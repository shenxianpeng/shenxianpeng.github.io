[{"content":"","date":"2025-08-24","externalUrl":null,"permalink":"/en/","section":"","summary":"","title":"","type":"page"},{"content":"Recently, while reviewing Google Analytics, I discovered an interesting phenomenon: My blog (shenxianpeng.github.io) has decent traffic from Google Search, but the primary language of visitors is surprisingly English, with Chinese coming in second.\nCome to think of it, it\u0026rsquo;s not surprising—I\u0026rsquo;ve written a few good English articles before, such as Using Gcov and LCOV for C/C++ Code Coverage, attracting many overseas readers.\nHowever, the problem is: I mainly write in Chinese, only occasionally in English. If readers want to see a version in another language, I have to manually translate, copy, paste, preview, and submit\u0026hellip; The whole process is tedious and time-consuming.\nSo I thought: Let\u0026rsquo;s automate this with AI.\nMy Solution # Using GitHub Actions + Gemini API to achieve automatic bilingual blog publishing.\nThe overall idea is simple:\nEvery time I write a new article and commit it to the repository, GitHub Actions will be automatically triggered; The workflow calls the Gemini API to translate the Chinese text into English; The translated article is committed to a new branch, and a PR is automatically created; Netlify deploys a preview; I can review it on GitHub and merge it with one click; After merging, the English version of the article will be published. This way, I only need to focus on writing in Chinese, and the English version will be generated automatically.\nCore Configuration # Here\u0026rsquo;s the core configuration of GitHub Actions (simplified version):\non: push: paths: - \u0026#39;content/posts/**/*.md\u0026#39; - \u0026#39;content/misc/**/*.md\u0026#39; schedule: - cron: \u0026#39;0 2 * * *\u0026#39; # Prevents missed translations and controls API call frequency workflow_dispatch: jobs: check-and-translate: runs-on: ubuntu-latest steps: - uses: actions/checkout@v5 - uses: actions/setup-python@v5 with: python-version: \u0026#39;3.13\u0026#39; - run: make install-deps - run: make translate env: GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }} GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} I put the dependencies and translation logic in the Makefile:\ninstall-deps: pip3 install -r requirements.txt translate: python3 .github/auto_translate.py Results and Inspiration # The final result is excellent:\nEffortless: No need for manual translation and copy-pasting; Efficient: An English version is generated almost simultaneously with a Chinese article; Cost-effective: Using Gemini API + Netlify, the cost is almost zero.\nMore importantly, many \u0026ldquo;seemingly troublesome little things\u0026rdquo; can be solved through AI + automation. Writing a blog is just one example; extending this idea, it can be applied to many scenarios: For example, bilingual support for technical documentation, internationalization of team knowledge bases, or even translation of internal company wikis.\nConclusion # Don\u0026rsquo;t waste time on \u0026ldquo;repetitive work\u0026rdquo;—let tools and automation handle it; save your energy for more creative things.\nIf you also have the habit of writing blogs or documents, I hope this article will inspire you.\nComplete code here 👉 GitHub Repository\n","date":"2025-08-24","externalUrl":null,"permalink":"/en/posts/auto-translate-post/","section":"Posts","summary":"Discovered my blog has more English than Chinese readers?  So I used GitHub Actions + Gemini API to automatically translate articles into English, saving time and effort.","title":"Blog Bilingual Publishing Made Easy — GitHub Actions + Gemini API in Practice","type":"posts"},{"content":"","date":"2025-08-24","externalUrl":null,"permalink":"/en/tags/gemini-api/","section":"Tags","summary":"","title":"Gemini API","type":"tags"},{"content":"","date":"2025-08-24","externalUrl":null,"permalink":"/en/tags/github-actions/","section":"Tags","summary":"","title":"GitHub Actions","type":"tags"},{"content":"","date":"2025-08-24","externalUrl":null,"permalink":"/en/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"Blowfish has full support for Hugo taxonomies and will adapt to any taxonomy set up. Taxonomy listings like this one also support custom content to be displayed above the list of terms.\nThis area could be used to add some extra descriptive text to each taxonomy. Check out the advanced tag below to see how to take this concept even further.\n","date":"2025-08-24","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"\u003cp\u003eBlowfish has full support for Hugo taxonomies and will adapt to any taxonomy set up. Taxonomy listings like this one also support custom content to be displayed above the list of terms.\u003c/p\u003e","title":"Tags","type":"tags"},{"content":"","date":"2025-08-22","externalUrl":null,"permalink":"/en/tags/career-development/","section":"Tags","summary":"","title":"Career Development","type":"tags"},{"content":"I\u0026rsquo;ve been asked: Why dedicate your free time to open source, especially after having children?\nFrankly, open source is more than a hobby for me; it\u0026rsquo;s a long-term investment in value.\nWhile direct financial returns are minimal, the rewards it brings far outweigh monetary gains.\nToday, I want to share three of them: these aren\u0026rsquo;t just for open-source contributors; every developer, and even every professional, can benefit from them.\n1. Making Your Work Truly Visible # In a company, our achievements are usually only known to colleagues or superiors.\nOpen-source projects are different; they showcase your abilities and accomplishments to a wider technical community.\nThis \u0026ldquo;visibility\u0026rdquo; is often more persuasive in career development than a few lines on a resume.\n👉 Key takeaway: Even without open source, you can create visibility by blogging, producing technical documentation, and sharing your work in communities. This is an intangible asset.\n2. Connecting with Exceptional People and Projects # Writing CRUD code in isolation might lead to career stagnation by age 35. Open source has exposed me to trends, best practices, and exceptional developers worldwide.\nFor example, I regularly watch the CPython repository, learning how they write PRs, propose PEPs, and implement CI/CD.\nLearning these processes alone has been invaluable. Participating in the English-speaking community has also improved my English.\n👉 Key takeaway: Even without contributing to open source, you can benefit by following excellent projects or large-company engineering practices and applying their lessons to your daily work.\n3. Long-Term Value Accumulation # Over the past four years, I\u0026rsquo;ve created several GitHub organizations and over ten projects, accumulating tens of thousands of users and hundreds of stars.\nThese accomplishments are the result of gradual accumulation during my free time.\nWhile there\u0026rsquo;s no direct financial return, this is a form of long-term asset.\nMy open-source experience and perspective help me grow faster at work, and my work experience, in turn, feeds back into my open-source contributions.\nMore importantly, it keeps me in a state of continuous learning and exploration.\n👉 Key takeaway: Whether it\u0026rsquo;s open source, writing articles, or side hustles, these can be seen as long-term investments. They may not immediately generate income, but they build reputation, skills, and opportunities.\nMy Open Source Story (Short Version) # In 2021, I took my first step into open source with cpp-linter-action. I was surprised when someone proactively offered contributions in an Issue; that moment ignited my open-source journey.\nLater, I collaborated with Brendan (2bndy5) on several projects, learning a great deal from him.\nWe also created cpp-linter, which has become one of the leading C/C++ linter projects on GitHub.\nI later developed commit-check and conventional-branch, gaining a significant user base.\nIn 2024, AI tools exploded. I initially worried about their potential to displace developers, but quickly realized that AI can rapidly generate runnable code, but is insufficient for building large, maintainable projects.\nFor example, when developing Jenkins Explain Error Plugin, AI helped me create a basic version quickly, but getting the project into the official Jenkins ecosystem required reviewer feedback and manual refinement.\nThis reinforced my belief that AI is a powerful tool, but ultimate value still comes from human thought and judgment.\nConclusion # Open source has:\nIncreased my visibility; Connected me with exceptional people and best practices; Kept me learning and growing. Even without direct financial rewards, I believe it\u0026rsquo;s a worthwhile endeavor.\nAs for whether AI will replace us? Perhaps someday.\nBut for now, it\u0026rsquo;s far from time to \u0026ldquo;lie flat.\u0026rdquo;\nContinuous learning and adaptation are key to maintaining value and competitiveness in this rapidly changing world.\n","date":"2025-08-22","externalUrl":null,"permalink":"/en/posts/why-open-source/","section":"Posts","summary":"Some say open source is useless, yielding neither profit nor time savings.  But through four years of dedication, I\u0026rsquo;ve discovered three unexpected rewards: increased visibility for my work, connections with exceptional people and projects, and the accumulation of long-term value.  These rewards are applicable to every developer.","title":"Four Years of Open Source —— Three Unexpected Rewards","type":"posts"},{"content":"","date":"2025-08-22","externalUrl":null,"permalink":"/en/tags/open-source/","section":"Tags","summary":"","title":"Open Source","type":"tags"},{"content":"","date":"2025-08-22","externalUrl":null,"permalink":"/en/tags/technical-growth/","section":"Tags","summary":"","title":"Technical Growth","type":"tags"},{"content":"","date":"2025-08-17","externalUrl":null,"permalink":"/en/tags/devops/","section":"Tags","summary":"","title":"DevOps","type":"tags"},{"content":"In daily work, we often talk about the buzzword DevOps. But have you ever wondered:\nWhat is our team\u0026rsquo;s actual DevOps maturity? How do we measure if one group is doing well and another needs improvement? Is there a unified standard for assessment? Based on these questions, we need a DevOps maturity model to help teams better evaluate and improve their practices.\nBut the assessment method itself is worth considering. If you just write a wiki or send out an Excel sheet for everyone to fill in, this \u0026ldquo;primitive\u0026rdquo; approach is not only inefficient, but also hard to track and accumulate results.\nThus, an open source project—DevOps Maturity—was born.\nDevOps Maturity Specification # This specification is based on the DevOps Maturity Model and Google DORA, combined with real-world implementation.\nIt defines the characteristics and metrics of different maturity levels, so teams clearly know:\nWhat stage are we currently at? What should we improve next? Key points:\nThe specification covers basics, quality, security, supply chain security, analytics and reporting, with actionable assessment items for each part:\nFor results, instead of complex percentages or scores, we use badge levels to motivate teams to keep improving:\nMore details can be found on the official site 👉 https://devops-maturity.github.io/en/\nDevOps Maturity Assessment Tool # With standards and specifications, we also need a tool for practical implementation.\nSo I developed the DevOps Maturity Assessment Tool, available in both Web and CLI versions.\nAssessment results are stored in a SQLAlchemy database and can be viewed and analyzed via Web or CLI.\nCLI example:\nCLI video:\nWeb video:\nFive Highlights # The DevOps Maturity project has five key highlights:\nStandardized Assessment: Based on industry-proven models, providing clear standards and metrics. Open Source Tools: Web and CLI options for easy use and integration. Visualized Results: Intuitive interfaces to quickly understand your team\u0026rsquo;s status. Continuous Improvement: Badge level incentives encourage ongoing DevOps practice improvement. Customizable Extension: Users can customize assessment items and metrics as needed, e.g., by editing criteria.yaml, without changing a single line of code to generate a unique assessment scheme. DevOps Maturity Enterprise Edition is also in development, with more enterprise features and support coming soon!\nJoin Us # DevOps Maturity is officially released—everyone is welcome to:\nAdopt it Share feedback Suggest improvements Contribute code All of these are the greatest support for open source 🙌\nIf you find it valuable, please give a ⭐ Star to help more people discover and benefit from it.\n","date":"2025-08-17","externalUrl":null,"permalink":"/en/posts/devops-maturity/","section":"Posts","summary":"How to assess and improve your team\u0026rsquo;s DevOps maturity. Official release of the DevOps Maturity open source project, with assessment tools and practical guides.","title":"DevOps Maturity — From Reflection to Open Source Practice","type":"posts"},{"content":"","date":"2025-08-13","externalUrl":null,"permalink":"/tags/pre-commit/","section":"标签","summary":"","title":"Pre-Commit","type":"tags"},{"content":"昨天在网上冲浪，突然看到了一个仓库叫 prek，一看介绍是 —— ⚡ Better pre-commit, re-engineered in Rust。这就引起了我的兴趣，毕竟 pre-commit 作为非常广泛的预提交的工具，如果能改进，尤其是性能方面的改进，肯定是好事。\n最最有意思的是 pre-commit 的作者也来到这个项目的 Issue 里发帖了，他先表示要合作，然后提到该项目违反了版权（已修复），再后面说这是一个恶意的、不道德，以及抄袭。\n让我们一起来看看这个帖子是怎么回事，我这里直接用 Google 翻译，带大家过一遍。\n最终，在 Airflow Maintainer 留言之后，作者点了一个 ❤ 然后锁了这个帖子。（这个操作可谓恰到好处）\n链接在这里：https://github.com/j178/prek/issues/73\n这件事有点像 uv 取代 pip 的故事，只不过我没看到像这次一样的争议。原因或许在于 pip 是由众多社区志愿者共同维护的项目，而 pre-commit 更像是 Anthony Sottile 的“个人”项目。虽然它是开源的，但原作者对项目拥有绝对的控制权。\n此外，它的衍生项目 pre-commit.ci 对开源项目免费，但对私有仓库（$10/月）、初创公司（$20/月）和大型组织（$100/月）则收取费用。如果出现有竞争力的替代方案，可能会对其收入造成影响。\n这里简单介绍一下 Anthony Sottile —— 他是 pre-commit 的作者，同时也是 pytest-dev、tox-dev 的核心开发者，维护 flake8，PyCQA 成员，GitHub Star 等等。只要你使用 Python，就很可能接触到他参与的项目。此外，他还是一名 YouTuber，会进行编程直播。我最初是因为使用 pre-commit 才了解到他，也看过他的视频，专业能力确实很强。不过，正如上文所提到的，他在 pre-commit 社区的互动方式，确实让部分人感到不适甚至不愉快。\n我的看法 # 除非 pre-commit 原作者 Anthony Sottile 能够更积极、更开放地推进，并加快 pre-commit-rs 的开发进度，否则 prek 对它的威胁将持续增加。从目前的趋势来看，prek 已经展现出强劲的势头。\n基于以下几点，我认为它很有可能走得很远：\n作者影响力： prek 作者是活跃且有影响力的开源贡献者，参与并贡献了 encode/httpx、astral-sh/uv、astral-sh/rye 等知名项目，具备长期获得社区信任与背书的能力。 重量级背书： prek 已经获得知名贡献者如 Jarek Potiuk 的积极背书——他是 Apache Airflow 的贡献者与 PMC 成员，Airflow 正在积极筹备切换到 prek。 社群形象差异： 与 pre-commit 原作者相比，其“高冷”风格可能限制了外部贡献者的参与意愿；反观 prek，作者听取的社区的建议将这个项目名字从 prefligit 改为 prek，我认为这个一个更好的名字，即简短，从发音上也更容易获得好感，这样也为替代方案的崛起创造了空间。 社区需求： 社区需要一个像 prek 这样积极推动 Rust 重写的项目，来打破 pre-commit 的现状。 除非 Anthony Sottile 做出 180 度转变，主动邀请外部贡献者共同加速 pre-commit-rs 的开发，并改变现有的社区互动方式，否则这种趋势短期内难以逆转。综合来看，我对 prek 的未来持乐观态度。\n在我截稿时，还看到作者将上述对话内容发布在 V2EX 和 Twitter 上，引发了更多关注。\n这部分我不作过多评价——开源社区本就是一个不断交流、切磋和竞争的舞台。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-08-13","externalUrl":null,"permalink":"/posts/pre-commit-competitors/","section":"Posts","summary":"昨天在网上冲浪，突然看到了一个仓库叫 \u003ccode\u003eprek\u003c/code\u003e，一看介绍是 —— ⚡ Better \u003ccode\u003epre-commit\u003c/code\u003e, re-engineered in Rust。这就引起了我的兴趣，毕竟 \u003ccode\u003epre-commit\u003c/code\u003e 作为非常广泛的预提交的工具，如果能改进，尤其是性能方面的改进，肯定是好事。","title":"被 Airflow Maintainer 一顿夸：Rust 重写版 pre-commit 项目 prek 的崛起","type":"posts"},{"content":"","date":"2025-08-10","externalUrl":null,"permalink":"/en/tags/blog/","section":"Tags","summary":"","title":"Blog","type":"tags"},{"content":"","date":"2025-08-10","externalUrl":null,"permalink":"/en/tags/blowfish/","section":"Tags","summary":"","title":"Blowfish","type":"tags"},{"content":"Eight years ago, I built my blog using Hexo + the landscape theme. From 2017 until today, it has hosted 236 posts, including technical articles and a small number of personal notes.\nOver time, Hexo no longer seemed like the best choice, and some limitations and inconveniences became more apparent, such as:\nPoor support for multiple languages The current theme couldn’t meet my needs, such as listing works or a resume Many features required third-party plugins, such as comments, reading time, and word count Content and images for posts were stored in different directories, making management inconvenient I had previously made a short-lived attempt to revamp the blog, but the results weren’t ideal, so I shelved the plan.\nRecently, however, I came across Hugo with the Blowfish theme, which immediately caught my eye. Combined with GitHub Copilot and my OpenAI Plus subscription, I decided to fully revamp my blog and migrate to Hugo.\nThat €30 per month AI subscription shouldn’t go to waste!\nEven with so many tools and resources at my disposal, it still took me three nights to complete a migration I was satisfied with. Here’s a comparison:\nHome Page Comparison # Previous home page (no localization applied)\nAbout Page Comparison # New Pages # Upgrade Notes # If you have a large number of posts like I do, create a new repository (e.g., new-blog), set it up with Hugo + Blowfish theme first. Then migrate your Hexo posts. I used Copilot to write a script to automatically convert Hexo posts into Hugo format. Use ChatGPT or other AI tools to generate post cover images. Use Copilot to copy the generated covers to the corresponding directories based on the post topics. Gains and Losses of the Upgrade # After the upgrade, my blog gained the following improvements:\nResolved the lack of multilingual support Added built-in post search functionality Provided better navigation and page layout Stored post content and images in the same directory for easier management The Blowfish theme offers many built-in features (comments, reading time, word count, etc.) without needing extra plugins Of course, there were also some losses:\nOld post URLs were not mapped to the new ones, impacting SEO (I can fix it if I want to) Lost previous comment data Time investment was needed for migration and adjustments Overall, the upgrade was definitely worth it — the new theme is a huge improvement over the old one.\nFinal Thoughts # Without the help of AI, this migration would have taken much more time and effort.\nAt the same time, with less reliance on search engines, the chances of technical articles being seen seem lower — perhaps now they exist more as training data for AI.\nRegardless, for IT professionals, having a personal writing space is still valuable, whether for recording, summarizing, or reflection.\nI will continue to share my learning and work experiences here, as well as record my thoughts, observations, and reflections as a programmer living in Eastern Europe. Perhaps that still holds some value.\n","date":"2025-08-10","externalUrl":null,"permalink":"/en/posts/upgrade-blog-using-hugo/","section":"Posts","summary":"This article records the process of revamping my blog after eight years, from migrating from Hexo to Hugo, to the functional and design improvements of the new blog.","title":"Eight Years Later, I Finally Revamped My Blog","type":"posts"},{"content":"","date":"2025-08-10","externalUrl":null,"permalink":"/en/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"上周我发布了 Jenkins Explain Error Plugin，旨在帮助 Jenkins 用户通过内置 AI 来更快地分析和解决 Jenkins 构建中的错误。\n有读者朋友在评论区提到，希望插件支持 Google Gemini 模型进行错误分析，因为他们公司只能使用 Google 的 AI 服务。\n今天，我很高兴地宣布，这个插件现在已经支持 Google Gemini 模型了！🎉\n插件更新 # 新增对 Google Gemini 模型的支持 优化文档并补充了示例视频 如何使用 Google Gemini # 在开始使用之前，请确保插件已更新到最新版本。 你可以在 Jenkins 插件管理器中找到 Explain Error Plugin，并将其升级到最新版\n更新后，在插件配置中，你可以选择使用 Google Gemini 模型进行错误分析。只需在 Manage Jenkins → Configure System 页面下的 Explain Error Plugin Configuration 中，将模型设置为 Google Gemini，并提供相应的 API 地址和密钥。\n点击 Test Configuration，确保你的 Google Gemini API Key、URL 和 Model 均已正确填写且可正常访问。\n插件示例视频 # 考虑到不少用户可能对插件的使用还不够熟悉，我录制了一段简短的视频，演示如何在 Jenkins 中使用 Explain Error Plugin 进行错误分析。\n你可以在 YouTube 上观看这个视频。\n结语 # 如果你在使用过程中有任何问题或建议，欢迎在 GitHub 上提交 issue，或在评论区留言。\n仓库地址：jenkinsci/explain-error-plugin\n欢迎 Star ⭐️ 支持一下！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-08-03","externalUrl":null,"permalink":"/posts/explain-error-plugin-support-gemini/","section":"Posts","summary":"本文介绍了 Jenkins Explain Error Plugin 的新功能，支持 Google Gemini 模型进行错误分析，并提供了配置方法和示例视频。","title":"Jenkins Explain Error Plugin 支持 Google Gemini 了！🤖","type":"posts"},{"content":"","date":"2025-07-29","externalUrl":null,"permalink":"/en/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2025-07-29","externalUrl":null,"permalink":"/en/tags/jenkins/","section":"Tags","summary":"","title":"Jenkins","type":"tags"},{"content":"As the title says, I recently launched my first Jenkins plugin! 🎉\nThe main function of this plugin is to eliminate the need to copy error messages from Jenkins builds into AI tools like ChatGPT for analysis. Instead, it provides a button directly in the Jenkins build log. Clicking this button automatically sends the error information to OpenAI for analysis. You can also add an explainError() step in your pipeline to get error explanations, helping developers locate and solve problems faster.\nThis is my first plugin project in the Jenkins community. Previously, I hadn\u0026rsquo;t attempted this because I believed many functionalities could be implemented via pipeline scripts, making a separate plugin unnecessary.\nHowever, with the increasing popularity of AI, I discovered that the Jenkins Plugin Center surprisingly lacked a similar plugin. So, I decided to implement this functionality myself. With the help of AI, and utilizing my evenings for development and testing, along with thorough code reviews from the Jenkins Hoster, I finally submitted it to the Jenkins Plugin Center last weekend, and it\u0026rsquo;s officially online.\nPlugin Introduction # Explain Error Plugin is a Jenkins plugin based on OpenAI. It automatically parses build failure log information and generates readable error explanations, suitable for common Jenkins job types such as Pipeline and Freestyle.\n🔍 One-click analysis of errors in console output ⚙️ Usable in Pipelines with a simple explainError() step 💡 AI-powered intelligent explanations based on the OpenAI GPT model 🌐 Provides two Web UI options to display AI-generated analysis results 🎯 Customizable: Supports setting the model, API address, log filters, and more Plugin Highlights # Feature Description ✅ One-click console analysis Adds an \u0026ldquo;Explain Error\u0026rdquo; button to the top of the console page ✅ Pipeline support Provides the explainError() step, automatically triggered on failure ✅ Model configuration support Customize to use GPT-3.5 or other models ✅ Jenkins CasC support Supports Configuration as Code ✅ Log filtering Supports regular expression filtering of logs to focus on error content Prerequisites # Jenkins version ≥ 2.479.3 Java version ≥ 17 An OpenAI API Key (available at OpenAI website) Quick Installation # You can install it through the Jenkins Plugin Manager:\nManage Jenkins → Manage Plugins → Available → Search for \u0026ldquo;Explain Error Plugin\u0026rdquo;\nPlugin Configuration # After installation, find Explain Error Plugin Configuration under Manage Jenkins → Configure System to configure your OpenAI API key and model:\nSetting Description Default Value Enable Explanation Enable AI analysis functionality ✅ Enabled API Key Your OpenAI Key Required API URL OpenAI API address https://api.openai.com/v1/chat/completions AI Model Model to use gpt-3.5-turbo You can also manage these settings through Jenkins Configuration as Code, for example:\nunclassified: explainError: enableExplanation: true apiKey: \u0026#34;${AI_API_KEY}\u0026#34; apiUrl: \u0026#34;https://api.openai.com/v1/chat/completions\u0026#34; model: \u0026#34;gpt-3.5-turbo\u0026#34; How to Use # Using in Pipelines # It\u0026rsquo;s recommended to call explainError() within the post { failure { ... } } statement to automatically analyze errors on build failure:\npost { failure { explainError() } } You can also set the log length and matching keywords:\nexplainError( maxLines: 500, logPattern: \u0026#39;(?i)(error|failed|exception)\u0026#39; ) Using in the Console # Suitable for any type of Jenkins job:\nOpen the console output page of the failed build Click the \u0026ldquo;Explain Error\u0026rdquo; button at the top The AI analysis results will be displayed below the button Preview # After a build failure, a sidebar entry will appear on the job page. Clicking it will display the AI analysis results:\nOr directly click \u0026ldquo;Explain Error\u0026rdquo; on the console page to view the analysis:\nFuture Plans # Error caching: Avoid repeated calls to OpenAI when analyzing the same error multiple times to save calls. (Already implemented, awaiting merge) Multi-model support: Support other AI platforms, such as Google Gemini, Claude, DeepSeek, etc. These features are still under development, conception, and refinement. Your feedback and suggestions are highly welcome.\nIn Closing # The plugin is completely open-source. Feel free to try it out and contribute code!\nIf you encounter any problems during use or have suggestions, please submit an issue or pull request on GitHub:\nGitHub address: https://github.com/jenkinsci/explain-error-plugin\nIf you find this plugin helpful, please share this article; you can also give it a Star ⭐ on GitHub to show your support!\nThis is the most direct support for open-source projects 🙌\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-07-29","externalUrl":null,"permalink":"/en/posts/explain-error-plugin/","section":"Posts","summary":"Introducing my first Jenkins plugin: Explain Error Plugin. It automatically analyzes build failure log information and generates readable error explanations, helping developers locate and solve problems faster.","title":"Jenkins Plugin Center Didn't Have an AI Plugin Yet? I Wrote One! 🤖","type":"posts"},{"content":"","date":"2025-07-13","externalUrl":null,"permalink":"/tags/thoughts/","section":"标签","summary":"","title":"Thoughts","type":"tags"},{"content":"今晚孩子睡得早，我坐在阳台的沙发上，打开昨天和同事聚餐剩下的啤酒，听着窗外邻居用我听不懂的语言聊天，心里有点感慨。\n前段时间，有人问我：“你怎么出国的？什么途径？孩子以后就在那边读书吗？”\n我也了解到，一些朋友正在纠结是否要出国生活，最后还是选择留在国内。\n今天刚和房东签了第二年的租约，想想自己一个人出来生活也满一年了，算是积累了一些经验。\n如果用一句话来概括我的感受，那就是：\n出不出国，是个非常个人化的决定，没有对错。 但对于打工人来说——打磨一项核心技能，学好英语，养好身体，做长期有价值的事。\n或许是这两年就业形势愈发严峻，大家越来越多地在思考一个问题：\n我适合出国吗？ # 有人是为了孩子教育； 有人想逃离内卷、追求更高的生活质量。\n但想法与现实之间，常常横亘着语言、文化、教育体系、效率与生活习惯。\n我自己作为一名在欧洲工作的程序员，也经常被问到这些问题：\n“欧洲适合养娃吗？” “不会英语，能活下来吗？” “孩子上学怎么办？光有英语够不够？” “工资能攒下钱吗？” 今天，我想以“程序员”和“孩子爸爸”的双重视角，结合自己的观察，聊聊：\n什么样的人适合来欧洲，什么样的人可能会不适应？ # 适合来欧洲的人群 # 1. 英语能力尚可，能应对日常沟通 # 在欧洲，尤其是技术领域，英语是最常用的工作语言。 你不需要说得像母语者一样流利，但至少得能看懂邮件、开会能跟上、和同事能沟通。\n2. 愿意学习当地语言和文化 # 欧洲不像英国、美国、加拿大那样是纯英语国家。 各国有自己的官方语言。掌握当地语言，会让你在超市购物、看病、租房、办事时省下不少麻烦，也能更快融入社会。\n当然，也有人并不想再学一门新语言，那可能生活上的不便就会多一些。\n3. 能接受“慢节奏 + 高质量”的生活 # 欧洲很多国家推崇“工作-生活平衡”：\n准时下班，周末不加班 假期随时请，没有人追问“项目怎么办” 网购慢、办事需预约，但节奏慢不代表低效，习惯就好 如果你追求“年入百万”或热衷于“996冲刺项目上线”，可能会觉得不适应； 但如果你希望生活稳定、有时间陪家人、有精力关注自身成长，那欧洲或许正适合你。\n4. 带学龄前小孩的家庭 # 小朋友的语言适应能力非常强，尤其是0–5岁之间。 这个阶段来欧洲，语言习得几乎是“自动完成”的，教育质量不错，还有政府补贴。\n对家庭来说，这是一个“软着陆”的黄金阶段。\n5. 追求长期发展与生活质量的人 # 在国内，程序员35岁之后经常面临职业焦虑； 但在欧洲，35岁依然是职场的黄金期，跳槽加薪、稳定成长都很常见。\n如果你关注长期发展、工作稳定、生活质量、教育与医疗保障—— 欧洲确实是一个值得考虑的选择。\n而对于想拿欧盟绿卡或长期居留的人来说，欧洲的移民政策整体比美国要友好。\n可能不太适应的人群 # 1. 英语较差，且不愿意学习当地语言 # 你可能会觉得“我写代码就行了”，但现实中：\n签证、银行、医院、驾照、孩子上学、日常生活……处处都需要语言。\n靠 Google 翻译应急可以，靠它生活几年，很容易焦虑和挫败。\n2. 急性子、习惯有服务配套的人 # 如果你习惯了“今天提需求，明天上线”的国内节奏，那么欧洲的办事效率可能会让你抓狂。\n这边很多事都需要预约、等待、排队，甚至很多服务根本没有——比如便宜的外卖、按摩、理发、搓澡等，要么很贵，要么根本没有。\n3. 对天气特别敏感的人 # 很多欧洲国家的天气对中国人来说确实不算友好：\n冬天漫长、阴冷、日照时间短，容易产生季节性抑郁 夏天短，但白天特别长，有时晚上10点天还没黑 雨水多、湿度大、衣服不好晒干 如果你特别需要阳光和四季分明的天气，这可能是个痛点。\n4. 孩子已经上初中或高中 # 这个阶段孩子的语言学习能力下降，学业压力却上来了。\n如果是在非英语国家（比如德国、法国、立陶宛等），课程是本地语言，考试也要本地化，孩子压力很大，家长也会跟着焦虑。\n5. 主要目标是“赚钱” # 如果你的目标是“积累原始资本”或者“挣快钱”，那么：\n欧洲的税高 消费也不低 程序员工资虽然还不错，但远不如美国，而且很多时候低于国内大厂 尤其是如果你成家了，考虑只身来欧洲拼几年，一定要谨慎衡量代价和收益。\n孩子的年龄，对家庭决策也很关键： # 年龄段 适应程度 建议 0–5 岁 适应最强 语言学习快，教育压力小，建议来 6–11 岁 中等适应 语言学习难度略高，需额外辅导 12 岁以上 适应较难 升学压力大，语言负担重，建议慎重考虑 18 岁以上 适应轻松 读大学或硕士是很好的选择，教育资源多、费用低 一眼判断适不适合你 # 特征 适合来欧洲 不太适合来欧洲 英语能力 能日常沟通 英语差、也不想学 性格 平和，愿意慢节奏适应 急性子，习惯高效率与被服务 天气接受度 能接受冬天漫长、阳光少 极度依赖阳光，怕冷 家庭阶段 孩子年幼（0–5 岁） 孩子已进入初高中 职业规划 追求稳定发展、生活质量 主要目标是赚钱或快速积累资本 文化态度 愿意融入、愿意学语言 抵触本地文化，只想待在“华人圈” 写在最后 # “出不出国”没有标准答案，每个人的背景、追求、资源都不一样。\n我个人更看重生活环境、教育资源、医疗保障和工作节奏。 最大的缺憾，就是离家太远，每年只能回国一次，哪怕呆上一个月也觉得时间不够。\n语言是生活中不便的来源之一，但我也在慢慢适应和学习。\n如果未来能实现“远程+回国工作几个月”的自由，那对我来说，就是最理想的状态。\n出国生活是一个重要的决定，尤其是有孩子的家庭。开启一段新的人生，并不轻松。\n希望这篇文章，能帮你更清晰地判断自己是否适合来欧洲。 如果你正在考虑移居，欢迎留言交流，也欢迎转发给身边的朋友～\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-07-13","externalUrl":null,"permalink":"/misc/run-or-not/","section":"Miscs","summary":"哪些人适合来欧洲发展，哪些人可能会感到不适？从程序员和孩子爸爸的双重视角，聊聊出国生活的真实体验。","title":"出国一年，我看清了：不是所有人都适合来欧洲","type":"misc"},{"content":"大家好，我是DevOps攻城狮。\n最近读了一份挺有意思也挺震撼的报告：JFrog发布的《2025软件供应链现状报告》。这是JFrog连续多年基于其平台数据、CVE趋势分析、安全研究团队研究和1400位开发/安全从业者调查所形成的一份行业报告。\n我挑了一些对我们DevOps从业者、尤其是负责CI/CD和软件交付的人来说非常有参考价值的内容，分享如下：\n软件供应链，真的变了 # 报告开头就给出了几个数字，让人警觉：\n64% 的企业开发团队使用 7种以上编程语言，44% 使用10种以上； 一个普通组织 每年引入458个新包，也就是每月平均38个； Docker Hub 和 Hugging Face 上的镜像和模型数量仍在指数级增长； npm依然是恶意包的“重灾区”，但 Hugging Face 上的恶意模型增长了6.5倍。 如果说过去我们担心“你用的包有没有CVE”，现在可能得先问一句：\n你真的知道自己用了哪些包、拉了哪些模型吗？\n风险激增，不只来自漏洞 # 2024年，全球共披露了 33,000多个CVE，比2023年多了27%。但这只是“冰山一角”。\n报告揭示了一个更加令人担忧的趋势：“漏洞≠风险”，而“风险”正在从更多方向袭来：\n秘钥泄露：JFrog在公开仓库中扫描出 25,229个秘密token，其中 6,790个是“活的”； XZ Utils后门：攻击者假装是OSS维护者，潜伏多年后埋入后门，影响OpenSSH； AI模型的投毒：某些Hugging Face模型在加载时自动执行恶意代码（Pickle注入），悄无声息入侵机器； 误配置的代价：比如微软Power Pages因权限配置问题泄露大量用户数据，Volkswagen旗下公司因SaaS误配置泄露了80万台电动车的定位数据。 说实话，这些问题和“有没有扫描CVE”没什么关系。很多时候，是我们根本没意识到“这里也会出事”。\nAI爆发，风险也同步升级 # 今年Hugging Face新增了超过100万个模型和数据集，但同时，恶意模型也增长了 6.5倍。很多组织都开始将AI模型纳入业务，但：\n有 37%的企业靠“人工白名单”筛选模型使用； 很多AI模型使用的是Pickle格式，加载即执行，一不小心就中招； Hugging Face等平台上也出现了“挂羊头卖木马”的开源模型。 对于我们DevOps或者平台团队来说，这意味着：\n“模型”正在变成新的“依赖包”，也应该纳入供应链治理和安全扫描的范畴。\n安全实践现状：工具变多，人却更焦虑 # 报告里还有一个很现实的观察：\n安全工具越多，反而让人越看不清真相。\n73%的企业使用了 7个以上安全工具，但只有 43%同时扫描代码和二进制； 71%的组织允许开发者直接从公网拉包； 52%的组织一边允许公网拉包，一边又靠自动规则追踪来源； CVSS评分“虚高”问题越来越严重，JFrog分析后发现，88%的Critical CVE其实并不适用。 作为一线DevOps，我看到的是：工具越来越多，但我们似乎并没有真正“安心”下来。\n我们能做什么？ # 报告里没有提供“万无一失”的解决方案，但给出了一些务实建议，我结合自己的理解补充几点：\n治理从源头做起：不要再让开发者从公网自由拉包，使用内部代理仓库如 Artifactory/Nexus/Harbor； 扫描不止于代码：二进制扫描、容器镜像扫描和SBOM（软件物料清单）都需要纳入CI流程； 引入“适用性评估”：别光看CVE得分，更重要的是它是否真的适用于你的场景； 把AI模型当“依赖”管理：构建模型白名单、扫描模型安全性，甚至做模型SBOM； 限制新“匿名包”引入：不要因为某个库“突然火了”就引入，回顾XZ事件足够令人警醒。 写在最后 # 2025年的软件供应链比以往更大、更快、更复杂，也更脆弱。\n安全问题不是“有没有风险”，而是“你知道风险藏在哪里吗”。 一不小心，风险可能来自一个新同事引入的PyPI包，或者一位AI工程师下载的模型。\nJFrog这份报告虽然没有解决所有问题，但给我们敲了一个不小的警钟。\n如果你也在构建自己的DevOps流程、AI平台、或者仅仅是维护日常构建环境， 建议你认真想一想：\n你的“依赖”到底靠不靠谱？ 你的“扫描”真的能发现问题吗？ 你的“策略”是否跟得上变化了？\n欢迎在评论区聊聊你看到的“供应链怪现象”。也希望这篇分享对你有所启发。\n🧡 欢迎关注，一起做更好的技术实践。 📥 如果你需要这份《JFrog Software Supply Chain Report 2025》原报告， 可以在公众号后台回复关键词：“jfrog report 2025” 来获取报告下载链接。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-06-28","externalUrl":null,"permalink":"/posts/jfrog-report/","section":"Posts","summary":"JFrog发布的《2025软件供应链现状报告》揭示了软件供应链的变化和风险，尤其是AI模型的安全问题。本文分享了报告中的关键发现和对DevOps从业者的启示。","title":"🧊2025软件供应链现状报告：开源时代，我们究竟在和谁打交道？","type":"posts"},{"content":" Issue # I have set up documentation publishing on Jenkins using the following Groovy code:\npublishHTML([ allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: \u0026#34;docs/build/html/\u0026#34;, reportFiles: \u0026#39;index.html\u0026#39;, reportName: \u0026#34;Documentation\u0026#34;, useWrapperFileDirectly: true ]) However, some badges from Shields.io do not display properly within the published documentation.\nHow to fix it # ✅ Working Script to Update Jenkins CSP in Script Console\nHere’s the correct and minimal Groovy script you should run in Manage Jenkins → Script Console:\nSystem.setProperty( \u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;default-src \u0026#39;self\u0026#39;; img-src * data:;\u0026#34; ) This allows images from any domain (img-src *), which includes Shields.io. If you want to restrict it more safely:\nSystem.setProperty( \u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;default-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; https://img.shields.io data:;\u0026#34; ) 🟡 Note: This change is temporary (in-memory only). It will reset if Jenkins restarts.\n✅ To Make It Permanent\nModify Jenkins startup arguments (depends on how you run Jenkins): If using /etc/default/jenkins (Debian/Ubuntu):\nJENKINS_JAVA_OPTIONS=\u0026#34;-Dhudson.model.DirectoryBrowserSupport.CSP=\\\u0026#34;default-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; https://img.shields.io data:;\\\u0026#34;\u0026#34; If using systemd service unit (CentOS/Red Hat-based or modern setups):\nEdit or override your jenkins.service file:\nEnvironment=\u0026#34;JAVA_OPTS=-Dhudson.model.DirectoryBrowserSupport.CSP=default-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; https://img.shields.io data:;\u0026#34; Then restart Jenkins: sudo systemctl restart jenkins Results # The issue with Shields.io badges not displaying on Jenkins has now been resolved.\nPlease credit the author and source when reposing this article. Commercial use is not permitted. You\u0026rsquo;re welcome to subscribe to my blog via RSS.\n","date":"2025-06-23","externalUrl":null,"permalink":"/en/posts/jenkins-show-badge/","section":"Posts","summary":"How to temporarily fix it via the Script Console, and how to make it permanent by modifying Jenkins startup parameters. This method is suitable for internal Jenkins environments and has been tested on modern Jenkins installations.","title":"How to Fix Shields.io Badges Not Displaying in Jenkins","type":"posts"},{"content":"","date":"2025-06-12","externalUrl":null,"permalink":"/en/tags/bitbucket/","section":"Tags","summary":"","title":"Bitbucket","type":"tags"},{"content":" Background # Recently, I had a discussion with colleagues about a seemingly simple yet crucial issue:\nHow can we ensure that valuable information in PRs isn\u0026rsquo;t lost over time and with changes in tools?\nWhile we currently use Bitbucket for collaborative development, we might migrate to GitHub, GitLab, or other platforms in the future. These hosting platforms may change, but Git itself, as the core record of code history, is likely to remain for a long time.\nThis leads to the problem:\nPR page descriptions of change context, solutions, and key discussions, if only stored within the PR tool, are likely to \u0026ldquo;disappear\u0026rdquo; after platform migration. This information should be part of the commit message.\nSolutions We Considered: # Manually recording the solution in git commit -m — but this is easily overlooked or incomplete. Mimicking pip projects using a NEWS file to record each change — while this preserves information, it\u0026rsquo;s more suitable for generating release notes than recording the motivation or reasons for a change. Mandating that members write the content in Jira tickets — this creates tool silos and hinders quick understanding of history within the code context. Ultimately, we decided to: Use Bitbucket\u0026rsquo;s Commit Message Templates feature to directly write the PR description into the Git commit.\nBitbucket\u0026rsquo;s Commit Message Templates Feature # Bitbucket supports automatically generating commit messages when merging PRs, allowing useful information to be inserted via templates. The documentation is available here: 🔗 Pull request merge strategies - Bitbucket Server\nI\u0026rsquo;ve also seen similar functionality in GitLab, but GitHub seems to lack this feature.\nSee the GitLab Commit Templates official documentation\nYou can use the following variables in the template:\nVariable Name Description title PR Title id PR ID description PR Description approvers Current Approved Reviewers fromRefName / toRefName Source / Target Branch Name fromProjectKey / toProjectKey Source / Target Project fromRepoSlug / toRepoSlug Source / Target Repository crossRepoPullRequestRepo Source repository information for cross-repository PRs How We Used It? # In Bitbucket\u0026rsquo;s repository settings, you can configure the PR merge commit template. Find the settings:\nRepository settings -\u0026gt; Merge strategies -\u0026gt; Commit message template\nAfter configuration, when you merge a PR, Bitbucket will automatically write the PR title, description, and related ID into the final merge commit message.\nThis way, regardless of whether the team continues to use Bitbucket in the future, key PR information will forever be preserved in Git history.\nHere are the actual results:\n📥 Template Configuration Interface: 📤 Final Generated Git commit message: Summary # This small change helps us procedurally protect the context of code changes. It prevents PRs from being \u0026ldquo;temporary information containers\u0026rdquo; and instead makes them a natural part of Git history.\nIf you\u0026rsquo;re also using Bitbucket, give this feature a try.\nLet Git commit messages be not just code submissions, but records of decisions and evolution.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-06-12","externalUrl":null,"permalink":"/en/posts/commit-message-template/","section":"Posts","summary":"Leverage Bitbucket\u0026rsquo;s Commit Message Templates feature to automatically write PR descriptions into Git commit messages, ensuring crucial information isn\u0026rsquo;t lost with tool changes.","title":"Enhancing Code Traceability — Automatically Including PR Descriptions in Git Commits","type":"posts"},{"content":"","date":"2025-06-12","externalUrl":null,"permalink":"/en/tags/git/","section":"Tags","summary":"","title":"Git","type":"tags"},{"content":" Background # In our Jenkins pipeline, we use the following configuration to manage resource consumption, especially for builds that typically take more than 30 minutes to complete:\ndisableConcurrentBuilds abortPrevious: true This setting prevents concurrent builds on the same branch or pull request. When a new build is triggered while a previous one is still running, the older build is automatically aborted.\nThis helps conserve resources and avoids redundant builds when developers push multiple updates shortly after opening a pull request.\nThe problem # But the problem is:\nSometimes, during merges, an ongoing build gets aborted midway or near completion because a new build was triggered for the same branch. They requested that if a build is already running, new builds triggered on the same branch should wait in the queue instead of aborting the current build.\nBefore the changes: # Let\u0026rsquo;s take a picture bellow of the release/x.x.x branch.\nJob #104 was aborted because a new merge was triggered on the release/x.x.x branch. Job #105 was also aborted for the same reason, due to another new merge on the release/x.x.x branch. After the changes: # Job #106 will continue running without being canceled, even if a new merge occurs. Job #107 will wait in the queue until Job #106 finishes before starting. Initial Thoughts # Initially, I believed that disableConcurrentBuilds was a global setting that applied uniformly across all branches and pull requests.\nAfter researching via ChatGPT and Google, I found that selectively applying this setting per branch is not straightforward and requires adding complexity to the existing pipeline.\nThe Solution # But there is a simple solution? Yes!\nI implemented and tested conditional logic that:\nRetains abortPrevious: true for pull request builds, Disables it for specific branches such as devel and release. Here the code snap about how to implement it.\n// vars/build.groovy def call() { def isAboutPrevious = true if (env.BRANCH_NAME == \u0026#39;devel\u0026#39; || env.BRANCH_NAME.startsWith(\u0026#39;release/\u0026#39;)) { isAboutPrevious = false // disable abortPrevious for devel and release branches. } pipeline { options { disableConcurrentBuilds abortPrevious: isAboutPrevious } stages{ // .... } } } The Result # The changes passed testing and have been merged into our shared pipeline library.\nNow, for the devel and release branches in multi-branch pipeline:\nJenkins no longer aborts running builds when new builds are triggered. Instead, it queues subsequent builds, improving stability and predictability for our QA workflows. Please credit the author and source when reposing this article. Commercial use is not permitted. You\u0026rsquo;re welcome to subscribe to my blog via RSS.\n","date":"2025-06-04","externalUrl":null,"permalink":"/en/posts/jenkins-concurrent-build/","section":"Posts","summary":"In Jenkins, the \u003ccode\u003edisableConcurrentBuilds\u003c/code\u003e option is used to manage concurrent builds. This article explains how to conditionally set the \u003ccode\u003eabortPrevious\u003c/code\u003e value based on the branch being built, allowing for more flexible build management.","title":"How to Change abortPrevious Value in Jenkins?","type":"posts"},{"content":"","date":"2025-06-02","externalUrl":null,"permalink":"/tags/ci/cd/","section":"标签","summary":"","title":"CI/CD","type":"tags"},{"content":"今天这篇文章，起因是我看到前同事发的一条朋友圈。\n我感觉他这是在赤裸裸地夸我 ：） （不好意思，得瑟一下~）\n不过说实话，这几年在 CI/CD 这块，我确实一直在做两件事：\n一边关注行业里最新的实践； 一边筛选出适合我们业务的部分落地实施，或者写文章进行分享。 但我发现一个常见的误区是：\n很多人觉得 CI/CD 做完了就完了，是一劳永逸的事情。\n其实并不是。\n罗马不是一天建成的，CI/CD 也不是 # 不论是 CI/CD 流水线，还是底层的程序、工具、库、平台，它们都属于基础设施的一部分。基础设施的一个显著特点是：\n需要长期投入与持续维护。\n否则，哪怕你现在搭建得再好，几年之后，也会因为无人维护而变成一个臃肿、失控的“技术债堆积场”。最终不得不推翻重来。\n举几个例子你就会明白：\n如果不维护，风险很快就会显现 # 1. 安全风险： # 如果你没关注 CVE-2024-23897，可能根本不知道你的 Jenkins 存在任意文件读取漏洞，有被攻击者入侵的风险。\n2. 功能缺失与兼容性问题： # 不看 Jenkins 的更新日志，可能会错过一些关键修复或新特性； 不更新 CI/CD 工具链，代码可能突然就编译不了了，测试也不再通过。\n3. 技术债积累与维护成本上升： # 没有引入新的实践和工具，流水线就会越来越复杂、难维护，开发体验也会越来越差。\n再来看 Python 项目这边 # 1. 依旧使用 setup.py？ # 那你可能错过了 PEP 518 推出的 pyproject.toml 所带来的统一构建生态。\n2. 依旧用 pip install 安装 CLI 工具？ # 那你可能还不了解 pipx 和 uvx 能如何更方便、高效地管理工具依赖。\n3. 不了解 PEP、不了解包结构和发布规范？ # 那很容易写出不规范、难以维护、别人无法复用的代码。\n这些问题，表面上看似无伤大雅，但背后隐藏的是维护成本、团队协作效率以及项目未来发展的可持续性。\n所以说，重构是软件工程的日常功课\n很多时候，这些工作并不显眼，甚至容易被认为是“做多了”“瞎折腾”或“浪费时间”。\n但实际上，恰恰是这些“隐性的工作”，才是软件工程真正的根基。\n没有重构，系统只会越来越臃肿； 没有持续进化，CI/CD 迟早变成一个烂摊子； 没有持续学习和探索，就会离最佳实践越来越远。\n所以，希望这篇文章能给你一点启发：\nCI/CD 不是一次性的建设项目，而是一个持续演进、不断重构和成长的 DevOps 系统。\n如果你也正在做这些“看不见的工作”，别灰心。你正在为整个系统的长期可持续打下基础。\n你是否也遇到过相关的坑？欢迎留言分享你的故事。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-06-02","externalUrl":null,"permalink":"/posts/code-refactor/","section":"Posts","summary":"在 DevOps 中，CI/CD 流水线需要持续维护和重构。本文探讨了为什么 CI/CD 不是一次性的建设项目，而是一个需要长期投入和持续演进的系统。","title":"CI/CD 不是一次性的项目，而是一个不断演进的系统","type":"posts"},{"content":"","date":"2025-05-29","externalUrl":null,"permalink":"/tags/asdf/","section":"标签","summary":"","title":"Asdf","type":"tags"},{"content":"最近，我在 cpp-linter 组织下发布了一个名为 asdf-clang-tools 的全新仓库。这个项目是从 amrox/asdf-clang-tools fork 而来。由于原作者多年没有维护，我对其进行了修复、升级和功能扩展，使其焕然一新。简单来说，asdf-clang-tools 是一个 asdf 插件，用于安装和管理 Clang Tools 相关工具（如 clang-format、clang-tidy、clang-query 和 clang-apply-replacements 等）。\n新的安装方式：除了 pip 还有 asdf # 在此之前，我曾推出过 clang-tools-pip 工具包，用户可以通过 pip install clang-tools 的方式安装包括 clang-format、clang-tidy、clang-query、clang-apply-replacements 在内的一整套 Clang 可执行工具。\n而 asdf-clang-tools 则提供了另一种途径——利用 asdf 版本管理器来安装这些工具。简而言之，这为喜欢用 asdf 管理工具版本的开发者多了一个选择。\n这两种方式并不是互斥的：你可以通过 pip 或 asdf 轻松安装和管理 Clang 工具。至于选择哪种方式取决于你的工作流和个人喜好。\n什么是 asdf 版本管理器 # 很多开发者可能还不太熟悉 asdf。asdf 是一个多语言、多工具的版本管理器。\n它可以用一个命令行工具管理多种运行时环境的版本，支持插件机制。\n举例来说，你可以通过 asdf 来管理 Python、Node.js、Ruby 等语言的版本，也可以管理 Clang 工具（像我介绍的 asdf-clang-tools）。\n所有工具的版本信息都记录在一个共享的 .tool-versions 文件中，这样团队可以轻松在不同机器间同步配置。\n总之，asdf 的好处就是“一个工具管理所有的依赖”，让项目所需的各类工具版本统一起来，免去在每个工具里使用不同版本管理器的麻烦。\n安装与使用示例 # 使用 asdf-clang-tools 安装 Clang 工具非常简单。假设你已经安装好了 asdf，只需按照官方仓库的说明进行操作：\n首先 添加插件：以 clang-format 为例，在终端运行：\nasdf plugin add clang-format https://github.com/cpp-linter/asdf-clang-tools.git 类似地，clang-query、clang-tidy、clang-apply-replacements 等工具也使用相同的仓库地址，只需把插件名改为对应的名称即可。\n查看可用版本：添加插件后，可以运行 asdf list all clang-format 来列出所有可安装的 clang-format 版本。\n安装工具：选择一个版本（例如最新的 latest），执行：\nasdf install clang-format latest 这会下载并安装指定版本的 clang-format 二进制文件。\n设置全局版本：安装完成后，可以执行：\nasdf set clang-format latest 这会把版本写入 ~/.tool-versions 文件，实现全局可用。此后，你就可以直接在命令行中使用 clang-format 等命令了。\n以上操作完成后，clang-format、clang-tidy 等工具就已集成到 asdf 管理下。更多细节可参考 asdf 官方文档。\n欢迎试用并反馈建议 # 总的来说，asdf-clang-tools 为需要 Clang Tools 的开发者提供了一种新的安装方式。\n它与 cpp-linter 组织的其它工具（如 clang-tools-pip）互为补充。\n我诚挚欢迎大家尝试 cpp-linter 提供的整个 C/C++ lint 解决方案，选择最适合自己工作流的工具。\n同时，如果在使用过程中有任何问题或改进建议，欢迎通过 GitHub Issues、讨论区等渠道提出，一起完善 Cpp Linter 工具链，让 C/C++ 格式化和静态分析工作更加便捷高效！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-05-29","externalUrl":null,"permalink":"/posts/asdf-clang-tools/","section":"Posts","summary":"asdf-clang-tools 是一个 asdf 插件，用于安装和管理 Clang Tools 相关工具（如 clang-format、clang-tidy、clang-query 和 clang-apply-replacements 等）。","title":"asdf-clang-tools：使用 asdf 安装 Clang 工具的新选择","type":"posts"},{"content":"","date":"2025-05-29","externalUrl":null,"permalink":"/tags/clang-tools/","section":"标签","summary":"","title":"Clang-Tools","type":"tags"},{"content":" 分享两个最近的体会 # AI 让人感到“虚” # 老同事来欧洲出差，和他聊了很多 —— 从工作到生活，从技术到 AI。\n当我问他：“你对 AI 有什么感受？”\n他说了一个字：虚！\n这让我有些意外。他可是我心中的技术大牛，工作了十几年，积累了很多经验和技能。但如今，他却觉得自己在 AI 面前变得“虚”了。\n我也有同感。可能在一年前，我还觉得，自己在工作、博客、开源上的不断积累，会在下一份工作中起到很大的帮助；但 AI 的出现，让我开始怀疑：\n我过去四五年的努力是否还有意义？\nAI 的确让每个人的工作效率都有了显著提升。\n有同事开玩笑说：“我路过每个人的桌面，都看到 ChatGPT 开着。”；另一位同事接着说：“我都同时开好几个。”\n这样的对话一度让我感到沮丧。\n我开始怀疑：\n自己在工作中是否还有不可替代的价值？ 自己在开源社区的贡献是否还有意义？ 自己还在坚持写博客是否还有用处？ 直到我在 LinkedIn 上看到还有不少岗位在招聘，意识到手头的工作依然需要推进，这才慢慢把我拉回现实：AI 不会取代我们的职位，但一定会改变我们的做事方式。\n每天一小步 # 另外还有一个小的体会是：每天一小步。\n无论是工作还是生活，有目标感是好事。但如果目标太大，往往难以坚持，甚至会被拖垮。\n过去我常常给自己设定理想化的目标，比如“写一本书”或“翻译一本书”。但这样的目标需要投入大量时间和精力，最后常常因为现实的工作和生活节奏被搁置。\n后来我发现，其实偶尔写一篇小文章，或者维护一个小项目，更容易坚持。\n当一件事不再那么“重”，它也就没那么难开始了。\n所以，我们既要抬头看路，树立目标，也要低头走好每一步。\n这两点体会，虽然简单，却是我在日常工作和生活中逐渐领悟到的。\nAI 带来的冲击还在继续，我们每个人都要拥抱变化；而“每天一小步”的坚持，也在悄悄积累力量。\n愿我们都能在变化中找到节奏，在前行中保持方向。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-05-26","externalUrl":null,"permalink":"/posts/ai/","section":"Posts","summary":"AI 的出现让很多人感到“虚”，但它不会取代我们的职位，而是改变我们的做事方式。本文分享了对 AI 的体会和每天一小步的坚持。","title":"ChatGPT 一开，谁还去“努力”？","type":"posts"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/confluence/","section":"标签","summary":"","title":"Confluence","type":"tags"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/documentation/","section":"标签","summary":"","title":"Documentation","type":"tags"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/wiki/","section":"标签","summary":"","title":"Wiki","type":"tags"},{"content":"不知道你是否也在企业中使用过 Confluence，或者其他类似的 Wiki 工具。刚接触它时，我觉得这玩意儿真不错：功能强大、支持各种排版样式、可以插入图片、视频、图标，还能查看历史版本，使用体验比 Git 轻松太多了。\n但慢慢地，我发现了它的一个巨大问题：每个人都可以创建并维护自己的 Wiki 页面。\n一开始，这种自由看起来是优势。但时间一长，问题就来了：同一个主题的内容，可能会被不同的人写了多个版本。尤其是在项目或产品从一个团队交接到另一个团队时（在外企这是很常见的操作），新团队成员可能不会在原有文档上继续维护，而是习惯另起炉灶，记录自己的理解。\n于是，旧的 Wiki 随着人员流动逐渐失效（原作者可能早已离职），新的 Wiki 内容又不够完善甚至有误。这样一来，知识沉淀不仅没有统一，反而更混乱了。\n我始终认为 Wiki 工具本身是好的，但如果缺乏统一的管理机制、没有像 Git 那样的 Pull Request 审批流程，那最终它就容易沦为垃圾信息的生产场。\n相比之下，开源社区在这方面反而做得很好。\n以 Python 社区为例：\nPython 官网 https://www.python.org 的内容，是通过 GitHub 上的 python/pythondotorg 仓库维护的； 开发者指南 https://devguide.python.org 的内容，托管在 GitHub 上的 python/devguide 仓库。 不管是谁，只要对这些文档有修改建议，都必须通过 PR 提交，经过审查、通过 CI 检查后才能合并。而且因为是开源项目，社区用户也会主动参与反馈和改进，这也帮助文档长期保持高质量。\n但反观企业内部，就完全不同了：\n多人各写各的 Wiki，质量参差不齐； 内容孤岛多，缺乏维护，一旦人员流动，旧文档就“失效”； 更重要的是，内部文档缺乏公开审核机制，也没有外部反馈入口，错误不容易被发现或纠正。 还有一点可能更真实也更残酷：在企业内部，员工缺乏维护文档的动力。因为一个写得面面俱到、毫无遗漏的“满分文档”，可能在某天就意味着你可以被“无缝替换”。相比之下，把关键细节掌握在自己脑子里，才更有“工作安全感”。\n所以文档治理这件事，本质上和工具无关，关键在人。在没有文化和流程支撑的前提下，再先进的工具也可能变成信息垃圾的堆放地。\n文档和代码其实不分家。在开源社区的经历让我深有感触：真正优秀的人，往往能独立撑起一个团队的质量与节奏。他们热爱技术、主动负责、乐于分享，推动项目健康发展。\n反观一些企业项目，问题往往出在相反的方向。当团队成员缺乏主人翁意识，人员流动频繁，或者有些人能力一般却意见最多，最终只会在屎山代码上继续堆。\n最后，你是否也有类似的经历？你所在的公司又是如何管理内部文档和代码的呢？欢迎在评论区留言交流。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-05-14","externalUrl":null,"permalink":"/posts/docs-and-code/","section":"Posts","summary":"在企业中，Wiki 和 Confluence 等文档工具如果缺乏统一管理和审核机制，可能会导致信息混乱和知识沉淀失败。本文探讨了如何避免这种情况，并借鉴开源社区的成功经验。","title":"还在用 Wiki/Confluence？你可能在生产垃圾","type":"posts"},{"content":"","date":"2025-05-05","externalUrl":null,"permalink":"/en/tags/pip/","section":"Tags","summary":"","title":"Pip","type":"tags"},{"content":"If you\u0026rsquo;re used to pip install, manually creating virtual environments, and managing requirements.txt yourself, you might be surprised by uv.\nThis is a Python package management tool developed by the Astral team and written in Rust. It not only replaces the functionality of pip, venv, and pip-tools, but also provides faster dependency resolution and a more modern project management approach.\nStart with uv init to create a project skeleton with one command # We won\u0026rsquo;t start with \u0026ldquo;how to install uv,\u0026rdquo; but with \u0026ldquo;how to use uv to create a project.\u0026rdquo;\nuv init After running this command, uv will help you:\nCreate a .venv virtual environment; Initialize the pyproject.toml configuration file; (Optional) Add dependencies; Generate a uv.lock lock file; Set .venv as the default environment for the current directory (no manual activation needed); The entire process only requires one command to complete what used to take multiple steps, making it a better starting point for building Python projects.\nInstall dependencies using uv add (instead of pip install) # The traditional way is:\npip install requests But in the uv world, adding dependencies looks like this:\nuv add requests The benefits are:\nAutomatically writes to pyproject.toml\u0026rsquo;s [project.dependencies]; Automatically installs into .venv; Automatically updates the uv.lock lock file; No need to maintain requirements.txt anymore. If you want to add development dependencies (such as testing or formatting tools), you can:\nuv add --dev pytest ruff Want to remove dependencies?\nuv remove requests Running project scripts or tools: uv venv + uvx # uv\u0026rsquo;s virtual environment is installed by default in .venv, but you don\u0026rsquo;t need to source activate every time. Just execute:\nuv venv This ensures that .venv exists and is automatically configured as the default Python environment for the current shell. After that, you can run scripts or tools without worrying about path issues.\nEven better, uv provides a uvx command, similar to pipx and Node.js\u0026rsquo;s npx, which allows you to directly run CLI tools installed in the project.\nFor example, let\u0026rsquo;s use ruff to check or format Python code:\nuvx ruff check . uvx ruff format . Now with uvx, you don\u0026rsquo;t need to install a bunch of global tools, nor do you need to use pre-commit to unify command calls—use it directly within the project, it\u0026rsquo;s cross-platform and convenient.\nExample Project Structure # After uv init and some uv add commands, a clean Python project structure might look like this:\nmy-project/ ├── .venv/ ← Virtual environment ├── pyproject.toml ← Project configuration (dependencies, metadata, etc.) ├── uv.lock ← Locked dependency versions ├── main.py ← Project entry script User Experience? # I\u0026rsquo;ve recently adopted uv as the default tool for new projects:\nNo more manually writing requirements.txt No more struggling with mixing poetry.lock and pyproject.toml Dependency installation is very fast, 3-5 times faster for large projects on the first installation. Combined with ruff as a Lint + Format tool, there\u0026rsquo;s no need to install black and flake8 separately. In CI, I\u0026rsquo;m also gradually replacing pip install -r requirements.txt with it, using the lock file to build the environment for stronger consistency.\nSummary # If you:\nAre dissatisfied with the slow speed of pip install; Don\u0026rsquo;t want to write a bunch of requirements files; Want a more modern, faster, and more automated Python project structure; Then you should try uv. It\u0026rsquo;s a faster and more modern package manager toolset.\nStarting with your next project, why not start with uv init?\nProject address: https://docs.astral.sh/uv/\n","date":"2025-05-05","externalUrl":null,"permalink":"/en/posts/uv/","section":"Posts","summary":"uv is a Python package management tool developed by the Astral team. It replaces the functionality of pip, venv, and pip-tools, offering faster dependency resolution and a more modern project management approach.","title":"Still using pip and venv? You're outdated! Try uv!","type":"posts"},{"content":"","date":"2025-05-05","externalUrl":null,"permalink":"/en/tags/uv/","section":"Tags","summary":"","title":"Uv","type":"tags"},{"content":"","date":"2025-04-29","externalUrl":null,"permalink":"/tags/lithuania/","section":"标签","summary":"","title":"Lithuania","type":"tags"},{"content":"","date":"2025-04-29","externalUrl":null,"permalink":"/tags/pycon/","section":"标签","summary":"","title":"PyCon","type":"tags"},{"content":"今天是我参加 PyCon LT 2025 的第三天：AI and ML Day，主题是 LLM 和 Neural Networks。\n全部日程可以在这里查看：PyCon LT 2025\nAI 的发展确实很快，我也想借这次大会的机会，听一听关于 AI 的相关讨论。虽然因为下午有工作需要回公司处理，只听了上午的议程，但依然有收获。\n主要的收获是：AI 已经成为每个人都不能不学习和应用的技术了。\n依旧是流水账记录一下今天的感受 # 今天同样是早上九点多到的会场，第一场演讲开始前，打上一杯咖啡就进去听了。\n第一场的主题是《Open-source Multimodal AI》。这位演讲嘉宾来自法国，是 Hugging Face 团队的成员，也许大家都见过这个表情 🤗 —— 著名的 Transformers、Diffusers 开源库就是他们开发的，另外他们还有 Hugging Face Hub 网站，用户可以上传、分享、下载各种 AI 模型和数据集。 本次演讲主要分享了多模态人工智能的内容，介绍了多模态和开源的一些背景知识、相关库、基本 API，以及如何开始使用开源模型，并提到了目前流行的一些开源模型。\n第二场听的是《Knowledge Bases \u0026amp; Memory for Agentic AI》，介绍了 Agent 人工智能（Agentic AI）的起源。演讲内容包括如何设计提示（prompt），指导 LLM 使用工具并规划解决复杂查询的方法；学习函数调用，以及如何利用 LLM 的这一特性作为代理（Agent）的基础。演讲者同样是一位女性，来自荷兰。\n第三场听的是《EGTL data-processing model prototype using Python》，分享了如何在传统 ETL（提取、转换、加载）流程中，新增由生成式 AI 支持的“生成”步骤，扩展为 EGTL（提取、生成、传输、加载）。演示了基于数据仓库的 Python 流水线如何自动提取数据、生成新洞察并实现优化转换，探讨了实用工作流程、实际用例和最佳实践，帮助数据项目应用 EGTL 方法。\n上午最后一场听的是《AI 360: From Theory to Transformation》，分享了 AI 的发展历程。从数据和模型的双重视角，回顾了人工智能的演变，讲述了从早期的符号系统到今天先进的数据驱动技术的发展过程。与会者了解了不断增长的数据集和日益复杂的模型架构如何相互作用，推动 AI 从理论好奇心变为全球创新催化剂的过程。\n后面因为下午要回公司处理工作，就在吃了午饭后离开了。\n启发 # 今天听了几位演讲，女性讲者的比例依旧很高，前两位讲得尤其好，逻辑清晰、表达有力。\n而今天关于 AI 的主题，更是让我意识到一件事：**AI 的发展根本不等人。**谁还在观望、犹豫，谁就已经落后了。唯一的出路，是尽快拥抱它，而不是浪费时间抗拒或怀疑。\n说白了，现在最该做的不是去学怎么“用” AI，而是反过来问：**哪些事情应该让 AI 重新来做一遍？**你过去那一整套工作方式，可能早已效率低下，只是 AI 还没来得及替你动刀而已。\n顺带一提，现在市面上 99% 的 AI 课程本质上都是智商税——用 DeepSeek、ChatGPT 摘点资料、套个 Notion 模板，然后卖给你。看似在教你用 AI，本质上是在‘用 AI 来教你交钱’。\n真正该做的，是盯紧你所在的行业和岗位，认真思考两个问题：AI 是否正在取代我的工作？我是否能用 AI 做出比别人更好的结果？\n总结 # 以上就是我参加 PyCon 第三天的简单记录。\n整体来看，这次 PyCon LT 之行非常值得。虽然谈不上收获了太多具体的技术细节，但能听到来自同行的分享，无论是从技术视角还是人际交流上，都是有价值的。\n这次也和几位同事一同参会，有更多时间深入交流，感觉非常难得。\n如果以后还有机会参加 PyCon，我希望能更多关注一些核心项目，比如 CPython、PyPA，也希望能接触更多 DevOps 和 AI 相关的话题。\n这次的 PyCon 随记就分享到这里，期待下一次再见！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-29","externalUrl":null,"permalink":"/misc/pycon-lt-d3/","section":"Miscs","summary":"今天是 PyCon LT 2025 的最后一天，主题是 AI and ML Day。分享了几位女性讲者的精彩演讲，以及对数据科学和 AI 的新认识。","title":"全程记录｜PyCon LT 2025 第三天：AI 能取代你吗？","type":"misc"},{"content":"今天是我参加 PyCon LT 2025 的第二天：Data Day，主题是 Dataframes、Databases、Orchestration。\n全部日程可以在这里查看：PyCon LT 2025\n这确实是我不太熟悉的领域，但还是希望来听听，看看能不能有所收获，毕竟门票里也包含了这一天的议程。没想到，今天真正打动我的，竟然是几位技术演讲者中的女性讲者——不仅讲得有料，还有热情、风格、甚至让我顺手关注了几个博客……\n依旧是流水账记录一下今天的感受 # 早上也是九点多到的会场，刚刚好赶上第一场演讲开始。打上一杯咖啡就进去听了。\n这场的主题是《Build Your Own (Simple) Static Code Analyzer》。这位演讲嘉宾来自纽约，是 numpydoc 的核心开发者，也是《Hands-On Data Analysis with Pandas》一书的作者。确实很有干货，她讲的是如何使用 AST（抽象语法树）构建一个自己的静态代码分析器，这也是我第一次了解到这些 Linter 工具的工作原理，非常有启发。\n另外我也觉得她不仅有技术实力，而且乐于公开演讲，打造个人影响力。看了她的博客 https://stefaniemolin.com/ 也很值得参考。\n接下来听了一场主题是《Data Warehouses Meet Data Lakes》，是一位意大利人带着浓重的意大利口音讲数据仓库面临的挑战，以及他们使用哪些技术来构建数据架构。这个领域我确实不太熟悉，但也了解到了，和 DevOps 一样，最终也需要构建 pipeline 来完成数据的收集和分析。\n然后又去听了《Cutting the price of Scraping Cloud Costs》，讲述了他们用哪些技术构建 pipeline 来计算云定价。\n上午最后一场我听了《cluster-experiments: A Python library for end-to-end A/B testing workflows》，主要介绍了他开发的这个 Python 库用来做 A/B 测试。我怀疑他的这个演讲多少有点是在宣传自己的开源项目。\n等我听完演讲已经是 12:30 以后了，看到很多人都在一楼排队点餐，我索性就去三楼找了个地方先处理工作。\n工作差不多处理完了，同事发消息说大家一起拍张照吧，于是我才下楼去点餐。\n点餐依然等了很久，吃完饭差不多快两点了，下午的演讲也要开始了。\n下午第一场我听的是《Accelerating privacy-enhancing data processing》，关于在真实世界中进行癌症研究的数据处理挑战。演讲者也介绍了他们用到的技术栈，而且特别有心地在开头准备了一个乐高积木，因为他在 PPT 里也使用乐高元素来表达观点。最后在提问环节，说谁答对了就送给谁，结果这个乐高被我同事答对并领走了。\n第二场我听了《Working for a Faster World: Accelerating Data Science with Less Resources》，这个分享里主要了解到一个工具叫 Panel，可以用来做数据探索和构建 Web 应用。其他的收获就不多了。\n接着参加了《Organize your data stack using Dagster》，介绍了一个开源的数据编排工具 Dagster。讲得非常好，我觉得作为 Dagster 的开发者应该感谢她。估计很多之前没听说过这个工具的人，现在也想上手试试看它能带来哪些好处，以及在实际工作中该怎么使用。值得一提的是，这位演讲者还是一位热爱技术的女性，在这次 PyCon 上她一共有两个演讲，令人佩服！\n最后一场我听了《Top 5 Lessons from a Senior Data Scientist》，这场就更有启发了，演讲者也是一位女性，目前是自由职业的数据科学家。我发现数据领域的女性确实不少呢？她的演讲不涉及技术，纯讲经验，也就是作为一名资深数据科学家的五大经验教训，其中的观点对我们这些在职场中的人都很适用。她的个人网站也维护得非常好，值得学习。\n启发 # 我发现今天我听的几位演讲者里，女性讲者在个人网站方面普遍比男性做得更好。\n但怎么说呢，所有的演讲者都很棒，至少他们都有热情、有想法，愿意站在台上把自己的所知所想分享给台下的听众，这一点就很值得我们学习了。\n尤其是那些技术过硬、表达自信、又乐于分享的女性讲者，确实有点让我圈粉了，也有启发。\n这就是我参加 PyCon 的第二天“流水账”。\n希望明天的 AI and ML Day 能带来更多启发。那就明天见了！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-24","externalUrl":null,"permalink":"/misc/pycon-lt-d2/","section":"Miscs","summary":"今天是 PyCon LT 2025 的 Data Day，主题是 Dataframes、Databases、Orchestration。虽然不太熟悉这些领域，但几位女性讲者的演讲让我印象深刻，收获颇丰。","title":"全程记录｜PyCon LT 2025 第二天：被几位女性开发者圈粉了","type":"misc"},{"content":"第一次参加了 Python 的大会，没想到是在国外。\n虽然这次的规模和演讲嘉宾的知名度不及即将六月份在捷克举办的 PyCon 欧洲大会，但这也是一次不错的体验。\n这次 PyCon LT 2025 一共是三天：\n第一天（4月22日）是 Python Day（Web、Cloud、IoT、Security） 第二天（4月23日）是 Data Day（Dataframes、Databases、Orchestration） 第三天（4月24日）是 AI and ML Day（LLM、Neural Networks） 全部的日程可以在这里查看：PyCon LT 2025\n先说说第一次参加的感受吧： # 我是早上九点刚过到的会场，扫了二维码后，工作人员给我一张写有我名字的参会证，然后我就去参加开幕式了。\n接着就是当天的第一场演讲，主题是《Ethics, Privacy and a Few Other Words》。说实话，我对这个话题并不太了解，主要讲的是道德、隐私等相关问题，听得不是特别明白。旁边的同事倒是听得津津有味，还时不时给我补充一些内容，这让我切实感受到自己在英语方面的不足——技术相关的内容还可以听懂，但一旦话题偏离技术，就有点跟不上了。\n这位讲者确实很敢说，有些观点我甚至担心可能会影响他以后进入一些大公司，甚至在前往某些国家时可能会受到限制。\n之后就是茶歇时间，大家出来喝水、喝咖啡，吃点甜点。\n接下来，会场会同时有多个主题在不同的房间进行，可以根据自己的兴趣自由选择。\n我接着听了一个主题是《Code Review the Right Way》。这个演讲没有讲太多技术细节或工具，而是分享了一些关于代码审查的最佳实践，以及如何建立代码审查文化。我觉得说得挺好的，特别是在审查别人代码时，要做到谦逊、委婉、有礼貌，避免冲突。\n然后我又去了另一场，主题是《What We Can Learn from Exemplary Python Documentation》。这个主题刚好也是我最近在做的事情之一，主要是将 Python 项目中的 Markdown 文档转换为 reStructuredText，并使用 Sphinx 生成 HTML 文档。\n听了一会儿之后，我觉得信息密度不是很高，就闪人去了另一场：《Using Trusted Publishing to Ansible Release》。\n主要是因为我对 Trusted Publishing 和 Ansible 都比较了解，去看看有没有什么新鲜的内容。演讲者是 PFS 的研究员、Ansible 的发布经理，但这个主题确实没有什么新意，可能因为我已经在使用这些内容了，她演示了一个 Demo ，通过 GitHub 使用 Trusted Publishing 发布一个 Python package 到 PyPI，确实没什么新鲜。\n听完这场已经 12:30 了，刚好是吃午饭的时间。这个会议是包含午餐的，主办方大概包下了会场内的一家餐厅，大家可以随便在餐厅的点餐机器上点单，然后直接拿着打出的单子等着取餐，就像麦当劳、肯德基那样。当然你想多吃几份也没人拦着你，自助点餐机就在那里。\n吃完饭后，两点钟才有第一场下午的分享。无所事事之下，就和同事一起去参加会场里的集章活动。\n主要是主办方联合赞助商安排了一些小游戏，比如玩贪吃蛇，还有其他小活动，通过参与可以收集邮戳。另外，还需要去找演讲者合影来获得特定邮戳。\n最终我集齐了 4 个邮戳，可以去工作人员那里参与转盘抽奖。我一共抽了两次，第一次抽中“再转一次”，第二次抽到了一件 PyCon T-shirt，把一起的同事羡慕坏了。\n下午参加的第一场是《Python Containers: Best Practices》。讲得很好，但大部分内容我都了解，因为我之前写过一篇关于 Docker 容器最佳实践的文章，内容差不多，感觉可以跳过。\n后面我又听了一场叫《Do Repeat Yourself》。这场稍微有点被标题“骗”了，但演讲者确实花了很大心思来准备。他没有用 PPT，而是用 FastAPI + CSS + HTML 做了一个网站来演讲，还配了音乐，整体很用心，也挺酷的。虽然技术内容不算新，但让我觉得 PPT 还能这么玩，启发是可以有空尝试用 FastAPI 做个 Web 网站练练手。\n接着是《Coding Aesthetics: PEP 8, Existing Conventions, and Beyond》，主要讲的是 Linter、Pythonic 等相关内容，这些我都挺熟的，就不展开说了。\n然后又到了下午的咖啡时间，和同事们聊了聊他们都听了哪些主题。期间还被主办方的工作人员随机采访了，说不定会出现在后续发布的视频中。\n最后一个主题是《Skip the Design Patterns: Architecting with Nouns and Verbs》，算是当天的“重磅”嘉宾，看着得有五六十岁了，现场也有很多人认识他。\n这位讲者主要讲的是：Python 程序员很少花时间去考虑设计模式，以及为什么设计模式在 Python 中似乎不再适用。他还带着大家一起重构一个 Python 项目，引出三种思维方式，并介绍了如何在架构上进行理解和应用。\n这就是我参加 PyCon 的第一天“流水账”。\n希望明天的 Data Day 能学习到更多新内容。那就明天再见啦！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-23","externalUrl":null,"permalink":"/misc/pycon-lt-d1/","section":"Miscs","summary":"参加了 PyCon LT 2025 的第一天，分享了演讲内容、个人感受以及对 Python 社区的思考。","title":"全程记录｜PyCon LT 2025 第一天：我在异国 Python 大会上的见闻","type":"misc"},{"content":"","date":"2025-04-16","externalUrl":null,"permalink":"/tags/life/","section":"标签","summary":"","title":"Life","type":"tags"},{"content":"上个月回国休假，结束之后从北京转机飞欧洲。因为我们三口之家行李比较多，就订了首都机场附近的酒店，方便第二天出发。趁着在北京停留的这个晚上，就约了几位大学同学小聚一下。\n那天是星期二，我大概下午四五点钟到酒店。感谢北京的同学们的热情招待，也挺不好意思的，让他们下班后还大老远跑来机场这边见我一面。\n有一个小感慨就是：北京人下班真的挺晚的。\n最早到的一位同学是晚上八点，最晚的则到了十点钟。因为第二天还要上班，加上有娃在场，需要早点休息，我们就聚到了十一点就散了。\n其实我离开北京已经十几年了，差不多都快忘了自己当年也是晚上九十点才下班，甚至偶尔加班到半夜才回家。\n这十年在大连工作，主要在外企，工作节奏相对温和。一般如果晚上有安排，我会早点去公司、早点下班，或者请个小时假，就算偶尔早走了，没人会去计算你在办公室到底坐了满不满八小时。\n现在再看到北京同学的生活节奏，我真的很难想象，要怎么平衡这么晚的下班时间和家庭生活，尤其是当你有了孩子。\n当然，每个人都有自己的应对方式。\n比如有的同学请父母过来帮忙带孩子；还有的因为孩子上学的问题，让孩子暂时在老家生活，只有放假的时候再团聚。\n另外他们还提到了北京幼儿园入园的问题。同学说，北京各类幼儿园在招生时会有一定的优先顺序，比如是否为本地户籍、是否在本区有房产等，综合考量户籍、房产、甚至是居住证等情况，顺位安排比较复杂。这也再次让我感受到：在北京，生活确实不容易，尤其是当了父母之后，再也承受不起北京的生活节奏。\n“逃离”未尝不是一种选择。避开这些现实的压力，让生活变得简单一点、纯粹一点。\n和队友聊天时，我们也不觉得外国的月亮就一定比中国圆。\n怎么说呢？可能这就是人口大国的特征吧。资源有限、人口多，哪怕你再努力，也时常感受到那种“被推着往前走”的无力感。\n想想看，如果欧洲哪个国家也有我们这么庞大的人口，竞争恐怕也会一样激烈吧。\n作为个体，我们所能做的，大概就是始终保持准备，继续前行，在适合的时机遇见并把握属于自己的机会。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-16","externalUrl":null,"permalink":"/misc/one-night-in-beijing/","section":"Miscs","summary":"在北京停留的一晚，和同学小聚，感受到北京人下班的晚节奏，以及生活在大城市的压力和挑战。","title":"回国休假的一点感慨：北京的夜，好晚","type":"misc"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/en/tags/clang/","section":"Tags","summary":"","title":"Clang","type":"tags"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/en/tags/clang-format/","section":"Tags","summary":"","title":"Clang-Format","type":"tags"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/en/tags/clang-tidy/","section":"Tags","summary":"","title":"Clang-Tidy","type":"tags"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/en/tags/cpp-linter/","section":"Tags","summary":"","title":"Cpp-Linter","type":"tags"},{"content":"Last week, the open-source project I created and maintain, cpp-linter-action, reached a small milestone:\n🌟 GitHub Star count surpassed 100!\nWhile this number isn\u0026rsquo;t huge, it\u0026rsquo;s a small milestone for me—it\u0026rsquo;s the first time one of my projects has received over 100 stars on GitHub. It\u0026rsquo;s a validation of the project and gives me the motivation to continue maintaining it.\nMy first commit to this project was on April 26, 2021, and almost 4 years have passed. Looking back, I\u0026rsquo;m glad I haven\u0026rsquo;t been idle and have left behind something useful for others.\nAs the project has evolved, so has its user base. I estimate that thousands of projects are currently using this Action.\nThese include several well-known organizations and open-source projects, such as:\nMicrosoft Apache NASA CachyOS nextcloud Jupyter Most importantly, this process has taught me many new skills and knowledge, and it\u0026rsquo;s maintained a \u0026ldquo;side hustle\u0026rdquo; habit for me:\nMy phone isn\u0026rsquo;t just for WeChat and Douyin (TikTok), but also for GitHub.\nThis project also became a springboard for my subsequent work. I later created the cpp-linter organization to maintain and release binary versions and Docker images of clang-tools with other developers. I also developed cpp-linter-hooks, providing users with clang-format and clang-tidy pre-commit hooks for easier use.\nWithout being immodest:\nIf your project is developed in C/C++, and you want to use clang-format and clang-tidy, then cpp-linter is an unavoidable option.\nFinally, I welcome your feedback and suggestions, and you\u0026rsquo;re welcome to contact me via Issue or Discussions!\nIf you find this project helpful, please leave a message on the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo; or give the project a star on GitHub to show your support!\n—— Written on 2025-04-15 12:49 AM\n","date":"2025-04-15","externalUrl":null,"permalink":"/en/posts/cpp-linter-action-milestone/","section":"Posts","summary":"cpp-linter-action is a GitHub Action that provides C/C++ code formatting and static analysis capabilities. It uses clang-format and clang-tidy, supporting various configurations and custom rules.  Since its creation in 2021, the project has been used by several well-known organizations and open-source projects.","title":"Microsoft and NASA Use It? My 4-Year-Old Side Project Hit 100 Stars","type":"posts"},{"content":"","date":"2025-04-13","externalUrl":null,"permalink":"/en/tags/github/","section":"Tags","summary":"","title":"GitHub","type":"tags"},{"content":"Yesterday, some people may have encountered the inability to access GitHub.\nSome people joked online that it was because of increased tariffs from the US, and GitHub\u0026rsquo;s response code upgraded from 200 to 403.\nJokes aside, GitHub later provided an explanation—it was a configuration error, and the problem has now been fixed.\nI recently returned to China for a trip, and once again experienced the convenience and affordability of express delivery, takeout, transportation, and payment. The only inconvenience was the network.\nMore accurately, it\u0026rsquo;s the websites and services that many programmers rely on daily, such as GitHub, Docker Hub, Docker Desktop, and ChatGPT. Of course, now we also have alternatives like DeepSeek.\nIn my personal experience, GitHub is accessible in China, but its stability is poor. Often, it works when I first open it, but then completely fails to load after a while.\nOutside of work, I already have limited free time in the evenings, but I often have to spend that time troubleshooting network problems.\nWhile I know I can modify my hosts file or try other methods to circumvent network restrictions, setting it up on my computer has always been problematic.\nAll this troubleshooting is exhausting. Sometimes I don\u0026rsquo;t even want to turn on my computer anymore; I just want to lie down and scroll through my phone.\nMy patience with accessing GitHub is wearing thin.\nOh well, I\u0026rsquo;ll just take it as a break.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-04-13","externalUrl":null,"permalink":"/en/posts/visit-github/","section":"Posts","summary":"Many programmers, especially those in China, are frustrated by access issues with GitHub. This article shares my personal experiences and reflections on network problems.","title":"My Patience with Accessing GitHub is Wearing Thin","type":"posts"},{"content":"","date":"2025-04-12","externalUrl":null,"permalink":"/en/tags/readthedocs/","section":"Tags","summary":"","title":"ReadTheDocs","type":"tags"},{"content":"","date":"2025-04-12","externalUrl":null,"permalink":"/en/tags/rst/","section":"Tags","summary":"","title":"RST","type":"tags"},{"content":"In daily open-source projects or team collaborations, we often need an easy-to-maintain, automatically deployable documentation system.\nRecently, while maintaining my own open-source project, I tried using Sphinx to generate documentation and ReadTheDocs to achieve automatic building and hosting. The overall experience was quite good.\nI\u0026rsquo;m documenting the configuration process here, hoping it can help others with similar needs.\nWhy Choose Sphinx and ReadTheDocs # Sphinx is a documentation generator written in Python, initially designed for the official Python documentation. It supports reStructuredText and Markdown (via plugins). ReadTheDocs is a documentation hosting platform that can automatically pull code from your Git repository, build, and publish documentation, supporting webhook auto-triggering. The combination of these two tools is ideal for continuously maintaining and updating documentation, and the community is mature with abundant resources.\nBasic Configuration Steps # Below is the complete process configured in a real project.\n1. Install Sphinx and Related Dependencies # It\u0026rsquo;s recommended to use a virtual environment, then install:\n# docs/requirements.txt sphinx==5.3.0 sphinx_rtd_theme==1.1.1 # If you need Markdown support, add: myst_parser==0.18.1 Install dependencies:\npip install -r docs/requirements.txt Notes:\nsphinx-rtd-theme is the default theme used by ReadTheDocs myst-parser is used to support Markdown 2. Initialize the Documentation Project Structure # In the project root directory, execute:\nsphinx-quickstart docs It is recommended to separate the source and build directories.\nAfter execution, the docs directory will generate files such as conf.py and index.rst.\n3. Modify the conf.py Configuration File # Several key settings are as follows:\n# Read the Docs configuration file # See https://docs.readthedocs.io/en/stable/config-file/v2.html for details from datetime import datetime project = \u0026#34;GitStats\u0026#34; author = \u0026#34;Xianpeng Shen\u0026#34; copyright = f\u0026#34;{datetime.now().year}, {author}\u0026#34; html_theme = \u0026#34;sphinx_rtd_theme\u0026#34; If you need Markdown support, you need to add the following to conf.py:\nextensions = [ \u0026#39;myst_parser\u0026#39;, # Support Markdown ] Configuring ReadTheDocs for Automatic Building # As long as the project structure is clear, ReadTheDocs can basically run with one click.\n1. Import the Project to ReadTheDocs # Log in to https://readthedocs.org/ Click \u0026ldquo;Import a Project\u0026rdquo; and select your GitHub or GitLab repository Ensure the repository contains docs/conf.py; the system will automatically recognize it. 2. Add the .readthedocs.yml Configuration File # To better control the build process, it is recommended to add .readthedocs.yml to the project root directory:\n# Read the Docs configuration file # See https://docs.readthedocs.io/en/stable/config-file/v2.html for details version: 2 build: os: ubuntu-24.04 tools: python: \u0026#34;3.12\u0026#34; sphinx: configuration: docs/source/conf.py python: install: - requirements: docs/requirements.txt After configuration, every time you submit a Pull Request, ReadTheDocs will automatically pull and build the latest documentation for preview, ensuring the documentation is as expected.\nFinal Result # After building, ReadTheDocs will provide a documentation address similar to https://your-project.readthedocs.io/, facilitating team collaboration and user consultation.\nMy current open-source project also uses this scheme, for example: GitStats documentation\nSummary # By following the above configuration, you can almost achieve \u0026ldquo;submit after writing documentation, and it goes live,\u0026rdquo; greatly improving the efficiency of documentation maintenance.\nIf you are writing open-source project documentation, or want to add a documentation system to a team project (especially Python projects), you might want to try Sphinx + ReadTheDocs.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the official account \u0026ldquo;DevOps攻城狮\u0026rdquo; (DevOps Engineer).\n","date":"2025-04-12","externalUrl":null,"permalink":"/en/posts/sphinx-readthedoc/","section":"Posts","summary":"In open-source projects or team collaborations, Sphinx + ReadTheDocs provides an easy-to-maintain, automatically deployable documentation system. This article documents the configuration process and considerations.","title":"Setting up Sphinx + ReadTheDocs from Scratch - Rapidly Deploying Automated Documentation","type":"posts"},{"content":"","date":"2025-04-12","externalUrl":null,"permalink":"/en/tags/sphinx/","section":"Tags","summary":"","title":"Sphinx","type":"tags"},{"content":"","date":"2025-04-11","externalUrl":null,"permalink":"/en/tags/markdown/","section":"Tags","summary":"","title":"Markdown","type":"tags"},{"content":"In daily work, whether writing READMEs, blogs, or project documentation, we always need to choose a markup language to format the content.\nCurrently, there are two mainstream choices: Markdown and reStructuredText (RST).\nWhat are the differences between them? And which one should be chosen in what scenario?\nRecently, I converted the documentation of the gitstats project from Markdown to RST and published it on ReadTheDocs. This article will discuss some of my practical experiences.\nWhat is Markdown? # Markdown is a lightweight markup language, originally designed by John Gruber and Aaron Swartz in 2004, aiming to make documents as \u0026ldquo;readable, writable, and convertible to well-formed HTML\u0026rdquo; as possible.\nAdvantages:\nSimple syntax, easy to learn Wide community support (GitHub, GitLab, Hexo, Jekyll, etc., all support it) Fast rendering speed, good format consistency Common Uses:\nREADME.md Blog posts Simple project documentation What is RST? # RST is a markup language used more frequently in the Python community, maintained by the Docutils project. Compared to Markdown, its syntax is richer and stricter.\nAdvantages:\nNative support for footnotes, cross-references, automatic indexing, code documentation, and other advanced features The preferred format for Sphinx, suitable for large-scale project documentation More friendly to structured documents Common Uses:\nDocumentation for Python projects (such as official documentation) Technical manuals generated using Sphinx Multilingual documentation (in conjunction with gettext) Syntax Comparison Summary # Feature Markdown RST Heading # at the beginning ===== or ----- underline Bold / Italic **text** / *text* **text** / *text* Hyperlink [text](url) `text \u0026lt;url\u0026gt;`_ Table Simple tables (extension supported) Requires strict indentation, complex writing Footnote / Citation Unsupported / Highly limited Native support Cross-reference Unsupported Native support Markdown is easier, RST is more professional.\nWhen to Use Markdown? # For smaller projects requiring only simple documentation When team members are unfamiliar with RST and want to write documentation quickly For blogs and daily notes, Markdown is more recommended When the platform used (such as GitHub Pages, Hexo) supports Markdown by default In short: Markdown is the preferred choice for lightweight documents.\nWhen to Use RST? # When using Sphinx to generate API documentation or technical manuals When the project structure is complex and requires advanced features such as automatic indexing, cross-referencing, and module documentation When integration with the Python toolchain (such as autodoc, napoleon, etc.) is required When publishing to ReadTheDocs (although it now supports Markdown, the RST experience is better) In short: RST is recommended for Python projects or structured technical documents.\nMy Personal Suggestion # If you are:\nA development engineer, using Markdown for daily documentation is sufficient A Python developer, it\u0026rsquo;s recommended to learn RST, especially when using Sphinx to write documentation An open-source project maintainer, for small-scale projects, using Markdown for the README is fine; but for documentation sites, consider using RST + Sphinx for building Summary in One Sentence # Markdown is more like a convenient notepad for writing, while RST is more like a typesetting system for writing books.\nThe choice depends on whether your goal is to write \u0026ldquo;notes\u0026rdquo; or \u0026ldquo;manuals\u0026rdquo;.\nIf you have your own insights on Markdown, RST, or documentation building tools, please feel free to leave a comment and share.\nNext time, I plan to share my experience with Sphinx configuration and automatic publication to ReadTheDocs～\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-04-11","externalUrl":null,"permalink":"/en/posts/md-vs-rst/","section":"Posts","summary":"Markdown and reStructuredText (RST) are two commonly used markup languages. This article compares their advantages and disadvantages and shares usage suggestions in different scenarios.","title":"Markdown — Not So Great Anymore? Why More and More Python Projects Use RST?","type":"posts"},{"content":"","date":"2025-03-12","externalUrl":null,"permalink":"/en/tags/nutanix/","section":"Tags","summary":"","title":"Nutanix","type":"tags"},{"content":"","date":"2025-03-12","externalUrl":null,"permalink":"/en/tags/vmware/","section":"Tags","summary":"","title":"VMware","type":"tags"},{"content":" Background # If you are an enterprise VMware user, you may be considering leaving VMware. Currently, many enterprise users are actively seeking alternatives to reduce costs and reduce dependence on the VMware ecosystem.\nMany companies are considering leaving VMware, mainly due to:\nThe impact of Broadcom\u0026rsquo;s acquisition of VMware. In 2023, Broadcom completed its acquisition of VMware and made a series of adjustments, including:\nCancellation of perpetual licenses for some products, pushing for a subscription model. Price increases, significantly increasing the costs for many companies. Cutting some non-core products and partnership programs, causing unease among some companies and partners. These changes have led many companies to start looking for alternatives to reduce costs and reduce their reliance on the VMware ecosystem.\nAlternatives # Today, I\u0026rsquo;ll share a potential alternative for enterprise users—Nutanix.\nIt is a hyper-converged infrastructure (HCI) (alternative to VMware vSAN / vSphere), suitable for enterprises that need to integrate computing, storage, and network resources and want to reduce their reliance on VMware vSAN or vSphere.\nNutanix is a major competitor to VMware vSphere, supporting KVM and providing enterprise-grade HCI solutions. It offers a simple management interface, similar to VMware vSAN, but at a lower cost. Suitable for companies that want to migrate from VMware but still want enterprise support. Advantages of Nutanix # Nutanix has several advantages over VMware in terms of architecture, management, scalability, and cost-effectiveness, as detailed below:\n1. Unified Hyper-Converged Architecture (HCI) # 🔹VMware： # Traditionally uses a vSphere + vSAN + NSX combination, requiring multiple components to work together. Requires separate configuration of vSAN storage, separated from computing and network resources, resulting in higher management complexity. 🔹Nutanix： # Uses a hyper-converged architecture (HCI), integrating computing, storage, and networking. Replaces VMware ESXi with Nutanix AHV (Acropolis Hypervisor), eliminating the need for additional hypervisor licensing fees. Storage-as-a-Service (Distributed Storage Fabric, DSF), storage performance scales linearly with cluster expansion. ✅ Advantages：\nSimplified management, eliminating the need for additional SAN/NAS storage devices, with unified management of storage and computing resources. Better scale-out, increasing computing and storage capacity by adding nodes. Reduced software licensing costs (VMware vSphere/vSAN costs are high, while Nutanix AHV is free). 2. More Flexible Hyper-Converged Storage # 🔹VMware： # Relies on vSAN for storage, requiring combined configuration with vSphere during expansion. vSAN requires additional licensing and has strict hardware compatibility requirements. 🔹Nutanix： # Built-in Nutanix AOS (Acropolis Operating System), providing distributed storage (Nutanix Files, Volumes, Objects). Higher storage elasticity, supporting hybrid cloud storage (AWS/Azure/Nutanix private cloud). Supports a combination of local NVMe SSDs and HDDs, automatically tiering storage based on data temperature. ✅ Advantages：\nNo additional storage licensing fees, lower cost compared to vSAN. Automatic data optimization, storage performance improves with cluster expansion. Supports external cloud storage, suitable for hybrid cloud deployments. 3. Built-in Free Hypervisor (AHV), Reduced Licensing Costs # 🔹VMware： # Requires paid use of ESXi and management through vCenter. Many companies using VMware need to purchase additional vSphere Enterprise Plus or vSAN licenses, resulting in high costs. 🔹Nutanix： # Provides Nutanix AHV (Acropolis Hypervisor), based on KVM, and is free to use. Managed directly through Prism Central, eliminating the need for vCenter. VMware VM compatible, supporting direct migration of existing VMware VMs to Nutanix AHV. ✅ Advantages：\nFree hypervisor, saving VMware vSphere/ESXi licensing fees. No vCenter needed, simpler management. VMware VM compatible, easier migration. 4. Higher Automation and Scalability # 🔹VMware： # Relies on vSphere and vSAN for VM resource management. Requires vRealize Automation (vRA) for automated operations, with high licensing costs. 🔹Nutanix： # Provides Prism Central for unified management of VMs, storage, and networking, making operations simpler. Provides Calm (automation orchestration tool), supporting one-click deployment of applications (K8s, Jenkins, DevOps-related tools). Supports direct API calls, allowing automation through the Nutanix Prism API. ✅ Advantages：\nHigher degree of automation, suitable for DevOps scenarios. Simpler management interface, better usability. Stronger API support, suitable for CI/CD automated integration. 5. Stronger Hybrid Cloud Support # 🔹VMware： # Requires additional purchase of VMware Cloud on AWS, Azure VMware Solution to extend to the public cloud. VMware vSAN and NSX require additional licensing for support in public cloud environments. 🔹Nutanix： # Built-in Nutanix Clusters, supporting hybrid cloud deployments (AWS, Azure, on-premises data centers). Automatic data synchronization, allowing data migration between on-premises Nutanix and cloud-based Nutanix. Provides Nutanix Xi as a SaaS solution, supporting cross-cloud management. ✅ Advantages：\nHybrid cloud support without additional licensing, lower cost compared to VMware. Simpler data migration, supporting automatic synchronization of on-premises and cloud data. Unified management of public and private clouds, reducing operational complexity. 6. Cost Advantages # 🔹VMware： # Requires a vSphere + vSAN + NSX combination, with high overall licensing costs. VMware uses CPU core billing, and VMware may further increase licensing costs in 2024 (due to the Broadcom acquisition). 🔹Nutanix： # Provides a free AHV Hypervisor, saving vSphere licensing fees compared to VMware. Pay-as-you-go, unlike VMware, which requires bundling multiple products. Integrated storage, computing, and networking, eliminating the need to purchase vSAN or NSX separately. ✅ Advantages：\nLower overall TCO (Total Cost of Ownership), potentially saving 30%-50% compared to VMware. Reduced hypervisor licensing fees, free AHV replacing vSphere. Fewer components, improved management efficiency. Summary: Nutanix vs VMware # Feature VMware Nutanix Hypervisor ESXi (Paid) AHV (Free) Management Tool vCenter (Additional Charge) Prism Central (Free) Storage vSAN (Additional Charge) Nutanix Files/Volumes (Built-in) Automation vRealize Automation (Paid) Calm (Built-in) Hybrid Cloud Requires VMware Cloud solutions Nutanix Clusters (Built-in support) Cost High cost of vSphere + vSAN + NSX Saves ESXi/vSAN costs Scalability Requires manual scaling of vSphere/vSAN HCI model, unified scaling of storage and computing When to choose Nutanix? # You want to reduce VMware licensing costs (eliminate vSphere/vSAN/NSX costs). You need simpler management and don\u0026rsquo;t want to configure vCenter, vSAN, NSX. You plan to use a hybrid cloud and want seamless migration between on-premises and cloud. You want stronger automation capabilities, suitable for DevOps scenarios. When to choose VMware? # You already have a large VMware ecosystem (vSphere, NSX, vSAN) and don\u0026rsquo;t want to switch architectures. Your enterprise applications rely on the VMware ecosystem, such as Horizon (desktop virtualization). You don\u0026rsquo;t mind additional licensing fees, and you already have a mature VMware operations team. If you are primarily focused on reducing costs, simplifying management, and enhancing hybrid cloud capabilities, Nutanix may be a better choice for enterprise users!\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2025-03-12","externalUrl":null,"permalink":"/en/posts/nutanix/","section":"Posts","summary":"Following Broadcom\u0026rsquo;s acquisition of VMware, many enterprise users are seeking alternatives. Nutanix, as a hyper-converged infrastructure (HCI) solution, offers lower costs and a simpler management interface, making it a good option.","title":"Why Are More and More Enterprise Users Abandoning VMware?","type":"posts"},{"content":"","date":"2025-02-27","externalUrl":null,"permalink":"/tags/cpython/","section":"标签","summary":"","title":"CPython","type":"tags"},{"content":"昨晚，哄完女儿睡觉已经是午夜十二点了。我回到自己的屋里，打开 GitHub，看看当晚有没有什么可以贡献的项目。\n这次，我决定去 CPython 的 Issue 区找找有没有适合自己的贡献机会。\nCPython 就是大名鼎鼎的 Python 编程语言的官方代码仓库。\n其实，早就想找机会为 CPython 贡献代码，但一直没能迈出第一步。这次，我想用自己的方式寻找突破口。\n这种想法的启发，来自 Tian Gao（GitHub ID：gaogaotiantian），他是 Python 的 Core Developer（核心开发者），专注于维护 pdb，并曾跻身 Python 贡献排行榜 #94 名。他可能是唯一一个前 100 名的中国开发者。\n于是，我筛选了一些自己感兴趣的类别，当然就是 Infra 和 DevOps 相关的问题。很快，我找到了一张合适的 Issue，修改代码、测试、提交 Pull Request，然后就去睡觉了。\n今天早上醒来，我发现我的 PR 已经被 Merge 到 CPython 主分支了！\n虽然这算不上什么了不起的成就，但却是一个很好的学习过程。比如，通过参与优秀的开源项目，了解他们是如何管理 Issue 和 Pull Request 的，学习他们做得好的地方。这些经验都有可能应用到自己的工作或项目中。\n在贡献优秀开源项目的过程中，不仅能提升相关技能，还能与这些优秀的开发者交流，学到新的知识。\n从短期来看，或许不会带来直接的收益，但如果这是你真正热爱的事情，那么长期投入一定是值得的。\n假如拥有 Python Core Developer 这样的身份认可，在国内可能有助于获得更理想的工作机会。然而，并非所有公司都青睐这种“双时区开发者”（白天工作，晚上开源）。\n但如果你的目标是寻找远程工作，或者申请欧美国家的签证，这样的经历无疑会成为一个重要的加分项。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-02-27","externalUrl":null,"permalink":"/posts/my-first-pr-to-cpython/","section":"Posts","summary":"在 CPython 的 Issue 区找到一个合适的 PR，修改代码、测试、提交 Pull Request，第二天醒来发现已经被 Merge 到主分支了！这是一个很好的学习过程，也是对开源社区的贡献。","title":"一觉醒来，我的 PR 已经被 Merge 到 CPython 主分支了！","type":"posts"},{"content":"","date":"2025-02-14","externalUrl":null,"permalink":"/en/tags/europython/","section":"Tags","summary":"","title":"EuroPython","type":"tags"},{"content":"","date":"2025-02-14","externalUrl":null,"permalink":"/en/tags/reviewer/","section":"Tags","summary":"","title":"Reviewer","type":"tags"},{"content":"Lately, I haven\u0026rsquo;t been contributing much code outside of work. I\u0026rsquo;ve mainly been spending my time reviewing proposals for EuroPython 2025 (the 2025 European Python Conference).\nWhile in China, I never considered participating in or volunteering for events. But since moving to Europe, I\u0026rsquo;ve felt a desire to participate more.\nBelow, I\u0026rsquo;ll discuss why I chose to volunteer and what I\u0026rsquo;ve learned during this week of activity.\nWhy I chose to participate # First, I wanted to challenge myself—to force myself to speak up, communicate with others, and actively listen.\nSecond, this experience is valuable for my resume and blog.\nThird, I don\u0026rsquo;t participate in just any event. For me, the ideal event falls into one of two categories: Python or DevOps.\nThis is the underlying logic behind my decision-making process.\nThe idea itself was the most important part. Once I had the idea, at the end of last year I searched for European events in 2025 and found EuroPython 2025, being held in Prague. I then applied to join the organizing committee.\nNo matter what, I really wanted to participate, to volunteer. It would be even better to attend in person—as long as the ticket is free. I would cover the flight and hotel myself, treating it as a family vacation, and I could also meet up with former colleagues. A win-win-win.\nWhat I\u0026rsquo;ve learned as a reviewer # First, I\u0026rsquo;ve gained insight into how such an event is organized, including the documentation, responsibilities, review process, requirements, and tools used. Overall, the organizers seem very experienced, and the event is progressing smoothly.\nSecond, as a reviewer, I\u0026rsquo;ve read hundreds of presentation proposals, shared my opinions, helped the organizing committee score them, and ultimately helped select which proposals will be presented at the conference in July 2025. This has broadened my horizons.\nA reflection # Open source is a powerful personal endorsement.\nWithin our company, sharing a Python linter tool like uv or pre-commit is fine as long as you\u0026rsquo;re knowledgeable.\nHowever, when presenting at a conference, you might have plenty of valuable content, but it might not be enough to get the judges\u0026rsquo; approval. If you are an author or maintainer of uv or pre-commit, your chances of getting selected are several times higher.\nFor example, while reviewing proposals, I saw a proposal from a PyPI member. Their proposal had great content, but being a PyPI member instantly boosted the value of the presentation.\nThere were also some Python Core Devs and PyPy Core Devs. Let\u0026rsquo;s just say, if these individuals want to share something about their contributions, it\u0026rsquo;s very easy—the organizers would be delighted.\nFinally # Although this requires a few extra hours each day, the rewards are significant. At the very least, it has broadened my horizons and exposed me to many new ideas.\nTherefore, I\u0026rsquo;ll try some projects I haven\u0026rsquo;t tackled before, such as PyPy, Pydantic, and FastAPI.\nI\u0026rsquo;ve completed 129 reviews, and there\u0026rsquo;s one day left. I hope I can finish close to 150-200, even though the committee only suggests 30 reviews per person.\nIn short, this has been a very positive learning experience. I\u0026rsquo;ve learned many new ideas, improved my English and communication skills, and gained experience beneficial for my career and resume.\nThis task was initially challenging, but not insurmountable—it\u0026rsquo;s the perfect level of challenge for growth.\nWhat do you think?\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Follow the \u0026ldquo;DevOps攻城狮\u0026rdquo; WeChat public account.\n","date":"2025-02-14","externalUrl":null,"permalink":"/en/posts/euro-python-review/","section":"Posts","summary":"I haven\u0026rsquo;t contributed much code lately, focusing instead on reviewing proposals for EuroPython 2025.","title":"Why I chose to review for EuroPython 2025","type":"posts"},{"content":"Hello everyone! Since my last post announcing the start of maintaining gitstats, I\u0026rsquo;ve been continuously improving this project. Here are the major updates over the past two months:\n✨ New Features and Improvements # Support for Generating JSON Files In addition to the original HTML report, gitstats can now generate JSON files! Use Case: Facilitates secondary development or programmatic use for developers, meeting more customized needs. Origin: This feature was implemented quickly based on user feedback.\nCode Refactoring A significant amount of refactoring and optimization has been performed on the previously mixed code. Benefits: The code structure is clearer and easier to maintain, laying the foundation for writing unit tests.\nReplacing getopt The deprecated getopt has been replaced with the more modern argparse. Advantages: Improves code readability and maintainability.\nCross-Platform Support In addition to Linux, gitstats has now been fully tested on Windows and macOS. Testing: I\u0026rsquo;ve conducted thorough testing on all three platforms to ensure real-time availability.\n📅 Next Steps # Support for Theme Switching In addition to the default theme, I plan to add a Dark Mode, catering to different user visual preferences.\nUnit Testing and Coverage Unit tests will be added, and coverage will be increased to 100% (a small goal) to prevent regression bugs.\n💡 Your Needs Matter! # If you have any other needs or feature suggestions, feel free to visit the following repository address and submit an Issue: 👉 https://github.com/shenxianpeng/gitstats\n🌟 Welcome to Use and Support # If you find gitstats helpful, welcome to Star🌟 it! Your recognition is the motivation for my continuous improvement!\nWhat new features would you like to see added to gitstats? Feel free to leave a comment or directly submit an Issue on GitHub!\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-02-05","externalUrl":null,"permalink":"/en/posts/gitstats-update/","section":"Posts","summary":"After two months of continuous improvement, gitstats now supports JSON output, code refactoring, argparse replacing getopt, and full compatibility with Windows and macOS. Welcome to use and Star support!","title":"🚀 gitstats Upgrade Arrives—JSON Output, Cross-Platform Compatibility, and Code Refactoring!","type":"posts"},{"content":"","date":"2025-02-05","externalUrl":null,"permalink":"/en/tags/gitstats/","section":"Tags","summary":"","title":"GitStats","type":"tags"},{"content":"","date":"2025-01-25","externalUrl":null,"permalink":"/en/tags/cloud/","section":"Tags","summary":"","title":"Cloud","type":"tags"},{"content":"","date":"2025-01-25","externalUrl":null,"permalink":"/en/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"Recently, I am working on Jenkins instance migration, this time I started to use Jenkins Docker Cloud insead of use docker { ... } in Jenkinsfile.\nJenkins cloud plugin # First you need to install Jenkins Docker Cloud plugin https://plugins.jenkins.io/docker-plugin/\nJenkins Docker Cloud is a plugin that allows Jenkins to use Docker containers as build agents.\nSo you need to config a Docker Host with remote API as follows.\nEnable Docker Remote API # Jenkins controller connects to the docker host using REST APIs. To enable the remote API for docker host, please follow the steps below.\nStep 1: Spin up a VM, and install docker on it. You can follow the official documentation for installing docker. based on the Linux distribution you use. Make sure the docker service is up and running.\nStep 2: Log in to the server and open the docker service file /lib/systemd/system/docker.service. Search for ExecStart and replace that line with the following.\nExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock Step 3: Reload and restart docker service.\nsystemctl daemon-reload service docker restart Step 4: Validate API by executing the following curl commands. Replace myhostname with your hostname or IP.\ncurl http://localhost:4243/version curl http://myhostname:4243/version Create custom docker image # For me, I use launch via JNLP for my custom docker image.\nFor example, you docker image is based on jenkins/inbound-agent, you can use the following Dockerfile to create your custom image.\nFROM jenkins/inbound-agent RUN apt-get update \u0026amp;\u0026amp; apt-get install XXX COPY your-favorite-tool-here ENTRYPOINT [\u0026#34;/usr/local/bin/jenkins-agent\u0026#34;] How to use in Jenkinsfile # Once you have configed Docker Cloud, you can use it in Jenkinsfile like a normal agent.\nFor example:\n// Jenkinsfile (Declarative Pipeline) pipeline { agent { node { \u0026#34;docker\u0026#34; } } } Which is does not like if you use docker { ... } directly.\nFor example of using docker { ... }:\n// Jenkinsfile (Declarative Pipeline) pipeline { agent { docker { image \u0026#39;node:22.13.1-alpine3.21\u0026#39; } } stages { stage(\u0026#39;Test\u0026#39;) { steps { sh \u0026#39;node --eval \u0026#34;console.log(process.platform,process.env.CI)\u0026#34;\u0026#39; } } } } Here is more details about using Docker with pipeline: https://www.jenkins.io/doc/book/pipeline/docker/\nFeel free to comment if you have any questions.\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-01-25","externalUrl":null,"permalink":"/en/posts/jenkins-docker-cloud/","section":"Posts","summary":"This article explains how to use Jenkins Docker Cloud for building and deploying applications, including setting up a Docker host and creating custom Docker images.","title":"How to use Jenkins Docker Cloud","type":"posts"},{"content":"","date":"2025-01-20","externalUrl":null,"permalink":"/tags/copyright/","section":"标签","summary":"","title":"Copyright","type":"tags"},{"content":"最近逛 CPython 的仓库发现了这个 Issue gh-126133:\nHugo van Kemenade 他作为 Python 3.14 \u0026amp; 3.15 发布经理提出是否可以停止更新 Copyright。\n在工作中，我其中的职责之一也是发布，因此我这个想法的提出也非常感兴趣，跟 CPython 项目一样，我们的项目在每年的年初都要更新 Copyright。\n下面我们就一起来看看 Hugo 提出的理由以及最终这个想法被 Python 项目法律团队采纳并最终合并到 CPython 的主分支的过程。\nHugo 提出问题的内容是这样的 # 每年一月，我们会在代码库中更新 PSF 的版权年份：\nCopyright © 2001-2024 Python Software Foundation. All rights reserved. 2025年即将到来。\n我们能否获得 PSF 的许可，采用一种无需每年更新的版权声明？\n我建议使用如下之一：\nCopyright © Python Software Foundation. All rights reserved. Copyright © 2001 Python Software Foundation. All rights reserved. Copyright © 2001-present Python Software Foundation. All rights reserved. 然后他列出了许多以前很多关于更新 Copyright 的 PR 的例子。\n其中很多还有重复的，浪费了开发者的时间。并表示不仅这些工作枯燥无趣，而且可能完全没有必要。\n许多大型项目已经停止更新版权年份，比如：\nCopyright (c) 2009 The Go Authors. All rights reserved. Copyright (c) 2015 - present Microsoft Corporation Copyright 2013 Netflix, Inc. Copyright (c) Meta Platforms, Inc. and affiliates. Copyright (c) Microsoft Corporation. All rights reserved. Copyright (c) .NET Foundation and Contributors 这些要么只有首年，要么完全省略年份，要么以“present”结尾。\n我们可以效仿类似的做法吗？\n这里面有其他人发表的观点。\nGregory P. Smith（Python 指导委员会成员）给出了很好的建议：\n他说：我不会从任何地方删除整个版权声明。Hugo 应该向 PSF（作为版权持有者）寻求建议。我们的目标是简化我们的生活，制定一项可参考的政策，减少劳累。我们应该将这些建议编入开发指南，并简单地回复任何试图修改版权声明的人，并提供该文档的链接。\n最终 Hugo 给 PFS 法律发了邮件，并得到如下回复 # 首先要理解的是，版权声明是可选的/信息性的。曾经有一段时间，版权所有者必须包含一份声明，以保护他们的版权；根据国际条约，这一要求已被废除。因此，版权声明的形式不会影响版权所有者的权利。\n然而，版权声明仍然可以发挥有用的信息作用。根据美国版权局的说法，有效的版权声明的要素包括：\n版权符号 ©；“copyright”一词；或缩写 “copr.”； 作品首次出版的年份；和 版权所有者的姓名。\n示例：© 2024 John Doe\n对于只出版过一次的作品，首次出版的年份很简单。另一方面，开源项目中的文件本质上是修订或衍生作品的累积，每个作品都有不同的首次出版年份。这就是为什么您会在相应的版权声明中看到多个日期或日期范围。例如，如果某个文件是在 2015 年创建的，并在 2017 年、2018 年、2019 年和 2022 年进行了修订，则您可以采用以下方式之一呈现版权声明：\n© 2015、2017-2019、2022 John Doe © 2015、2017、2018、2019、2022 John Doe 在这种情况下，使用“2015-2022”作为日期范围可能相当普遍，尽管这提供的有关各个修订的首次发布日期的信息较少，并且可能错误地暗示该文件在该范围内的每一年都进行了修订。\n无论相应文件是否最近更新过，在每个版权声明的日期范围末尾更新声明以添加当前年份都没有特别的意义。但它也不会影响任何人的权利，也不会使用“-present”来代替。\n所以我认为你提出的包含年份的两种格式都可以。我不建议完全省略首次出版的年份，因为它是有效版权声明的要素之一。\n我还要指出的是，“Python 软件基金会”可能不是 PSF 项目作者的准确陈述，因为 PSF 不需要（据我所知）版权转让。话虽如此，为文件的所有作者包含声明也是不切实际且容易出错的。一些常见的替代方案是 “FOO 项目贡献者”或“贡献者中列出的作者”。这些都不是理想的，因为它们要么实际上没有命名作者，要么随着作者的贡献随着时间的推移而消失，它们可能会变得不准确。\n简而言之，没有理想的解决方案，但幸运的是声明并不那么重要。选择一些合理的内容并尽力坚持下去。\n以上就是 PFS 法律的回复。\n期间还有人提议 Hugo 要争得 PFS 董事会的同意。\n但有人这样回复这个提议：PSF 法律列表由 PSF 员工（包括我自己）监控，是解决此类问题的正确途径。需要董事会全员参与的主题会被提交，不需要董事会全员参与的主题会在那里决定，根据主题的不同，是否需要法律顾问。在这个特定情况下，我们确实请来了律师，以确保 PSF 履行我们作为版权持有人的法律义务。\n此回答并获得了 Hugo 及其他人的点赞👍和❤️。\n最终 Hugo 的提议 “只包含首次出版的年份，删除结束年份” 修改被合并到了 CPython 的主分支了。详见：https://github.com/python/cpython/pull/126236\n总结 # 从这个问题我们可以学到，关于 Copyright 中的年份，我们可以仿照 Python，谷歌，微软和 Netflix，只包含首次出版的年份即可。\n我不是律师，所以不要把这当作我的法律建议。\n我只想说，如果这对 Python，谷歌、微软和 Netflix 的律师来说是可以的，那么对我来说也是可以的。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-01-20","externalUrl":null,"permalink":"/posts/copyright/","section":"Posts","summary":"CPython 停止更新 Copyright 的原因和过程。了解 Python 项目法律团队的建议，以及如何处理开源项目中的版权声明。","title":"CPython 停止更新 Copyright 了，为什么？","type":"posts"},{"content":"时间过得很快，2024 年即将过去。如果不记录下在这一年里发生的事情，过不了多久就很难回想起这一年都发生了什么。按照惯例，这篇年终总结如期而至。\n回看自己年初写下的 Flag，有些实现了，有些做了但不是很好，还有一些没有做到。\n职业发展：希望能顺利度过职业发展期，并迎来一个崭新的、充满挑战的开始。✅ 家庭健康：家人身体健康，工作和家庭的平衡，多带他们出去走走。✅ 提升技能：补足 DevOps 领域的一些不足，争取通过实际项目提升自己。⏳ 分享与成长：通过博客和公众号持续分享知识，以教促学。✅ 保持运动：不论是跑步还是踢球，努力减重。❌ 总结来说，2024 年的目标除了没有减重之外，总体完成得不错。\n回顾 2024 # 回顾这一年，最大的变化就是工作，生活因此也做出了改变。这也是我结束沪漂、北漂回连之后十年来最大的变动。\n珍惜每一天 # 从 2013 年 12 月份，我知道自己将在 2024 年 6 月底离开工作了快10年的公司，并有可能去欧洲工作。\n从那一刻起，我开始更加珍惜工作和生活中的每一天。有时间就尽可能带着家人多出去走走。\n接下来是一些流水账，记录我无比珍惜在国内的七个月：\n一月：我的脚踝在打羽毛球时受伤了，活动参加一场少一场，所以我还是拖着脚参伤加了公司在金石滩鲁能温泉的活动。 二月：孩子意外烫伤，对我们全家来说是至暗时刻。整个春节都是在医院里度过的，现在回想起来依然觉得痛心。 三月：月初去了姐姐家，月中去了大连森林动物园，月末带爸妈去了丹东，看了鸭绿江大桥和抗美援朝纪念馆。 四月：多次走亲访友，动物园，还参加了公司组织的小黑山登山、摘蓝莓和草莓活动。 五月：月初父母来大连一起给孩子过了一个生日；月底我们去了日本东京办理签证。 六月：月初和孩子的小伙伴家长们一起露营；月中回了趟老家，带爸妈去了一趟省会转转；又去了动物园；月底正式结束在大连公司的工作。 七月：月初和朋友们聚餐告别；独自带孩子回了一趟老家，与父母分别开上车的那一刻，我就哭了；随后去日本取签证，顺便去了东京迪士尼；月底启程去欧洲，临行时心情沉重。 这些点滴构成了2024年上半年的珍贵记忆。\n享受每一天的工作 # 上半年，我一边准备交接，一边完成本职工作，始终不遗余力地践行 DevOps 最佳实践。尽管结果尚不可知。\n但我真的很享受在岗的每一天，包括咖啡、午休散步、听喜欢的内容、偶尔跑步，以及每周二、四的羽毛球时光。\n业余时间，坚持开源和写作 # 和去年一样，业余时间我还做着相同的事情：\n开源项目 cpp-linter-action：已有上千用户，包括微软、Jupyter、Linux Foundation 等知名项目。 commit-check：优化并新增用户需求，用户数稳步增长。 gitstats：这是我今年新纳入维护的一个开源项目，希望能帮助更多人。 写作 — 全年更新 27 篇博客、41 篇公众号文章，大幅超出预期。 学习 — 加入知名社区学习和贡献的计划仍在继续，但工作之余带孩子确实让业余时间变得很少。 展望 2025 # 2024 年对于很多人来说都是特殊的一年。无论身处何地，都能感受到不稳定的政治因素和不乐观的经济形势。\n2025 年会更好吗？应该不会，但仍怀抱希望：希望未来能有工作，有饭吃，有地方住，家人健康平安。\n如果有什么愿望，那就是：\n祝愿家人身体健康。 去其他欧洲国家旅行，争取看一场喜欢的球队比赛。 争取获得 Azure 认证。 争取参与 DevOps 或 Python 大会。 争取加入 PyPA 或 Python GitHub Organization。 争取别长胖，保持运动。 没有信心完成的愿望，一律使用“争取”来描述。:）\n过去的年终总结 # 2023 年终总结 2022 年终总结 2020 年终总结 2019 年终总结 2018 从测试转开发\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-12-28","externalUrl":null,"permalink":"/misc/2024-summary/","section":"Miscs","summary":"\u003cp\u003e时间过得很快，2024 年即将过去。如果不记录下在这一年里发生的事情，过不了多久就很难回想起这一年都发生了什么。按照惯例，这篇年终总结如期而至。\u003c/p\u003e","title":"2024 年终总结","type":"misc"},{"content":"","date":"2024-12-28","externalUrl":null,"permalink":"/tags/summary/","section":"标签","summary":"","title":"Summary","type":"tags"},{"content":"女儿已经两岁七个月了，我还没有单独写过一篇关于她的文章。\n我的女儿是个好动的小姑娘，每天充满活力，以至于经常我从下班到家六点半一直陪她到半夜十二点多才能停下来睡觉。\n今天是周五晚上，十二点半了还是蹦蹦跳跳不想睡觉，终于在凌晨一点钟熄灯睡觉了。但她突然下床要吃苹果。\n放在以前晚上十一点为了哄她睡觉我会尽量满足她，但今天实在是太晚了，我就严厉地再次告诉她：熄灯之后只能喝水，其他的都不能吃和喝了。\n显然她对我的拒绝表示抗拒，开始又哭又闹！但这次我不想让她得逞，坐在床上任凭她怎么推我拉我去厨房，我都一动不动。\n她被我的举动气得不行了，哭得上气不接下气。\n说实在我心里是很心疼的，同时也是很犹豫的。\n心疼是哭得实在太厉害；犹豫是我这样做真的就是对吗？\n最终结果是，在我拿了卫生间的纸准备给她擦眼泪和鼻涕的时候，两岁多的女儿因“秩序敏感期”，更是哭不成泣了，而不再想着吃苹果的事情了。\n最后，她就趴在我的身上，不跟我说话，不让我离开厨房。我就这么抱着她大概有五分钟。\n然后我问她：你爱爸爸吗？她不说话。\n我又问她：你爱妈妈吗？她还不说话。\n不记得是哪句话，她同意跟我回卧室睡觉了。\n当她躺在我的身边，我搂着她，听着她的呼吸声。当我的手臂离开她的时候，她说：爸爸抱抱！\n那时我的心都要融化了。\n我再次问她：你爱爸爸吗？她说爱！你爱妈妈吗？她说爱！\n我说爸爸妈妈也爱你！\n随着她的呼吸声加重，我知道了她应该已经睡着了。\n此时，我回想刚刚发生的一切，觉得是我错了。是我从一开始就没有把睡觉前可以做的和不可以做的事情清楚地让她理解。\n以前她可以吃苹果，而今天爸爸突然不让吃苹果了，显然她是不能理解了。（虽然从我的角度是因为实在是太晚了）。\n我在心中默念，女儿，对不起！也请给爸爸一点耐心，爸爸也是第一次当爸爸，还需要学会如何做一个好爸爸。\n写下这篇文章的时候，女儿就熟睡在我的身边。 —— 写于 2024 年 12 月 28 日凌晨 2:41\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-12-28","externalUrl":null,"permalink":"/misc/my-daughter/","section":"Miscs","summary":"女儿已经两岁七个月了，我还没有单独写过一篇关于她的文章。记录下我和女儿的点滴生活。","title":"我的女儿","type":"misc"},{"content":"Recently, in the evenings (usually after my child has gone to sleep), I\u0026rsquo;ve been working on something: reviving the long-dormant GitStats project.\nPreviously, I wrote two articles about GitStats. If you\u0026rsquo;re interested, you can check them out:\nGit History Statistics Generator GitStats Automatically Providing Multi-Dimensional Code Analysis Reports to My Boss Regularly via Jenkins What is GitStats # GitStats is a Python-based tool for analyzing the history of a Git repository and generating an HTML report.\nHowever, it only supports Python 2, and the author has stopped maintaining it (the last commit was 9 years ago).\nIn current development environments, compatibility and ease of use are significantly limited, but its value remains undeniable.\nTherefore, I decided to modernize this project.\nCompleted Work # Migration to Python 3.9+: Refactored the code to support Python 3 versions. ✅ Creation of a Modern CI/CD Pipeline: Added CI/CD tools for easier continuous development and release. ✅ Publication to PyPI: Users can now install it via pip install gitstats. ✅ Docker Image Provided: Users no longer need to handle dependencies themselves; running gitstats is more convenient. ✅ Online Preview Provided: Created a demonstration page to allow users to intuitively understand GitStats\u0026rsquo; functionality. ✅ Special Thanks # Here, I would like to express my special thanks to Sumin Byeon (@suminb). From his introduction, he seems to be a programmer living in South Korea.\nThe original owner of GitStats on PyPI was him, so I couldn\u0026rsquo;t directly use that name. I tried other names, such as git-stats and gitstory, but they were rejected by PyPI due to similarity to other projects.\nSeeing that his project hadn\u0026rsquo;t been maintained for five years, I sent him an email on a whim, asking if he would be willing to transfer the GitStats name to me, as I was reviving the project.\nUnexpectedly, he replied quickly and eventually agreed to transfer the GitStats name to me. His only condition was that if I stopped maintaining GitStats in the future and someone else needed the name, I would do the same as he did and transfer the name to them.\nI agreed and promised to maintain GitStats long-term. (Hopefully, I can do it.)\nFuture Plans # Addressing Valuable Issues: Reviewing unresolved issues in the original repository and selecting valuable ones for fixing. Reviewing Existing Pull Requests: Evaluating PRs from the original repository and merging them into the current project as appropriate. Updating Documentation: Improving the documentation to make it clearer and easier to understand. Adding New Features: Adding features to make the project more powerful and useful. UI Optimization: Improving the visual appeal and user experience of the interface. How to Participate # If you are interested in improving GitStats, you are welcome to participate in this project! You can:\nSuggest Features: Propose ideas or feature requests to help the project better meet user needs. Contribute Code: Fix bugs or add features to directly contribute to the project. Share and Promote: Recommend GitStats to friends or communities who might be interested. Finally, let\u0026rsquo;s work together to bring GitStats back to life!\nWritten at 2:50 AM on November 28, 2024\nPlease indicate the author and source when reprinting this article. Please do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2024-11-28","externalUrl":null,"permalink":"/en/posts/gitstats/","section":"Posts","summary":"This post details the revival of the GitStats project, including its migration to Python 3, the creation of a modern CI/CD pipeline, publication to PyPI and Docker, and future improvement plans.","title":"Reviving GitStats — Breathing New Life into Git History Analysis","type":"posts"},{"content":" pip vs pipx Differences # In the Python ecosystem, both pip and pipx are software tools used for package management, but they have different design goals and usage scenarios. Some developers may be confused about the differences between the two and how to choose.\n1. pip: The General-Purpose Python Package Manager # pip is the officially recommended package manager for Python, used to install and manage Python packages (libraries).\nMain Features:\nSuitable for any Python package: Can install libraries and command-line tools. Installation in global or virtual environments: Packages are installed by default in the global Python environment or in a virtual environment (such as venv, virtualenv). Simple commands: pip install package-name Use Cases:\nInstalling development dependencies (such as requests, flask). Creating project-specific environments (usually used in conjunction with virtual environments). Limitations:\nIf installed directly into the global environment, it can easily lead to version conflicts. Installation and management of command-line tools (CLI) is more cumbersome because they share the same environment. 2. pipx: Focused on Isolated Installation of Command-Line Tools # pipx is a tool specifically designed for Python command-line tools (CLIs), providing an isolated installation environment.\nMain Features:\nCreates an independent environment for each tool: Each CLI tool runs in its own virtual environment, avoiding conflicts. Automatic dependency management: When installing a tool, it automatically handles dependency version management. Simplified user experience: CLI tools are directly usable without extra path configuration. Simple commands: pipx install package-name Use Cases:\nInstalling and managing Python CLI tools (such as black, httpie, commit-check). Avoiding dependency conflicts between tools. Users with high requirements for the development tool or script runtime environment. Limitations:\nOnly suitable for CLI tools, not suitable for installing ordinary Python libraries. Requires installing the pipx tool first: python -m pip install pipx Comparison Summary # Feature pip pipx Purpose Install and manage all Python packages Install and manage CLI tools Installation Scope Global environment or virtual environment Independent virtual environment for each tool Dependency Isolation Requires manual management (better with virtual environments) Automatic isolation, tools do not affect each other Use Cases Development project dependency management Independent installation and use of CLI tools Example pip install flask pipx install black How to Choose? # If you are building a Python project and need to install project dependencies, use pip. If you need to install Python CLI tools, such as pytest or pre-commit, it is recommended to use pipx to ensure independence and stability. In short: pip is a general-purpose tool, pipx is a dedicated solution for CLI tools.\n","date":"2024-11-26","externalUrl":null,"permalink":"/en/posts/pip-vs-pipx/","section":"Posts","summary":"This article introduces the differences between pip and pipx, helping developers choose the right tool to manage Python packages and command-line tools.","title":"pip vs pipx Differences","type":"posts"},{"content":"","date":"2024-11-26","externalUrl":null,"permalink":"/en/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"2024-11-18","externalUrl":null,"permalink":"/en/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"Recently, while setting up a new Windows Server 2022, I encountered an issue where my Ansible playbook, which previously worked without problems, failed to execute.\nHere’s the configuration I used for the Windows host in my Ansible inventory:\n[jenkins-agent-windows:vars] ansible_user= ansible_ssh_pass= ansible_connection=winrm ansible_winrm_transport=ntlm ansible_winrm_server_cert_validation=ignore However, when I ran the playbook, the following error occurred:\nwinrm send_input failed; stdout: stderr \u0026#39;PowerShell\u0026#39; is not recognized as an internal or external command, operable program or batch file. Cause of the Issue # This is usually the case when the SYSTEM\u0026rsquo;s PATH environment variable has been changed and is no longer able to find PowerShell.exe in the path.\nPlease verify the PATH environment contains the entry C:\\Windows\\System32\\WindowsPowerShell\\v1.0 in there.\nSolution # Right-click This PC \u0026gt; Properties \u0026gt; Advanced system settings \u0026gt; Environment Variables.\nAfter adding C:\\Windows\\System32\\WindowsPowerShell\\v1.0 to PATH, the error disappeared, and my Ansible playbook executed successfully.\n","date":"2024-11-18","externalUrl":null,"permalink":"/en/posts/ansbile-playbook-issue/","section":"Posts","summary":"Explaining a recent issue with Ansible playbook execution on Windows Server 2022, where PowerShell was not recognized, and how to resolve it.","title":"PowerShell is not recognized as an internal or external command","type":"posts"},{"content":"很多人可能会好奇，作为一名 DevOps 工程师，每天究竟忙些什么呢？今天就来简单聊聊，作为 DevOps/Build/Release 工程师，我的日常工作节奏是怎样的。\n工作准备 # 每天早上九点半到公司，第一件事就是打开 Slack 和邮箱，优先处理那些紧急或容易回复的消息。遇到比较复杂的内容，就会设置提醒，以防漏掉。之后，会把当天的任务列入 To-Do List，再检查 Jenkins 上是否有失败的任务需要关注。这一系列动作大概会花半小时左右。\n咖啡时间 # 十点左右是咖啡时间——一天的正式开始。如果十点半有站会，那就是一个快速的回顾和计划环节，主要是分享昨天的进展、当天的任务安排，也和团队同步一下各自的状态。\n日常工作 # 开始工作后，我会打开 VSCode，接着前一天没完成的任务。平时常用的代码仓库包括 pipeline-library、ansible-playbook、docker-images 和 infra，它们分别负责管理流水线、自动化脚本、容器和基础设施。几乎每天都会对这些仓库进行一些更新或优化。\nBuild 和 Release 也是我的主要工作之一。构建任务已经实现了自动化，团队成员通过 Multibranch Pipeline 自行构建；我主要负责分支管理、发布时的合并和冲突解决，确保发布信息的准确和版本的合规性。\n此外，还有一些日常任务，比如：\n维护和升级构建环境 收集代码覆盖率，生成报告 升级编译器，处理相关问题 管理虚拟机分配，帮团队解决环境问题 上午的主要工作还是消息回复和问题处理，之后再逐一处理 To-Do List。\n午餐与休息 # 中午十二点半左右和同事一起午餐。吃饭时聊聊天，也是练习英语的机会。饭后，有时会和同事一起在附近散步，或者自己跑步运动一下。\n下午 # 下午是主要的产出时间。从一点半到四点半，专心处理 To-Do List 上的任务，尽量推进工作进度。四点半之后，美国同事上线，可能会有会议或讨论需求。\n晚间时光 # 晚上是家人时间。天气渐冷，不方便带孩子出门散步，我们偶尔会去超市采购。如果孩子自己看书或看动画片，我会利用时间给开源社区做点贡献，或是写文章。\n这就是我在 DevOps 岗位上的一天，一边忙工作，一边兼顾家庭和爱好。希望这个小分享能让大家更了解这个岗位的日常。\n​你更喜欢我分享一些技术，还是更偏爱这种工作、生活的日常？欢迎留言告诉我！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-10-27","externalUrl":null,"permalink":"/posts/devops-everyday/","section":"Posts","summary":"本文介绍了作为 DevOps 工程师的日常工作节奏，从早上到晚上，涵盖工作准备、会议、代码管理、构建发布等环节。","title":"从早到晚，我的 DevOps 一天","type":"posts"},{"content":" Background # Recently, I’ve been working on migrating and upgrading Jenkins. The main motivation is the vulnerability CVE-2024-23897.\nJenkins 2.441 and earlier, LTS 2.426.2 and earlier does not disable a feature of its CLI command parser that replaces an \u0026lsquo;@\u0026rsquo; character followed by a file path in an argument with the file\u0026rsquo;s contents, allowing unauthenticated attackers to read arbitrary files on the Jenkins controller file system.\nTo address this, Jenkins needs to be upgraded to at least version 2.442 or LTS 2.426.3 or above. This was also an opportunity to rework parts of the setup that weren’t initially optimized.\nPre-Upgrade Jenkins # Before the upgrade, we were using Jenkins 2.346.3, the last version supporting Java 8. Because older operating systems don’t support Java 17, this blocked us from upgrading Jenkins.\nThat said, our initial setup was already well-structured:\nWe followed the Infrastructure as Code principle, deploying Jenkins through Docker Compose. We adhered to the Configuration as Code principle, managing all Jenkins Pipelines with a Jenkins Shared Library. We used Jenkins Multibranch Pipeline to build and test projects. However, there were some limitations, such as:\nThe Jenkins server didn’t have a fixed domain name like jenkinsci.organization.com, but instead had a format like http://234.345.999:8080. Whenever the IP or hostname changed, Webhook configurations for this Jenkins instance had to be updated manually in the Git repository. We hadn’t adopted Docker Cloud. While many tasks used Docker for builds, we weren’t utilizing Docker JNLP agents to create dynamic agents for builds that would automatically be destroyed upon completion. The naming and code structure of the Jenkins Shared Library needed refactoring, as it was initially created for a single team, which limited repository naming. We hadn’t yet used Windows Docker Containers. Some Jenkins plugins were likely outdated or unused but still present. Jenkins and its plugins weren’t regularly updated due to the Jenkins Agent version restrictions. Post-Upgrade Jenkins # Building on prior best practices, we made the following improvements:\nContinued following the Infrastructure as Code principle, and using Nginx as a reverse proxy, we deployed Jenkins with Docker Compose to ensure a stable domain name. Continued to follow the Configuration as Code principle. Continued using Jenkins Multibranch Pipeline for building and testing projects. Where possible, implemented Docker Cloud for builds. Renamed the Jenkins Shared Library to pipeline-library (aligning with Jenkins\u0026rsquo; naming conventions) and refactored many Jenkinsfiles and Groovy files. Introduced Windows Docker Containers to build Windows components. Utilized the Jenkins Configuration as Code plugin and scheduled regular configuration backups. Installed only necessary Jenkins plugins and exported the current plugin list using the plugin command. Attempted to automate plugin backups before upgrading, enabling quick rollback if the upgrade fails. Summary # I hope these efforts enable Infrastructure maintenance and Pipeline development to be managed through GitOps.\nThrough continuous exploration, experimentation, and application of best practices, I aim to establish CI/CD as a healthy, sustainable, self-improving DevOps system.\n","date":"2024-10-25","externalUrl":null,"permalink":"/en/posts/jenkins-upgrade/","section":"Posts","summary":"This article discusses the optimizations made during the Jenkins upgrade, including using Docker Compose for deployment, refactoring the Jenkins Shared Library, introducing Windows Docker Containers, and more to enhance the efficiency and security of the CI/CD process.","title":"What Optimizations I Made During the Jenkins Upgrade","type":"posts"},{"content":"偶尔深夜躺下时，我常常在想，我是怎么就走到这了？这都是源于毕业后的一系列选择吧！\n也时常感慨，选择往往比努力更重要。回顾过去十余年，这几个决定对我走到今天起到了至关重要的影响。\n1. 毕业之后去上海 # 2009年刚毕业时，我的目标很简单，就是找到一份和专业相关的工作，不论在哪，不论具体做什么，没任何计划，只要能提供一份能养活自己的工作就行。\n当时，有同学已经找到手机测试的工作，并被派到上海出差，我得知之后他们还在招聘，也应聘了这家公司。\n还记得应聘完不久，我就坐上了回家的火车。列车从沈阳北站出发时，我接到了公司的电话，通知我面试通过，可以来签合同了。就在火车到达沈阳站时，我毫不犹豫地下了车，还退掉了火车票，开始了人生的第一次真正选择。\n我的第一份测试工作就这样开始了。那时候真的很敢闯，说走就走，然后就独自一个人去了上海。\n上海对我来说是一个新鲜的、充满魔力的城市，即使是住在公司郊区的宿舍，生活依然快乐和精彩。\n在上海，我去过世博会、首家苹果店开业，当然还有南京路、外滩、杭州西湖；还看过刘翔在钻石联赛跑110米跨栏，看周杰伦的上海演唱会。周末的晚上还会和同事一起聚餐到半夜，那时候大家都是刚毕业的小伙子，同事之间相处起来更像是同学。\n2. 从上海回到沈阳 # 在上海出差了一年多，又到了选择的时候，是回到沈阳分公司还是辞职在上海找一份新的工作？我和一部分同事选择回到沈阳分公司。\n回到沈阳，那里的办公室、食堂、工厂和宿舍都在一个大大的厂区里，周围是郊区，只有周末才能出去逛逛。\n我感觉到在这个郊区，我就是这个工厂里的螺丝钉，看到了那种一眼望到头的生活。\n日子虽然过得平稳，但不应该是我这个二十来岁的年龄该追求的，我知道这不会持续太久，如果不及时调整，等到有一天没什么一技之长时会很被动。我开始思考：如果将来我想去大连，我现在的这份手机测试是很难找到工作的，因此我要转做 Web 测试，哪里要我，我就去哪里，我需要的就是相关工作经验。\n后来我是在沈阳东软面试上了北京东软的岗位，去北京的工资是3000，于是我毅然决然去了北京。\n3. 从沈阳到北京 # 那是2011年3月，来之前我联系了大学同学刚哥。我可以在找到房子之前在他那里借宿一段时间。\n就这样，我就坐上了去北京的火车。\n我如愿做上了我心中真正的软件测试工作，也从那时候开始，我才开始上手了一点脚本、数据库相关的知识。\n在北京东软，我经历了两个外派项目，发现我不喜欢这样的外派方式。一是上班无定所，二是定期更换并认识新同事。不到一年时间，我打算换工作了。\n我第一个投递的简历是百度，面试了但不知道为什么反正就是没有通过，可能那时候确实是太菜了，才做了不到一年的 web 测试新手，还想面试百度？想想也不太可能。\n后来又投了几个简历，误打误撞我就面试了京东，且最终通过了面试。记得当时的工资是6500，这对我来说真的是一笔巨款啊，足足比我刚来北京的时候翻了一倍。\n在京东工作的两三年，是我真正入行测试的几年。身边确实有一些值得学习的同事，在他们身上我学到了 Python、Jenkins、性能测试、以及功能测试。\n虽然我学到了这些技能，但我知道如果我想回到大连找到一份不错的工作，光会点技术还不够，我还得会英语，因为在大连只有外企的待遇还不错。\n因此我在北京就开始搜索大连的外企，另外也开始准备学习英语。\n我报名了新东方，学费好几千块钱，我记得上一堂课可能都要几十到上百，另外我觉得这样性价比不高。听了一两次试听课后，我果断退了学费。\n我觉得最好的学习英语的环境就是加入外企。我开始在北京尝试去面试大连的外企，正好有一家在北京有分公司，我就去面试了，但可惜面试没通过。\n4. 从北京回到大连 # 没有办法，在北京想找到大连外企实在不太现实，我辞掉了京东的工作回到大连找。\n回到大连之后休息了几天，发现自己没有工作实在没心情做其他任何事情，然后就开始了找工作。\n去面试过花旗的外派，也没有成功。那时候我还有一个想法就是转开发！\n这是受在京东的同事影响。当时有一个测试同事，他会开发且技术不错，负责给开发团队写单元测试。我当时觉得这个很厉害，我也想做那种被认为有技术含量的工种。\n因此我当时的另一个计划是：如果有任何一家公司愿意招聘我这个只有测试工作经验的人，培养我做开发，我其实愿意少拿钱，甚至免费为他们工作。我当时离开京东时的工资已经有一万二了，尽管如此但我也想要转开发，即使是从一个实习生开始。可惜当时市场没有给我这个机会，我没有在短期内找到这样的开发岗位。因为我需要一份工作，后来就找到一个6000多薪资的测试小主管工作。在这个岗位上我工作了几个月，发现这不是我想继续工作的环境，并在骑驴找马继续寻找一份外企工作。\n后来我又误打误撞投了我之前在北京投递过的外企，这次面试得挺不错的，并正式被录用进入了外企。在这里开启了一段十年的职业生涯，是我最为努力的十年。\n5. 在外企从测试转开发然后DevOps # 上面说过我想转开发。在我加入到新公司后，组内有机会可以从测试转前端开发的机会。我当时极度羡慕这样的机会，可惜晚了一步，人够了，因此我就只能在测试岗位上继续发光发热。\n直到2018年，因为公司业务调整，我又有了转开发的机会，但需要考核。\n虽然对我来说是一个很大的挑战，但我认为这对我来说是在做对的事情。两个原因：\n如果编程技能比较强，我可以成为测试里技术最好的人； 通常开发比测试有更多的话语权，更多的机会，比如说技术移民。 我毫不犹豫地申请去做开发！ 那是一段压力非常大的日子，我之前在#码农随笔系列里写过文章记录过那段日子。但最后我还是成功做了开发。再后来团队需要一个Build工程师，说真的，真是为我准备的角色，太适合我了！\n之前这个角色还叫Build工程师，因为我非常有热情把这个工作做好，因此我做了很多Build之外的事情，因此我的职责也在慢慢扩大到DevOps相关领域，之后大家都称我为DevOps工程师，负责DevOps/Build/Release相关的事情。\n6. 从大连到欧洲 # 早在几年前，我开始有了想走出去看看的想法，想在日本、欧美范围内找一个可以提供签证的DevOps工作。如果再走不出去，到了四十岁我可能就没什么机会了。\n这个想法的产生可能源自于我此前自由行去过泰国、日本、美国的缘故。之后我喜欢听《静说日本》、《随口说美国》这些音频节目。\n我更新了自己的LinkedIn，期间偶尔有人联系过我新加坡的机会。说实话，能不能去成还不好说，但我对那里不是很感冒，给我的感觉那边比较卷。\n在经历了疫情、孩子出生后，这个想法就搁置了，也没有任何进展。在经历了众所周知的大环境后，突然有了机会。\n但说实话这时候我顾虑比当初去上海，回沈阳、去北京、回大连时多得多。\n因为家庭、因为孩子、因为父母，做出决定是不容易的。但在过往的所有选择里，没有完美的机会，在这个不确定的时代，唯一不变的就是变化。况且这样的机会以后几乎都不会再有了。\n留下来可能需要被迫去卷无意义的事情，就像大人卷工作、教育，小孩卷学习。想到此，我知道又到了该选择的时候，那就是走出去。\n最后 # 自毕业以来，特别是近十年的外企工作，我挺努力的。努力工作、不妥协地去完成；坚持业余时间以教促学的写了七八年的博客和公众号，贡献开源项目。\n我坚信努力的价值，但同时也提醒自己不要陷入“自我感动”的误区。努力固然重要，但不能让战术上的勤奋掩盖战略上的懒惰。我们必须抬头看路，抓住关键的机会，做出明智的决策。毕竟，如果方向错误，再多的努力也可能是徒劳的。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-27","externalUrl":null,"permalink":"/misc/power-of-choice/","section":"Miscs","summary":"偶尔深夜躺下时，我常常在想，我是怎么就走到这了？这都是源于毕业后的一系列选择吧！","title":"选择往往比努力更重要","type":"misc"},{"content":"想象一下，你是一名 DevOps 工程师，不论初级、中级还是高级，老板总有一天拍拍你的肩膀说：“加油干，兄弟，未来你就是我们的首席 DevOps 工程师了！”你可能会心想：“啥是首席 DevOps？这是个什么新饼？”\n今天就带你了解一下，所谓的“首席 DevOps 工程师”到底干啥，职责是什么？\n我们一起看看，顺便找准未来的职业发展方向。毕竟，谁都希望能进阶到高级角色嘛，对吧？\n首席 DevOps 工程师是干啥的？ # 在今天这个技术跑得比人快的世界里，首席 DevOps 工程师是关键角色，帮助企业搞定基础设施，让软件交付又快又稳。这可不光是架个服务器那么简单，真正的大活儿是当好开发和运营团队之间的“桥梁”，推动 DevOps 文化在公司生根发芽。\n那么他们的日常是啥呢？总结起来，有三个主要工作：\n设计并维护基础设施 —— 和开发团队配合，搭建弹性、可扩展的基础设施，满足业务需求。 自动化所有能自动化的事情 —— 减少手动操作，提升代码发布和测试的效率。 推动团队文化变革 —— 推广 CI/CD 最佳实践，优化大家的工作方式。 核心职责 # 1. 协调开发和运营 # 在你成为首席 DevOps 工程师后，你的头号任务就是让开发和运营两边配合得像一个人。持续集成、持续部署这些词你得说得像背诗一样顺溜，同时，基础设施得稳如老狗。\n2. 实现自动化和流程优化 # 自动化是 DevOps 的灵魂，首席 DevOps 工程师就是得把那些繁琐的手动任务尽可能自动化，不断优化，让所有事情跑得又快又稳。\n3. 保证系统可靠性和效率 # 系统跑不稳，CI/CD就得停摆，所以你要设计出能撑得住风浪的基数设施。遇到高负载或者故障，系统照样得稳住。定期监控、优化，是你的日常功课。\n成为首席 DevOps 工程师需要哪些技能？ # 1. 技术要硬 # 技术基础是标配，Python、Bash 这些脚本语言得熟悉，Docker 这种容器技术也得懂，Ansible、Chef 这些配置管理工具是你日常操作。最重要的是，云平台（比如 AWS 和 Azure）管理经验不能少。\n2. 领导力和管理能力 # 技术大牛不稀奇，领导力大牛才是硬通货。你得激励团队，帮他们成长，创造出协作的氛围。别忘了，技术再牛，不会跟不同层级的利益相关者沟通，那也白搭。\n3. 解决问题的能力 # 技术上碰到过问题可以迅速找到问题的根源，给出靠谱的解决方案。更重要的是，做决策时得能平衡技术需求和业务目标，让公司上下都买账。\n对公司有啥好处？ # 1. 加强沟通协作 # 作为首席 DevOps，你不仅仅写代码，还得跨团队沟通协调，让大家更默契，工作更顺畅。减少内部摩擦，提升效率。\n2. 加快产品交付 # 优化流程、自动化任务，你能让产品的交付速度快得像坐火箭。市场变化快，你得让公司跟得上，产品能快速迭代，企业才能有竞争力。\n3. 提高系统稳定性和安全性 # 稳定性和安全性很重要，你得建立起强大的监控体系，防止潜在的威胁，在安全和稳定方面给公司打下坚实基础。\n总结一下 # 首席 DevOps 工程师可不是单纯的技术专家，你要靠技术提升效率，推动合作，保证产品交付和系统的稳定性。这过程里，你不仅是一个技术领袖，更是团队文化的推动者。\n想成为首席 DevOps？那就不仅要技术过硬，还要培养领导力和解决问题的能力。\n希望这篇轻松的文章能帮你找到未来的努力方向，毕竟，我们都在为更高的目标努力着！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-26","externalUrl":null,"permalink":"/posts/principal-devops-engineer/","section":"Posts","summary":"本文介绍了首席DevOps工程师的职责、核心技能和对公司的价值，帮助你了解如何在DevOps领域实现职业发展。","title":"DevOps进阶：揭秘首席DevOps工程师的职责与技能","type":"posts"},{"content":" Summary # Conventional Branch refers to a structured and standardized naming convention for Git branches which aims to make branch more readable and actionable. We\u0026rsquo;ve suggested some branch prefixes you might want to use but you can also specify your own naming convention. A consistent naming convention makes it easier to identify branches by type.\nKey Points # Purpose-driven Branch Names: Each branch name clearly indicates its purpose, making it easy for all developers to understand what the branch is for. Integration with CI/CD: By using consistent branch names, it can help automated systems (like Continuous Integration/Continuous Deployment pipelines) to trigger specific actions based on the branch type (e.g., auto-deployment from release branches). Team Collaboration : It encourages collaboration within teams by making branch purpose explicit, reducing misunderstandings and making it easier for team members to switch between tasks without confusion. Specification # Branch Naming Prefixes # The branch specification by describing with feature/, bugfix/, hotfix/, release/ and chore/ and it should be structured as follows:\n\u0026lt;type\u0026gt;/\u0026lt;description\u0026gt; main: The main development branch (e.g., main, master, or develop) feature/: For new features (e.g., feature/add-login-page) bugfix/: For bug fixes (e.g., bugfix/fix-header-bug) hotfix/: For urgent fixes (e.g., hotfix/security-patch) release/: For branches preparing a release (e.g., release/v1.2.0) chore/: For non-code tasks like dependency, docs updates (e.g., chore/update-dependencies) Basic Rules # Lowercase and Hyphen-Separated: Always use lowercase letters, and separate words with hyphens. For example, feature/new-login or bugfix/header-styling. Alphanumeric and Hyphens Only: Use only lowercase letters (a-z), numbers (0-9), and hyphens. Avoid special characters like spaces, punctuation, and underscores. No Consecutive Hyphens: Ensure that hyphens are used singly, with no consecutive hyphens (e.g., feature/new-login, not feature/new--login). No Trailing Hyphens: Do not add a hyphen at the end of the branch name. For instance, use feature/new-login instead of feature/new-login-. Clear and Concise: Make branch names descriptive but concise. The name should clearly indicate the work being done. Include Jira (or Other Tool) Ticket Numbers: If applicable, include the ticket number from your project management tool to make tracking easier. For example, for a ticket T-123, the branch name could be feature/T-123-new-login. Conclusion # Clear Communication: The branch name alone provides a clear understanding of its purpose the code change. Automation-Friendly: Easily hooks into automation processes (e.g., different workflows for feature, release, etc.). Scalability: Works well in large teams where many developers are working on different tasks simultaneously. In summary, conventional branch is designed to improve project organization, communication, and automation within Git workflows.\nFAQ # What tools can be used to automatically identify if a team member does not meet this specification? # You can used commit-check to check branch specification or commit-check-action if your codes are hosted on GitHub.\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-18","externalUrl":null,"permalink":"/en/posts/conventional-branch/","section":"Posts","summary":"This article introduces the Conventional Branch specification, which provides a structured naming convention for Git branches to enhance readability and collaboration.","title":"Conventional Branch Specification Released!","type":"posts"},{"content":"PyPA（Python Packaging Authority）是管理和维护 Python 包相关工具的一个社区组织。PyPA 管理的知名项目包括 pip、packaging、setuptools、wheel、twine、build 等等。了解这些项目的关于有助于我们更好的了解 Python 的生态系统。\n以下是这些项目的介绍以及它们之间的关系：\npip 作用：pip 是 Python 的包管理工具，用于安装和管理 Python 库和依赖项。通过 pip，用户可以从 Python Package Index (PyPI) 或其他包源下载并安装所需的 Python 包。 关系：pip 依赖于 setuptools 和 wheel 来处理包的构建和安装。它是最常用的 Python 包管理工具，也是官方推荐的包安装方法。\nsetuptools 作用：setuptools 是一个用于打包 Python 项目的工具，可以创建分发包（distribution packages）并发布到 PyPI。它扩展了 Python 标准库中的 distutils，提供了更多功能，如依赖管理、插件系统等。 关系：setuptools 是创建 Python 包时常用的工具，pip 使用 setuptools 来安装源代码形式的 Python 包。setuptools 生成的分发包通常是 .tar.gz 或 .zip 文件格式。\npackaging 作用：packaging 提供了用于与 Python 包打包和分发相关的核心实用工具和标准实现。它实现了一些与包版本、依赖关系解析等有关的 PEP（Python Enhancement Proposals）。 关系：packaging 是 setuptools 和 pip 等工具的底层依赖，用于处理包的版本比较、依赖解析等低层次操作。\nwheel 作用：wheel 是一种 Python 包的打包格式，作为 setuptools 打包格式 .egg 的替代方案。它是目前推荐的发布格式，可以避免编译步骤，安装速度更快。 关系：pip 优先安装 wheel 格式的包，因为它可以直接安装，而不需要像 .tar.gz 那样进行编译。setuptools 可以生成 wheel 格式的包。\nvirtualenv 作用：virtualenv 用于创建独立的 Python 环境，可以避免不同项目之间的包依赖冲突。每个虚拟环境都有自己独立的 Python 可执行文件和库集合。 关系：pip 被用于管理 virtualenv 中的包。virtualenv 是创建隔离环境的工具，但近年来 Python 标准库中的 venv 模块也能提供类似功能。\ntwine 作用：twine 是用于将 Python 包上传到 PyPI 的工具。与 setuptools 的 python setup.py upload 方法不同，twine 更加安全，支持上传 wheel 文件和 .tar.gz 文件。 关系：twine 通常与 setuptools 或 wheel 一起使用，负责将构建好的包上传到 PyPI。\nbuild 作用：build 是一个简单的命令行工具，用于构建 Python 项目。它可以使用 PEP 517 接口构建包，而不依赖于 setuptools。 关系：build 提供了比 setuptools 更简洁的构建方式，但它依赖于 setuptools 或其他构建后端（如 flit、poetry）来实际完成构建过程。\npyproject.toml 作用：pyproject.toml 不是一个工具，而是一种配置文件格式。它被用来描述项目的构建需求，并支持使用不同的构建后端，如 setuptools、flit、poetry 等。 关系：pip 和 build 等工具会读取 pyproject.toml 文件，了解如何构建和安装项目。\n他们之间的关系总结 # pip 作为包管理工具，与所有这些项目都有交互关系，尤其依赖 setuptools 和 wheel 来安装包。 setuptools 负责包的创建和打包，并使用 packaging 处理版本和依赖。 wheel 是打包格式，pip 更倾向于安装 wheel 格式的包。 virtualenv 用来创建隔离的环境，pip 被用来在这些环境中安装依赖。 twine 用来安全地上传包到 PyPI，通常与 setuptools 和 wheel 结合使用。 build 是一个新兴的构建工具，使用 PEP 517 接口，可以使用 setuptools 作为构建后端。 这些工具共同构成了 Python 包管理和分发的完整生态系统，简化了 Python 开发者的工作流程。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-05","externalUrl":null,"permalink":"/posts/pypa/","section":"Posts","summary":"本文介绍了 PyPA（Python Packaging Authority）下的知名项目，包括 pip、setuptools、wheel 等，并分析了它们之间的关系，帮助读者更好地理解 Python 包管理和分发的生态系统。","title":"初步了解 PyPA（Python Packaging Authority）下的知名项目和关系","type":"posts"},{"content":"最近开车的时候我总爱听赵雷的歌，尤其是那首《我记得》。\n我觉得这首歌就是回忆的交响曲，每每听到就仿佛看到了一部老电影。它像是一种声音，一种触动人心的触感，让我们回忆起那些已经消逝的时光，反思我们的生活方式和人生的方向。\n歌词和旋律中都充满了满满的怀旧情感，总是让我回想起来这一段时间来的所有人多我的问候和祝福。\n趁着他们都还很鲜活我想把他们全都记录一下，也叫《我记得》：\n6月14日（周五）我所在的1024足球队就为我准备了聚餐，当时还只有我的签证获批，这已经是一个好的开始了，当晚把我这几年的啤酒全喝了，还好酒精度数不是很大，回到家也没有迷糊。 6月27日（周四）与公司的同事和领导去吃了钱库里自助，当时的心情还是复杂的，因为明天就要毕业了，我们在一起照了一张欢快的合影。 6月28日（周五）毕业当天很多同事过来跟我告别、以及他们把我送出公司的门和大楼门的情景，最后我和大超一起下山，他回家，我等公交车。 6月30日（周日）我去小平岛恰巧碰到了同事大叔，我去送洗鞋，他去买菜。我们就站在马路边站着聊了很久，最后分别前拍了一张合影。 7月1日（周一）跟当年的发小和老同桌一起吃了午饭，绕着腾飞走了一圈，最后他陪我一起去4s保养、补胎，聊了一下午，好久没有单独和一个人聊这么久了，感觉很好，拍了合影，留作纪念 7月4日（周四）我独自带着女儿回到老家让二老再看看他们的大孙女，整个往返车程6个小时，女儿都是坐在宝宝安全座椅上，表现的非常棒。 7月7日（周日）和妻子、女儿去老丈人家吃了一个美味的午饭；还是当天，下午和教练去市馆游泳，回到腾飞后喝了杯咖啡、吃了点小食、最后绕着腾飞走了半圈，聊到意犹未尽。 7月8日（周一）去跟原公司同事蹭场打羽毛球， 7月9日（周二）跟1024足球队踢了我在这边的最后一场球了，对方今天有一个超强的守门员，给人印象深刻，另外一个深刻的地方可能就是我的一记后场吊射进球了吧。 7月10日（周三）启程再次去日本拿卡，另外还去了一趟东京迪斯尼，尽管当天下了雨，但感受还是很好。 7月14日（周日）从日本回来，当天下午前同事朋友去羊汤馆吃了一顿饭，聊了几个小时。 7月15日（周一）见了发小，他在海边钓鱼，我也去了，那是我第一次使用鱼竿钓鱼，并且钓到了一条小小小鱼。 7月16日（周二）启程出发了，虽然心中有万般不舍，但到了改走的时候了。我知道此时的难过是真的，我也知道我一定会走出这一步也是真的。当晚做高铁到了北京，见到机场见到大学同学，一起聊了一会，但因为已经很晚了，就叙旧到此。 7月17日（周三）经过了十几个小时的飞行，我们一家人终于到了维村。至此，我短暂的毕业季就结束了。 很怀念，因此写下只言片语，记录一切发生过的美好。\n这些记忆会提醒我，无论生活如何变迁，我们都不会忘记过去，但也要对未来充满希望和期待，体验到了生活的美好和希望。\n生活里虽然充满了困难和挑战，但只要我们对过去有回忆，对未来有期待，那么生活就充满了意义和价值。\n美好的音乐也会触动着我们的情感，引发我们的思考，带给我们力量和希望。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-07-09","externalUrl":null,"permalink":"/misc/remember/","section":"Miscs","summary":"最近开车的时候我总爱听赵雷的歌，尤其是那首《我记得》。这篇文章记录了我在Rocket中国分公司的毕业季和同事们的祝福。","title":"我记得","type":"misc"},{"content":"​今天（6月28日）是我在Rocket中国分公司的最后一天，也是效力的第十个年头。这是我职业生涯中效力过最长的一家公司，想借此机会写下一些文字，为这十年画上一个句号。\n光阴似箭，岁月如梭。转眼间，十年悄然逝去。唯有认真努力地生活，才能不被时光的流逝所感叹。我一直很喜欢的一句话是：“种一棵树最好的时间是十年前，其次是现在。\n受同事的影响，从加入到公司以后我开始爱上软件这个行业，也想成为别人眼中的技术专家。我从最初的测试工程师转为开发人员，最终投身于DevOps领域，在这个过程中收获了快乐和成就。\n十年征程，感恩有你。在Rocket中国的十年里，我结识了众多优秀的同事和朋友，从他们身上我学到了很多，也受到了他们的深刻影响。\n无数次的团队聚餐、活动、旅游、足球、羽毛球、游泳等等，感谢你们一路的相伴，让这段旅程更加精彩和难忘。\n也感谢我的家人，一直以来对我的包容、理解、支持和爱。是你们给了我坚强的后盾，让我无畏前行。感谢这十年里所有美好的回忆。\n三十年风雨，五座城市。三十年来，我辗转生活过五个城市：出生在辽宁庄河，大学就读于沈阳，第一份工作在上海，后来又在北京工作了三年。直到2014年，我从京东裸辞，回到了家乡大连。\n大连是我最喜欢的城市之一，这里风景优美、气候宜人。我在这里买房、结婚、生子，完成了人生中许多重要的阶段。\n天下没有不散的宴席。随着最后一天日期的临近，我的心情也变得复杂。\n其中既有对未来的期待和憧憬，也有对挑战和未知的担忧，以及对家人的牵挂。我期待着自己、妻子和孩子都能在未来的道路上变得更好、更强；但从工作到生活，这无疑将充满挑战。我无法确定这一过程是否会顺利，也不确定需要多长时间。\n选择欧洲，开启新篇章。或许有人会问，在国内难道不能养家糊口吗？为什么要跑到那么远的地方？其实，这并非对错之分，而是选择。一个人的选择往往与他的经历息息相关。\n以前，我从未想过出国，直到加入Rocket中国之后，身边优秀的同事陆续带着家人和孩子去了澳大利亚、欧洲、日本等地。此外，我也有机会去美国出差、去日本旅游，看到了更加广阔精彩的世界。\n这些经历逐渐在我心中萌生了一个想法：如果有机会走出去，那将是一次充满挑战和乐趣的人生体验。\n我钦佩那些主动打破舒适圈的人，而我则一直被动地等待着机会的降临。如果机会来临，我愿意勇敢地尝试。\n恰逢公司业务调整，项目要移至欧洲。对于其他人来说，这可能是一个坏消息，但对我而言，这是一个难得的机会。虽然这并非一个完美的契机，但人生哪有那么多完美呢？\n这次机会将让我有机会在全英文的工作环境中锻炼自己；我的两岁女儿也将有机会接受不同的教育，开启她的第二甚至第三外语学习之旅。\n对于我们一家来说，这都是一次全新的挑战和开始。虽然我们已经过了35岁，但我始终觉得，我们依然年轻，可以不断学习和探索，保持对世界的好奇心。\n选择欧洲的另一个原因是，如果留在国内，程序员想要延续职业生涯到四十岁甚至五十岁、六十岁会非常困难，而在欧美国家相对容易一些。\n此外，随着年龄的增长，我对“好公司”的定义也在不断变化。\n比如刚毕业去上海工作时，我觉得SIMCom是最好的公司；后来去北京东软工作时，我觉得东软也还不错；再后来，我加入了京东，觉得这可能是我这辈子能加入的最好的公司了；直到我加入到现在这家外企，这十年来我一直觉得这才是一家好公司。\n如果还按照现在的标准在大连甚至其他城市来寻找新的工作，恐怕只能在梦里才能实现了。\n最后，感谢在Rocket中国这十年里遇到的每一位朋友和同事。\n衷心感谢每位同事的临别前的问候和祝福。\n希望未来我们在人生的旅途上能够再次相聚。\n再见！👋\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-06-28","externalUrl":null,"permalink":"/misc/farewell/","section":"Miscs","summary":"2024年6月28日是我在Rocket中国分公司的最后一天，回顾十年征程，展望未来新篇章。","title":"告别Rocket中国，回连十年再启程","type":"misc"},{"content":"软件真是个有趣又深奥的东西，它由看似神奇的代码片段组成，这些代码运行在最终的终端上，本身却并非生命体，但拥有自己的生命周期。\n软件最初是源代码的形式，仅仅是存放在某个仓库的文本文件，然后通过独特的构建过程，这些源代码会转变为其他形式。例如交付到 web 服务器的压缩 JavaScript 代码块、包含框架代码和业务逻辑的容器镜像，或者针对特定处理器架构编译的原始二进制文件。\n这种最终的形态转变，也就是源代码生成的其他形式，通常被称为“软件制品”。在创建之后，制品通常会处于休眠状态，等待被使用。它们可能会被放置在软件包注册表（例如 npm、RubyGems、PyPI 等）或容器注册表（例如 GitHub Packages、Azure Container Registry、AWS ECR 等）中，也可能作为附加到 GitHub 版本发布的二进制文件，或者仅仅以 ZIP 文件的形式存储在某个 Blob 存储服务中。\n最终，有人会决定拾取该制品并使用它。他们可能会解压缩包、执行代码、启动容器、安装驱动程序、更新固件 - 无论采用何种方式，构建完成的软件都将开始运行。\n这标志着生产生命周期的顶峰，该周期可能需要大量人力投入、巨额资金，并且鉴于现代世界依赖软件运行，其重要性不言而喻。\n然而，在许多情况下，我们并不能完全保证所运行的制品就是我们构建的制品。制品经历的旅程细节要么丢失，要么模糊不清，很难将制品与其来源的源代码和构建指令联系起来。\n这种缺乏对制品生命周期的可见性是当今许多最严峻的安全挑战的根源。在整个软件开发生命周期 (SDLC) 中，有机会保护代码转换为制品的流程 - 如此一来，可以消除威胁行为者破坏最终软件并造成严重后果的风险。一些网络安全挑战似乎难以成功应对，但这种情况并非如此。让我们深入了解一些背景知识。\n哈希值和签名 # 假设你的目录中有一个文件，并且你想要确保它明天与今天完全相同。你该怎么做？一个好的方法是通过安全的哈希算法生成文件的哈希值。\n以下是如何使用 OpenSSL 和 SHA-256 算法完成此操作：\nopenssl dgst -sha256 ~/important-file.txt 现在，你拥有了一个哈希值（也称为散列值），它是一个由字母和数字组成的 64 字符字符串，代表该文件的唯一指纹。只要更改该文件中的任何内容，然后再次运行哈希函数，你就会得到不同的字符串。你可以将哈希值写在某个地方，然后第二天回来尝试相同的过程。如果你两次没有得到相同的哈希值字符串，则文件中的某些内容已发生更改。\n到目前为止，我们可以确定某个文件是否被篡改。如果我们想要对制品进行声明怎么办？如果我们想说“我今天看到了这个制品，我（系统或人）保证这个东西就是我看到的东西”，该怎么办？此时，你需要的是软件制品签名；你需要将哈希值字符串通过加密算法进行处理，以生成另一个字符串，代表使用唯一密钥“签名”该指纹的过程。 如果你随后希望其他人能够确认你的签名，则需要使用非对称加密：使用你的私钥签名哈希值，并提供相应的公钥，以便任何获取你文件的人都可以进行验证。\n你可能已经知道，非对称加密是互联网上几乎所有信任的基础。它可以帮助你安全地与银行互动，也可以帮助 GitHub 安全地交付你的存储库内容。我们使用非对称加密来支持诸如 TLS 和 SSH 等技术，以创建可信赖的通信通道，但我们也使用它通过签名来创建信任软件的基础。\nWindows、macOS、iOS、Android 等操作系统都具有用于确保可执行软件制品的可信来源的机制，方法是强制要求存在签名。这些系统是现代软件世界中极其重要的组件，构建它们非常困难。\n不仅仅是签名 - 还要证明 # 当我们思考如何展示关于软件制品的更多可信赖信息时，签名是一个好的开端。它表示“某个可信赖的系统确实看到了这个东西”。但是，如果你真的想在整个软件开发生命周期 (SDLC) 的安全性方面取得重大进步，那么你就需要超越简单的签名，而是要考虑证明。\n证明是一种事实断言，是对制品或制品所做的声明，并由可被认证的实体创建。之所以可以进行认证，是因为声明已签名，并且用于签名的密钥是可信的。\n最重要和最基础的证明类型之一是断言有关制品来源和创建的事实 - 它来自的源代码和将源代码转换为制品的构建指令，我们称之为来源证明。\n我们选择的来源证明规范来自 SLSA 项目。SLSA 是考虑软件供应链安全性的绝佳方式，因为它为软件的生产者和消费者提供了一个通用的框架，用于推理安全保证和边界，而这与特定的系统和技术栈无关。\nSLSA 基于 in-toto 项目的工作，提供了一种用于生成软件制品来源证明的标准化架构。in-toto 是一个 CNCF 毕业项目，其存在目的之一是提供一系列有关供应链和构建过程的相关信息的标准化元数据架构。\n构建这样的东西需要什么？ # GitHub 作为托管大量代码和构建管道的全球最大软件开发平台，对此进行了大量的思考。构建认证服务需要许多活动部件。\n这样做意味着有一种方法可以：\n颁发证书（本质上是绑定到某个经过身份验证的身份的公钥）。 确保这些证书不会被滥用。 在众所周知的上下文中启用工件的安全签名。 以最终用户可以信任的方式验证这些签名。 这意味着设置证书颁发机构 (CA) 并拥有某种客户端应用程序，你可以使用它来验证与该颁发机构颁发的证书相关联的签名。\n为了防止证书被滥用，你需要 (1) 维护证书吊销列表或 (2) 确保签名证书是短期的，这意味着需要某种时间戳机构的反签名（可以提供权威印章，表明证书仅在其有效期内用于生成签名）。\n这就是 Sigstore 的作用所在，它是一个开源项目，提供 X.509 CA 和基于 RFC 3161 的时间戳机构。它还允许你使用 OIDC 令牌进行身份验证，许多 CI 系统已经生成了令牌并将其与其工作负载相关联。\nSigstore 对软件签名的作用与 Let\u0026rsquo;s Encrypt 对 TLS 证书的作用相同：使其简单、透明且易于采用。\nGitHub 通过在技术指导委员会中的席位帮助监督 Sigstore 项目的治理，是服务器应用程序和多个客户端库的维护者，并且（与来自 Chainguard、Google、RedHat 和 Stacklok 的人员一起）组成了 Sigstore 公共物品实例的运营团队，该团队的存在是为了支持 OSS 项目的公共证明。\nSigstore 需要符合更新框架 (TUF) 规定的标准的安全信任根。这允许客户端跟上 CA 底层密钥的轮换，而无需更新其代码。TUF 的存在是为了缓解在现场更新代码时可能出现的大量攻击媒介。许多项目都使用它来更新长期运行的遥测代理、提供安全的固件更新等。\n有了 Sigstore，就可以创建防篡改的纸质跟踪，将工件与 CI 联系起来。这一点非常重要，因为以无法伪造的方式签署软件和捕获出处细节，意味着软件消费者有办法执行他们自己的规则，以确定他们正在执行的代码的来源。\n原文：https://github.blog/2024-04-30-where-does-your-software-really-come-from/\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-06-13","externalUrl":null,"permalink":"/posts/where-does-your-software-come-from/","section":"Posts","summary":"本文介绍了软件制品的来源证明，强调了在软件开发生命周期中保护代码转换为制品的流程的重要性，并介绍了 SLSA 项目和 Sigstore 的作用。","title":"你的软件究竟从哪里来？","type":"posts"},{"content":"上次我在 代码签名（Code Signing）的文章中时候提到了 GaraSign，这是我在工作中使用到的另一个代码签名工具。\n鉴于关于 GaraSign 的使用并没有多少中文资料，本篇我将介绍关于 GaraSign 的一些实线，希望对你有帮助。\n代码签名 # 这里再次说明什么是代码签名。代码签名证书用于对应用程序、驱动程序、可执行文件和软件程序进行数字签名，客户可通过这种方式验证他们收到的代码未被网络罪犯和黑客篡改或破坏。签名后的交付产品结合了加密令牌和证书，用户可在安装产品前对其进行验证。代码签名可确认谁是软件作者，并证明代码在签名后未被修改或篡改。\nGarasign 解决方案 # GaraSign 是一个基于 SaaS 的安全协调平台，可对企业基础设施、服务和数据进行集中管理。GaraSign 可与所有主要操作系统、平台和工具的本地客户端集成，确保现有工作流不受干扰，同时改善其整体安全态势和合规性。\nGaraSign 由以下组件组成： # 加密令牌 - 存储签名密钥的加密设备（通常是一个或多个 HSM - Hardware Security Modules） GaraSign 签名服务器 - 位于存储签名密钥的加密令牌前的 REST 服务器 GaraSign Signing 客户端 - 允许与之集成的签名工具在本地散列数据并将签名生成脱载至 GaraSign Signing 服务器的客户端。 Garasign 代码签名散列方法 - 大幅提高速度\n安装 GaraSign # 关于如何安装 GaraSign 这里不过去介绍，可以到官网找相关的安装文档。这里要注意目前 GaraSign 对操作系统版本的要求还是很高的，比如\nWindows 最低要求是 Windows 2019, Win10 and Win11 Linux 最低要求是 RHEL 7.9, 8.0, 9.0，CentOS 7.9, 8.0, 9.0，Rocky 8.0 如果你的构建环境还是比较旧的或是不符合其支持的版本，建议你向我一样设置一台专用的 GaraSign 机器（推荐 Linux）。\n如果你使用 Jenkins 来构建，可以将这台机器设置为一台 Jenkins agent，通过创建一个 Jenkins pipeline，这样其他所有的要需要发布的包都可以通过这个 pipeline 来进行签名。\n如何签署独立签名 # 如果你已经设置好了 GaraSign 环境，以 Linux 为例，那么就可以通过下面的命令进行签署。\n注：在 Windows 与 Linux 在签署不同的类型文件所使用到的命令不同，因此推荐在 Linux 进行签名会更加简单。\nopenssl dgst -sign \u0026lt;private key file\u0026gt; -keyform PEM -sha256 -out \u0026lt;signature-file-name.sig\u0026gt; -binary \u0026lt;binary file to sign\u0026gt; 具体的实施 # 加入你的 Artifacts 存在 Artifactory 上面，下面就 Jenkins 为例，来实施一个可以自动签名的 pipeline。包括：\n从 Artifactory 上下载需要签名的 Artifacts 使用 GaraSign 进行签名 验证 GaraSign 是否成功 上传签名文件和公钥到 Artifactory 上的同一个目录下 pipeline{ agent { node { label \u0026#39;garasign\u0026#39; } } parameters { string( name: \u0026#39;REPO_PATH\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Repository Path on Artifactory. eg. generic-stage/test_repo/devel/54/mybuild_1.1.0_752d0821_64bit.exe\u0026#39; ) } environment { BOT = credentials(\u0026#34;BOT-credential\u0026#34;) ART_URL = \u0026#34;https://my.org.com/artifactory\u0026#34; } stages { stage(\u0026#39;GaraSign\u0026#39;){ steps { script { if (! params.REPO_PATH){ error \u0026#34;REPO_PATH can not empty, exit!\u0026#34; } // Update Job description def manualTrigger = true currentBuild.upstreamBuilds?.each { b -\u0026gt; currentBuild.description = \u0026#34;Triggered by: ${b.getFullDisplayName()}\\n${REPO_PATH}\u0026#34; manualTrigger = false } if (manualTrigger == true) { currentBuild.description = \u0026#34;Manual sign: ${REPO_PATH}\u0026#34; } sh \u0026#39;\u0026#39;\u0026#39; # download artifacts curl -u${BOT_USR}:${BOT_PSW} -O ${ART_URL}/${REPO_PATH} file_name=$(basename ${REPO_PATH}) repo_folder=$(dirname ${REPO_PATH}) # garasign openssl dgst -sign grs.privkey.pem -keyform PEM -sha256 -out $file_name.sig -binary $file_name # verify grs.pem.pub.key output=$(openssl dgst -verify grs.pem.pub.key -keyform PEM -sha256 -signature $file_name.sig -binary $file_name) if echo \u0026#34;$output\u0026#34; | grep -q \u0026#34;Verified OK\u0026#34;; then echo \u0026#34;Output is Verified OK\u0026#34; else echo \u0026#34;Output is not Verified OK\u0026#34; exit 1 fi # upload signature file (.sig) and public key (.pem.pub.key) curl -u${BOT_USR}:${BOT_PSW} -T $file_name.sig ${ART_URL}/${repo_folder}/ curl -u${BOT_USR}:${BOT_PSW} -T grs.pem.pub.key ${ART_URL}/${repo_folder}/ \u0026#39;\u0026#39;\u0026#39; } } } } } 如何验证独立签名 # 还是以 Linux 为例，使用如下命令可以进行签名的验证。\nopenssl dgst -verify \u0026lt;public key file\u0026gt; -signature \u0026lt;signature\u0026gt; \u0026lt;file to verify\u0026gt; 当你的 Artifacts 已经进行了签名，在提供给客户的时候，你不但需要提供发布的包，而且需要提供签名文件 (.sig) 和公钥 (.pem.pub.key)。\n举个例子，如下 CLI 产品分别提供了 Windows，Linux 和 AIX 三个平台的安装包，客户可以参考如下进行签名验证。\n# 下载安装包、签名文件和公钥 $ ls cli.pem.pub.key CLI_AIX_1.1.0.zip CLI_AIX_1.1.0.zip.sig CLI_LINUXX86_1.1.0.zip CLI_LINUXX86_1.1.0.zip.sig CLI_WINDOWS_1.1.0.zip CLI_WINDOWS_1.1.0.zip.sig # 验证签名 openssl dgst -verify cli.pem.pub.key -signature CLI_AIX_1.1.0.zip.sig CLI_AIX_1.1.0.zip Verified OK openssl dgst -verify cli.pem.pub.key -signature CLI_LINUXX86_1.1.0.zip.sig CLI_LINUXX86_1.1.0.zip Verified OK openssl dgst -verify cli.pem.pub.key -signature CLI_WINDOWS_1.1.0.zip.sig CLI_WINDOWS_1.1.0.zip Verified OK # 当包和签名文件不符时会验证失败 openssl dgst -verify cli.pem.pub.key -signature CLI_AIX_1.1.0.zip.sig CLI_LINUXX86_1.1.0.zip Verification Failure 以上就是关于 GaraSign 的实现分享，如有任何问题或是建议咱们评论区见。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-06-10","externalUrl":null,"permalink":"/posts/garasign/","section":"Posts","summary":"本文介绍了 GaraSign 代码签名工具的安装、使用和验证方法，帮助开发者实现安全的代码签名。","title":"代码签名（Code Signing） - GaraSign","type":"posts"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/infrastructure/","section":"标签","summary":"","title":"Infrastructure","type":"tags"},{"content":"Python 软件基金会 (PFS) 或许大家比较熟知，它是开源 Python 编程语言背后的组织，致力于为 Python 和 Python 社区的发展壮大创造条件。\n继上次我们看完了 Apache 的基础设施介绍，本篇文章我们一起来看看 Python 软件基金会 (PFS) 的基础设施，看看可以从中学到哪些。\nPSF 基础设施概述 # PSF 运行各种基础设施服务来支持其使命，从 PyCon 站点 到 CPython Mercurial 服务器。本页的目标是列举所有这些服务，它们在哪里运行，以及主要联系人是谁。\n基础架构团队 # 基础架构团队最终负责维护 PSF 基础设施。但是，它不需要成为运行 PSF 服务的基础设施。事实上，大多数的日常运营服务由不在基础设施团队中的人员处理。这基础设施团队可以协助建立新服务并为维护人员提供建议的个别服务。其成员通常还处理对敏感的更改全球系统，例如 DNS。目前的团队成员是：\nAlex Gaynor (has no responsibilities) Benjamin Peterson Benjamin W. Smith Donald Stufft Ee Durbin (PSF Director of Infrastructure) Noah Kantrowitz 联系基础架构团队的最佳方式是发送邮件 infrastructure-staff@python。也经常有人在 Libera 的 #python-infra 频道联系他们。\n基础设施提供商 # PSF 为其基础架构使用多个不同的云提供商和服务。\nFastly Fastly 慷慨捐赠其内容分发网络（CDN）到 PSF。我们最高的流量服务（即 PyPI, www.python.org, docs.python.org）使用此 CDN 来改善最终用户延迟。\nDigitalOcean DigitalOcean 是当前的主要托管对于大多数基础设施，此处部署的服务由 Salt 管理。\nHeroku Heroku 托管了许多 CPython 核心工作流机器人，短暂的或概念验证的应用程序，以及其他适合部署在 Heroku 的 Web 应用程序。\nGandi Gandi 是我们的域名注册之星。\nAmazon Route 53 Amazon Route 53 托管所有域的 DNS，它目前由基础设施人员手动管理。\nDataDog DataDog 提供指标、仪表板和警报。\nPingdom Pingdom 提供监控，当服务中断时向我们投诉。\nPagerDuty PagerDuty 用于 PSF 的待命轮换基础设施员工在一线，志愿者作为后援。\nOSUOSL 俄勒冈州立大学开源实验室举办一个 PSF 的硬件服务器，speed.python.org 用于运行基准测试，此主机是使用 Chef 和他们的配置管理位于 PSF-Chef Git 存储库中。\n数据中心 # PSF DC Provider Region ams1 Digital Ocean AMS3 nyc1 Digital Ocean NYC3 sfo1 Digital Ocean SFO2 各种服务的详细信息 # 本部分列举了 PSF 服务、有关其托管的一般情况以及所有者的联系信息。\nBuildbot buildbot master 是由 python-dev@python 运行的服务。特别是 Antoine Pitrou and Zach Ware.\nbugs.python.org bugs.python.org 由 PSF 在 DigitalOcean 上托管，由 Roundup 提供支持。它还部署了 bugs.jython.org 和 issues.roundup-tracker.org。\ndocs.python.org Python 文档托管在 DigitalOcean 上，通过 Fastly 提供。负责人是 Julien Palard。\nhg.python.org CPython Mercurial 存储库托管在 Digital Ocean VM 上。负责人是 Antoine Pitrou 和 Georg Brandl。\nmail.python.org python.org Mailman 实例托管在 https://mail.python.org 和 SMTP（Postfix）上。所有查询都应定向到 postmaster@python。\nplanetpython.org 和 planet.jython.org 它们托管在 DigitalOcean VM 上。Planet 代码和配置托管在 GitHub 上，并由团队在 planet@python。\npythontest.net pythontest.net 托管 Python 测试套件。python-dev@python 对其最终负责维护。\nspeed.python.org speed.python.org 是一个跟踪 Python 性能的 Codespeed 实例。Web 界面托管在 DigitalOcean VM 上，基准测试在 strongfy 上运行机器在 OSUOSL 上，由 Buildbot 主节点调度。由 speed@python 和 Zach Ware 维护。\nwiki.python.org 它托管在 DigitalOcean VM 上。负责人是 Marc-André Lemburg。\nwww.jython.org 这是从 Amazon S3 存储桶托管的。设置非常简单，不应该需要很多调整，但基础设施工作人员如有需要可以对它进行调整。\nwww.python.org 主要的 Python 网站是一个 Django 应用程序，托管在 Heroku。它的源代码可以在 GitHub 上找到，并且该网站的问题可能是报告给 GitHub 问题跟踪器。 Python 下载 （即 https://www.python.org/ftp/ 下的所有内容）都托管在单独的 DigitalOcean 虚拟机。整个网站都在 Fastly 后面。还有用于测试站点的 https://staging.python.org。http://legacy.python.org 是从静态镜像托管的旧网站。\nPyCon PyCon 网站托管在 Heroku 上。联系地址是 pycon-site@python。\nPyPI Python 包索引的负载最多 任何 PSF 服务。它的源代码可在 GitHub 上找到。 它的所有基础设施都在 由 pypi-infra 配置的 AWS，它以 Fastly 为首。基础设施是由 Ee Durbin, Donald Stufft, 和 Dustin Ingram 维护的，联系地址是 admin@pypi。\nPyPy properties PyPy 网站托管在 DigitalOcean VM 上并进行维护作者：pypy-dev@python。\n如需要参看原文。可访问地址。\n最后 # 从 PFS 基础设施来看，他们更多的使用了 Cloud 服务商和技术来部署他们的应用。因此想要参与到 PFS 基础设施管理用，需要对 CDN，DigitalOcean，Heroku，Amazon Route 53，Amazon S3，DataDog，Pingdom 这些技术有比较深入的使用经验。\n我们羡慕那些可以为开源软件全职工作的人，他们拿着薪水做着很多程序员都羡慕的事情。\n但拥有这样的工作需要我们自己的技能可以匹配的上，并且积极主动的去寻求这些机会，才有可能得到一份全职开源软件工程师的职位。共勉！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-05-28","externalUrl":null,"permalink":"/posts/psf-infra/","section":"Posts","summary":"本文介绍了 Python 软件基金会 (PFS) 的基础设施，包括其服务、提供商和团队成员，帮助读者了解 PFS 如何支持 Python 社区。","title":"Python 软件基金会 (PFS) 基础设施概览","type":"posts"},{"content":"When it comes to software development and security, Code Signing is a crucial concept. In this article, we will explore what code signing is, why it\u0026rsquo;s important, and compare two code signing tools.\nWhat is Code Signing? # Code signing is a digital signature technique used to verify the authenticity and integrity of software or code. It uses cryptography to attach a digital signature to a software file, proving that the file was created by a specific developer and has not been tampered with.\nThe code signing process typically involves the following steps:\nGenerate a digital certificate: Developers use a digital certificate to create a digital signature. Certificates are usually issued by a trusted third-party Certificate Authority (CA). Sign the software: Developers use specialized tools, such as Microsoft\u0026rsquo;s SignTool or Apple\u0026rsquo;s codesign tool, to digitally sign the software. Distribute the signed software: Digitally signed software can be distributed to user devices. Why is Code Signing Important? # The importance of code signing is reflected in several aspects:\nIntegrity verification: Code signing ensures that the software has not been tampered with during distribution. Users can verify the signature to confirm the software\u0026rsquo;s integrity and ensure it comes from a trusted source.\nAuthentication: The digital certificate accompanying the signature can be used to verify the identity of the software publisher. Users can view the certificate to learn about the software manufacturer and assess its trustworthiness.\nEnhanced security: Digital signatures prevent malicious software from being inserted into legitimate software packages, ensuring that the software downloaded by users is safe and reliable.\nOperating system trust: Most operating systems and app stores require developers to code-sign software before publishing. Unsigned software may be considered insecure or untrusted.\nCode Signing Tools # In my work, I mainly use two code signing tools: Code Signing Certificates Code Signing Certificates and GaraSign, which are representative of two different types of tools.\nHere\u0026rsquo;s a brief comparison of Code Signing Certificates and GaraSign:\nCode Signing Certificates and GaraSign are both solutions for verifying software integrity and origin, but they have some key differences in how they work and their functionality.\nFeature Code Signing Certificates GaraSign Issuer Trusted Certificate Authority (CA) GaraSign Form Digital Certificate Cloud Service Verification Method Cryptographic Hash Cryptographic Hash Functionality Verify software integrity, ensure software hasn\u0026rsquo;t been tampered with Verify software integrity, ensure software hasn\u0026rsquo;t been tampered with, provide centralized management, support audit and compliance Cost $100 to thousands of dollars per year Pay-as-you-go Management Requires purchasing and managing certificates No need to manage certificates Scalability Suitable for organizations that need to sign a large amount of software Suitable for organizations of any size In general, both Code Signing Certificates and GaraSign are effective solutions for verifying software integrity and origin. The best choice for you will depend on your specific needs and budget. Here are some factors to consider:\nCost: If you need to sign a large amount of software, GaraSign may be more cost-effective. Scalability: If you need to sign a large amount of software, GaraSign may be a better choice. Audit and compliance: If you need to meet stringent audit and compliance requirements, GaraSign may be a better choice. Ease of use: Based on my current experience, Code Signing Certificates are easier to set up and use. GaraSign requires setting up services and installing and configuring clients, which is very cumbersome1. Other signing tools include Microsoft\u0026rsquo;s SignTool.exe, Docker trust sign, etc., which will not be introduced here.\nConclusion # Through code signing, developers can increase the security and trustworthiness of software, while helping users avoid malicious software and tampering risks. In today\u0026rsquo;s digital environment, code signing is an important part of ensuring software integrity and security, improving software supply chain security.\nFor more information on software supply chain security, see this series of articles.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\nI am still in the early stages of using GaraSign. If necessary, I will write separate articles about my user experience with it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-04-29","externalUrl":null,"permalink":"/en/posts/code-signing/","section":"Posts","summary":"This article introduces the concept and importance of code signing, along with a comparison of two common code signing tools, emphasizing its role in software supply chain security.","title":"Code Signing","type":"posts"},{"content":"","date":"2024-04-29","externalUrl":null,"permalink":"/en/tags/devsecops/","section":"Tags","summary":"","title":"DevSecOps","type":"tags"},{"content":"","date":"2024-04-29","externalUrl":null,"permalink":"/en/tags/slsa/","section":"Tags","summary":"","title":"SLSA","type":"tags"},{"content":"","date":"2024-04-21","externalUrl":null,"permalink":"/tags/contributor/","section":"标签","summary":"","title":"Contributor","type":"tags"},{"content":"今天翻译一篇我在 Jenkins Contributors 页面上看到的一篇文章。\n其作者是 Hervé Le Meur，我早在关注 Jenkins-Infra 的项目的时候就关注到他，他是一个法国人。以下是关于他如何通过 Jenkins-X 社区最终进入到 Jenkins 基础设施团队成为 SRE 的经历。\n说实话有点羡慕。希望这个分享能给每一个想加入开源、并且在开源组织中找到一份全职工作带来一点启发。\n以下是我对原文的翻译：\nHervé Le Meur 是一名 SRE（网站可靠性工程师），目前是 Jenkins 基础设施团队的成员。他是通过 Jenkins X 进入开源社区的，随后转到 Jenkins 基础设施工作。\nHervé 的父亲是一名木匠，母亲是一名室内装潢师，他出身于一个模拟技术背景的家庭，但在六岁时就通过 Amstrad CPC 464 电脑第一次真正接触到了技术。\n如今，在不从事 Jenkins 任务的时候，Hervé 喜欢和家人一起到户外散步、阅读和观看自己喜欢的节目。\n在加入 Jenkins 之前，你的背景是什么？ # 大学毕业后，我在一家小型 B2B 营销咨询公司工作了 10 年，当时我是开发我们所用工具的万事通，但那里既没有 CI/CD，也没有开源。\n之后，我进入一家建筑信息建模（BIM）软件公司，从软件开发人员做起。有些团队在使用 Jenkins，但对他们来说，当时的 Jenkins 看起来既麻烦又缓慢。\n随着我逐渐成长为软件架构师，我的任务是基于 Jenkins X 建立一个新的 CI/CD，这花了我几个月的时间。 由于 Jenkins X 刚刚出炉，而我又是 Kubernetes 的新手，这项任务比预期的要困难得多，尤其是 Jenkins X 进入测试阶段后，我不得不多次重做大部分工作。\n通过这项工作，我学到了很多关于 Kubernetes 和 CI/CD 的知识，同时也为 Jenkins X 做出了不少贡献。 被这份工作解雇后，我联系了 James Strachan 和 James Rawlings，他们给了我一个链接，让我从 CloudBees 公司的 Oleg Nenashev 那里获得一个职位空缺，也就是我现在的职位。\n在我的脑海中，我是一名程序员，而不是系统管理员。因此，当 Mark Waite 解释说最后一轮面试将与人际网络有关时，我有点害怕。 我以为我会因此失去机会，因为这可能是不可能完成的任务。然而，当我与 Mark、Damien Duportal 和 Olivier Vernin 面谈时，他们却问我如何将 CI/CD 与 Jenkins X 集成：这真是一次奇妙的经历。我们进行了有意义的讨论，这让我感觉更舒服，也让我更容易做出决定。\n面试前15分钟，我收到了另一家公司（ Damien 和 Jean-Marc Meessen 之前恰好在这家公司工作过）的最终录用通知，当时我犹豫了一下，但结果是最好的，因为我现在仍然是 Jenkins 项目的一员，这可以说是我梦寐以求的工作。\n我还有过主持在线论坛的经验，所以社区部分的工作对我来说很熟悉。\n你使用 Jenkins 多久了？ # 我从 Jenkins X 开始，但从未接触过 Jenkins 本身。除了共享 Jenkins 的名称外，它们没有任何共同之处。 我对 Jenkins 的预想是负面的。我认为它是一个笨重、过时、复杂的 Java 程序。这些印象都是我在以前的公司里从其他使用它的人那里听来的。然而，当我开始使用 Jenkins 后，这种对比简直是天壤之别。 我的先入之见是，与其他程序相比，它既笨重又缓慢。我并不是唯一一个认为 Jenkins 不一定是最好、最快或最新的项目的人，但事实证明，一旦我开始使用这个项目，我就错了。\n为什么选择 Jenkins 而不是其他项目？ # 我并不一定选择 Jenkins，因为它是我工作的主要部分。当我开始查看 Tyler、Olivier、Damien 和 Mark 为 Jenkins 基础设施所做的工作时，我意识到 Jenkins 比我想象的要完善和高效得多。 我还喜欢我们使用 Jenkins 开发和发布 Jenkins 的事实。这种用法是独一无二的，因为大多数开源工具都不具备转发成功的能力。 Jenkins 花费了大量的时间和精力，以配合开发人员的流程和功能。在我看来，这是 Jenkins 成功的主要原因之一。Jenkins 还集成了 Terraform、Puppet、Kubernetes 和 Helmfile 等其他工具，但 Jenkins 仍然是这些工具的协调者。\n对我来说，为 Jenkins 工作是我的最高成就，因为我一直喜欢创建和开发工具，即使不是开发 Jenkins 的核心。\n加入 Jenkins 项目后，你看到 Jenkins 有哪些增长？ # 我们已经有越来越多的实例被定义为代码。因此，我们可以重新创建任何实例，而无需手动配置设置，这大大提高了我们的恢复能力。我们还在慢慢实现将 ci.jenkins.io 定义为代码并进行管理。\n你对 Jenkins 的哪方面特别感兴趣？ # 现在，我正在重构 Jenkins 控制器和代理的官方 Docker 镜像的构建过程，这让我感到非常有趣。 我也喜欢在贡献者方面工作，因为这就像一个谜题，知道我需要达到什么目标以及我的起点会让我更愉快。\n什么样的贡献最成功或最有影响力？ # Basil Crow 提出了使用 SQLite 代替文件系统的有趣想法。改用 JDK 17 非常成功，随着 JDK 21 的推出，Jenkins 可以在更新的平台上运行，并跟上时代的进步。 由于我们喜欢让 Jenkins 基础架构保持领先（例如始终运行最新的 Jenkins Weekly），下一步将引入 JDK22。插件健康分数标识对于可视化整个插件生态系统的健康状况非常有用。\n给新开发人员和开源社区新成员的建议 # 首先要提醒我的是项目的庞大性，并指示我一开始要选择一件事来专注。\n不要犹豫，大胆尝试；开源意味着对所有人开放。不要害怕提交 pull request，它并不需要完美无缺。\n你可能最终会喜欢它，并继续提交贡献！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-04-21","externalUrl":null,"permalink":"/posts/jenkins-contributors/","section":"Posts","summary":"本文介绍了 Hervé Le Meur 如何通过 Jenkins-X 社区的贡献，最终成为 Jenkins 基础设施团队的一名 SRE，并分享了他的经历和对 Jenkins 的看法。","title":"通过 Jenkins-X 社区最终进入到 Jenkins 基础设施团队成为 SRE 的经历","type":"posts"},{"content":"相信大家最近都总会看到这样或那样的新闻：哪个科技巨头又裁员了。裁员潮似乎成为了这个时代的常态，让许多打工人感到焦虑和不安。\n身在大连的我确实深有感触，外企和私企都有在裁员，与前两年相比，岗位越来越少，失业的人越来越多，因此想找到一个满意的岗位将会变得越来越难。\n再加上随着人工智能（AI）的发展，作为 DevOps 打工人常常在想，需要掌握哪些关键技能和能力才能让自己保持竞争力。\n以下是我认为在 2024 年至关重要的关键技能和能力：\n深入理解 DevOps 理念和工具：\n熟练掌握持续集成/持续交付（CI/CD）工具和流程。如 Jenkins，GitLab CI/CD，GitHub Actions。 能够设计和优化自动化部署流程，包括自动化测试、构建和发布。 精通容器化技术，如 Docker，以及容器编排工具，如 Kubernetes，Helm。 云计算和基础设施：\n对主流云服务提供商（如 AWS、Azure、Google Cloud）的基础设施和服务有深入了解。 能够进行云原生架构设计和实施，包括使用云原生服务和技术。 自动化和编程能力：\n精通至少一种编程语言（如 Python、Go、Java 等），能够编写脚本和工具来实现自动化。 对基础架构即代码（IaC）工具有熟练掌握，例如 Terraform、Ansible 等。 监控和日志管理：\n熟悉监控和日志管理工具，能够建立完善的监控系统和日志分析平台。 掌握应用性能监控和故障排除技术。如 Prometheus，Grafana，ELK Stack。 安全和合规性：\n了解容器和云安全最佳实践，能够设计安全的部署架构。 理解数据隐私和合规性要求，能够实施符合法规的解决方案。如 HashiCorp Vault，Chef InSpec。 持续学习和技术更新：\n持续关注新技术和行业趋势，参与培训和研讨会，多于同行交流。 不断学习和提升自身的技能，保持适应快速变化的技术环境。 团队协作和沟通能力：\n良好的团队合作和沟通能力，能够与开发团队、运维团队和其他利益相关者有效地协作。 熟练使用版本控制系统和协作工具。 问题解决和创新思维：\n具备快速定位和解决问题的能力，善于思考创新解决方案。 鼓励并参与团队中的持续改进和创新活动。 业务理解和领导能力（对于高级岗位）：\n具备对业务需求的理解和洞察，能够为业务提供技术支持和解决方案。 如果担任领导职务，需要具备领导团队和推动项目的能力。 只有通过不断学习和拓展技能，保持对最新技术的了解，注重团队协作和创新，才能够在市场不好，AI崛起的环境中继续保持竞争力。\n最后，希望大家都能在 2024 年工作顺利，不被裁员；裁员 N+x (x\u0026gt;=1)，并顺利过渡到下一份更好的工作 💪\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-04-08","externalUrl":null,"permalink":"/posts/devops-skills-2024/","section":"Posts","summary":"本文介绍了在2024年DevOps工程师需要掌握的关键技能和能力，以应对裁员潮和人工智能的挑战，保持竞争力。","title":"2024年如何保持竞争力：DevOps工程师的关键技能","type":"posts"},{"content":" What are Reusable Workflows # If you\u0026rsquo;ve used GitHub Actions, you must know about the Reusable Workflows feature. It allows you to define workflows and reuse them across multiple repositories.\nGitHub Actions is GitHub\u0026rsquo;s own CI/CD tool. Other mainstream CI/CD tools include Jenkins, Azure DevOps, Travis CI, etc.\nWith GitHub Reusable Workflows, you can define common workflows in a separate Git repository and then reference these workflows in other repositories without having to redefine them in each repository. The benefits include:\nConsistency: Ensures your team or organization uses the same standardized workflows across different repositories, maintaining consistency. Maintainability: Changes or updates to the workflow only need to be made in one place, eliminating the need to modify code in multiple repositories. Reusability: Separates common workflows, allowing reuse in any project as needed, improving code reusability and maintainability. In general, GitHub Reusable Workflows makes managing and organizing workflows in GitHub Actions more flexible and maintainable.\nHow to Use Reusable Workflows # GitHub Reusable Workflows allows you to create a workflow in .github or another repository and then call that workflow from other repositories.\nHere are the general steps for using GitHub Reusable Workflows:\nCreate a Reusable Workflow:\nCreate a new repository under your GitHub account to store your reusable workflows. Create a directory named .github/workflows in the repository (if it doesn\u0026rsquo;t exist). Create a YAML file in this directory to define your workflow. Define a Parameterized Workflow (Optional):\nIf you want your workflow to be parameterized, you can define parameters using the inputs keyword in your workflow. Commit the Workflow to the Repository:\nCommit your created workflow YAML file to the repository, ensuring it\u0026rsquo;s located in the .github/workflows directory. Use the Workflow in Other Repositories:\nOpen the other repository where you want to use this workflow. Create a YAML file in the .github/workflows directory that points to the YAML file of your previously created reusable workflow. Commit Changes and Trigger the Workflow:\nCommit the changes to the repository and push them to the remote repository. GitHub will automatically detect the new workflow file and trigger workflow execution based on triggers (e.g., push, pull_request, etc.). Here\u0026rsquo;s a simple example demonstrating how to create and use a reusable workflow:\nSuppose you create a workflow file named build.yml in the .github/workflows directory of the reuse-workflows-demo repository to build your project. The file contents are as follows:\nIf not in the .github/workflows directory, you will encounter this error invalid value workflow reference: references to workflows must be rooted in '.github/workflows'\nname: Build on: workflow_call: inputs: target: required: true type: string default: \u0026#34;\u0026#34; jobs: build: strategy: matrix: target: [dev, stage, prod] runs-on: ubuntu-latest steps: - name: inputs.target = ${{ inputs.target }} if: inputs.target run: echo \u0026#34;inputs.target = ${{ inputs.target }}.\u0026#34; - name: matrix.targe = ${{ matrix.target }} if: matrix.target run: echo \u0026#34;matrix.targe = ${{ matrix.target }}.\u0026#34; Then, in the .github/workflows directory of your other repository, you can create a workflow build.yml pointing to this file, for example:\nname: Build on: push: pull_request: workflow_dispatch: jobs: call-build: uses: shenxianpeng/reuse-workflows-demo/.github/workflows/build.yml@main with: target: stage For more real-world examples of Reusable Workflows, refer to the .github repository under the cpp-linter organization.\n# cpp-linter/.github/.github/workflows . ├── codeql.yml ├── main.yml ├── mkdocs.yml ├── pre-commit.yml ├── py-coverage.yml ├── py-publish.yml ├── release-drafter.yml ├── snyk-container.yml ├── sphinx.yml └── stale.yml 7 Best Practices for GitHub Reusable Workflows: # Modular Design: Break down workflows into small, reusable modules, each focusing on a specific task. This improves workflow flexibility and maintainability, making them easier to reuse and combine. Parameterized Configuration: Allow workflows to accept parameters for configuration with each use. This makes workflows more versatile, adapting to different project and environment needs. Version Control: Ensure your reusable workflows are version-controlled and updated regularly to reflect changing project requirements. Use GitHub branches or tags to manage different workflow versions and easily switch or roll back when needed. Documentation and Comments: Provide clear documentation and comments for workflows to help other developers understand their purpose and steps. Comment on the purpose and implementation details of key steps so others can easily understand and modify the workflow. Security: Carefully handle workflow files containing sensitive information (such as credentials, keys, etc.) to prevent accidental leaks. Store sensitive information in GitHub Secrets and use Secrets to access this information in the workflow. Testing and Validation: Test and validate reusable workflows before introducing them to projects to ensure they integrate and execute correctly. Simulate and test workflows in a separate test repository to ensure correctness and reliability. Performance Optimization: Optimize workflow performance, minimizing unnecessary steps and resource consumption to ensure workflows complete tasks within a reasonable time and minimize system resource usage. Following these best practices helps you better utilize GitHub Reusable Workflows and provides your projects and teams with more efficient and maintainable automated workflows.\nDifferences and Similarities between Reusable Workflows and Jenkins Shared Library # Finally, let\u0026rsquo;s discuss my understanding and summary of GitHub Reusable Workflows and Jenkins Shared Library. There are similarities, but also differences.\nSimilarities:\nReusability: Both aim to provide a mechanism to define common automated workflows as reusable components, sharing and reusing them across multiple projects. Organization: Both help better organize and manage automated workflows, making them easier to maintain and update. Parameterization: Both support parameterization, allowing workflows to be customized and configured in different contexts as needed. Differences:\nPlatform: Reusable Workflows are part of GitHub Actions, while Shared Library is a Jenkins feature. They run on different platforms with different ecosystems and working principles. Syntax: Reusable Workflows use YAML syntax to define workflows, while Shared Library uses Groovy to define shared libraries. Ease of Use: Reusable Workflows are relatively simple to use on the GitHub platform, especially for projects already hosted on GitHub. Shared Library requires configuring the Jenkins server and related plugins and requires some understanding of the Jenkins build process. In summary, although GitHub Reusable Workflows and Jenkins Shared Library both aim to provide reusable automated workflows and share some similarities, they have significant differences in platform, syntax, and ease of use.\nThe specific choice depends on your needs, workflow, and the platform you need to use.\n","date":"2024-03-25","externalUrl":null,"permalink":"/en/posts/reusable-workflows/","section":"Posts","summary":"This article introduces the Reusable Workflows feature of GitHub Actions, helping developers and teams manage and reuse CI/CD processes more efficiently.","title":"Must-Know GitHub Action Feature - Reusable Workflows","type":"posts"},{"content":"最近看到一篇非常有信息量的关于人工智能、云原生、开源的趋势报告，出自于GitHub，翻译并分享给大家，以下是报告全文。\n英文原文在这里：https://github.blog/2023-11-08-the-state-of-open-source-and-ai/?utm_source=banner\u0026amp;utm_medium=github\u0026amp;utm_campaign=octoverse\n新技术成为主流意味着什么？\nGit 于 2005 年首次发布，当我们创建 GitHub 时，Git 还是一个新的开源版本控制系统。如今，Git 已成为现代开发人员体验的基本元素 — 93% 的开发人员使用它在各地构建和部署软件。\n2023 年，GitHub 数据凸显了另一种技术如何迅速开始重塑开发者体验：人工智能。去年，越来越多的开发人员开始使用人工智能，同时也尝试构建人工智能驱动的应用程序。 Git 从根本上改变了当今的开发人员体验，现在人工智能正在为软件开发的下一步奠定基础。\n在 GitHub，我们知道开发人员喜欢边做边学，开源可以帮助开发人员更快地采用新技术，将其集成到他们的工作流程中，并构建下一代技术。开源还为几乎所有现代软件提供动力——包括大部分数字经济。当我们探索技术如何成为主流时，GitHub 在弥合开源技术实验与广泛采用之间的差距方面继续发挥着关键作用，这些技术支撑着我们软件生态系统的基础。\n在今年的报告中，我们将研究围绕人工智能、云和 Git 的开源活动如何改变开发人员体验，并日益增强对开发人员和组织的影响。\n我们发现了三大趋势:\n开发人员正在大量使用生成式人工智能进行构建。 我们看到越来越多的开发人员尝试使用 OpenAI 和其他 AI 参与者的基础模型，开源生成式 AI 项目甚至会在 2023 年进入按贡献者数量计算的前 10 个最受欢迎的开源项目。几乎所有开发人员 (92%) 都在使用或试验借助 AI 编码工具，我们期望开源开发人员能够在 GitHub 上推动下一波 AI 创新浪潮。 开发人员正在大规模运营云原生应用程序。 我们看到使用基于 Git 的基础设施即代码 (IaC) 工作流程的声明性语言有所增加，云部署的标准化程度更高，开发人员使用 Dockerfile 和容器、IaC 和其他云原生的速度急剧增加技术。 2023 年首次开源贡献者数量最多。 我们继续看到商业支持的开源项目在首次贡献者和总体贡献中占据最大份额，但今年，我们还看到生成式 AI 项目进入了首次贡献者最受欢迎的前 10 个项目。我们还看到 GitHub 上的私人项目显著增长，同比增长 38%，占 GitHub 上所有活动的 80% 以上。 在 GitHub 上构建的全球开发者社区 # 在全球范围内，开发人员正在使用 GitHub 来构建软件并进行比以往更多的协作，而且涉及公共和私人项目。这不仅证明了 Git 在当今开发者体验中的基础价值，也展示了全球开发者社区使用 GitHub 构建软件的情况。\n美国拥有 2020 万开发者，过去一年开发者增长 21%，继续拥有全球最大的开发者社区。\n但自 2013 年以来，我们不断看到其他社区在整个平台上实现了更多增长，我们预计这种情况会持续下去。 GitHub 上开发人员的全球分布显示了哪些地区拥有最多的开发人员。\n亚太地区、非洲、南美洲和欧洲的开发者社区逐年扩大，其中印度、巴西和日本处于领先地位。\n预测未来五年排名前 10 的开发者社区 # 为了了解哪些开发者社区将在未来五年内增长最快，我们根据当前的增长率进行了预测。在此标题下，我们预计到 2027 年印度将取代美国成为 GitHub 上最大的开发者社区。\n亚太地区发展最快的开发者社区 # 我们继续看到，在印度、日本和新加坡等经济中心的推动下，亚太地区出现了可观的增长。\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 表 1：2023 年开发者总数增长，较 2022 年增长百分比。\n印度的开发者社区继续实现同比大幅增长 # 在去年的 Octoverse 中，我们预测印度的开发者总数将超过美国。这仍然有望发生。印度的开发者人数同比增长 36%，2023 年有 350 万新开发者加入 GitHub。\n作为联合国支持的数字公共产品联盟的一部分，印度一直在利用开放材料（从软件代码到人工智能模型）建设数字公共基础设施，以改善数字支付和电子商务系统。以下是印度开发人员在 GitHub 上构建并贡献的开源软件 (OSS) 项目列表。\n新加坡今年是亚太地区开发者人数增长最快的国家 # 并且以开发者占总人口的比例最高而位居全球第一。新加坡国立大学计算机学院将 GitHub 纳入其课程，高增长也可能归因于该国在东南亚的监管重要性。\n由于对技术和初创公司的投资，我们还可能在明年看到日本的开发人员持续增长。\n非洲发展最快的开发者社区 # 非洲地区拥有世界上增长最快的人口和不断增加的开发人员，已被认为是有前途的科技公司中心。（例如，在肯尼亚，小学和中学必须教授编程。）\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 表 2：2023 年开发者总数增长，较 2022 年增长百分比。\n尼日利亚是 OSS 采用和技术投资的热点，其 45% 的同比增长率（全球增幅最高）反映了这一点。GitHub 上还有至少 200 个由尼日利亚开发者制作的项目集合，可以在“非洲制造”集合下找到。\n南美洲发展最快的开发者社区 # 南美洲的开发者增长率与亚太和非洲一些增长最快的开发者社区相当。\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 表 3：2023 年开发者总数增长，较 2022 年增长百分比。\n2023年，巴西的开发者人数是该地区最多的，并继续以两位数增长，同比增长30%。此前，巴西的私人和公共组织持续投资。查看巴西开发人员在 GitHub 上创建和贡献的 OSS 项目列表。\n我们还看到阿根廷和哥伦比亚的持续增长，这两个国家在过去几年中已成为组织的热门投资目标。\n欧洲发展最快的开发者社区 # 整个欧洲的社区开发人员总数继续增加，但他们的发展现在更接近于美国的总体发展，因为南美洲、非洲和亚太地区的社区增长超过了他们。\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 值得注意的是，法国的增长是在政府推动吸引更多科技初创企业之后实现的。我们还看到西班牙和意大利的增长有所上升，这说明这两个地区为支持其国内技术市场所做的努力。\n2023 年生成式 AI 爆发式增长 # 虽然生成式人工智能在 2023 年引起了轰动，但对于 GitHub 上的开发者来说，它并不是全新的。事实上，过去几年我们已经在 GitHub 上看到了几个生成式 AI 项目的出现，以及许多其他专注于 AI 的项目。\n但 2023 年的 GitHub 数据反映了这些人工智能项目如何从更面向专业的工作和研究发展到更主流的采用，开发人员越来越多地使用预先训练的模型和 API 来构建由人工智能驱动的生成应用程序。\n就在去年过半的时候，我们看到 2023 年的生成式 AI 项目数量是 2022 年全年的两倍多。 我们知道这只是冰山一角。\n随着越来越多的开发人员尝试这些新技术，我们期望他们能够推动软件开发中的人工智能创新，并继续将该技术快速发展的功能带入主流。\n开发人员越来越多地尝试人工智能模型。 在过去的几年里，我们看到开发人员使用 tensorflow/tensorflow、pytorch/pytorch 等机器学习库构建项目，而现在我们看到更多的开发人员尝试使用 AI 模型和LLM（例如 ChatGPT API）。\n保持聪明： 我们预计企业和组织也将利用预先训练的人工智能模型，特别是随着越来越多的开发人员熟悉如何使用它们进行构建。\n开源人工智能创新多种多样，顶级人工智能项目由个人开发者拥有。 分析 GitHub 上排名前 20 的开源生成式 AI 项目，其中一些顶级项目归个人所有。这表明 GitHub 上的开源项目继续推动创新，并向我们所有人展示行业的未来发展，社区围绕最令人兴奋的进步而构建。\n生成式人工智能正在推动生成式人工智能项目的个人贡献者在全球范围内大幅增长，同比增长 148%，生成式人工智能项目总数也同比增长 248%。 值得注意的是，美国、印度和日本在开发者社区中处于领先地位，其他地区（包括香港特别行政区）、英国和巴西紧随其后。\n了解生成式人工智能的开发人员数量大幅增加将对企业产生影响。 随着越来越多的开发人员熟悉构建基于人工智能的生成式应用程序，我们预计不断增长的人才库将支持寻求开发自己的基于人工智能的产品和服务的企业。\n底线： 在过去的一年里，我们看到基于基础模型（例如 ChatGPT）构建的应用程序呈指数级增长，因为开发人员使用这些 LLM 来开发面向用户的工具，例如 API、机器人、助手、移动应用程序和插件。全球开发人员正在帮助为主流采用奠定基础，而实验正在帮助组织建立人才库。\n最流行的编程语言 # 自从我们在 2019 年看到云原生开发的巨大增长以来，IaC 在开源领域也持续增长。 2023 年，Shell 和 Hashicorp 配置语言 (HCL) 再次成为开源项目中的顶级语言，这表明运维和 IaC 工作在开源领域越来越受到重视。\nHCL 采用率同比增长 36%，这表明开发人员正在为其应用程序利用基础设施。 HCL 的增加表明开发人员越来越多地使用声明性语言来指示他们如何利用云部署。 JavaScript 再次夺得第一大最受欢迎语言的桂冠，并且我们继续看到 Python 和 Java 等熟悉的语言逐年保持在前五名语言之列。\nTypeScript 越来越受欢迎。 今年，TypeScript 首次取代 Java，成为 GitHub 上 OSS 项目中第三大最受欢迎的语言，其用户群增长了 37%。 TypeScript 是一种集语言、类型检查器、编译器和语言服务于一体的语言，它于 2012 年推出，标志着渐进类型的到来，它允许开发人员在代码中采用不同级别的静态和动态类型。\n用于数据分析和操作的流行语言和框架显著增加。 T-SQL 和 TeX 等古老语言在 2023 年不断发展，这凸显了数据科学家、数学家和分析师如何越来越多地使用开源平台和工具。\n底线： 编程语言不再仅仅局限于传统软件开发领域。\n与 GitHub 上使用的总体最流行语言相比，我们发现 2023 年创建的项目中使用的最流行语言具有显著的一致性。一些值得注意的异常值包括 Kotlin、Rust、Go 和 Lua，它们在 GitHub 上的新项目中出现了更大的增长。\nRust 和 Lua 都以其内存安全性和效率而闻名，并且都可以用于系统和嵌入式系统编程，这可以归因于它们的增长。Go 最近的增长是由 Kubernetes 和 Prometheus 等云原生项目推动的。\n开发者活动是新技术采用的领头羊 # 2023 年初，我们庆祝了超过 1 亿开发者使用 GitHub 的里程碑 —— 自去年以来，我们看到 GitHub 上的全球开发者帐户数量增长了近 26%。更多的开发人员跨时区协作并构建软件。私人和公共存储库中的开发人员活动强调了哪些技术正在被广泛采用，以及哪些技术有望得到更广泛的采用。\n开发人员正在自动化更多的工作流程。 在过去的一年里，开发人员使用 GitHub Actions 分钟数增加了 169%，用于自动化公共项目中的任务、开发 CI/CD 管道等。\n平均而言，开发人员在公共项目中每天使用 GitHub Actions 的时间超过 2000 万分钟。随着 GitHub Marketplace 中的 GitHub Actions 数量在 2023 年突破 20,000 个大关，社区不断发展。 这凸显了开源社区对 CI/CD 和社区管理自动化的认识不断增强。 超过 80% 的 GitHub 贡献都贡献给私有存储库。 其中，私人项目贡献超过 42 亿美元，公共和开源项目贡献超过 3.1 亿美元。这些数字显示了通过免费、团队和 GitHub Enterprise 帐户在公共、开源和私人存储库中发生的活动的巨大规模。大量的私人活动表明了内部源代码的价值，以及基于 Git 的协作不仅有利于开源代码的质量，而且也有利于专有代码的质量。\n事实上，在最近 GitHub 赞助的一项调查中，所有开发人员都表示他们的公司至少采用了一些内部源实践，超过一半的开发人员表示他们的组织中有活跃的内部源文化。\nGitHub 是开发人员操作和扩展云原生应用程序的地方。 2023 年，430 万个公共和私有存储库使用 Dockerfile，超过 100 万个公共存储库使用 Dockerfile 来创建容器。过去几年，我们看到 Terraform 和其他云原生技术的使用量不断增加。 IaC 实践的增加也表明开发人员正在为云部署带来更多标准化。\n生成式人工智能进入 GitHub Actions。 人工智能在开发者社区中的早期采用和协作能力在 GitHub Marketplace 中的300 多个人工智能驱动的 GitHub Actions和40 多个 GPT 支持的 GitHub Actions中显而易见。开发人员不仅继续尝试人工智能，还通过 GitHub Marketplace 将其带入开发人员体验及其工作流程的更多部分。\n底线： 开发人员尝试新技术并在公共和私人存储库中分享他们的经验。这项相互依赖的工作凸显了容器化、自动化和 CI/CD 在开源社区和公司之间打包和发布代码的价值。\n开源的安全状况 # 今年，我们看到开发人员、OSS 社区和公司等通过自动警报、工具和主动安全措施更快地响应安全事件，这有助于开发人员更快地获得更好的安全结果。我们还看到 GitHub 上共享了负责任的 AI 工具和研究。\n更多的开发人员正在使用自动化来保护依赖关系。 与 2022 年相比，2023 年开源开发人员合并的针对易受攻击包的自动化Dependabot拉取请求数量增加了 60%，这凸显了共享社区对开源和安全性的奉献精神。得益于 GitHub 上的免费工具（例如 Dependabot、代码扫描和密钥扫描），开源社区的开发人员正在修复更多易受攻击的软件包并解决代码中的更多漏洞。\n**更多的开源维护者正在保护他们的分支机构。**受保护的分支为维护者提供了更多方法来确保其项目的安全性，我们已经看到超过 60% 的最受欢迎的开源项目 使用它们。自从我们今年早些时候在 GA 中在 GitHub 上推出存储库规则以来，大规模管理这些规则应该会变得更加容易。\n开发人员正在 GitHub 上分享负责任的 AI 工具。 在实验性生成人工智能时代，我们看到人工智能信任和安全工具的发展趋势。开发人员正在围绕负责任的人工智能、人工智能公平、负责任的机器学习和道德人工智能创建和共享工具。\n乔治城大学安全与新兴技术中心也在确定哪些国家和机构是值得信赖的人工智能研究的顶级生产者，并在 GitHub 上分享其研究代码。\n底线： 为了帮助 OSS 社区和项目保持更加安全，我们投资了 Dependabot、受保护的分支、CodeQL 和秘密扫描，免费向公共项目提供。2023 年的新采用指标显示这些投资如何成功帮助更多开源项目提高整体安全性。我们还看到软件开发人员和机构研究人员对创建和共享负责任的人工智能工具感兴趣。\n开源状态 # 2023 年，开发者为 GitHub 上的开源项目做出了总计 3.01 亿的贡献，其中包括 Mastodon 等热门项目到 Stable Diffusion 和 LangChain 等生成式 AI 项目。\n商业支持的项目继续吸引一些最开源的贡献，但 2023 年是生成式 AI 项目也进入 GitHub 上十大最受欢迎项目的第一年。说到生成式 AI，几乎三分之一拥有至少一颗星的开源项目都有一位使用 GitHub Copilot 的维护者。\n商业支持的项目继续领先。 2023 年，贡献者总数最大的项目获得了压倒性的商业支持。这是去年以来的持续趋势，microsoft/vscode、flutter/flutter 和 vercel/next.js 在 2023 年再次跻身前 10 名。\n生成式人工智能在开源和公共项目中快速发展。 2023 年，我们看到基于 AI 的生成式 OSS 项目，如 langchain-ai/langchain 和 AUTOMATIC1111/stable-diffusion-webui，在 GitHub 上按贡献者数量跃居榜首。越来越多的开发人员正在使用预先训练的人工智能模型构建法学硕士应用程序，并根据用户需求定制人工智能应用程序。\n开源维护者正在采用生成式人工智能。 几乎三分之一拥有至少一颗星的开源项目都有使用 GitHub Copilot 的维护者。这是我们向开源维护人员免费提供 GitHub Copilot 的计划，并表明生成式 AI 在开源领域的采用日益广泛。\n开发人员看到了组合包和容器化的好处。 正如我们之前指出的，2023 年有 430 万个存储库使用了 Docker。另一方面，Linux 发行版 NixOS/nixpkgs 在过去两年中一直位居贡献者开源项目的榜首。\n首次贡献者继续青睐商业支持的项目。 去年，我们发现，与其他项目相比，围绕流行的、商业支持的项目的品牌认知度吸引了更多的首次贡献者。这种情况在 2023 年继续出现，一些在 Microsoft、Google、Meta 和 Vercel 支持的首次贡献者中最受欢迎的开源项目。\n但社区驱动的开源项目（从 home-assistant/core 到 AUTOMATIC1111/stable-diffusion-webui、langchain-ai/langchain 和Significant-Gravitas/Auto-GPT）也见证了首次贡献者的活动激增。这表明，基础模型的开放实验增加了生成人工智能的可及性，为新的创新和更多合作打开了大门。\n2023 年，首次为开源项目做出贡献的贡献者数量最多。 新的开发人员通过 freeCodeCamp、First Contributions 和 GitHub Education 等计划参与到开源社区中。我们还看到大量开发人员参与了 Google 和 IBM 等公司的在线开源教育项目。\n**底线是：**开发人员正在为开源生成式人工智能项目做出贡献，开源维护者正在采用生成式人工智能编码工具，而公司则继续依赖开源软件。这些都表明，公开学习并分享新技术实验的开发人员提升了整个全球开发人员网络 - 无论他们是在公共存储库还是私人存储库中工作。\n总结 # 正如 Git 已成为当今开发人员体验的基础一样，我们现在也看到了人工智能成为主流的证据。仅在过去一年，就有高达 92% 的开发人员表示在工作内外使用基于人工智能的编码工具。在过去的一年里，GitHub 上托管的各种开源项目的人工智能实验也出现了爆炸性增长。\n我们给您留下三个要点：\nGitHub 是生成式 AI 的开发者平台。 生成式 AI 将于 2023 年从专业领域发展成为主流技术，开源活动的爆炸式增长反映了这一点。随着越来越多的开发人员构建和试验生成式 AI，他们正在使用 GitHub 进行协作和集体学习。 开发人员正在 GitHub 上大规模运行云原生应用程序。 2019 年，我们开始看到开源中使用基于容器的技术的开发人员数量大幅增加，并且越来越多的开发人员使用基于 Git 的 IaC 工作流程、容器编排和其他云原生技术的速度急剧增加2023 年。如此大量的活动表明开发人员正在使用 GitHub 来标准化他们将软件部署到云的方式。 GitHub 是开源社区、开发人员和公司构建软件的地方。 2023 年，我们看到私有存储库的数量增加了 38%，占 GitHub 上所有活动的 81% 以上。但我们看到开源社区持续增长，他们使用 GitHub 来构建未来并推动行业向前发展。数据显示新的开源开发人员的增加以及开放社区可能实现的快速创新步伐，很明显开源从未如此强大。 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-02-22","externalUrl":null,"permalink":"/posts/open-source-state/","section":"Posts","summary":"本文介绍了 GitHub 发布的 2023 年开源状况和人工智能的崛起报告，分析了开发者社区的增长、生成式 AI 的应用以及云原生技术的发展趋势。","title":"2023 年开源状况和人工智能的崛起（GitHub）","type":"posts"},{"content":"","date":"2024-02-22","externalUrl":null,"permalink":"/tags/oss/","section":"标签","summary":"","title":"OSS","type":"tags"},{"content":"As the creator and contributor of cpp-linter, I am pleased to announce that cpp-linter-action now supports Pull Request Review functionality starting from version v2.9.0 👏\nHere are the release notes for cpp-linter-action.\nAmong them, Bump cpp-linter from 1.6.5 to 1.7.1 by @dependabot in #191 is the most significant change. Note: the cpp-linter package is the core dependency of cpp-linter-action.\nWhat is cpp-linter-action # If you are unfamiliar with cpp-linter-action, you can check out my previous article.\nIn short, cpp-linter-action is a GitHub Action under the cpp-linter organization, designed for C/C++ code to format code, diagnose, and fix typical programming errors.\nCurrently, cpp-linter-action is used by over 500 open-source projects (closed-source projects cannot be counted), including well-known organizations or projects such as Microsoft, Linux Foundation, CachyOS, nextcloud, and Jupyter.\nIt can be said that cpp-linter-action is currently the best Linter choice for C/C++ projects on GitHub.\nAbout the Pull Request Review Feature # The newly added Pull Request Review feature allows you to provide review suggestions directly after cpp-linter-action completes the check, eliminating the need for developers to manually modify and push the detected errors to the remote repository.\nInstead, you can directly click the Commit suggestions button on GitHub to merge the suggested changes into the current pull request, saving the effort of manual modification and pushing.\nOnce all suggestions have been addressed, the GitHub Actions bot will automatically approve your pull request.\nOther Features Supported by cpp-linter-action # Besides the Pull Request Review feature, cpp-linter-action currently supports three other options: Annotations, Thread Comment, and Step Summary.\nGitHub Annotations # This displays the execution results at the specified code lines that need modification.\nThread Comment # This adds the execution results as comments on the Pull Request.\nStep Summary # This adds and displays the execution results in the GitHub Action job execution interface.\nThe Story Behind This Release # I finally found some time to sit down and write an article about the story behind this release on the evening of the eighth day of the Lunar New Year after my child fell asleep.\nThis release is particularly thanks to the contribution of cpp-linter co-creator @2bndy5. I have never met him in person, yet we have \u0026ldquo;worked\u0026rdquo; together for three years. We met because of his proactive contribution, but our communication has been limited to discussions on GitHub issues and pull requests, with only non-public information communicated via email.\nAs @2bndy5\u0026rsquo;s self-introduction states: passionate about programming, enjoys DIY electronics, and insists on writing easy-to-understand documentation. He is one of the few people I know who is technically comprehensive and has incredibly user-friendly documentation—a true geek.\nNot long ago, I received an email from him saying that due to a family emergency, he needed to take a break and had lost the motivation to code. He told me that all the changes for the Pull Request Review seemed to have passed the tests. If I wanted to lead the release, he could provide support.\nHere, I want to express my deepest sympathy and condolences 🙏 for what happened to him.\nTo continue his work, I needed to carefully read his changes and understand this functionality, but I didn\u0026rsquo;t have enough time to do so at the end of the year. I wanted to wait until the Lunar New Year holiday to catch up.\nHowever, before the Lunar New Year holiday, my child fell ill on the 27th day of the 12th lunar month and needed to be hospitalized. Because we discovered it early, the illness wasn\u0026rsquo;t serious, and the child recovered well on New Year\u0026rsquo;s Eve. We hoped that after a couple of days of observation, they could be discharged. Alas, an accident happened—the child accidentally burned their arm. As parents, we were heartbroken. This is the darkest moment I never want to recall. It was just one setback after another. So we stayed in the hospital from the 27th day of the 12th lunar month until the 6th day of the first lunar month—10 days in total. The day after the child was discharged, my wife and I both fell ill, probably because we relaxed and became exhausted.\nDuring this time, @2bndy5 completed the testing, documentation updates, and release of the Pull Request Review feature. He said in a comment that spending time coding allowed him to temporarily escape reality.\nFinally, the day before I returned to work, I had almost recovered my energy and eagerly opened GitHub to review and test the contribution code from another contributor. This contributor\u0026rsquo;s title came from an astrophysicist from the University of Dortmund in Germany, which was quite surprising.\nBut that\u0026rsquo;s the fun part of open source; it allows you to have the opportunity to freely spar with any expert at close range. If you submit code to the Linux kernel, you might even get guidance from Linus himself :)\nFinally, welcome to use any project under the cpp-linter organization and provide your valuable opinions, suggestions, or contribute code.\n———— February 17, 2024, 23:34\nPlease indicate the author and source when reprinting this article and do not use it for any commercial purposes. Follow the \u0026ldquo;DevOps攻城狮\u0026rdquo; WeChat official account.\n","date":"2024-02-17","externalUrl":null,"permalink":"/en/posts/cpp-linter-action/","section":"Posts","summary":"This article introduces the new feature of cpp-linter-action Pull Request Review, allowing developers to directly submit code modification suggestions on GitHub, improving code quality and collaboration efficiency.","title":"cpp-linter-action—Latest Version Now Supports Pull Request Review Functionality 👏","type":"posts"},{"content":"","date":"2024-01-21","externalUrl":null,"permalink":"/tags/apache/","section":"标签","summary":"","title":"Apache","type":"tags"},{"content":"本篇介绍的是大名鼎鼎的开源软件基金会 Apache 所使用的服务(Services)和工具(Tools)，这或许能帮助你打开视野，在选择工具的时候提供参考。如果你是一名 DevOps、SRE 或是 Infra 工程师，通过本篇文章内容结果帮助你更好的展示团队所提供的服务有哪些，以及窥探到 Apache Infra 是怎样组织和管理他们的。\nApache 是谁 # 如果你不太了解 Apache，下面是关于 Apache 的简要介绍。\nApache 是一个开源软件基金会（Apache Software Foundation，简称 ASF）的缩写。ASF 是一个非营利性的组织，致力于支持和发展开源软件项目。Apache 软件基金会通过提供法律、财务和基础设施支持，帮助开发者共同合作创建和维护开源软件。其中，Apache 软件基金会最为著名的项目之一是 Apache HTTP 服务器，也称为 Apache Web 服务器。此外，ASF 还托管了许多其他流行的开源项目，像 ECharts，Superset，Dubbo，Spark，Kafka 等等。\n服务与工具 # Apache Infra 团队维护着供 PMC（项目管理委员会）、项目提交者和 Apache 董事会使用的各种工具。这些工具中的部分工具只提供给有特定职责或角色的人员使用。其他工具，如显示 Apache 基础设施各部分状态的监控工具，则向所有人开放。\n为顶级项目（TLP）提供的服务 网站 电子邮件 ASF 自助服务平台 ASF 账户管理 支持 LDAP 的服务 项目孵化服务 ASF 项目工具 版本控制 问题跟踪和功能请求 将版本库与 Jira 票据集成 源码库发布者/订阅者服务 构建服务 产品命名 代码签名 代码质量 代码分发 虚拟服务器 在线投票 其他工具 DNS URL 短缩器 共享片段 机器列表 奇思妙想 为顶级项目（TLP）提供的服务 # 网站 # www.apache.org 这是 Apache 的主要网站。 Apache 项目网站检查器 会定期检查所有为顶级项目（TLP）提供的网站，并报告它们是否符合 Apache 的 TLP 网站政策。 这里只列出了几个挺有意思的连接，比如项目网址检查器，它会检查顶级项目是否有 License, Donate, Sponsors, Privacy 等正确的连接。\n电子邮件 # 所有新建电子邮件列表的申请都应通过自助服务系统进行。 电子邮件服务器 - QMail/QSMTPD ASF自助服务平台 # Infra 的目标之一是让 ASF 成员、PMC 和提交者有能力完成他们需要做的大部分工作，而无需向 Infra 求助。例如，自助服务平台提供了许多方便的工具，拥有 Apache 电子邮件地址的人（基本上是项目提交者、PMC 成员和 ASF 成员）可以使用这些工具：\n创建 Jira 或 Confluence 项目、Git 仓库或电子邮件列表（PMC 主席和 Infra 成员）。 编辑你的 ASF 身份或更新你的 ASF 密码。如果要更新密码，则需要访问与 Apache 帐户相关联的电子邮件帐户。重置密钥的有效期只有 15 分钟，因此请务必在收到密钥后立即使用。 同步 Git 仓库。 使用 OTP 计算器为 OTP 或 S/Key 一次性密码系统生成一次性密码（一般用于 PMC 成员）。 将 Confluence Wiki 空间存档并设置为只读。 不属于 ASF 社区但希望提交有关 ASF 项目产品的 Jira 票据的人员可使用该平台申请 Jira 账户。\nASF账户管理 # 如果你想更新账户详情或丢失了账户访问权限，ASF 账户管理可为你提供指导。\n支持LDAP的服务 # Infra 支持许多 LDAP 的 ASF 服务。你可以使用 LDAP 凭据登录这些服务。\n孵化项目服务 # Infra 支持孵化项目。\nInfra 孵化器介绍，展示了建立孵化项目的步骤。 项目或产品名称选择指南 ASF项目工具 # Infra 支持一系列工具和服务，以帮助项目开发和支持其应用程序及其社区，包括\n每个项目都可以在 Confluence 维基上使用专用空间。 如何管理项目维基空间的用户权限。 如何授予用户编辑维基空间的权限。 Reporter 提供有关项目的活动统计和其他信息，并提供编辑工具，帮助你撰写和提交项目的季度董事会报告。 你可以创建并运行项目博客。 你可以建立一个 Slack 频道，用于团队实时讨论。一旦你建立了 Slack 频道，Infra 就可以建立 Slack-Jira 桥接，这样你就可以在频道中收到新的或更新的 Jira 票据通知。 团队可以使用 ASFBot 通过 Internet Relay Chat (IRC) 进行并记录会议。不过，你必须按照 Apache 投票流程，在相应的项目电子邮件列表中对决策进行正式投票。 本地化工具。 Apache 发布审核工具 (RAT) 可帮助你确认所提议的产品发布符合 ASF 的所有要求。 ASF OAuth 系统为希望使用身份验证的服务提供了一个协调中心，而不会对存储敏感用户数据造成安全影响。许多 Apache 服务使用它来验证请求访问的用户是否是项目中的提交者，以及是否拥有对相关系统的合法访问权限。了解更多有关 Apache OAuth 的信息。 版本控制 # Apache 提供并由 Infra 维护代码库，Apache 项目可使用这些代码库来保证项目代码的安全、团队成员的可访问性以及版本控制。\n关于使用 【Git 的信息](https://infra.apache.org/git-primer.html)\nSVN 代码库的只读 Git 镜像 可写的 Git 代码库 Apache 与 GitHub GitHub 仓库的访问角色 关于使用 Subversion 的信息\nSubversion (SVN) 版本库 ViewVC（SVN 主版本库的浏览器界面） 问题跟踪和功能请求 # ASF 支持以下用于跟踪问题和功能请求的选项： * Jira * GitHub 问题跟踪功能\n由于历史原因，一些项目使用 Bugzilla。我们将继续支持 Bugzilla，但不会为尚未使用它的项目设置。\nApache Allura 是另一个问题跟踪选项。如果你觉得它可以满足你的项目需求，请通过 users@allura.apache.org 邮件列表直接咨询 Allura 项目。\n请参阅 issues.apache.org，查看各项目使用的问题列表。\n以下是为你的项目申请 bug 和问题跟踪器的方法。\n以下是撰写优秀错误报告的指南。\n将你的版本库与Jira票据集成 # Infra 可以为你的项目激活 Subversion 和 Git 与 Jira 票据的集成。\n源码库发布者/订阅者服务 # SvnPubSub PyPubSub 构建服务 # Apache 支持并模拟持续集成和持续部署（或 CI/CD）。ASF 构建和支持的服务页面提供了有关 ASF 提供和/或支持的 CI 服务的信息和链接。\n其他可考虑的工具:\nTravis CI Appveyor 产品命名 # 请参阅产品名称选择指南\n代码签名 # 数字证书源码库发布者/订阅者服务\n请求访问 Digicert 代码签名服务 使用 Digicert 通过苹果应用程序商店发布\n关于代码签名和发布的更多信息\n代码质量 # SonarCloud 是一款代码质量和安全工具，开源项目可免费使用。它允许对代码质量进行持续检查，因此你的项目可以通过对代码进行静态分析来执行自动审查，以检测 20 多种编程语言中的错误、代码气味和安全漏洞。\n你可以检查许多 Apache 项目软件源的状态。\n有关在 ASF 项目中使用 SonarCloud 的指导，请点击此处。\n代码分发 # 使用 ASF Nexus Repository Manager 浏览和审查 ASF 项目的代码发布。\n发布 # 当前发布 历史发布存档 Rsync 分发镜像 Nexus 虚拟服务器 # Infra 可为项目提供 Ubuntu 虚拟机。\n虚拟机策略 申请虚拟机的流程 使用nightlies.a.o # nightlies.a.o 如其名称所示，是一种 \u0026ldquo;短期 \u0026ldquo;存储解决方案。请参阅 nightlies 使用政策。\n在线投票 # 项目可使用 Apache STeVe 投票系统实例（不使用时离线）。工具名称指的是作为投票选项之一的单一可转移投票系统。为 Infra 开立 Jira 票单，以便为你的项目使用 STeVe 做好准备。\n其他工具 # DNS # Infra 管理在 Namecheap 注册的 ASF DNS。\nURL短缩器 # URL 短缩器\n分享代码片段 # Paste 是一项服务，ASF 成员可以发布代码片段或类似的文件摘要，以说明代码问题或供重复使用，通常是与其他项目成员共享。你可以以纯文本形式发布内容，也可以以多种编码和格式发布内容。\n机器列表 # 主机密钥和指纹\n奇思妙想 # Apache Whimsy 自称为 \u0026ldquo;以易于使用的方式提供有关 ASF 和我们项目的组织信息，并帮助 ASF 实现企业流程自动化，使我们众多的志愿者能够更轻松地处理幕后工作\u0026rdquo;。\nWhimsy 有许多对项目管理委员会和个人提交者有用的工具，例如提交者搜索。\n总结 # 以上就是 Apache 开源软件基金会用到的一些服务和工具，总体的感觉就是写的很全面，并且每个连接都对应着完整的文档，这也是这种开源协作方式最重要的地方：通读文档。另外这种组织方式对于想参与的人来说很清晰，值得学习。\n另外我们看到了一些常见的服务和工具，像是 Jira，Confluence，Slack，Git，GitHub，SonarCloud，Digicert，Nexus。 也看到了不太常见的工具，像在 CI 工具上的选择是 Travis CI 和 Appveyor。 还有一些有意思的工具，像是 URL缩短器，代码片段分享，奇思妙想等工具，从访问的网址来看它们是部署在内部。 由于历史原因，还有项目还在使用 Bugzilla 和 SVN 等工具。 以上 Apache 所使用的服务和工具，借用理财中风险评估等级划分是属于稳健型，而非一味的追求“新”、“开源”和“免费”。\n为了文章的可读性，本文做了部分修改和删减。原文在这里。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-01-21","externalUrl":null,"permalink":"/posts/apache-services-and-tools/","section":"Posts","summary":"本篇介绍的是大名鼎鼎的开源软件基金会 Apache 所使用的服务(Services)和工具(Tools)，这或许能帮助你打开视野，在选择工具的时候提供参考。","title":"看看顶级的开源组织都在用哪些服务和工具","type":"posts"},{"content":"时间过得很快，2023 年转瞬即逝。如果不记录下自己在这一年里发生的事情，过不了多久就很难回想起来这一年都发生过什么。\n因此按照惯例还是要先回顾 2023 年，然后展望 2024 年。\n回顾 2023 # 回顾这一年，我想用三个关键词来形容生活、工作以及业余。\n生活中，是“奶爸” # 从孩子一出生就是我和我的队友独立带娃，白天我上班，她带娃；晚上下班我火速回去接班、陪玩、家务直到孩子睡觉。周末至少有一天会带孩子出去溜达。\n带娃的过程中会觉得辛苦，漫长，然而回顾这一年会发现孩子一眨眼就长大了。年初时候还只会爬、扶站；到了年末孩子已经能爬桌子能跑了，有消耗不完的能量。\n接下来就是流水账的记录了，记录一下平凡一年中的小事件：\n四月：跟公司活动去了一趟发现王国、爬了童牛岭、玩了三国杀、在渔公码头吃了海鲜自助 五月：在星海和庄河分别给孩子办了生日宴 九月：第一次带娃住酒店，去了金石滩希尔顿住了一晚、吃了一顿酒店自助早餐 十月：国庆回庄河住了一晚，吃了一顿家庭烧烤；回来去了一趟横山寺，吃了一顿自助素食餐；假期踢了一场球；办了大连森林动物园的卡开始打卡动物园 十一月：去了大连森林动物园、旅顺太阳沟、大连自然博物馆、参加了公司在钱库里的聚餐 十二月：收到非常突发的消息（以后有机会再细说）、参加组内团建、大连博物馆、陪爸爸住院检查、逛旅顺博物馆 工作中，是“最佳实践”的坚守着 # 这一年我依旧坚持 DevOps 最佳实践：\n继续拓展 Ansible Playbook 仓库的应用范围 创建了 Infrastructure as Code 仓库，用 Code 来管理 Infra 创建了 Docker Image 仓库来容器化产品，在 Bitbucket 仓库中应用了很多我的 DevOps 实践 承担了更多产品的 Build 和 Release 工作 提出并实施了 Software Supply Chain Security 想法 在团队内部分享 DevOps 实践，以 Jenkins 共享库的形式分享给其他团队 业余时间，是“开源和写作”的爱好者 # 和去年一样，业余时间我还做同样的事情：\n开源项目 cpp-linter-action 截至目前已经有 400 多个用户在使用，包括微软、Jupyter、Waybar、libvips、desktop、chocolate-doom 等知名开源项目，希望之后还有更多的想法去改进 commit-check 目前处在初期发展阶段，目前的用户很少，还需要优化它以及引入更多好的想法 写作 - 2023 年我一共更新了 20 篇博客 + 6 篇公众号文章，距离年初设定的 24（博客）+ 12（公众号）打了一些折扣 学习 - 加入的知名社区里学习和贡献这件事没有做到，有了孩子之后确实没有多少业余时间了，希望 2024 娃能早睡我能有更多自己的时间 展望 2024 # 每年的愿望或是 Flag 还是要设置的，万一实现了呢？\n希望能顺利的度过职业发展期，希望会是一个崭新的、充满挑战的开始 家人身体健康，工作和家庭的平衡，带她们多出去走走 补足在 DevOps 一些方面的不足，争取参与到实际的项目中来获得提升 以教促学，持续在博客、公众号等分享，坚持个人成长 保持运动，不论是跑步还是踢球，把体重降下来 过去的年终总结 # 2022 年终总结 2020 年终总结 2019 年终总结 2018 从测试转开发\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-12-31","externalUrl":null,"permalink":"/misc/2023-summary/","section":"Miscs","summary":"\u003cp\u003e时间过得很快，2023 年转瞬即逝。如果不记录下自己在这一年里发生的事情，过不了多久就很难回想起来这一年都发生过什么。\u003c/p\u003e","title":"2023 年终总结","type":"misc"},{"content":"如果你使用过 GitHub 发布过项目，你会知道 GitHub 可以自动生成 Release Notes。\n就像这样 GitHub 自动生成的 Release Notes。\n这个截图里的 Release Notes 内容很少，看起来还很清晰。但如果内容很多，以 Jenkinsci 组织下的 configuration-as-code-plugin 项目为例，可以看出来这里的 Release Notes 中的内容是按照标题进行分类的，假如这些内容混在一起将会非常糟糕的体验。(不要误以为这是手动进行分类的，程序员才不愿意干这种事😅)\n本文将分享针对需要对 GitHub Release Notes 的内容按照标题进行自动分类的两种方式。\n方式一：使用 GitHub 官方提供的功能 # 方式一是通过 GitHub 提供的功能对 Release Notes 进行自动分类，即在仓库下面创建配置文件 .github/release.yml。这个功能与 GitHub 的 Issue Template 和 Pull Request Template 类似。具体的配置选项可以参考官方文档\n以下我是在 commit-check-action 项目的配置\nchangelog: exclude: labels: - ignore-for-release categories: - title: \u0026#39;🔥 Breaking Changes\u0026#39; labels: - \u0026#39;breaking\u0026#39; - title: 🏕 Features labels: - \u0026#39;enhancement\u0026#39; - title: \u0026#39;🐛 Bug Fixes\u0026#39; labels: - \u0026#39;bug\u0026#39; - title: \u0026#39;👋 Deprecated\u0026#39; labels: - \u0026#39;deprecation\u0026#39; - title: 📦 Dependencies labels: - dependencies - title: Other Changes labels: - \u0026#34;*\u0026#34; 针对上面的示例，在添加了 .github/release.yml 配置文件之后，当再次生成 Release Notes 时就会自动将其内容进行自动归类（下图中的标题 📦 Dependencies 是自动添加的）\n方式二：使用 Release Drafter # 方式二是使用 Release Drafter，即在仓库创建配置文件 .github/release-drafter.yml。\n从 Release Drafter 项目提供的配置参数可以看出来它提供的功能更多，使用也更加复杂。另外它还支持将配置文件放到组织下的中央仓库 .github 来实现统一的配置、并将其共享给其他仓库。\n目前方式一 .github/release.yml 不支持通过中央仓库 .github 来实现统一的配置，详见这个讨论。\n这里还以 jenkinsci/configuration-as-code-plugin 为例看到它的 .github/release-drafter.yml 的配置。\n_extends: .github 这个配置的 _extends: .github 表示从中央仓库 .github/.github/release-drafter.yml 继承过来的配置。\n# Configuration for Release Drafter: https://github.com/toolmantim/release-drafter name-template: $NEXT_MINOR_VERSION tag-template: $NEXT_MINOR_VERSION # Uses a more common 2-digit versioning in Jenkins plugins. Can be replaced by semver: $MAJOR.$MINOR.$PATCH version-template: $MAJOR.$MINOR # Emoji reference: https://gitmoji.carloscuesta.me/ # If adding categories, please also update: https://github.com/jenkins-infra/jenkins-maven-cd-action/blob/master/action.yaml#L16 categories: - title: 💥 Breaking changes labels: - breaking - title: 🚨 Removed labels: - removed - title: 🎉 Major features and improvements labels: - major-enhancement - major-rfe - title: 🐛 Major bug fixes labels: - major-bug - title: ⚠️ Deprecated labels: - deprecated - title: 🚀 New features and improvements labels: - enhancement - feature - rfe - title: 🐛 Bug fixes labels: - bug - fix - bugfix - regression - regression-fix - title: 🌐 Localization and translation labels: - localization - title: 👷 Changes for plugin developers labels: - developer - title: 📝 Documentation updates labels: - documentation - title: 👻 Maintenance labels: - chore - internal - maintenance - title: 🚦 Tests labels: - test - tests - title: ✍ Other changes # Default label used by Dependabot - title: 📦 Dependency updates labels: - dependencies collapse-after: 15 exclude-labels: - reverted - no-changelog - skip-changelog - invalid template: | \u0026lt;!-- Optional: add a release summary here --\u0026gt; $CHANGES replacers: - search: \u0026#39;/\\[*JENKINS-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[JENKINS-$1](https://issues.jenkins.io/browse/JENKINS-$1) - \u0026#39; - search: \u0026#39;/\\[*HELPDESK-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[HELPDESK-$1](https://github.com/jenkins-infra/helpdesk/issues/$1) - \u0026#39; # TODO(oleg_nenashev): Find a better way to reference issues - search: \u0026#39;/\\[*SECURITY-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[SECURITY-$1](https://jenkins.io/security/advisories/) - \u0026#39; - search: \u0026#39;/\\[*JEP-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[JEP-$1](https://github.com/jenkinsci/jep/tree/master/jep/$1) - \u0026#39; - search: \u0026#39;/CVE-(\\d{4})-(\\d+)/g\u0026#39; replace: \u0026#39;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-$1-$2\u0026#39; - search: \u0026#39;JFR\u0026#39; replace: \u0026#39;Jenkinsfile Runner\u0026#39; - search: \u0026#39;CWP\u0026#39; replace: \u0026#39;Custom WAR Packager\u0026#39; - search: \u0026#39;@dependabot-preview\u0026#39; replace: \u0026#39;@dependabot\u0026#39; autolabeler: - label: \u0026#39;documentation\u0026#39; files: - \u0026#39;*.md\u0026#39; branch: - \u0026#39;/docs{0,1}\\/.+/\u0026#39; - label: \u0026#39;bug\u0026#39; branch: - \u0026#39;/fix\\/.+/\u0026#39; title: - \u0026#39;/fix/i\u0026#39; - label: \u0026#39;enhancement\u0026#39; branch: - \u0026#39;/feature\\/.+/\u0026#39; body: - \u0026#39;/JENKINS-[0-9]{1,4}/\u0026#39; 以上是中央仓库的 .github/.github/release-drafter.yml 配置，可以看到 Jenkins 官方使用了很多特性，比如模板、替换、自动加 label 等，需要在通读 Release Drafter 的文档之后能更好的理解和使用。\n总结 # 以上两种方式都可以帮助你在自动生成 Release Notes 的时候自动进行标题分类，但两者有一些如下差别，了解它们可以帮助你更好的进行选择。\nGitHub 官方提供的方式更容易理解和配置，可以满足绝大多数的项目需求。主要的不足是不支持从中央仓库 .github 中读取 .github/release.yml。 Release Drafter 提供了更为强大的功能，比如模板、排序、替换、自动对 pull request 加 label 等等，尤其是可以通过中央仓库来设置一个模板，其他项目来继承其配置。 如果是大型的开源组织，Release Drafter 是更好的选择，因为它提供了强大的功能以及支持继承中央仓库配置。如果是个人项目，GitHub 官方提供的方式基本满足需求。\n以上就是我对两个生成 GitHub Release Notes 并进行自动分类的分享。\n如果有任何疑问或建议欢迎在评论区留言。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-12-27","externalUrl":null,"permalink":"/posts/automatic-categorize-release-notes/","section":"Posts","summary":"本文将分享针对需要对 GitHub Release Notes 的内容按照标题进行自动分类的两种方式。","title":"如何把 GitHub Release Notes 按照 New features、Bug Fixes ... 进行自动分类","type":"posts"},{"content":"Sometimes you don\u0026rsquo;t want Jenkins pipeline failed for a specific error occurs. so you can use catchError to catch error and update stage or build result to SUCCESSFUL or UNSTABLE or FAILURE (if you want)\nHere is the Jenkinsfile of pipeline\npipeline { agent { node { label \u0026#39;linux\u0026#39; } } stages { stage(\u0026#39;Catch Error\u0026#39;) { steps { catchError(buildResult: \u0026#39;UNSTABLE\u0026#39;, stageResult: \u0026#39;UNSTABLE\u0026#39;, message: \u0026#39;abc: command not found\u0026#39;) { sh \u0026#34;abc\u0026#34; } } } } } Here is the output of pipeline\n17:14:07 [Pipeline] Start of Pipeline 17:14:08 [Pipeline] node 17:14:08 Running on linux in /agent/workspace/Stage-Job/catch-error 17:14:08 [Pipeline] { 17:14:08 [Pipeline] stage 17:14:08 [Pipeline] { (Catch Error) 17:14:08 [Pipeline] catchError 17:14:08 [Pipeline] { 17:14:08 [Pipeline] sh 17:14:08 + abc 17:14:08 /agent/workspace/Stage-Job/catch-error@tmp/durable-303b03ca/script.sh: line 1: abc: command not found 17:14:08 [Pipeline] } 17:14:08 ERROR: abc: command not found 17:14:08 ERROR: script returned exit code 127 17:14:08 [Pipeline] // catchError 17:14:08 [Pipeline] } 17:14:08 [Pipeline] // stage 17:14:08 [Pipeline] } 17:14:08 [Pipeline] // node 17:14:08 [Pipeline] End of Pipeline 17:14:08 Finished: UNSTABLE 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-12-15","externalUrl":null,"permalink":"/en/posts/jenkins-catch-error/","section":"Posts","summary":"Introducing the \u003ccode\u003ecatchError\u003c/code\u003e step in Jenkins pipeline to handle specific errors without failing the entire build, allowing for more flexible error management.","title":"How to make Jenkins pipeline not fail if a specific error occurs","type":"posts"},{"content":" Why Software Supply Chain Security is important? # Software supply chain security is the act of securing the components, activities, and practices involved in creating software.\nAttacks in the software supply chain have become more and more frequent in recent years, SonaType reported more than 700% of attacks in open-source software from 2019 to 2022.\nIn this Google Security Blog, there are many real examples of software supply chain attacks that pose growing threats to users and Google proposed a solution called SLSA in 2021.\nAlso, some well-known organizations such as Linux Foundation and CNCF have created standards and tools to address the issue of how to produce trusted software and attestations.\nBased on this background, many organizations want to incorporate best practices from the open-source community into our CICD pipeline.\nHow to adopt Supply Chain Security for GitHub and Non-GitHub projects # Next, I will show you how to adopt on both GitHub and Rocket Bitbucket as an example to show you how we integrate software supply chain security\nGitHub projects # On GitHub, the easiest and most popular option is to use slsa-github-generator, a tool provided by the official slsa-framework for native GitHub projects to create attestations during the build process and upload signed attestations to Rekor a transparency log system created by Sigstore. Here is the demo reposistory for reference.\nBefore installing your product package, the user can download the package and verify the provenance file at the end of .intoto.jsonl first, then run the following command manually or in their CI pipeline to verify whether the artifact is tampered with or not\nbash-4.4$ slsa-verifier verify-artifact test-1.0.0-py3-none-any.whl --provenance-path test-1.0.0-py3-none-any.whl.intoto.jsonl --source-uri github.com/shenxianpeng/slsa-provenance-demo Verified signature against tlog entry index 49728014 at URL: https://rekor.sigstore.dev/api/v1/log/entries/24296fb24b8ad77af7063689e8760fd7134f37e17251ec1d5adc16af64cb5cb579493278f7686e77 Verified build using builder \u0026#34;https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.9.0\u0026#34; at commit fb7f6df9f8565ed6fa01591df2af0c41e5573798 Verifying artifact test-1.0.0-py3-none-any.whl: PASSED PASSED: Verified SLSA provenance Non-GitHub projects # However, there are many organizations\u0026rsquo; codes are hosted on Non-GitHub SCM, so you can use the Witness, a tool from CNCF in-toto, which can help us generate and verify attestations.\nIt’s easy to scale Witness to your products, just integrate witness command into the existing build command it will generate proof of the software build and release execution process and can be verified.\nYou can follow this demo to integrate with witness, then will generate the build package along with attestations file, policy-signed.json file, and a public key.\nBefore user installing your product package, they can run the following command manually or in their CI pipeline to verify whether the artifact is tampered or not.\nwitness verify -f dist/witness_demo-1.0.0-py3-none-any.whl -a witness-demo-att.json -p policy-signed.json -k witness-demo-pub.pem INFO Using config file: .witness.yaml INFO Verification succeeded INFO Evidence: INFO 0: witness-demo-att.json ","date":"2023-12-02","externalUrl":null,"permalink":"/en/posts/supply-chain-security/","section":"Posts","summary":"This article introduces how to implement software supply chain security using SLSA and Witness for both GitHub and non-GitHub projects, enhancing the security of software development and deployment processes.","title":"How to adopt Supply Chain Security for GitHub and Non-GitHub projects","type":"posts"},{"content":"","date":"2023-12-02","externalUrl":null,"permalink":"/en/tags/witness/","section":"Tags","summary":"","title":"Witness","type":"tags"},{"content":"Due to the increasing frequency of attacks targeting software supply chains in recent years, Google proposed a solution in 2021: Supply chain Levels for Software Artifacts (\u0026ldquo;SLSA\u0026rdquo;).\nThis article will describe how to generate and verify the provenance of software artifacts outside the GitHub ecosystem to improve your project\u0026rsquo;s SLSA Level.\nWitness is a pluggable software supply chain risk management framework that automates, standardizes, and verifies software artifact provenance. It is a CNCF project under CNCF, originally authored by Testifysec and later donated to in-toto.\nWhat is Witness # Witness is a pluggable supply chain security framework that creates a provenance trace throughout the software development lifecycle (SDLC), ensuring the integrity of software from source code to target. It supports most major CI and infrastructure providers, offering a versatile and flexible solution for securing software supply chains.\nThe use of a secure Public Key Infrastructure (PKI) system and the ability to verify Witness metadata further enhance the security of the process and help mitigate many software supply chain attack vectors.\nWitness works by encapsulating commands executed within a continuous integration process, providing a provenance trace for each operation in the software development lifecycle (SDLC). This allows for a detailed and verifiable record of how the software was built, by whom, and what tools were used.\nThis evidence can be used to assess policy compliance, detect any potential tampering or malicious activity, and ensure that only authorized users or machines can complete a given step in the process.\nSummary - What Witness can do\nVerify who built the software, how it was built, and what tools were used Detect any potential tampering or malicious activity Ensure that only authorized users or machines can complete each step in the process Distribute Attestations and Policies How to use Witness # It mainly consists of three steps:\nwitness run - Run the provided command and record an attestation about the execution. witness sign - Sign the provided file using the provided key. witness verify - Verify the witness policy. Quick Start # This is a Witness Demo repository I created to demonstrate the Witness workflow. You can follow these steps.\nPrepare the environment # Install Witness and download the demo project\nbash \u0026lt;(curl -s https://raw.githubusercontent.com/in-toto/witness/main/install-witness.sh) Latest version of Witness is 0.1.14 Downloading for linux amd64 from https://github.com/in-toto/witness/releases/download/v0.1.14/witness_0.1.14_linux_amd64.tar.gz expected checksum: f9b67ca04cb391cd854aec3397eb904392ff689dcd3c38305d38c444781a5a67 file checksum: f9b67ca04cb391cd854aec3397eb904392ff689dcd3c38305d38c444781a5a67 witness v0.1.14-aa35c1f Witness v0.1.14 has been installed at /usr/local/bin/witness git clone https://github.com/shenxianpeng/witness-demo.git Create key pair # openssl genpkey -algorithm ed25519 -outform PEM -out witness-demo-key.pem openssl pkey -in witness-demo-key.pem -pubout \u0026gt; witness-demo-pub.pem Prepare the Witness configuration file .witness.yaml # run: signer-file-key-path: witness-demo-key.pem trace: false verify: attestations: - \u0026#34;witness-demo-att.json\u0026#34; policy: policy-signed.json publickey: witness-demo-pub.pem Integrate the build step into the Attestation # witness run --step build -o witness-demo-att.json -- python3 -m pip wheel --no-deps -w dist . INFO Using config file: .witness.yaml INFO Starting environment attestor... INFO Starting git attestor... INFO Starting material attestor... INFO Starting command-run attestor... Processing /tmp/witness-demo Building wheels for collected packages: witness-demo Running setup.py bdist_wheel for witness-demo: started Running setup.py bdist_wheel for witness-demo: finished with status \u0026#39;done\u0026#39; Stored in directory: /tmp/witness-demo/dist Successfully built witness-demo INFO Starting product attestor... That is, add the witness run command to the previous build command.\nView the verification data of the signed Attestation # Because the Attestation data is base64 encoded, it needs to be decoded before viewing.\ncat witness-demo-att.json | jq -r .payload | base64 -d | jq # Partial output below { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/command-run/v0.1\u0026#34;, \u0026#34;attestation\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;pip\u0026#34;, \u0026#34;wheel\u0026#34;, \u0026#34;--no-deps\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;dist\u0026#34;, \u0026#34;.\u0026#34; ], \u0026#34;stdout\u0026#34;: \u0026#34;Processing /tmp/witness-demo\\nBuilding wheels for collected packages: witness-demo\\n Running setup.py bdist_wheel for witness-demo: started\\n Running setup.py bdist_wheel for witness-demo: finished with status \u0026#39;done\u0026#39;\\n Stored in directory: /tmp/witness-demo/dist\\nSuccessfully built witness-demo\\n\u0026#34;, \u0026#34;exitcode\u0026#34;: 0 }, \u0026#34;starttime\u0026#34;: \u0026#34;2023-11-29T05:15:19.227943473-05:00\u0026#34;, \u0026#34;endtime\u0026#34;: \u0026#34;2023-11-29T05:15:20.078517025-05:00\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/product/v0.1\u0026#34;, \u0026#34;attestation\u0026#34;: { \u0026#34;dist/witness_demo-1.0.0-py3-none-any.whl\u0026#34;: { \u0026#34;mime_type\u0026#34;: \u0026#34;application/zip\u0026#34;, \u0026#34;digest\u0026#34;: { \u0026#34;gitoid:sha1\u0026#34;: \u0026#34;gitoid:blob:sha1:b4b7210729998829c82208685837058f5ad614ab\u0026#34;, \u0026#34;gitoid:sha256\u0026#34;: \u0026#34;gitoid:blob:sha256:473a0f4c3be8a93681a267e3b1e9a7dcda1185436fe141f7749120a303721813\u0026#34;, \u0026#34;sha256\u0026#34;: \u0026#34;471985cd3b0d3e0101a1cbba8840819bfdc8d8f8cc19bd08add1e04be25b51ec\u0026#34; } } }, \u0026#34;starttime\u0026#34;: \u0026#34;2023-11-29T05:15:20.078579187-05:00\u0026#34;, \u0026#34;endtime\u0026#34;: \u0026#34;2023-11-29T05:15:20.081170078-05:00\u0026#34; } Create the Policy file # policy.json is used to define or require that a step has specific attributes or meets certain values, thereby requiring verification of the Attestation for successful verification. For example, if the expires field expires (is less than the current time), the execution of witness verify will fail.\n{ \u0026#34;expires\u0026#34;: \u0026#34;2033-12-17T23:57:40-05:00\u0026#34;, \u0026#34;steps\u0026#34;: { \u0026#34;build\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;attestations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/material/v0.1\u0026#34;, \u0026#34;regopolicies\u0026#34;: [] }, { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/command-run/v0.1\u0026#34;, \u0026#34;regopolicies\u0026#34;: [] }, { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/product/v0.1\u0026#34;, \u0026#34;regopolicies\u0026#34;: [] } ], \u0026#34;functionaries\u0026#34;: [ { \u0026#34;publickeyid\u0026#34;: \u0026#34;{{PUBLIC_KEY_ID}}\u0026#34; } ] } }, \u0026#34;publickeys\u0026#34;: { \u0026#34;{{PUBLIC_KEY_ID}}\u0026#34;: { \u0026#34;keyid\u0026#34;: \u0026#34;{{PUBLIC_KEY_ID}}\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;{{B64_PUBLIC_KEY}}\u0026#34; } } } More information about policy attributes and settings can be found here: https://github.com/in-toto/witness/blob/main/docs/policy.md\nSign the Policy file # Before signing, you need to replace the variables in the Policy file.\nid=`sha256sum witness-demo-pub.pem | awk \u0026#39;{print $1}\u0026#39;` \u0026amp;\u0026amp; sed -i \u0026#34;s/{{PUBLIC_KEY_ID}}/$id/g\u0026#34; policy.json pubb64=`cat witness-demo-pub.pem | base64 -w 0` \u0026amp;\u0026amp; sed -i \u0026#34;s/{{B64_PUBLIC_KEY}}/$pubb64/g\u0026#34; policy.json Then use witness sign to sign.\nwitness sign -f policy.json --signer-file-key-path witness-demo-key.pem --outfile policy-signed.json INFO Using config file: .witness.yaml Verify that the binary file meets policy requirements # witness verify -f dist/witness_demo-1.0.0-py3-none-any.whl -a witness-demo-att.json -p policy-signed.json -k witness-demo-pub.pem INFO Using config file: .witness.yaml INFO Verification succeeded INFO Evidence: INFO 0: witness-demo-att.json Finally # This is a demonstration of using Witness for Non-GitHub projects.\nIf your project code is on GitHub, the easiest and most popular way is to use slsa-github-generator, a tool provided by the SLSA Framework, and then use slsa-verifier to verify the Provenance. Refer to my previous article Python and SLSA 💃 for details.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2023-11-30","externalUrl":null,"permalink":"/en/posts/witness-and-slsa/","section":"Posts","summary":"This article introduces the concept and working mechanism of Witness, and how to use Witness to generate and verify the provenance of software artifacts, emphasizing its importance in improving software supply chain security.","title":"Witness and SLSA 💃","type":"posts"},{"content":"由于近些年针对软件的供应链的攻击越来越频繁，据 SonaType 的统计从 2019 年到 2022 年针对开源软件的攻击增长了 742%，因此 2021 年 Google 提出的解决方案是软件工件供应链级别（Supply chain Levels for Software Artifacts，\u0026ldquo;SLSA\u0026rdquo;）\n本篇将介绍在 Python 生态系统中，我们如何使用 SLSA 框架来生成和验证 Python 工件的来源，从而让你的 SLSA Level 从 L0/L1 到 L3。\n注意：本文介绍的是针对托管在 GitHub 上的 Python 项目。SLSA 框架可通过 GitHub Actions 来实现开箱即用，只需较少的配置即可完成。\n对于托管在非 GitHub 上的项目（例如 Bitbucket）可以尝试 Witness，下一篇我将更新关于如何使用 Witness。\n内容 # 构建纯净的Python包 生成出处证明 上传到PyPI 验证Python包的来源 文中用到的项目 下面是从维护人员到用户的端到端工作流程：从构建 Wheel package -\u0026gt; 生成出处 -\u0026gt; 验证出处 -\u0026gt; 发布到 PyPI -\u0026gt; 以及用户验证出处 -\u0026gt; 安装 wheel。接下来让我们一起来完成这其中的每一步。\n如果你想了解 Python 打包的流程或是术语可以参见Python 打包用户指南。\n构建纯净的Python包 # 构建纯 Python 包通常只有两个工件：即纯 Python Wheel Package 和源代码 distribution。可以使用命令 python3 -m build 从源代码构建。\n下面是 GitHub Actions job 定义来构建 Wheel Package 和源代码 distribution，并为每个工件创建 SHA-256 哈希值：\njobs: build: steps: - uses: actions/checkout@... - uses: actions/setup-python@... with: python-version: 3.x - run: | # 安装 build，创建 sdist 和 wheel python -m pip install build python -m build # 收集所有文件的哈希值 cd dist \u0026amp;\u0026amp; echo \u0026#34;hashes=$(sha256sum * | base64 -w0)\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - uses: actions/upload-artifacts@... with: path: ./dist 这里将 build 完的 wheel package 上传到 GitHub Artifacts 存起来，用作后续在 “上传到PyPI” job 中使用。另外还将 dist 下的所有文件的哈希值存储在 hashes 用作后续的 provenance job 的输入。\n注意： SLSA 使用 sha265sum 的输出作为出处证明中 subject-base64 字段的输入。sha256sum 的输出是一个或多个对散列 + 名称。\n生成出处证明 # 现在我们已经构建了 sdist 和 wheel，我们可以从文件哈希生成来出处证明。\n因为我们需要将 Build 阶段的的输出作为这里生成出处的输入，因此这里使用了 needs 选项来作为 provenance job 执行的前提条件。可以看到上面生成的哈希值在这里被 subject-base64 所使用。\njobs: provenance: needs: [build] uses: slsa-framework/slsa-github-builder/.github/workflows/generator_generic_slsa3.yml@v1.9.0 permissions: # 需要检测 GitHub 操作环境 actions: read # 需要通过 GitHub OIDC 创建出处 id-token: write # 需要创建并上传到 GitHub Releases contents: write with: # 生成的 package SHA-256 哈希值 subject-base64: ${{ provenance.needs.build.output.hashes }} # 将出处文件上传到 GitHub Release upload-assets: true 你会注意到 SLSA builders 使用可重用工作流功能来证明给定的 builders 行为不能被用户或其他进程修改。\n出处证明文件是 JSON lines，以 .intoto.jsonl 结尾。*.intoto.jsonl 文件可以包含多个工件的证明，也可以在同一文件中包含多个出处证明。该 .jsonl 格式意味着该文件是一个 “JSON lines” 文件，即每行一个 JSON 文档。\n注意：这里有一点令人困惑的是 GitHub job 中的 id-token 需要 write 权限才能读取 GitHub OIDC 令牌。read 不允许你读取 OIDC\u0026hellip;🤷。有关 id-token 权限的更多信息，请参阅 GitHub 文档。\n上传到PyPI # 我们使用官方 pypa/gh-action-pypi-publish GitHub Action 将 wheel 包上传到 PyPI。\n注意：publish job 需要在 build 和 provenance 都完成后开始执行，这意味着我们可以假设 provenance job 已经为我们起草了 GitHub Release（因为 upload-assets: true 的设置），并且我们可以假设该 job 已成功。如果不先创建来 provenance 文件，我们不想将这些 wheel 包上传到 PyPI，因此我们最后上传到 PyPI。\npublish: needs: [\u0026#34;build\u0026#34;, \u0026#34;provenance\u0026#34;] permissions: contents: write runs-on: \u0026#34;ubuntu-latest\u0026#34; steps: # 下载已构建的 distributions - uses: \u0026#34;actions/download-artifact@...\u0026#34; with: name: \u0026#34;dist\u0026#34; path: \u0026#34;dist/\u0026#34; # 上传 distributions 到 GitHub Release - env: GITHUB_TOKEN: \u0026#34;${{ secrets.GITHUB_TOKEN }}\u0026#34; run: gh release upload ${{ github.ref_name }} dist/* --repo ${{ github.repository }} # 发布 distributions 到 PyPI - uses: \u0026#34;pypa/gh-action-pypi-publish@...\u0026#34; with: user: __token__ password: ${{ secrets.PYPI_TOKEN }} 验证Python包的来源 # 让我们使用一个真正的 Python 项目来验证它的出处。以 urllib3 项目为例，它在 GitHub Releases 发布了版本中包含出处证明，这里演示的是使用它的最新版本 2.1.0 。\n首先我们需要下载 slsa-verifier 用来验证出处。下载完 slsa-verifier 工具后，让我们从 PyPI 获取 urllib3 wheel 包，而不使用 pip download. 我们使用该 --only-binary 选项强制 pip 下载 wheel。\npython3 -m pip download --only-binary=:all: urllib3 Collecting urllib3 Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB) Downloading urllib3-2.1.0-py3-none-any.whl (104 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.6/104.6 kB 761.0 kB/s eta 0:00:00 Saved ./urllib3-2.1.0-py3-none-any.whl Successfully downloaded urllib3 下载软件包后，我们需要从 GitHub 版本下载出处证明。我们需要使用与包版本相同的 GitHub Release 来确保获得正确的出处证明，因此 tag 也是 2.1.0。\ncurl --location -O https://github.com/urllib3/urllib3/releases/download/2.1.0/multiple.intoto.jsonl 该出处文件的名称为 multiple.intoto.jsonl，这是一个包含多个工件证明的出处证明的标准名称。\n此时，我们当前的工作目录中应该有两个文件：wheel 和出处证明，ls 浏览一下确保已经准备好了：\nls multiple.intoto.jsonl urllib3-2.1.0-py3-none-any.whl 从这里我们可以使用 slsa-verifier 来验证出处。我们可以验证最重要的事情，即哪个 GitHub 仓库实际构建了 wheel，以及其他信息，例如 git 标签、分支和建造者 ID：\n源存储库 (--source-uri) 建造者 ID (--builder-id) Git 分支 (--source-branch) git 标签 (--source-tag)\n# 这里仅验证 wheel package 的 GitHub 仓库 slsa-verifier verify-artifact --provenance-path multiple.intoto.jsonl --source-uri github.com/urllib3/urllib3 urllib3-2.1.0-py3-none-any.whl Verified signature against tlog entry index 49513169 at URL: https://rekor.sigstore.dev/api/v1/log/entries/24296fb24b8ad77a08c2f012d69948ed5d12e8e020852bb7936ea9208d684688e5108cca859a3302 Verified build using builder \u0026#34;https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.9.0\u0026#34; at commit 69be2992f8a25a1f27e49f339e4d5b98dec07462 Verifying artifact urllib3-2.1.0-py3-none-any.whl: PASSED PASSED: Verified SLSA provenance 成功了！🥳 我们已经验证了这个 wheel 的出处，所以现在我们可以放心的安装它，因为我们知道它是按照我们的预期构建的：\npython3 -m pip install urllib3-2.1.0-py3-none-any.whl Defaulting to user installation because normal site-packages is not writeable Processing ./urllib3-2.1.0-py3-none-any.whl Installing collected packages: urllib3 Attempting uninstall: urllib3 Found existing installation: urllib3 2.0.5 Uninstalling urllib3-2.0.5: Successfully uninstalled urllib3-2.0.5 Successfully installed urllib3-2.1.0 文中用到的项目 # 以下这些是本文使用的所有项目和工具：\nSLSA GitHub Builder slsa-framework/slsa-verifier pypa/gha-action-pypi-publish pypa/build urllib3/urllib3 英文原文：https://sethmlarson.dev/python-and-slsa\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-11-15","externalUrl":null,"permalink":"/posts/python-and-slsa/","section":"Posts","summary":"本文介绍了如何在 Python 生态系统中使用 SLSA 框架来生成和验证 Python 工件的来源，从而提升软件供应链的安全性。","title":"Python 和 SLSA 💃","type":"posts"},{"content":"","date":"2023-10-08","externalUrl":null,"permalink":"/en/tags/aix/","section":"Tags","summary":"","title":"AIX","type":"tags"},{"content":"In this article, I would like to document the problems encountered when upgrading from IBM XLC 10.1 to XLC 17.1 (IBM Open XL C/C++ for AIX 17.1.0) and how to fix the following 12 errors.\nIf you\u0026rsquo;ve encountered any other errors, feel free to share your comments with or without a solution.\n1. Change cc to ibm-clang # First you need to change all the related cc to ibm-clang in the the global Makefile. for example:\n- CC=cc - CXX=xlC_r - XCC=xlC_r - MAKE_SHARED=xlC_r + CC=ibm-clang + CXX=ibm-clang_r + XCC=ibm-clang_r + MAKE_SHARED=ibm-clang_r And check following link of Mapping of options to map new Clang options if any.\n2. error: unknown argument: \u0026lsquo;-qmakedep=gcc\u0026rsquo; # - GEN_DEPENDENTS_OPTIONS=-qmakedep=gcc -E -MF $@.1 \u0026gt; /dev/null + GEN_DEPENDENTS_OPTIONS= -E -MF $@.1 \u0026gt; /dev/null 3. should not return a value [-Wreturn-type] # - return -1; + return; 4. error: non-void function \u0026lsquo;main\u0026rsquo; should return a value [-Wreturn-type] # - return; + return 0; 5. error: unsupported option \u0026lsquo;-G\u0026rsquo; for target \u0026lsquo;powerpc64-ibm-aix7.3.0.0\u0026rsquo; # - LIB_101_FLAGS := -G + LIB_101_FLAGS := -shared -Wl,-G 6. Undefined symbol (libxxxx.so) # - LIB_10_FLAGS := -bexport:$(SRC)/makefiles/xxxx.def + LIB_10_FLAGS := -lstdc++ -lm -bexport:$(SRC)/makefiles/xxxx.def 7. unsupported option -qlongdouble # - drv_connect.c.CC_OPTIONS=$(CFLAGS) -qlongdouble -brtl + drv_loadfunc.c.CC_OPTIONS=$(CFLAGS) $(IDIR) -brtl 8. Undefined symbol: ._Z8u9_closei # - extern int u9_close(int fd) ; + extern \u0026#34;C\u0026#34; int u9_close(int fd) ; 9. ERROR: Undefined symbol: .pow # - CXXLIBES = -lpthread -lC -lstdc++ + CXXLIBES = -lpthread -lC -lstdc++ -lm 10. \u0026lsquo;main\u0026rsquo; (argument array) must be of type \u0026lsquo;char **\u0026rsquo; # - d_char *argv[]; + char *argv[]; 11. first parameter of \u0026lsquo;main\u0026rsquo; (argument count) must be of type \u0026lsquo;int\u0026rsquo; # - int main(char *argc, char *argv[]) + int main(int argc, char *argv[]) 12. ERROR: Undefined symbol: ._ZdaPv # - LIB_3_LIBS\t:= -lverse -llog_nosig + LIB_3_LIBS\t:= -lverse -llog_nosig -lstdc++ ","date":"2023-10-08","externalUrl":null,"permalink":"/en/posts/upgrade-xlc-10-to-xlc-17.1/","section":"Posts","summary":"This article documents the problems encountered when upgrading from IBM XLC 10.1 to XLC 17.1 (IBM Open XL C/C++ for AIX 17.1.0) and how to fix the errors.","title":"Problems and solutions when upgrading XLC from 10.1 to IBM Open XL C/C++ for AIX 17.1.0","type":"posts"},{"content":"","date":"2023-10-08","externalUrl":null,"permalink":"/en/tags/xlc/","section":"Tags","summary":"","title":"XLC","type":"tags"},{"content":"","date":"2023-09-11","externalUrl":null,"permalink":"/en/tags/artifactory/","section":"Tags","summary":"","title":"Artifactory","type":"tags"},{"content":"","date":"2023-09-11","externalUrl":null,"permalink":"/en/tags/java/","section":"Tags","summary":"","title":"Java","type":"tags"},{"content":"Recently, I encountered an issue where artifacts could not be uploaded from a Jenkins agent to Artifactory. The specific error is as follows:\n[2023-09-11T08:21:53.385Z] Executing command: /bin/sh -c git log --pretty=format:%s -1 [2023-09-11T08:21:54.250Z] [consumer_0] Deploying artifact: https://artifactory.mycompany.com/artifactory/generic-int-den/my-project/hotfix/1.2.0.HF5/3/pj120_bin_opt_SunOS_3792bcf.tar.Z [2023-09-11T08:21:54.269Z] Error occurred for request GET /artifactory/api/system/version HTTP/1.1: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target. [2023-09-11T08:21:54.282Z] Error occurred for request PUT /artifactory/generic-int-den/my-project/hotfix/1.2.0.HF5/3/pj120_bin_opt_SunOS_3792bcf.tar.Z;build.timestamp=1694418199972;build.name=hotfix%2F1.2.0.HF5;build.number=3 HTTP/1.1: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target. [2023-09-11T08:21:54.284Z] [consumer_0] An exception occurred during execution: [2023-09-11T08:21:54.284Z] java.lang.RuntimeException: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target [2023-09-11T08:21:54.284Z] at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:44) [2023-09-11T08:21:54.284Z] at org.jfrog.build.extractor.producerConsumer.ConsumerRunnableBase.run(ConsumerRunnableBase.java:11) [2023-09-11T08:21:54.284Z] at java.lang.Thread.run(Thread.java:745) [2023-09-11T08:21:54.285Z] Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This problem occurs because the Java security certificate does not validate the HTTPS connection when uploading files. The solution is to regenerate the certificate file and import it. The steps are as follows.\nGenerating the Security Certificate File # Steps:\nFirst, open your Artifactory URL in a browser. There should be a lock icon on the left side of the URL. Click on \u0026ldquo;Connection is secure\u0026rdquo; -\u0026gt; \u0026ldquo;Certificate is valid\u0026rdquo; -\u0026gt; \u0026ldquo;Details\u0026rdquo; -\u0026gt; \u0026ldquo;Export\u0026rdquo;. Select \u0026ldquo;DER-encoded binary, single certificate (*.der)\u0026rdquo; to generate the certificate file. For example, I named my security certificate file: artifactory.mycompany.der (the name can be arbitrary, as long as the extension remains unchanged).\nImporting the Security Certificate via Command Line # Log in to the problematic Solaris agent, upload artifactory.mycompany.der to a specified directory, find the path to cacerts, and execute the following command:\nroot@mysolaris:/# keytool -import -alias example -keystore /usr/java/jre/lib/security/cacerts -file /tmp/artifactory.mycompany.der # Then select yes You will be prompted to enter the password. The default password is changeit. Enter it and restart your JVM or VM. Once you retry uploading artifacts through this agent, everything should return to normal.\nReference: https://stackoverflow.com/questions/21076179/pkix-path-building-failed-and-unable-to-find-valid-certification-path-to-requ\nPlease indicate the author and source when reprinting this article. Do not use for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2023-09-11","externalUrl":null,"permalink":"/en/posts/unable-to-find-valid-certification-path/","section":"Posts","summary":"This article describes how to resolve SSL certificate validation issues when uploading artifacts from a Jenkins agent to Artifactory, including generating a security certificate file and importing it into Java\u0026rsquo;s cacerts.","title":"Resolving Jenkins Artifactory Plugin Artifact Upload Failure \"unable to find valid certification path to requested target\"","type":"posts"},{"content":"DevOps 运动仍然是一个不断发展的领域，受到技术进步、行业趋势和组织需求等多种因素的影响。这使得很难对 DevOps 工程的未来做出具体预测。然而我认为有一些趋势可能会在来年继续影响 DevOps 领域的发展。\n云原生技术的持续采用 # 容器化、微服务和无服务器计算等云原生技术会继续在 DevOps 环境中广泛采用，这些技术为各种项目提供了许多好处，包括：\n提高可扩展性和可靠性：云原生技术可以让 DevOps 团队更轻松地构建、部署和管理可扩展且有弹性的应用程序。 更快的部署和迭代：云原生技术可以使团队更快地部署和迭代应用程序，帮助组织更快地行动并响应不断变化的业务需求。 更大的灵活性和敏捷性：云原生技术可以为 DevOps 团队提供更大的灵活性和敏捷性，使他们能够使用各种工具和平台构建和部署应用程序。 总体而言，随着组织寻求提高数字时代的效率和竞争力，采用云原生技术可能会继续成为未来几年 DevOps 的趋势。 Kubernetes 和类似的编排平台仍将是这一过程的重要组成部分，因为它们为构建、部署和管理容器化提供了一致的、基于云的环境、应用程序和基础设施。\n更加关注安全性和合规性 # 随着安全性和合规性的重要性不断增长，组织会更加重视将安全性和合规性最佳实践纳入其流程和工具中。\n未来几年，安全性和合规性在一些特定领域尤为重要：\n云安全：随着越来越多的组织采用基于云的基础设施和服务，DevOps 团队将更加需要关注云安全性和合规性，包括确保云环境配置安全、数据在传输和静态时加密以及对云资源的访问进行控制和监控。 应用程序安全性：DevOps 团队需要专注于构建安全的应用程序并确保它们以安全的方式进行测试和部署，包括实施安全编码实践、将安全测试纳入开发过程以及使用安全配置部署应用程序。 遵守法规和标准：团队需要确保他们的流程和工具符合相关法规和行业标准，包括实施控制措施来保护敏感数据，确保系统配置为以合规的方式，通过审计和评估证明合规性，以及在适用的情况下将部分责任推给云提供商。 总而言之，随着组织寻求保护其系统、数据和客户免受网络威胁并确保遵守相关法规和标准，DevOps 中对安全性和合规性的关注肯定会增加，它还将使 DevSecOps 专家 在 DevOps 团队中发挥更大的作用。\n开发和运营团队之间加强协作 # DevOps 运动的理念是将开发和运营团队聚集在一起，以便更紧密、更有效地工作。未来几年，开发和运营团队之间的协作在一些关键领域可能会特别重要：\n持续集成和交付 (CI/CD)：开发和运营团队需要密切合作，以确保有效地测试、部署和监控代码更改，并快速识别和解决任何问题。 事件管理：开发和运营团队需要合作识别和解决生产环境中出现的问题，并制定策略以防止将来发生类似问题。 容量规划和资源管理：开发和运营团队需要共同努力，以确保系统拥有必要的资源来满足需求，并规划未来的增长。 DevOps 的成功取决于开发和运营团队之间的强有力合作，这可能仍然是 2023 年的重点。\n自动化和人工智能的持续发展 # 自动化和人工智能 (AI) 可能在 DevOps 的发展中发挥重要作用。自动化工具可以帮助 DevOps 团队对重复性任务进行编程、提高效率并降低人为错误的风险，而人工智能可用于分析数据、识别模式并协助做出更明智的决策。\n人工智能可能对 DevOps 产生重大影响的一个潜在领域是预测分析领域。通过分析过去部署和性能指标的数据，人工智能算法可以识别模式并预测未来的结果，从而使团队能够更好地优化其流程并提高整体性能。 人工智能可能产生影响的另一个领域是事件管理领域。人工智能算法可用于分析日志数据并在潜在问题发生之前识别它们，从而使团队能够在出现重大事件之前主动解决出现的问题。 总体而言，DevOps 中自动化和人工智能的发展可能会带来更高效、更有效的流程，并提高应用程序和系统的性能和可靠性。然而，组织必须仔细考虑这些技术的潜在影响，并确保它们的使用方式符合其业务目标和价值观。 自动化和人工智能的实施应该成为战略的一部分：包括从一开始就需要集成到业务流程中，调整期望和目标，估计成本以及相关的风险和挑战。仅仅为了实现两者而实施并不一定会从中获益，相反，从长远来看，它可能会因维护它们而导致其他问题。\n基础设施即代码的多云支持 # 基础设施即代码 (IaC) 正在成为一种越来越流行的实践，涉及使用与管理代码相同的版本控制和协作工具来管理基础设施。这使得组织能够将其基础设施视为一等公民，并且更容易自动化基础设施的配置和管理。\n多云基础设施是指在单个组织内使用多个云计算平台，例如 AWS、Azure 和 Google Cloud。这种方法可以为组织提供更大的灵活性和弹性，因为它们不依赖于单个提供商。\n结合这两个概念，对 IaC 的多云支持是指使用 IaC 实践和工具来管理和自动化跨多个云平台的资源调配和配置的能力。这可以包括使用 IaC 定义和部署基础设施资源，例如虚拟机、网络和存储，以及管理这些资源的配置。\n使用 IaC 管理多云基础设施可以带来多种好处。它可以帮助组织在其环境中实现更高的一致性和标准化，并降低在多个云平台中管理资源的复杂性。它还可以使组织能够更轻松地在云平台之间迁移工作负载，并利用每个平台的独特功能。\n总体而言，对于寻求优化多个云平台的使用并简化多云基础设施管理的组织来说，对 IaC 的多云支持可能仍然是一个重要因素。\n更加强调多样性和包容性 # 随着技术行业继续关注多样性和包容性，DevOps 团队可能会更加重视建立多元化和包容性的团队并创造更具包容性的工作环境 - 包括：\n为团队提供具有新的非技术技能的人员，以填补深刻的技能差距。根据 《提高 IT 技能 2022》报告，IT 需要七种关键技能：流程和框架技能、人员技能、技术技能、自动化技能、领导技能、数字技能、业务技能和高级认知技能。不幸的是，大多数 IT 经理首先追求的是技术技能，而忽视了“软技能” —— 这可能是阻碍 DevOps 在组织中进一步发展的最大因素。 整个团队以及单个 DevOps 成员的技能发展。近年来，每个人都面临着考验（新冠等流行病、不确定的经济形势），并使越来越多的 IT 专业人员不确定自己的技能，并感到需要进一步的培训和发展。与此同时，根据《2022 年 IT 技能提升》报告，接受调查的 IT 组织中只有 52% 制定了正式的技能提升计划。这实际上导致全球 IT 团队面临的最大挑战是缺乏技能。 向外部团队开放，保证能力和预期质量的快速提高。无论是外包以及其他形式的合作。这还可以在优化生产成本方面带来切实的好处（这无疑是当今开展业务最重要的因素之一）。 概括 # 上述 6 个方面是我们认为 2023 年 DevOps 的主要趋势。当然，每个人都可以从自己的角度添加其他要点，例如：无服务器计算、低代码和无代码解决方案、GitOps 等。\n过往 DevOps 趋势文章 # 2022 年最值得关注的 DevOps 趋势和问答 参考 # Trends for DevOps engineering in 2023 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-09-01","externalUrl":null,"permalink":"/posts/devops-trends-2023/","section":"Posts","summary":"本文介绍了2023年DevOps领域的主要趋势，包括云原生技术的持续采用、加强安全性和合规性、开发与运营团队协作、自动化和人工智能的发展等。","title":"2023 年最值得关注的 DevOps 趋势","type":"posts"},{"content":"","date":"2023-09-01","externalUrl":null,"permalink":"/tags/kubernetes/","section":"标签","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"2023-08-29","externalUrl":null,"permalink":"/en/tags/troubleshooting/","section":"Tags","summary":"","title":"Troubleshooting","type":"tags"},{"content":"Recently my CI pipeline suddenly does not work on AIX 7.1 with error Caused by: javax.net.ssl.SSLHandshakeException: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath..\nClick to see more details about the failure log. 22:13:30 Executing command: /bin/sh -c git log --pretty=format:%s -1 22:13:36 [consumer_0] Deploying artifact: https://artifactory.company.com/artifactory/generic-int-den/myproject/PRs/PR-880/1/myproject_bin_rel_AIX_5797b20.tar.Z 22:13:36 Error occurred for request GET /artifactory/api/system/version HTTP/1.1: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error. 22:13:36 Error occurred for request PUT /artifactory/generic-int-den/myproject/PRs/PR-880/1/cpplinter_bin_rel_AIX_5797b20.tar.Z;build.timestamp=1693273923987;build.name=PR-880;build.number=1 HTTP/1.1: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error. 22:13:36 [consumer_0] An exception occurred during execution: 22:13:36 java.lang.RuntimeException: javax.net.ssl.SSLHandshakeException: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:44) 22:13:36 at org.jfrog.build.extractor.producerConsumer.ConsumerRunnableBase.run(ConsumerRunnableBase.java:11) 22:13:36 at java.lang.Thread.run(Thread.java:785) 22:13:36 Caused by: javax.net.ssl.SSLHandshakeException: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.jsse2.j.a(j.java:3) 22:13:36 at com.ibm.jsse2.as.a(as.java:213) 22:13:36 at com.ibm.jsse2.C.a(C.java:339) 22:13:36 at com.ibm.jsse2.C.a(C.java:248) 22:13:36 at com.ibm.jsse2.D.a(D.java:291) 22:13:36 at com.ibm.jsse2.D.a(D.java:217) 22:13:36 at com.ibm.jsse2.C.r(C.java:373) 22:13:36 at com.ibm.jsse2.C.a(C.java:352) 22:13:36 at com.ibm.jsse2.as.a(as.java:752) 22:13:36 at com.ibm.jsse2.as.i(as.java:338) 22:13:36 at com.ibm.jsse2.as.a(as.java:711) 22:13:36 at com.ibm.jsse2.as.startHandshake(as.java:454) 22:13:36 at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:436) 22:13:36 at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384) 22:13:36 at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) 22:13:36 at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376) 22:13:36 at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) 22:13:36 at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) 22:13:36 at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) 22:13:36 at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) 22:13:36 at org.apache.http.impl.execchain.ServiceUnavailableRetryExec.execute(ServiceUnavailableRetryExec.java:85) 22:13:36 at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) 22:13:36 at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) 22:13:36 at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) 22:13:36 at org.jfrog.build.client.PreemptiveHttpClient.execute(PreemptiveHttpClient.java:76) 22:13:36 at org.jfrog.build.client.PreemptiveHttpClient.execute(PreemptiveHttpClient.java:64) 22:13:36 at org.jfrog.build.client.JFrogHttpClient.sendRequest(JFrogHttpClient.java:133) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.JFrogService.execute(JFrogService.java:112) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.artifactory.services.Upload.execute(Upload.java:77) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.artifactory.ArtifactoryManager.upload(ArtifactoryManager.java:267) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.artifactory.ArtifactoryManager.upload(ArtifactoryManager.java:262) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:39) 22:13:36 ... 2 more 22:13:36 Caused by: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.jsse2.util.f.a(f.java:107) 22:13:36 at com.ibm.jsse2.util.f.b(f.java:143) 22:13:36 at com.ibm.jsse2.util.e.a(e.java:6) 22:13:36 at com.ibm.jsse2.aA.a(aA.java:99) 22:13:36 at com.ibm.jsse2.aA.a(aA.java:112) 22:13:36 at com.ibm.jsse2.aA.checkServerTrusted(aA.java:28) 22:13:36 at com.ibm.jsse2.D.a(D.java:588) 22:13:36 ... 29 more 22:13:36 Caused by: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.engineBuild(PKIXCertPathBuilderImpl.java:422) 22:13:36 at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:268) 22:13:36 at com.ibm.jsse2.util.f.a(f.java:120) 22:13:36 ... 35 more 22:13:36 Caused by: java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.security.cert.BasicChecker.\u0026lt;init\u0026gt;(BasicChecker.java:111) 22:13:36 at com.ibm.security.cert.PKIXCertPathValidatorImpl.engineValidate(PKIXCertPathValidatorImpl.java:199) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.myValidator(PKIXCertPathBuilderImpl.java:749) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.buildCertPath(PKIXCertPathBuilderImpl.java:661) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.buildCertPath(PKIXCertPathBuilderImpl.java:607) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.engineBuild(PKIXCertPathBuilderImpl.java:368) 22:13:36 ... 37 more 22:13:36 Caused by: java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.security.cert.CertPathUtil.findIssuer(CertPathUtil.java:316) 22:13:36 at com.ibm.security.cert.BasicChecker.\u0026lt;init\u0026gt;(BasicChecker.java:108) 22:13:36 ... 42 more 22:13:36 I have tried to download certificate.pem from my Artifactory and run this command, but the issue still there on my AIX 7.3.\n/usr/java8_64/jre/bin/keytool -importcert -alias cacertalias -keystore /usr/java8_64/jre/lib/security/cacerts -file /path/to/your/certificate.pem It investing it can not reproduce on my Windows, Linux and AIX 7.3 build machines.\nWhat\u0026rsquo;s the different between them? the only different is Java runtime. On my problematic AIX 7.1 build machine, I used a shared runtime which is a link to path /tools/AIX-7.1/Java8_64-8.0.0.401/usr/java8_64/bin/java\nbash-5.0$ /tools/AIX-7.1/Java8_64-8.0.0.401/usr/java8_64/bin/java -version java version \u0026#34;1.8.0\u0026#34; Java(TM) SE Runtime Environment (build pap6480sr4fp1-20170215_01(SR4 FP1)) IBM J9 VM (build 2.8, JRE 1.8.0 AIX ppc64-64 Compressed References 20170209_336038 (JIT enabled, AOT enabled) J9VM - R28_20170209_0201_B336038 JIT - tr.r14.java.green_20170125_131456 GC - R28_20170209_0201_B336038_CMPRSS J9CL - 20170209_336038) JCL - 20170215_01 based on Oracle jdk8u121-b13 And I have anther Java runtime installed my that machine which is /usr/java8_64/bin/java\nbash-5.0$ /usr/java8_64/bin/java -version java version \u0026#34;1.8.0_241\u0026#34; Java(TM) SE Runtime Environment (build 8.0.6.5 - pap6480sr6fp5-20200111_02(SR6 FP5)) IBM J9 VM (build 2.9, JRE 1.8.0 AIX ppc64-64-Bit Compressed References 20200108_436782 (JIT enabled, AOT enabled) OpenJ9 - 7d1059c OMR - d059105 IBM - c8aee39) JCL - 20200110_01 based on Oracle jdk8u241-b07 Actually the versions of these two java versions are different. I just changed configuration of JavaPath from /tools/AIX-7.1/Java8_64-8.0.0.401/usr/java8_64/bin/java to /usr/java8_64/bin/java in the Jenkins node and disconnect then launch agent again, it works.\nI don\u0026rsquo;t why it works, I don\u0026rsquo;t know much about Java certificate, if you know the reason please leave comments and let me know. Thank you.\n","date":"2023-08-29","externalUrl":null,"permalink":"/en/posts/upload-artifacts-failed-on-aix/","section":"Posts","summary":"This article explains how to resolve an SSL certificate verification issue on AIX when uploading artifacts to Artifactory via Jenkins, including updating Java’s cacerts file.","title":"Upload artifacts failed to Artifactory from AIX","type":"posts"},{"content":"","date":"2023-08-25","externalUrl":null,"permalink":"/tags/nuget/","section":"标签","summary":"","title":"NuGet","type":"tags"},{"content":"其实创建包管理平台账户没什么可说的，但最近准备在 https://www.nuget.org 上面发布产品前创建 Organization 的时候确遇到了问题。\n事情是这样的 # 作为一名公司员工在首次打开 NuGet 网站 (www.nuget.org) 的时候，点击【Sign in】，映入眼帘的就是【Sign in with Microsoft】，点击，下一步、下一步，我就顺利的就用公司邮箱注册了我的第一个 NuGet 的账户。\n此时我准备创建一个 Organization 的时候，输入自己的公司邮箱提示这个邮箱地址已经被使用了，What ？？？\nOK。那我就填写同事的公司邮箱地址吧。\n同事在收到邮件通知后也是一步步操作，先是打开 NuGet.org，点击【Sign in with Microsoft】，然后也是需要填写自己的账户名，结果完成这一系列的操作之后，再输入他的邮件地址去注册 Organization 的时候也同样提示这个邮箱已经被使用了？？？什么操作！！！醉了\u0026hellip;\n如何解决的 # 就在这千钧一发焦急得等待发布之际，我突然灵机一动，以死马当活马医的心态，将我注册的 NuGet 的个人账户绑定的公司邮箱修改为了自己的 Gmail 邮箱，然后此时再去创建 Organization 的时候输入的是自己的公司邮箱，创建 Organization 成功了！！\n好了，谨以此记录一下在注册 NuGet 时候遇到的问题。不知道对你是否有用，如果真的有帮助到你，举手示意一下哦。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-08-25","externalUrl":null,"permalink":"/posts/nuget/","section":"Posts","summary":"本文记录了在创建 NuGet Organization 时遇到的问题和解决方法，特别是关于公司邮箱地址的使用。","title":"创建 NuGet Organization 的遇到的坑","type":"posts"},{"content":"随着容器化技术的普及和应用场景的增多，构建和管理多平台镜像变得越来越重要。Docker Buildx 是 Docker 官方对于 Docker CLI 的一个扩展，为 Docker 用户提供了更强大和灵活的构建功能。包括：\n多平台构建：Docker Buildx 允许用户在一个构建命令中为多个不同的平台构建容器镜像。这样，你可以一次性构建适用于多种 CPU 架构的镜像，比如 x86、ARM 等，从而在不同的硬件设备上运行相同的镜像。 构建缓存优化：Docker Buildx 改进了构建过程中的缓存机制，通过自动识别 Dockerfile 中哪些部分是可缓存的，从而减少重复构建和加快构建速度。 并行构建：Buildx 允许并行构建多个镜像，提高了构建的效率。 多种输出格式：Buildx 支持不同的输出格式，包括 Docker 镜像、OCI 镜像、以及 rootfs 等。 构建策略：通过支持多种构建策略，用户可以更好地控制构建过程，例如，可以在多个节点上构建、使用远程构建等。 使用 docker buildx 需要 Docker Engine 版本不低于 19.03。\n其中，Docker Buildx Bake 是 Buildx 的一个子命令，也是本篇文章要重点介绍包括概念、优势、使用场景以及如何使用该功能来加速构建和管理多平台镜像。\n什么是 Docker Buildx Bake？ # Docker Buildx Bake 是 Docker Buildx 的一项功能，它旨在简化和加速镜像构建过程。Bake 是一种声明式的构建定义方式，它允许用户在一个命令中定义多个构建配置和目标平台，实现自动化批量构建和发布跨平台镜像。\n为什么使用 Docker Buildx Bake？ # 1. 提高构建效率 # Bake 通过并行构建和缓存机制来提高构建效率。使用 Bake 可以一次性定义和构建多个镜像，而无需为每个镜像分别执行构建过程，这样可以大大节省构建时间，提高工作效率。\n2. 支持多个平台和架构 # Docker Buildx Bake 的另一个优势是它能够构建多个平台和架构的镜像。通过在 Bake 配置中指定不同的平台参数就可以轻松构建适用于不同操作系统和架构的镜像。这对于跨平台应用程序的开发和部署非常有用。\n3. 一致的构建环境 # 通过 docker-bake.hcl （除了 HCL 配置文件之外还可以是 JSON 或是 YAML 文件）文件描述构建过程确保一致的构建环境，使不同的构建配置和目标平台之间具有相同的构建过程和结果。这种一致性有助于减少构建过程中的错误，而且构建配置更易于维护和管理。\n如何使用 Docker Buildx Bake？ # 以下是使用 Docker Buildx Bake 进行高效构建的基本步骤，首先确保你已经安装了 Docker Engine 或 Docker Desktop 版本 19.03 以及以上。\n然后你的 docker 命令将变成这样 docker buildx bake。以下 docker buildx bake --help 的帮助输出：\ndocker buildx bake --help Usage: docker buildx bake [OPTIONS] [TARGET...] Build from a file Aliases: docker buildx bake, docker buildx f Options: --builder string Override the configured builder instance -f, --file stringArray Build definition file --load Shorthand for \u0026#34;--set=*.output=type=docker\u0026#34; --metadata-file string Write build result metadata to the file --no-cache Do not use cache when building the image --print Print the options without building --progress string Set type of progress output (\u0026#34;auto\u0026#34;, \u0026#34;plain\u0026#34;, \u0026#34;tty\u0026#34;). Use plain to show container output (default \u0026#34;auto\u0026#34;) --provenance string Shorthand for \u0026#34;--set=*.attest=type=provenance\u0026#34; --pull Always attempt to pull all referenced images --push Shorthand for \u0026#34;--set=*.output=type=registry\u0026#34; --sbom string Shorthand for \u0026#34;--set=*.attest=type=sbom\u0026#34; --set stringArray Override target value (e.g., \u0026#34;targetpattern.key=value\u0026#34;) 接下来尝试一下如何使用 docker buildx bake\n1. 创建 Bake 配置文件 # 比如创建一个名为 docker-bake.dev.hcl 的 Bake 配置文件，并在其中定义构建上下文、目标平台和其他构建选项。以下是一个简单的示例：\n# docker-bake.dev.hcl group \u0026#34;default\u0026#34; { targets = [\u0026#34;db\u0026#34;, \u0026#34;webapp-dev\u0026#34;] } target \u0026#34;db\u0026#34; { dockerfile = \u0026#34;Dockerfile.db\u0026#34; tags = [\u0026#34;xianpengshen/docker-buildx-bake-demo:db\u0026#34;] } target \u0026#34;webapp-dev\u0026#34; { dockerfile = \u0026#34;Dockerfile.webapp\u0026#34; tags = [\u0026#34;xianpengshen/docker-buildx-bake-demo:webapp\u0026#34;] } target \u0026#34;webapp-release\u0026#34; { inherits = [\u0026#34;webapp-dev\u0026#34;] platforms = [\u0026#34;linux/amd64\u0026#34;, \u0026#34;linux/arm64\u0026#34;] } 2. 运行 Bake 构建 # 运行以下命令开始使用 Bake 构建镜像：\n$ docker buildx bake -f docker-bake.dev.hcl db webapp-release\n3. 打印构建选项 # 你还可以无需构建打印构建选项，使用用 --print 来查看某个目标构建是否符合预期。例如：\n$ docker buildx bake -f docker-bake.dev.hcl --print db [+] Building 0.0s (0/0) { \u0026#34;group\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;db\u0026#34; ] } }, \u0026#34;target\u0026#34;: { \u0026#34;db\u0026#34;: { \u0026#34;context\u0026#34;: \u0026#34;.\u0026#34;, \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile.db\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;xianpengshen/docker-buildx-bake-demo:db\u0026#34; ] } } } 4. 发布构建镜像 # 通过添加 --push 选项可以将构建完成的镜像一键发布的镜像仓库，例如 $ docker buildx bake -f docker-bake.dev.hcl --push db webapp-release\n以上示例中的 demo 放在这里了：https://github.com/shenxianpeng/docker-buildx-bake-demo\n5. Buildx Bake 高级用法 # Buildx Bake 还有其他更多的使用技巧，比如 variable, function , matrix 等这里就不一一介绍了，详情请参见官方 Buildx Bake reference 文档。\n总结 # Docker Buildx Bake 是一个功能强大的构建工具，它提供了一种简化和加速构建过程的方法。通过使用 Bake 你可以高效地构建和测试多个镜像，并且可以跨多个平台和架构进行构建。所以说 Bake 是开发人员和构建工程师的重要利器，掌握 Docker Buildx Bake 的使用方法将帮助你更好地应对多镜像构建的带来的挑战、加快应用程序的交付速度。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-07-17","externalUrl":null,"permalink":"/posts/buildx-bake/","section":"Posts","summary":"本文介绍 Docker Buildx Bake 的概念、优势、使用场景以及如何使用该功能来加速构建和管理多平台镜像。","title":"Docker Buildx Bake 加速构建和管理多平台镜像的利器","type":"posts"},{"content":"","date":"2023-06-10","externalUrl":null,"permalink":"/en/tags/sbom/","section":"Tags","summary":"","title":"SBOM","type":"tags"},{"content":" What is SBOM # SBOM stands for Software Bill of Materials. It is a detailed inventory of all components, libraries, and dependencies used in the software building process.\nAn SBOM is similar to a product\u0026rsquo;s ingredient list. It lists the various elements that make up a software application, including open-source software components, third-party libraries, frameworks, and tools. Each element in the SBOM will have detailed information such as name, version number, license information, and dependencies.\nThe purpose of SBOM is to increase the visibility and transparency of the software supply chain and provide better risk management and security. It helps software developers, vendors, and users understand the components and dependencies used in their software, allowing for better management of potential vulnerabilities, security risks, and compliance issues. Through SBOM, users can identify and track any potential vulnerabilities or known security issues in the software and take appropriate remedial measures in a timely manner.\nSBOM can also be used for software audits, compliance requirements, and regulatory compliance. Some industry standards and regulations (such as the Software Supply Chain Security Framework (SSCF) and the EU Network and Information Security Directive (NIS Directive)) already require software vendors to provide SBOMs to improve the security and trustworthiness of the software supply chain.\nIn short, an SBOM is a record of all components and dependencies used in the software building process. It provides visibility into the software supply chain, helps manage risks, improves security, and meets compliance requirements.\nRelationship and Differences Between SBOM and SLSA # SBOM (Software Bill of Materials) and SLSA (Supply Chain Levels for Software Artifacts) are two different but related concepts.\nSBOM is a software bill of materials that provides visibility into the software supply chain, including component versions, license information, vulnerabilities, etc. SBOM aims to help organizations better manage and control the software supply chain, identifying and addressing potential vulnerabilities, compliance issues, and security risks. SLSA is a supply chain security framework that defines different levels of security requirements and practices to ensure the security of the software supply chain. SLSA aims to strengthen the credibility and security of software and prevent the spread of malicious code, supply chain attacks, and vulnerabilities. SLSA focuses on the security of the entire software supply chain, including the origin, verification, build process, and release mechanism of components. Regarding the differences:\nDifferent Perspectives: SBOM focuses on the inventory and visibility of software building materials, providing details on components and dependencies. SLSA focuses on supply chain security, defining security levels and practices, emphasizing ensuring the credibility and security of the software supply chain. Different Uses: SBOM is used to identify and manage components, vulnerabilities, and compliance issues in software builds. It provides a tool for managing software supply chain risks. SLSA provides a security framework that helps organizations ensure the security of their software supply chain by defining security levels and requirements. Correlation: SLSA can utilize SBOM as part of its implementation. SBOM provides the details of components and dependencies required by SLSA, which helps to verify and audit the security of the supply chain. SLSA practices may include requiring the generation and verification of SBOMs to ensure the visibility and integrity of the software supply chain. SBOM and SLSA are both key concepts in software supply chain security. They can be used in conjunction with each other and complement each other to strengthen software supply chain security and management.\nDifferences Between SBOM and Black Duck # SBOM (Software Bill of Materials) and Synopsys Black Duck are two related but distinct concepts. Here\u0026rsquo;s a comparison:\nSBOM:\nDefinition: An SBOM is a document or inventory that records all components and dependencies used in the software building process. It provides visibility and transparency into the software supply chain. Content: An SBOM lists details for each component, including name, version number, author, license information, etc. It helps track and manage software components, dependencies, vulnerabilities, and license compliance. Uses: SBOM is used for software supply chain management, security audits, compliance verification, and risk management. It helps organizations understand the components used in software builds, identify potential vulnerabilities and risks, and ensure compliance. Synopsys Black Duck:\nFunctionality: Synopsys Black Duck is a supply chain risk management tool. It can scan software projects, identify the open-source components and third-party libraries used, and analyze their license compliance, security vulnerabilities, and other potential risks. Features: Black Duck has an extensive vulnerability database and license knowledge base, integrates with development processes and CI/CD tools, and provides vulnerability alerts, license compliance reports, and risk analysis. Purpose: Black Duck helps organizations manage and control software supply chain risks, providing real-time security and compliance information on open-source components and third-party libraries to support decision-making and appropriate actions. In summary, SBOM records the components and dependencies used in software builds, providing visibility and management of the software supply chain. Black Duck is a supply chain risk management tool that provides license compliance, security vulnerability, and risk analysis by scanning and analyzing open-source components and third-party libraries in software projects. Black Duck can be used to generate SBOMs and provide more comprehensive risk and compliance analysis. Therefore, Black Duck is a specific tool, while SBOM is a concept for recording and managing software supply chain information.\nSBOM Best Practices # Automated Generation: Use automated tools to generate SBOMs, avoiding manual creation and maintenance to ensure accuracy and consistency. Include Detailed Information: Include as much detail as possible in the SBOM, such as component name, version number, author, license information, dependencies, and vulnerability information. Regular Updates: Regularly update the SBOM to reflect the latest build materials, ensuring its accuracy and completeness. Version Control: Establish and manage corresponding SBOM versions for each software version to track software versions and their corresponding build materials. Integration into the Software Lifecycle: Integrate SBOM into the entire software lifecycle, including development, build, test, deployment, and maintenance phases. Vulnerability Management and Risk Assessment: Utilize vulnerability information in the SBOM, integrate with vulnerability databases, and perform vulnerability management and risk assessments. Vendor Collaboration: Share and obtain SBOM information with vendors and partners, ensuring they also provide accurate SBOMs and continuously monitor their vulnerability management and compliance measures. SBOM Generation Tools # CycloneDX: CycloneDX is an open software component description standard used to generate and share SBOMs. It supports multiple languages and build tools and has a broad ecosystem and tool integrations. SPDX: SPDX (Software Package Data Exchange) is an open standard for describing software components and related license information. It provides a unified way to generate and exchange SBOMs. OWASP Dependency-Track: Dependency-Track is an open-source supply chain security platform that can generate and analyze SBOMs, providing vulnerability management, license compliance, and supply chain visualization. WhiteSource: WhiteSource is a supply chain management tool that provides automated open-source component identification, license management, and vulnerability analysis, generating SBOMs and performing risk assessments. JFrog Xray: JFrog Xray is a software supply chain analysis tool that can scan and analyze bills of materials, providing vulnerability alerts, license compliance, and security analysis. Microsoft sbom-tool: A highly scalable, enterprise-ready tool for creating SPDX 2.2-compliant SBOMs for various artifacts. trivy: Supports finding vulnerabilities, misconfigurations, secrets in containers, Kubernetes, code repositories, Cloud, etc., and generating SBOMs. In addition to these, several other tools provide SBOM generation, management, and analysis capabilities. You can choose the appropriate tool based on your specific needs to implement SBOM best practices.\nSummary # This article aims to help you understand the concept of SBOM, its relationship and differences with SLSA and Black Duck, best practices, and available generation tools to better manage software supply chain security.\n","date":"2023-06-10","externalUrl":null,"permalink":"/en/posts/sbom/","section":"Posts","summary":"This article introduces the definition of SBOM, its relationship and differences with SLSA and Black Duck, best practices, and available generation tools, helping readers better understand and apply SBOM.","title":"Understanding SBOM - Definition, Relationships, Differences, Best Practices, and Generation Tools","type":"posts"},{"content":"想必你也见到过很多开源项目中的 CONTRIBUTION.md 文档中通常都会让贡献者 Fork 仓库，然后做修改。\n那么如果你是该开源项目中的成员是否需要 Fork 仓库进行修改呢？\n以前我没有认真去想过这个问题，对于项目成员感觉 Fork 或不 Fork 好像差不多，但仔细想想 Fork 仓库与不 Fork 仓库其实是有以下几个主要的差别的：\n修改权限 # 在原始仓库中，你可能没有直接修改代码的权限，当你 Fork 一个仓库时，会创建一个属于你自己的副本，你可以在这个副本中拥有完全的修改权限，你可以自由地进行更改、添加新功能、解决bug等，而不会对原始仓库产生直接影响。\n做实验性的工作 # 如果你计划进行较大的修改或实验性工作，并且不希望直接影响原始仓库，那么 fork 仓库并在 fork 的中进行修改更为合适。\n比如你需要实验性的去大量清理现有仓库里的一些垃圾文件或是代码，如果你需要需要多次尝试，并将多次修改直接 git push 到推送原始仓库进行保存或是测试，这大大增加原始仓库的存储空间，如果你的修改是大型文件，那么对原始仓库的存储空间影响则会更大；如果你是 Fork 仓库则不会造成原始仓库的影响，直到你完成修改通过 Pull Request 合并到原始仓库时才会产生新的存储空间。\n代码审查和协作 # 当你 Fork 一个仓库并在自己的副本中进行修改后，你必须通过 Pull Request（PR）向原始仓库合并修改，有助于确保代码质量和功能正确性。（当然不 Fork 也可以这样做或不做，但 Fork 了就必须这样做了）\n版本控制和历史记录 # Fork 一个仓库后，你可以在自己的副本中维护独立的版本控制历史。你可以跟踪自己的更改、回溯历史、管理代码版本，而不会影响到原始仓库的版本控制。同时，你可以从原始仓库同步最新的更改，保持你的副本与原始仓库的同步。\n总结 # Fork 仓库与不 Fork 仓库的主要差别在于修改权限、做实验性的工作、代码审查和协作，以及版本控制和历史记录。\n个人认为只要一个仓库的贡献者超过 3 人，都建议所有人都 Fork 原始仓库，通过 Pull Request 方式合并代码。\n但也有例外情况可能不适合 Fork：项目在 Fork 之后 CI/CD 无法独立工作，但是你需要它们。比如 Fork 后的仓库因为环境等原因不支持独立的运行 CI/CD 而你需要在提交 Pull Request 之前通过自动化对分支进行测试。\n另外还要为原始仓库需要做适当的分支权限设置，以防止就算两个人的团队另外一个人不熟悉 Git 使用了非常危险的操作，比如强制推送（Force Push），变基（Rebasing），强制检出（Force Checkout）可能导致代码丢失、数据损坏或版本控制问题。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-06-09","externalUrl":null,"permalink":"/posts/fork-or-unfork/","section":"Posts","summary":"本文讨论了在开源项目中，作为项目成员是 Fork 原始仓库还是直接在原始仓库中修改代码的利弊，帮助开发者做出更合适的选择。","title":"如果你是项目成员，是 Fork 原始仓库还是直接原始仓库中修改代码？","type":"posts"},{"content":"Git commit message and Git branch naming conventions are a very important part of team collaboration. They can make the codebase more standardized, easier to maintain, and easier to understand.\nWe need to use tools to help implement Git commit message and branch creation conventions. This article will introduce how to use the Commit Check tool to verify commit messages, branch names, committer usernames, and committer email addresses to ensure they conform to specifications.\nFor more information on Git commit messages and branch creation conventions, please refer to my previous article, “Programmer\u0026rsquo;s Self-Cultivation—Git Commit Message and Branch Creation Conventions”. I will not reiterate them here.\nCommit Check Introduction # Commit Check is a tool that can check Git commit messages, branch names, committer usernames, committer emails, and more. It is an open-source alternative to Yet Another Commit Checker.\nCommit Check Configuration # Using Default Settings # Without custom settings, Commit Check will use the default settings. Specific settings are available here.\nBy default, commit messages follow Conventional Commits, and branch naming follows the branch model details.\nUsing Custom Configuration # You can create a configuration file .commit-check.yml in your Git repository to customize the settings. For example:\nchecks: - check: message regex: \u0026#39;^(build|chore|ci|docs|feat|fix|perf|refactor|revert|style|test){1}(\\([\\w\\-\\.]+\\))?(!)?: ([\\w ])+([\\s\\S]*)|(Merge).*|(fixup!.*)\u0026#39; error: \u0026#34;The commit message should be structured as follows:\\n\\n \u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt;\\n [optional body]\\n [optional footer(s)]\\n\\n More details please refer to https://www.conventionalcommits.org\u0026#34; suggest: please check your commit message whether matches above regex - check: branch regex: ^(bugfix|feature|release|hotfix|task)\\/.+|(master)|(main)|(HEAD)|(PR-.+) error: \u0026#34;Branches must begin with these types: bugfix/ feature/ release/ hotfix/ task/\u0026#34; suggest: run command `git checkout -b type/branch_name` - check: author_name regex: ^[A-Za-z ,.\\\u0026#39;-]+$|.*(\\[bot]) error: The committer name seems invalid suggest: run command `git config user.name \u0026#34;Your Name\u0026#34;` - check: author_email regex: ^\\S+@\\S+\\.\\S+$ error: The committer email seems invalid suggest: run command `git config user.email yourname@example.com` You can modify the values of regex, error, and suggest according to your needs.\nCommit Check Usage # Commit Check supports multiple usage methods.\nRunning with GitHub Actions # For example, create a GitHub Actions workflow file .github/workflows/commit-check.yml:\nname: Commit Check on: push: pull_request: branches: \u0026#39;main\u0026#39; jobs: commit-check: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: commit-check-action@v1 with: message: true branch: true author-name: true author-email: true dry-run: true job-summary: true For details, please refer to https://github.com/commit-check-action\nRunning with pre-commit hook # First, you need to install pre-commit.\nThen add the following settings to your .pre-commit-config.yaml file.\n- repo: https://github.com/commit-check rev: the tag or revision hooks: # support hooks - id: check-message - id: check-branch - id: check-author-name - id: check-author-email Running from the Command Line # Install via pip:\npip install commit-check Then run the commit-check --help command to see how to use it. Details can be found in the documentation.\nRunning with Git Hooks # To configure Git Hooks, you need to create a new script file in the .git/hooks/ directory of your Git repository.\nFor example, .git/hooks/pre-push, the file contents are as follows:\n#!/bin/sh commit-check --message --branch --author-name --author-email Make it executable with chmod +x .git/hooks/pre-push. Then, when you run the git push command, this push hook will execute automatically.\nCommit Check Failure Example # Commit message check failure:\nCommit rejected by Commit-Check. (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) / ._. \\ / ._. \\ / ._. \\ / ._. \\ / ._. \\ __\\( C )/__ __\\( H )/__ __\\( E )/__ __\\( C )/__ __\\( K )/__ (_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._) || E || || R || || R || || O || || R || _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ (.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.) `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ Invalid commit message =\u0026gt; test It doesn\u0026#39;t match regex: ^(build|chore|ci|docs|feat|fix|perf|refactor|revert|style|test){1}(\\([\\w\\-\\.]+\\))?(!)?: ([\\w ])+([\\s\\S]*) The commit message should be structured as follows: \u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt; [optional body] [optional footer(s)] More details please refer to https://www.conventionalcommits.org Suggest: please check your commit message whether matches above regex Branch name check failure:\nCommit rejected by Commit-Check. (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) / ._. \\ / ._. \\ / ._. \\ / ._. \\ / ._. \\ __\\( C )/__ __\\( H )/__ __\\( E )/__ __\\( C )/__ __\\( K )/__ (_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._) || E || || R || || R || || O || || R || _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ (.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.) `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ Commit rejected. Invalid branch name =\u0026gt; test It doesn\u0026#39;t match regex: ^(bugfix|feature|release|hotfix|task)\\/.+|(master)|(main)|(HEAD)|(PR-.+) Branches must begin with these types: bugfix/ feature/ release/ hotfix/ task/ Suggest: run command `git checkout -b type/branch_name` This concludes the introduction to Commit Check. For more information, please refer to https://github.com/commit-check\nPlease indicate the author and source when reprinting this article. Do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2023-05-09","externalUrl":null,"permalink":"/en/posts/commit-check/","section":"Posts","summary":"This article introduces how to use the Commit Check tool to verify whether Git commit messages, branch names, committer usernames, and committer email addresses conform to specifications.","title":"Programmer's Self-Cultivation — Git Commit Message and Branch Creation Conventions (Tools)","type":"posts"},{"content":"With the increasing number of attacks targeting software supply chains in recent years, Google has released a series of guidelines to ensure the integrity of software packages, aiming to prevent unauthorized code modifications from affecting the software supply chain.\nGoogle\u0026rsquo;s SLSA framework (Supply-chain Levels for Software Artifacts) provides recommendations for achieving more secure software development and deployment processes by identifying and mitigating issues in CI/CD pipelines.\nTable of Contents # What is SLSA Problems in the Software Supply Chain 2.1 What are Supply Chain Attacks? 2.2 Real-world Examples SLSA Levels 3.1 Detailed Explanation 3.2 Limitations SLSA Implementation Other Tools What is SLSA # SLSA stands for Supply chain Levels for Software Artifacts, or SLSA (pronounced \u0026ldquo;salsa\u0026rdquo;).\nSLSA is an end-to-end framework, a standard and checklist of controls to ensure the security of software building and deployment processes, preventing threats arising from tampering with source code, build platforms, and component repositories.\nProblems in the Software Supply Chain # Any software supply chain can introduce vulnerabilities. As systems become increasingly complex, it becomes crucial to implement best practices to ensure the integrity of delivered artifacts. Without certain standards and systematic development plans, it is difficult to cope with the next hacker attack.\nWhat are Supply Chain Attacks? # A Submitting unauthenticated modifications B Leaking the source code repository C Building from modified source code D Leaking the build process E Using compromised dependencies F Uploading modified packages G Leaking the package repository H Using compromised packages\nReal-world Examples # Integrity Threat Known Example How SLSA Helps A Submitting unauthenticated modifications Researchers attempting to introduce vulnerabilities into the Linux kernel via patches on the mailing list. Research Two-person review caught most (but not all) vulnerabilities. B Leaking the source code repository PHP: Attackers compromised PHP\u0026rsquo;s self-hosted git server and injected two malicious commits. A better-protected source code platform would make it more difficult for attackers to succeed. C Building from modified source code Webmin: Attackers modified the build infrastructure to use source files that didn\u0026rsquo;t match source control. SLSA compliant build servers generate provenance to identify the actual sources used, enabling consumers to detect such tampering. D Leaking the build process SolarWinds: Attackers compromised the build platform and installed implants that injected malicious behavior during each build. Higher SLSA levels require stronger security controls on the build platform making compromise and gaining persistence more difficult. E Using compromised dependencies event-stream: Attackers added a seemingly harmless dependency, then updated that dependency to add malicious behavior. The update didn\u0026rsquo;t match the code submitted to GitHub (i.e., attack F). Recursively applying SLSA to all dependencies blocks this specific vector because provenance would show it wasn\u0026rsquo;t built by the proper builder or the source wasn\u0026rsquo;t from GitHub. F Uploading modified packages CodeCov: Attackers used leaked credentials to upload malicious artifacts to Google Cloud Storage (GCS) from which users could download directly. Provenance of artifacts in GCS showed the artifacts weren\u0026rsquo;t built from the expected source code repository in the expected way. G Leaking the package repository Attacks on package mirrors: Researchers ran mirrors for several popular package repositories, which could be used to serve malicious packages. Similar to (F) above, the provenance of malicious artifacts would show they weren\u0026rsquo;t built as expected nor from the expected source code repository. H Using compromised packages Browserify typosquatting: Attackers uploaded a malicious package with a similar name to the original. SLSA doesn\u0026rsquo;t directly address this threat, but linking provenance back to source control enables and enhances other solutions. SLSA Levels # Level Description Example 1 Documentation of the build process Unsigned provenance 2 Tamper-resistant build service Hosted source/build, signed provenance 3 Additional resistance to specific threats Security controls on the host, unforgeable provenance 4 Highest level of confidence and trust Two-person review + hermetic build Detailed Explanation # Level Requirements 0 No guarantees. SLSA 0 indicates a lack of any SLSA level. 1 The build process must be fully scripted/automated and generate provenance. Provenance is metadata about how an artifact was built, including the build process, top-level source, and dependencies. Understanding provenance allows software consumers to make risk-based security decisions. SLSA 1 Provenance doesn\u0026rsquo;t prevent tampering, but it provides a basic level of code origin identification and helps with vulnerability management. 2 Requires version control and a hosted build service that generates authenticated provenance. These additional requirements give software consumers more confidence in the software\u0026rsquo;s origin. At this level, provenance is tamper-resistant to the degree that the build service is trusted. SLSA 2 also provides an easy path to upgrade to SLSA 3. 3 Source and build platforms meet specific standards to guarantee source auditability and provenance integrity, respectively. We envision a certification process where auditors can attest that platforms meet requirements, which consumers can then trust. SLSA 3 provides stronger tamper resistance than earlier levels by preventing specific classes of threats (e.g., cross-build contamination). 4 Requires two-person review of all changes and a hermetic, reproducible build process. Two-person review is an industry best practice for finding errors and preventing malicious behavior. Hermetic builds guarantee the dependency list of the provenance is complete. Reproducible builds, while not strictly required, provide numerous auditability and reliability benefits. Overall, SLSA 4 gives consumers high confidence that the software hasn\u0026rsquo;t been tampered with. Limitations # SLSA can help reduce supply chain threats in software artifacts, but it also has limitations.\nMany artifacts have a large number of dependencies in the supply chain, and the complete dependency graph can be very large. Teams actually doing security work need to identify and focus on important components in the supply chain. This can be done manually, but the workload can be significant. The SLSA level of an artifact is not transitive and dependencies have their own SLSA rating, meaning that an SLSA 4 artifact can be built from SLSA 0 dependencies. Therefore, while the main artifact has strong security, there may still be risks elsewhere. The sum of these risks will help software consumers understand how and where to use SLSA 4 artifacts. While automation of these tasks would help, a full review of the entire graph for every software artifact for every consumer isn\u0026rsquo;t practical. To bridge this gap, auditors and certification bodies can verify and attest that something meets SLSA requirements. This might be particularly valuable for closed-source software. As part of the SLSA roadmap, the SLSA team will also continue to explore how to identify important components, how to determine the overall risk of the entire supply chain, and the role of certification.\nSLSA Implementation # SLSA is a standard, but how do we implement it?\nWe can use the summary table of SLSA Requirements to check one by one and see which security level our current CI/CD workflow is at.\nAre there tools that can better help us check and guide us on how to improve the security level?\nCurrently, there are only a few tools that can achieve this, and the vast majority are limited to GitHub.\nOpenSSF Scorecard is an automated tool from the Open Source Security Foundation (OpenSSF) for checking security metrics of open-source software. It helps open-source maintainers improve their security best practices and helps open-source consumers judge whether their dependencies are secure.\nIt does this by assessing many important projects related to software security and assigning a score of 0-10 to each check. You can use these scores to understand specific areas that need improvement to strengthen the security posture of the project. It can also assess risks introduced by dependencies and make informed decisions about accepting those risks, assessing alternative solutions, or collaborating with maintainers to make improvements.\nOther Tools # slsa-verifier - Verifies build provenance compliant with SLSA standards Sigstore - A new standard for signing, verifying, and securing software Please indicate the author and source when reprinting this article. Please do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2023-03-23","externalUrl":null,"permalink":"/en/posts/slsa/","section":"Posts","summary":"This article introduces the concept, purpose, and levels of the SLSA framework, and how to apply SLSA in the software supply chain to improve security. It helps readers understand the importance of SLSA in software development and deployment.","title":"The SLSA Framework and Software Supply Chain Security Protection","type":"posts"},{"content":"","date":"2023-02-26","externalUrl":null,"permalink":"/tags/chatgpt/","section":"标签","summary":"","title":"ChatGPT","type":"tags"},{"content":"随着 DevOps 的流行，越来越多的开发团队正在寻找一些工具来帮助他们更好地完成任务。ChatGPT 是一款基于人工智能的自然语言处理工具，它可以用来帮助开发团队在 DevOps 任务中更加高效地工作。\n本文将探讨如何在 DevOps 任务中使用 ChatGPT。\n一、ChatGPT 简介 # ChatGPT 是一款由 OpenAI 开发的人工智能自然语言处理工具。它可以用于许多不同的应用程序，例如语音识别、自然语言处理、文本生成等。 ChatGPT 使用深度学习技术，可以生成与输入内容相关的文本。它是一款非常强大的工具，可以帮助开发团队更加高效地工作。\n二、ChatGPT 在 DevOps 中的应用 # 在 DevOps 中，开发团队通常需要快速解决问题，并与团队成员和客户进行有效沟通。ChatGPT 可以用来帮助解决这些问题。\n自动化代码审查 开发团队通常需要花费大量时间来进行代码审查。ChatGPT 可以用来自动化这个过程。它可以根据代码库中的样本代码，生成与样本代码风格相似的代码，并对新代码进行审查。这可以帮助开发团队更快地进行代码审查，并减少人为错误的可能性。\n自动化测试 测试是 DevOps 中不可或缺的一部分。ChatGPT 可以用来自动化测试。它可以根据测试用例生成相应的测试代码，并对测试结果进行评估。这可以帮助开发团队更快地进行测试，并减少人为错误的可能性。\n自动化部署 部署是 DevOps 中不可或缺的一部分。ChatGPT 可以用来自动化部署。它可以根据部署规则生成相应的部署代码，并对部署结果进行评估。这可以帮助开发团队更快地进行部署，并减少人为错误的可能性。\n自动化文档生成 文档是 DevOps 中不可或缺的一部分。ChatGPT 可以用来自动化文档生成。它可以根据项目的代码库和测试用例生成相应的文档，并对文档的质量进行评估。这可以帮助开发团队更快地生成文档，并减少人为错误的可能性。\n三、如何使用 ChatGPT # 要使用 ChatGPT，开发团队需要进行以下步骤：\n收集数据 收集数据是使用 ChatGPT 的第一步。开发团队需要收集开发团队需要收集与其任务相关的数据，例如代码库、测试用例、部署规则和文档。这些数据将用于训练 ChatGPT 模型，以生成与任务相关的文本。\n训练 ChatGPT 模型 训练 ChatGPT 模型是使用 ChatGPT 的第二步。开发团队可以使用已有的数据来训练模型，也可以使用迁移学习技术，将已有的 ChatGPT 模型进行微调，以适应其任务的需求。训练好的 ChatGPT 模型将用于生成与任务相关的文本。\n集成 ChatGPT 模型 集成 ChatGPT 模型是使用 ChatGPT 的第三步。开发团队可以将 ChatGPT 模型集成到其 DevOps 工具链中。例如，可以将 ChatGPT 模型集成到自动化代码审查工具、自动化测试工具、自动化部署工具和自动化文档生成工具中。这将使这些工具更加智能化，并帮助开发团队更加高效地工作。\n优化 ChatGPT 模型 优化 ChatGPT 模型是使用 ChatGPT 的第四步。开发团队需要定期监控 ChatGPT 模型的性能，并对其进行优化。例如，可以增加更多的训练数据、调整模型的超参数、增加正则化约束等。这将有助于提高 ChatGPT 模型的准确性和性能，并使其更加适应任务需求。\n四、结论 # 在 DevOps 任务中使用 ChatGPT 可以帮助开发团队更加高效地工作。ChatGPT 可以用来自动化代码审查、自动化测试、自动化部署和自动化文档生成等任务。开发团队需要收集与其任务相关的数据，训练 ChatGPT 模型，并将其集成到其 DevOps 工具链中。开发团队还需要定期监控 ChatGPT 模型的性能，并对其进行优化。这将有助于提高 ChatGPT 模型的准确性和性能，并使其更加适应任务需求。\n对了，本篇文章是由 ChatGPT 生成的，你觉得它写的怎么样，像个油腻的中年人？\n","date":"2023-02-26","externalUrl":null,"permalink":"/posts/chatgpt-for-devops/","section":"Posts","summary":"本文探讨如何在 DevOps 任务中使用 ChatGPT，包括自动化代码审查、测试、部署和文档生成等方面的应用。","title":"如何在 DevOps 任务中使用 ChatGPT?","type":"posts"},{"content":"As the title says, the reason why your Jenkins Controller is getting slower might be due to not following some best practices in Jenkins pipeline writing.\nTherefore, this article mainly introduces some best practices for Jenkins pipelines, aiming to show pipeline authors and maintainers some \u0026ldquo;anti-patterns\u0026rdquo; they might not have been aware of in the past.\nI will try to list all possible Pipeline best practices and provide some concrete examples from practice.\nGeneral Issues # Ensure using Groovy code as glue in your pipeline # Use Groovy code to connect a set of actions rather than as the main functionality of the pipeline.\nIn other words, instead of relying on pipeline features (Groovy or pipeline steps) to drive the build process forward, use individual steps (e.g., sh) to accomplish multiple parts of the build.\nPipelines, as they increase in complexity (amount of Groovy code, number of steps used, etc.), require more resources (CPU, memory, storage) on the controller. Think of the Pipeline as a tool to accomplish the build, not the core of the build.\nExample: Drive the build through its build/test/deploy process using a single Maven build step.\nRunning shell scripts in Jenkins pipeline # Using shell scripts in Jenkins Pipeline can help simplify builds by merging multiple steps into a single stage. Shell scripts also allow users to add or update commands without having to modify each step or stage individually.\nUsing shell scripts in Jenkins Pipeline and its benefits:\nAvoid complex Groovy code in pipelines # For pipelines, Groovy code is always executed on the controller, meaning it consumes controller resources (memory and CPU).\nTherefore, it is crucial to reduce the amount of Groovy code executed by the Pipeline (this includes any methods called on classes imported within the Pipeline). Here are examples of the most common Groovy methods to avoid:\nJsonSlurper: This function (and several other similar functions like XmlSlurper or readFile) can be used to read data from a file on disk, parse the data in that file into a JSON object, and then use JsonSlurper().parseText(readFile(\u0026quot;$LOCAL_FILE\u0026quot;)). This command loads the local file into the controller\u0026rsquo;s memory twice, which will require a lot of memory if the file is large or the command is executed frequently.\nSolution: Instead of using JsonSlurper, use a shell step and return standard output. This shell would look something like this: def JsonReturn = sh label: '', returnStdout: true, script: 'echo \u0026quot;$LOCAL_FILE\u0026quot;| jq \u0026quot;$PARSING_QUERY\u0026quot;'. This will use agent resources to read the file, and $PARSING_QUERY will help parse the file into a smaller size.\nHttpRequest: This command is frequently used to fetch data from external sources and store it in variables. This is not ideal because not only is the request made directly from the controller (which might give erroneous results for things like HTTPS requests if the controller doesn\u0026rsquo;t have certificates loaded), but the response to that request is stored twice.\nSolution: Use a shell step to perform the HTTP request from the agent, for example using tools like curl or wget, depending on the context. If the result must be in the Pipeline afterwards, try filtering the result on the agent side so that only the minimal necessary information has to be sent back to the Jenkins controller.\nReduce repetition of similar pipeline steps # Combine pipeline steps into single steps as much as possible to reduce the overhead caused by the pipeline execution engine itself.\nFor example, if you run three shell steps consecutively, each step must start and stop, requiring the creation and cleanup of connections and resources on the agent and controller.\nHowever, if you put all commands into a single shell step, only one step needs to be started and stopped.\nExample: Instead of creating a series of echo or sh steps, combine them into one step or script.\nAvoid calling Jenkins.getInstance # Using Jenkins.instance or its accessor methods in a Pipeline or shared library is indicative of code abuse within that Pipeline/shared library.\nUsing the Jenkins API from a non-sandboxed shared library means the shared library is both a shared library and a kind of Jenkins plugin.\nGreat care must be taken when interacting with the Jenkins API from a pipeline to avoid serious security and performance issues. If you must use the Jenkins API in your build, the recommended approach is to create a minimal plugin in Java that implements a secure wrapper around the Jenkins API that you want to access using the Pipeline’s Step API.\nUsing the Jenkins API directly from a sandboxed Jenkinsfile means you might have to whitelist methods allowed by the sandbox protection, which can be bypassed by anyone who can modify the pipeline, which is a major security risk. Whitelisted methods run as the system user, with overall administrator permissions, which might give developers more permissions than expected.\nSolution: The best solution is to address the calls being made, but if these calls must be done, then it’s best to implement a Jenkins plugin that can collect the data needed.\nClean up old Jenkins builds # As a Jenkins administrator, deleting old or unnecessary builds can allow the Jenkins controller to run efficiently.\nResources for updates and related versions are reduced when you don\u0026rsquo;t delete old builds. buildDiscarder can be used in each pipeline job to keep a specific number of historical builds.\nUsing Shared Libraries # Don\u0026rsquo;t override built-in pipeline steps # Stay away from custom/overridden pipeline steps as much as possible. Overriding built-in pipeline steps is the process of overriding standard pipeline APIs (like sh or timeout) using shared libraries. This practice is dangerous because pipeline APIs can change at any time, causing custom code to break or give different results than expected.\nTroubleshooting is difficult when custom code breaks due to Pipeline API changes, because even if the custom code hasn\u0026rsquo;t changed, it might not work after an API update.\nTherefore, even if the custom code hasn’t changed, it doesn’t mean it will remain unchanged after an API update.\nFinally, due to the widespread use of these steps throughout the pipeline, if something is coded incorrectly/inefficiently, the results can be catastrophic for Jenkins.\nAvoid large global variable declaration files # Having large variable declaration files can require significant memory with little to no benefit, since the file is loaded for every pipeline regardless of whether the variables are needed.\nIt is recommended to create small variable files containing only the variables relevant to the current execution.\nAvoid very large shared libraries # Using large shared libraries in Pipelines requires checking out a very large file before the Pipeline starts and loading the same shared library for every job currently executing, which leads to increased memory overhead and slower execution times.\nAnswering Other Common Questions # Handling concurrency in pipelines # Try to avoid sharing workspaces across multiple pipeline executions or multiple different pipelines. This practice might lead to unexpected file modifications or workspace renaming in each pipeline.\nIdeally, the shared volume/disk is mounted in a separate location, files are copied from that location to the current workspace, and then, when the build completes, if any updates are done, files can be copied back.\nBuild different containers, creating the needed resources from scratch (cloud-type agents are very suitable for this). Building these containers will ensure the build process starts from scratch every time and is easily repeatable. If building containers doesn’t work, disable concurrency on the pipeline or use a lockable resources plugin to lock the workspace at runtime so that other builds can’t use it while it’s locked.\nWarning: Disabling concurrency at runtime or locking the workspace might cause pipelines to be blocked while waiting for the resources if these resources are arbitrarily locked.\nAlso, note that both these approaches take longer to get build results compared to using unique resources for each job.\nAvoiding NotSerializableException # Pipeline code undergoes CPS transformation to allow pipelines to resume after a Jenkins restart. That is, you can shut down Jenkins or lose connection to the agent while the pipeline is running your script. When it comes back, Jenkins remembers what it was doing, and your pipeline script will resume execution as if it had never been interrupted. An execution technique called \u0026ldquo;Continuous Pipe Style (CPS)\u0026rdquo; plays a key role in resuming the pipeline. However, due to CPS transformation, some Groovy expressions will not work correctly.\nBehind the scenes, CPS relies on being able to serialize the current state of the pipeline, as well as the remainder of the pipeline to be executed. This means using unserializable objects in the pipeline will trigger a NotSerializableException being thrown when the pipeline tries to persist its state.\nFor more details and some examples that might be problematic, see Pipeline CPS Method Mismatches.\nTechniques to ensure pipelines run as expected will be described below.\nEnsuring persistent variables are serializable # During serialization, local variables are captured as part of the pipeline state. This means storing unserializable objects in variables during pipeline execution will result in a NotSerializableException being thrown.\nDon\u0026rsquo;t assign unserializable objects to variables # One strategy is to leverage the fact that unserializable objects always infer their values “just in time” instead of computing their values and storing that value in a variable.\nUsing @NonCPS # If necessary, you can use the @NonCPS annotation to disable CPS transformation for specific methods, whose body will not execute correctly if it is CPS transformed. Note that this also means the Groovy function will have to restart completely, since it is not transformed.\nAsynchronous pipeline steps (such as sh and sleep) are always CPS-transformed and cannot be used inside methods annotated with @NonCPS. Generally, you should avoid using pipeline steps inside methods annotated with @NonCPS.\nPipeline Durability # It is worth noting that changing the durability of the pipeline might result in NotSerializableException not being thrown where they would otherwise be thrown. This is because lowering the pipeline\u0026rsquo;s durability via PERFORMANCE_OPTIMIZED means the pipeline\u0026rsquo;s current state is persisted far less frequently. As a result, the pipeline never attempts to serialize unserializable values, and thus, no exception is thrown.\nThe presence of this note is to inform the user of the root cause of this behavior. Setting the pipeline\u0026rsquo;s durability to performance optimized is not recommended purely to avoid serialization problems.\nhttps://www.jenkins.io/doc/book/pipeline/pipeline-best-practices/ https://www.cloudbees.com/blog/top-10-best-practices-jenkins-pipeline-plugin https://github.com/jenkinsci/pipeline-examples/blob/master/docs/BEST_PRACTICES.md https://devopscook.com/jenkinsfile-best-practices/ ","date":"2023-02-06","externalUrl":null,"permalink":"/en/posts/jenkins-pipeline-best-practices/","section":"Posts","summary":"This article introduces some best practices for Jenkins pipelines, aiming to help developers and operations personnel optimize Jenkins\u0026rsquo; performance and maintainability.","title":"Why is my Jenkins Controller getting slower—Possible mistakes you might be making...","type":"posts"},{"content":"时间过得好快，又过完了一年。\n今年想写一些总结回顾一下过去的一年发生在自己身上的重要事件。\n由于 2021 年没有写年终总结，2021 年在我的脑海里已经变化模糊，我只能凭着一些照片和日记才想起来的一些事情。看来以后的年终总结不能落下。\n回顾 2021 # 2021 年我的个人关键词是“最后的潇洒”。\n年初我搬家了，新家离公司开车只要十几分钟。这节省了很多花在路上的时间，周末我也更愿意往公司跑。\n四月，第一次 Fork 并开始维护 cpp-linter-action 这个开源项目，并且吸引了另外一个开发者与我共同维护。\n七月，广鹿岛旅行\n八月，老婆怀孕了。现在回看 2021 的照片，两个人的生活真的是太自在和潇洒了，两人吃饱全家不饿。\n十一月，被告知为次密接，要求去酒店隔离。就这样我们被拉去酒店隔离了一周，然后回家后又要求隔离一周。现在想想呵呵不可思议！\n回顾 2022 # 2022 的我个人的关键词就是“责任”。\n五月，随着女儿的出生的，除了工作之外，几乎所有的时间都花在了照顾家庭方面，留给我学习和输出时间寥寥无几了。\n孩子的出生最直接的感受就是身上责任的重大，养育一个孩子不但需要付出时间还有金钱，我真正地成为了上有老下有小的中年人了，一刻都不能倒下。\n以前觉得自己无所不能，未来可期，现在觉得自己肩膀上的责任重大，从此再也无法躺平了。\n六月至九月，陪产假。当全职奶爸，在孩子的睡觉的时候放弃了一些休息时间用来读书和开源。\n十月至十二月，回归岗位，因为疫情关系大部时间都在家办公。上班工作，下班看娃，再也没有大块时间来学习和输出了。\n因此我今年输出的文章很少，博客上一共发布了 19 篇，其中公众号只输出了 11 篇文章。这些输出绝大多数发生在上半年孩子还没出生的时候。\n在业余时间相较于输出文章，第一优先级还是在开源项目，这能让我学到更多。我在 cpp-linter 这个项目上花了比较多的业余时间，目前 cpp-linter-action 已经其他被超过 100 个其他项目所使用（依赖），我希望能把 cpp-linter 做成所有 C/C++ 项目的首选的代码格式化和静态检查工具。\n展望 2023 # 工作和家庭的平衡，希望女儿健康成长，早点睡整觉，这样爸爸可以晚上工作和学习了 英语和技术能够有进步，比如通过 TOEIC Tests 和加入知名的 Python Org 和学习 Cloud 方面的技术 超过 2022 年在博客和公众号的输出文章数，完成 24（博客）+ 12（公众号）要求不高吧 保持身体健康，恢复游泳、足球、运动，让体重回到 160 斤以下 过去的年终总结 # 2020 年终总结 2019 年终总结 2018 从测试到开发的五个月\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-12-28","externalUrl":null,"permalink":"/misc/2022-summary/","section":"Miscs","summary":"\u003cp\u003e时间过得好快，又过完了一年。\u003c/p\u003e\n\u003cp\u003e今年想写一些总结回顾一下过去的一年发生在自己身上的重要事件。\u003c/p\u003e","title":"2022 年终总结","type":"misc"},{"content":"","date":"2022-10-09","externalUrl":null,"permalink":"/en/tags/ci/","section":"Tags","summary":"","title":"CI","type":"tags"},{"content":"When I want to implement [skip ci] or [ci skip] for Jenkins multi-branch pipeline, the existing plugin seems broken.\nJENKINS-35509\nJENKINS-34130\nMy advice: try not to use the Jenkins plugin if possible.\nGood, it\u0026rsquo;s time to implement [skip ci] myself.\nIf you like me used Jenkins shared library, you can create a function like SkipCI from src/org/cicd/utils.groovy, then other jobs can reused this function.\n// src/org/cicd/utils.groovy def SkipCI(number = \u0026#34;all\u0026#34;){ def statusCodeList = [] String[] keyWords = [\u0026#39;ci skip\u0026#39;, \u0026#39;skip ci\u0026#39;] // add more keywords if need. keyWords.each { keyWord -\u0026gt; def statusCode = null if (number == \u0026#34;all\u0026#34;) { statusCode = sh script: \u0026#34;git log --oneline --all | grep \\\u0026#39;${keyWord}\\\u0026#39;\u0026#34;, returnStatus: true } else { statusCode = sh script: \u0026#34;git log --oneline -n ${number} | grep \\\u0026#39;${keyWord}\\\u0026#39;\u0026#34;, returnStatus: true } statusCodeList.add(statusCode) } if (statusCodeList.contains(0)) { return true } else { return false } } Then I can call this function from other jobs.\n// The following is not the complete code, it is just sample code and may not be run successfully. import org.cicd.utils def call(){ pipeline { agent { node { label \u0026#39;linux\u0026#39; } } parameters { booleanParam defaultValue: true, name: \u0026#39;Build\u0026#39;, summary: \u0026#39;Uncheck to skip build.\u0026#39; } def utils = new org.cicd.utils() stage(\u0026#34;Checkout\u0026#34;) { checkout scm // just check the latest commit message. SkipCI = utils.SkipCI(\u0026#39;1\u0026#39;) } stage(\u0026#34;Build\u0026#34;){ when { beforeAgent true expression { return params.Build \u0026amp;\u0026amp; !SkipCI } } steps { script { sh \u0026#34;make build\u0026#34; } } } } } Please let me know if any questions or suggestions.\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-10-09","externalUrl":null,"permalink":"/en/posts/jenkins-skip-ci/","section":"Posts","summary":"This article explains how to implement [skip ci] functionality in Jenkins multi-branch pipelines, allowing you to skip builds based on commit messages.","title":"How to implement [skip ci] for Jenkins multi-branch pipeline","type":"posts"},{"content":" Problem # I have encountered a problem when I ping google.com failed and return some error like \u0026ldquo;Temporary failure in name resolution\u0026rdquo;\nHow to fix # Inside WSL2, create or append file: /etc/wsl.conf\nPut the following lines in the file in order to ensure the your DNS changes do not get blown away\nsudo tee /etc/wsl.conf \u0026lt;\u0026lt; EOF [network] generateResolvConf = false EOF In a cmd windows (!!), run wsl --shutdown\nStart WSL2\nRun the following inside WSL2 (line with search is optional)\nsudo rm -rf /etc/resolv.conf sudo tee /etc/resolv.conf \u0026lt;\u0026lt; EOF search yourbase.domain.local nameserver 8.8.8.8 nameserver 1.1.1.1 EOF In my case, I can remove /etc/resolv.conf and error is \u0026ldquo;rm: cannot remove \u0026lsquo;/etc/resolv.conf\u0026rsquo;: Operation not permitted\u0026rdquo;\n# use following command instead fixed. sudo chattr -a -i /etc/resolv.conf Reference links # https://askubuntu.com/questions/1192347/temporary-failure-in-name-resolution-on-wsl https://askubuntu.com/questions/125847/un-removable-etc-resolv-conf\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-09-27","externalUrl":null,"permalink":"/en/posts/fix-wsl-networking-issue/","section":"Posts","summary":"This article explains how to resolve the \u0026ldquo;Temporary failure in name resolution\u0026rdquo; issue in WSL by configuring DNS settings and ensuring persistent changes.","title":"How to fix \"Temporary Failure in name resolution\" in WSL","type":"posts"},{"content":"","date":"2022-09-27","externalUrl":null,"permalink":"/en/tags/wsl/","section":"Tags","summary":"","title":"WSL","type":"tags"},{"content":"","date":"2022-09-16","externalUrl":null,"permalink":"/en/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"If you have a critical machine like your team\u0026rsquo;s CI server that runs on Linux, so you don\u0026rsquo;t want every members in your group to access it.\nModifying this setting /etc/security/access.conf on Linux can do it.\nHow to setup # I commented out the access settings for TEAM A, and add some user accounts can access.\n#+ : (SRV_WW_TEAM_A_CompAdmin) : ALL + : shenx, map, xiar : ALL Be careful not to restrict everyone including yourself.\nIt would be best to allow several people can also access it to prevent any issues to log in with your account or you leave the organization.\nLet\u0026rsquo;s test # Then when I try to use another account not in the list to access this machine and the connection shows closed.\n$ ssh test@devciserver.organization.com test@devciserver.organization.com\u0026#39;s password: Connection closed by 10.84.17.119 port 22 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-09-16","externalUrl":null,"permalink":"/en/posts/restrict-connect-server/","section":"Posts","summary":"This article explains how to restrict access to a critical Linux machine by modifying the \u003ccode\u003e/etc/security/access.conf\u003c/code\u003e file, allowing only specific users to log in.","title":"Restrict others from login your important Linux machine","type":"posts"},{"content":"This article shares practical experience with C/C++ code formatting and static analysis.\nCurrently, the most widely used tools for C/C++ code formatting and checking are Clang-Format and Clang-Tidy from the LLVM project.\nThe LLVM project is a collection of modular and reusable compiler and toolchain technologies.\nFor C/C++ code formatting and static analysis, we use clang-format and clang-tidy from the LLVM project; together, we call them clang-tools.\nAlthough we have the tools, how to better integrate the tools into our workflow is the focus of this article.\nThe cpp-linter organization was created to provide a one-stop workflow for C/C++ code formatting and static analysis, including:\nEasy download of clang-tools, providing both Docker images and binaries; Easy integration with workflows, including integration with CI and git hooks. Below is how to use clang-tools, download tools, and integrate them into your workflow.\nclang-tools Docker images # If you want to use clang-format and clang-tidy via Docker, the clang-tools project provides Docker images specifically for this purpose.\nJust download the clang-tools Docker image, and you can use clang-format and clang-tidy. For example:\n# Check clang-format version $ docker run xianpengshen/clang-tools:12 clang-format --version Ubuntu clang-format version 12.0.0-3ubuntu1~20.04.4 # Format code (helloworld.c is in the demo directory of the repository) $ docker run -v $PWD:/src xianpengshen/clang-tools:12 clang-format --dry-run -i helloworld.c # Check clang-tidy version $ docker run xianpengshen/clang-tools:12 clang-tidy --version LLVM (http://llvm.org/): LLVM version 12.0.0 Optimized build. Default target: x86_64-pc-linux-gnu Host CPU: cascadelake # Diagnose code (helloworld.c is in the demo directory of the repository) $ docker run -v $PWD:/src xianpengshen/clang-tools:12 clang-tidy helloworld.c \\ -checks=boost-*,bugprone-*,performance-*,readability-*,portability-*,modernize-*,clang-analyzer-cplusplus-*,clang-analyzer-*,cppcoreguidelines-* clang-tools binaries # If you need to use clang-tools binaries, taking Windows as an example, downloading a specific version of clang-tools usually requires installing the large LLVM package first to obtain tools like clang-format \u0026amp; clang-tidy; it\u0026rsquo;s much easier on Linux, where you can use commands to download, but downloading specific versions of clang-format \u0026amp; clang-tidy might require manual download and installation.\nclang-tools-pip provides and supports downloading any specified version of clang-tools executables on Windows, Linux, and macOS via the command line.\nSimply install clang-tools using pip (i.e., pip install clang-tools), and then you can install any version of the executable using the clang-tools command.\nFor example, to install clang-tools version 13:\n$ clang-tools --install 13\nYou can also install it to a specific directory:\n$ clang-tools --install 13 --directory .\nAfter successful installation, you can check the installed version:\n$ clang-format-13 --version clang-format version 13.0.0 $ clang-tidy-13 --version LLVM (http://llvm.org/): LLVM version 13.0.0 Optimized build. Default target: x86_64-unknown-linux-gnu Host CPU: skylake The clang-tools CLI also provides other options, such as automatically creating links for you. Check its CLI documentation for help.\nIntegrating clang-tools into your workflow # We\u0026rsquo;ve introduced two convenient ways to download clang-tools: Docker images and binaries. How to integrate them into your workflow is our ultimate concern.\nMainstream IDEs can use clang-format and clang-tidy via plugins, but this has problems:\nDifferent developers may use different IDEs, requiring a high learning cost to install plugins on different IDEs; It cannot guarantee that all developers will run Clang-Format or Clang-Tidy when submitting code. So how can we ensure that Clang-Format or Clang-Tidy is run every time code is submitted?\ncpp-linter-action provides CI checks. If unformatted code or diagnostic errors are found, the CI will fail, preventing code that hasn\u0026rsquo;t passed code checks from being merged into the main branch; cpp-linter-hooks uses git hooks to automatically run clang-format and clang-tidy when code is submitted. If the code doesn\u0026rsquo;t meet the standards, the submission fails, and a prompt appears with automatic formatting. cpp-linter-action for Automatic Checks Before Code Merge # If you\u0026rsquo;re using GitHub, we highly recommend using the cpp-linter-action GitHub Action.\nCurrently, cpp-linter does not have API integration with SCMs other than GitHub.\nHere are some of its key features:\nResults are displayed using Annotations and Thread Comments. Supports GitHub\u0026rsquo;s public and private repositories. Supports most Clang versions. Many other optional-inputs are available. To use this Action, simply create a cpp-linter.yml file under .github/workflows/, with the following content:\nOf course, you can also add the following configuration to an existing Workflow, such as build.\nname: cpp-linter on: pull_request: types: [opened, reopened] push: jobs: cpp-linter: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: cpp-linter-action@v1 id: linter env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: style: file - name: Fail fast?! if: steps.linter.outputs.checks-failed \u0026gt; 0 run: | echo \u0026#34;Some files failed the linting checks!\u0026#34; exit 1 If unformatted code or static code analysis errors are found, the CI workflow will fail, and the following comments will appear; annotations are enabled by default.\nIf the Thread Comment option (thread-comments: true) is enabled, the following error comments will be automatically added to the Pull Request.\nMany well-known projects already depend on this Action, and its ranking in the GitHub Marketplace is very high; you can use it with confidence.\nNote: The annotations and comment features currently only support GitHub. This project plans to support other SCMs like Bitbucket and GitLab in the future.\ncpp-linter-hooks for Automatic Checks on Code Submission # cpp-linter-hooks uses git hooks for automatic checks on code submission; this method is not limited to any SCM.\nJust add a .pre-commit-config.yaml configuration file to your project repository, then add the cpp-linter-hooks hook to .pre-commit-config.yaml, as follows:\n.pre-commit-config.yaml is the default configuration file for the pre-commit framework.\nInstall pre-commit\npip install pre-commit Create the .pre-commit-config.yaml configuration file, setting it as follows:\nrepos: - repo: https://github.com/cpp-linter-hooks rev: v0.2.1 hooks: - id: clang-format args: [--style=file] # to load .clang-format - id: clang-tidy args: [--checks=.clang-tidy] # path/to/.clang-tidy Here, file refers to .clang-format. clang-format supports LLVM, GNU, Google, Chromium, Microsoft, Mozilla, and WebKit encoding formats by default. If you need special settings, you can create a .clang-format configuration file in the root directory of your repository. Similarly, if the default static analysis settings do not meet your requirements, you can create a .clang-tidy configuration file in the root directory of the repository.\nFor more configurations, see the README.\nInstall the git hook script\n$ pre-commit install pre-commit installed at .git/hooks/pre-commit After this, every git commit will automatically run clang-format and clang-tidy.\nIf unformatted code or static analysis errors are detected, the following error message will appear:\nclang-format output\nclang-format.............................................................Failed - hook id: clang-format - files were modified by this hook And it will automatically format your code:\n--- a/testing/main.c +++ b/testing/main.c @@ -1,3 +1,6 @@ #include \u0026lt;stdio.h\u0026gt; -int main() {for (;;) break; printf(\u0026#34;Hello world!\\n\u0026#34;);return 0;} - +int main() { + for (;;) break; + printf(\u0026#34;Hello world!\\n\u0026#34;); + return 0; +} clang-tidy output\nclang-tidy...............................................................Failed - hook id: clang-tidy - exit code: 1 418 warnings and 1 error generated. Error while processing /home/ubuntu/cpp-linter-hooks/testing/main.c. Suppressed 417 warnings (417 in non-user code). Use -header-filter=.* to display errors from all non-system headers. Use -system-headers to display errors from system headers as well. Found compiler error(s). /home/ubuntu/cpp-linter-hooks/testing/main.c:3:11: warning: statement should be inside braces [readability-braces-around-statements] for (;;) break; ^ { /usr/include/stdio.h:33:10: error: \u0026#39;stddef.h\u0026#39; file not found [clang-diagnostic-error] #include \u0026lt;stddef.h\u0026gt; ^~~~~~~~~~ Conclusion # CI or git hooks?\nIf your team is already using pre-commit, we recommend using git hooks. Just add cpp-linter-hooks. If you don\u0026rsquo;t want to introduce pre-commit, you can use CI for checking. Of course, you can also choose both. The cpp-linter organization is an open-source project I created and maintained by Brendan Doherty and me as the primary contributors. We are developers who pursue code quality and strive to build the best software. I\u0026rsquo;ve spent a lot of my free time on it, but I\u0026rsquo;ve also learned a lot. I\u0026rsquo;ll share some interesting implementation methods later.\nCurrently, cpp-linter provides the best C/C++ Linter Action and clang-tools on GitHub. We welcome your use, and you can provide feedback on any suggestions or problems through Issues.\n","date":"2022-08-23","externalUrl":null,"permalink":"/en/posts/cpp-linter/","section":"Posts","summary":"This article introduces tools and workflows for C/C++ code formatting and static analysis, focusing on the use and integration of clang-tools.","title":"C/C++ Code Formatting and Static Analysis—A One-Stop Workflow with Cpp Linter","type":"posts"},{"content":"","date":"2022-07-28","externalUrl":null,"permalink":"/en/tags/gpg/","section":"Tags","summary":"","title":"GPG","type":"tags"},{"content":" First, List your GPG key # # If folders does not exist will create be related automatically $ gpg --list-keys gpg: directory \u0026#39;/home/ubuntu/.gnupg\u0026#39; created gpg: keybox \u0026#39;/home/ubuntu/.gnupg/pubring.kbx\u0026#39; created gpg: /home/ubuntu/.gnupg/trustdb.gpg: trustdb created $ gpg --list-key Second, generate GPG key # $ gpg --gen-key gpg (GnuPG) 2.2.19; Copyright (C) 2019 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Note: Use \u0026#34;gpg --full-generate-key\u0026#34; for a full featured key generation dialog. GnuPG needs to construct a user ID to identify your key. Real name: shenxianpeng Email address: xianpeng.shen@gmail.com You selected this USER-ID: \u0026#34;shenxianpeng \u0026lt;xianpeng.shen@gmail.com\u0026gt;\u0026#34; Change (N)ame, (E)mail, or (O)kay/(Q)uit? O We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. gpg: key 5F72A7D009FC935A marked as ultimately trusted gpg: directory \u0026#39;/home/ubuntu/.gnupg/openpgp-revocs.d\u0026#39; created gpg: revocation certificate stored as \u0026#39;/home/ubuntu/.gnupg/openpgp-revocs.d/F0F32CB8C65536ECE0187EAD5F72A7D009FC935A.rev\u0026#39; public and secret key created and signed. pub rsa3072 2022-07-28 [SC] [expires: 2024-07-27] F0F32CB8C65536ECE0187EAD5F72A7D009FC935A uid shenxianpeng \u0026lt;xianpeng.shen@gmail.com\u0026gt; sub rsa3072 2022-07-28 [E] [expires: 2024-07-27] Third, get your public key content # # get with your email gpg --armor --export xianpeng.shen@gmail.com # or with your pub key id pg --armor --export F0F32CB8C65536ECE0187EAD5F72A7D009FC935A # # public key content output below # Fourth, add the public key content (GPG keys) to GitHub # Open GitHub, Settings -\u0026gt; SSH and GPG keys -\u0026gt; New GPG key\nThen when you commit with command git commit -S -m \u0026quot;Your commit message\u0026quot;, then a verified signature will show on GitHub\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-07-28","externalUrl":null,"permalink":"/en/posts/git-create-gpg-keys/","section":"Posts","summary":"This article explains how to create GPG keys, export the public key, and add it to GitHub for signing commits.","title":"How to create GPG keys and add to GitHub","type":"posts"},{"content":" Hi! I\u0026#39;m Xianpeng Shen Welcome Welcome to my personal site I write blog posts, develop free software, and actively contribute to open source projects. My Life Vilnius, Lithuania Explore · Learn · Share I currently live in Vilnius, Lithuania. I speak: Chinese English A bit of Lithuanian Get in Touch Let\u0026#39;s Connect Collaboration · Writing · Exchange Feel free to reach out via email: xianpeng.shen@gmail.com My Projects Open Source Contributions Welcome All forms of contributions are welcome—if you find them interesting or useful, I'd love to have your participation! Check out the ‘Portfolio’ for details. ","date":"2022-06-13","externalUrl":null,"permalink":"/en/about/","section":"","summary":"\u003col class=\"border-l-2 border-primary-500 dark:border-primary-300 list-none\"\u003e\n\n\n\n\n\n\u003cli\u003e\n  \u003cdiv class=\"flex flex-start\"\u003e\n    \u003cdiv\n      class=\"bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full -ml-12 mt-5\"\u003e\n      \u003cspan class=\"relative block icon\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 448 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M48 32H197.5C214.5 32 230.7 38.74 242.7 50.75L418.7 226.7C443.7 251.7 443.7 292.3 418.7 317.3L285.3 450.7C260.3 475.7 219.7 475.7 194.7 450.7L18.75 274.7C6.743 262.7 0 246.5 0 229.5V80C0 53.49 21.49 32 48 32L48 32zM112 176C129.7 176 144 161.7 144 144C144 126.3 129.7 112 112 112C94.33 112 80 126.3 80 144C80 161.7 94.33 176 112 176z\"/\u003e\u003c/svg\u003e\n\u003c/span\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"block p-6 rounded-lg shadow-2xl min-w-full ml-6 mb-10 break-words\"\u003e\n      \u003cdiv class=\"flex justify-between\"\u003e\n        \n          \u003ch2 class=\"mt-0\"\u003eHi! I\u0026#39;m Xianpeng Shen\u003c/h2\u003e\n        \n        \n          \u003ch3 class=\"\"\u003e\n            \u003cspan class=\"flex cursor-pointer\"\u003e\n  \u003cspan\n    class=\"rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400\"\u003e\n    Welcome\n  \u003c/span\u003e\n\u003c/span\u003e\n\n          \u003c/h3\u003e\n        \n      \u003c/div\u003e\n      \n        \u003ch4 class=\"mt-0\"\u003e\n          Welcome to my personal site\n        \u003c/h4\u003e\n      \n      \u003cdiv class=\"mb-6\"\u003e\nI write blog posts, develop free software, and actively contribute to open source projects.\n\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/li\u003e\n\n\n\n\n\n\n\u003cli\u003e\n  \u003cdiv class=\"flex flex-start\"\u003e\n    \u003cdiv\n      class=\"bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full -ml-12 mt-5\"\u003e\n      \u003cspan class=\"relative block icon\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M266.3 48.3L232.5 73.6c-5.4 4-8.5 10.4-8.5 17.1v9.1c0 6.8 5.5 12.3 12.3 12.3c2.4 0 4.8-.7 6.8-2.1l41.8-27.9c2-1.3 4.4-2.1 6.8-2.1h1c6.2 0 11.3 5.1 11.3 11.3c0 3-1.2 5.9-3.3 8l-19.9 19.9c-5.8 5.8-12.9 10.2-20.7 12.8l-26.5 8.8c-5.8 1.9-9.6 7.3-9.6 13.4c0 3.7-1.5 7.3-4.1 10l-17.9 17.9c-6.4 6.4-9.9 15-9.9 24v4.3c0 16.4 13.6 29.7 29.9 29.7c11 0 21.2-6.2 26.1-16l4-8.1c2.4-4.8 7.4-7.9 12.8-7.9c4.5 0 8.7 2.1 11.4 5.7l16.3 21.7c2.1 2.9 5.5 4.5 9.1 4.5c8.4 0 13.9-8.9 10.1-16.4l-1.1-2.3c-3.5-7 0-15.5 7.5-18l21.2-7.1c7.6-2.5 12.7-9.6 12.7-17.6c0-10.3 8.3-18.6 18.6-18.6H400c8.8 0 16 7.2 16 16s-7.2 16-16 16H379.3c-7.2 0-14.2 2.9-19.3 8l-4.7 4.7c-2.1 2.1-3.3 5-3.3 8c0 6.2 5.1 11.3 11.3 11.3h11.3c6 0 11.8 2.4 16 6.6l6.5 6.5c1.8 1.8 2.8 4.3 2.8 6.8s-1 5-2.8 6.8l-7.5 7.5C386 262 384 266.9 384 272s2 10 5.7 13.7L408 304c10.2 10.2 24.1 16 38.6 16H454c6.5-20.2 10-41.7 10-64c0-111.4-87.6-202.4-197.7-207.7zm172 307.9c-3.7-2.6-8.2-4.1-13-4.1c-6 0-11.8-2.4-16-6.6L396 332c-7.7-7.7-18-12-28.9-12c-9.7 0-19.2-3.5-26.6-9.8L314 287.4c-11.6-9.9-26.4-15.4-41.6-15.4H251.4c-12.6 0-25 3.7-35.5 10.7L188.5 301c-17.8 11.9-28.5 31.9-28.5 53.3v3.2c0 17 6.7 33.3 18.7 45.3l16 16c8.5 8.5 20 13.3 32 13.3H248c13.3 0 24 10.7 24 24c0 2.5 .4 5 1.1 7.3c71.3-5.8 132.5-47.6 165.2-107.2zM0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256zM187.3 100.7c-6.2-6.2-16.4-6.2-22.6 0l-32 32c-6.2 6.2-6.2 16.4 0 22.6s16.4 6.2 22.6 0l32-32c6.2-6.2 6.2-16.4 0-22.6z\"/\u003e\u003c/svg\u003e\u003c/span\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"block p-6 rounded-lg shadow-2xl min-w-full ml-6 mb-10 break-words\"\u003e\n      \u003cdiv class=\"flex justify-between\"\u003e\n        \n          \u003ch2 class=\"mt-0\"\u003eMy Life\u003c/h2\u003e\n        \n        \n          \u003ch3 class=\"\"\u003e\n            \u003cspan class=\"flex cursor-pointer\"\u003e\n  \u003cspan\n    class=\"rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400\"\u003e\n    Vilnius, Lithuania\n  \u003c/span\u003e\n\u003c/span\u003e\n\n          \u003c/h3\u003e\n        \n      \u003c/div\u003e\n      \n        \u003ch4 class=\"mt-0\"\u003e\n          Explore · Learn · Share\n        \u003c/h4\u003e\n      \n      \u003cdiv class=\"mb-6\"\u003e\nI currently live in Vilnius, Lithuania. I speak:\n\u003cul\u003e\n  \u003cli\u003eChinese\u003c/li\u003e\n  \u003cli\u003eEnglish\u003c/li\u003e\n  \u003cli\u003eA bit of Lithuanian\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/li\u003e\n\n\n\n\n\n\n\u003cli\u003e\n  \u003cdiv class=\"flex flex-start\"\u003e\n    \u003cdiv\n      class=\"bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full -ml-12 mt-5\"\u003e\n      \u003cspan class=\"relative block icon\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M48 64C21.5 64 0 85.5 0 112c0 15.1 7.1 29.3 19.2 38.4L236.8 313.6c11.4 8.5 27 8.5 38.4 0L492.8 150.4c12.1-9.1 19.2-23.3 19.2-38.4c0-26.5-21.5-48-48-48H48zM0 176V384c0 35.3 28.7 64 64 64H448c35.3 0 64-28.7 64-64V176L294.4 339.2c-22.8 17.1-54 17.1-76.8 0L0 176z\"/\u003e\u003c/svg\u003e\u003c/span\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"block p-6 rounded-lg shadow-2xl min-w-full ml-6 mb-10 break-words\"\u003e\n      \u003cdiv class=\"flex justify-between\"\u003e\n        \n          \u003ch2 class=\"mt-0\"\u003eGet in Touch\u003c/h2\u003e\n        \n        \n          \u003ch3 class=\"\"\u003e\n            \u003cspan class=\"flex cursor-pointer\"\u003e\n  \u003cspan\n    class=\"rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400\"\u003e\n    Let\u0026#39;s Connect\n  \u003c/span\u003e\n\u003c/span\u003e\n\n          \u003c/h3\u003e\n        \n      \u003c/div\u003e\n      \n        \u003ch4 class=\"mt-0\"\u003e\n          Collaboration · Writing · Exchange\n        \u003c/h4\u003e\n      \n      \u003cdiv class=\"mb-6\"\u003e\nFeel free to reach out via email: xianpeng.shen@gmail.com\n\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/li\u003e\n\n\n\n\n\n\n\u003cli\u003e\n  \u003cdiv class=\"flex flex-start\"\u003e\n    \u003cdiv\n      class=\"bg-primary-500 dark:bg-primary-300 text-neutral-50 dark:text-neutral-700 min-w-[30px] h-8 text-2xl flex items-center justify-center rounded-full -ml-12 mt-5\"\u003e\n      \u003cspan class=\"relative block icon\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 640 512\"\u003e\n\u003cpath fill=\"currentColor\"  d=\"M392.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6zm80.6 120.1c-12.5 12.5-12.5 32.8 0 45.3L562.7 256l-89.4 89.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l112-112c12.5-12.5 12.5-32.8 0-45.3l-112-112c-12.5-12.5-32.8-12.5-45.3 0zm-306.7 0c-12.5-12.5-32.8-12.5-45.3 0l-112 112c-12.5 12.5-12.5 32.8 0 45.3l112 112c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l89.4-89.4c12.5-12.5 12.5-32.8 0-45.3z\"/\u003e\u003c/svg\u003e\u003c/span\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"block p-6 rounded-lg shadow-2xl min-w-full ml-6 mb-10 break-words\"\u003e\n      \u003cdiv class=\"flex justify-between\"\u003e\n        \n          \u003ch2 class=\"mt-0\"\u003eMy Projects\u003c/h2\u003e\n        \n        \n          \u003ch3 class=\"\"\u003e\n            \u003cspan class=\"flex cursor-pointer\"\u003e\n  \u003cspan\n    class=\"rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400\"\u003e\n    Open Source\n  \u003c/span\u003e\n\u003c/span\u003e\n\n          \u003c/h3\u003e\n        \n      \u003c/div\u003e\n      \n        \u003ch4 class=\"mt-0\"\u003e\n          Contributions Welcome\n        \u003c/h4\u003e\n      \n      \u003cdiv class=\"mb-6\"\u003e\n\nAll forms of contributions are welcome—if you find them interesting or useful, I'd love to have your participation! Check out the ‘Portfolio’ for details.\n\n\u003c!-- \u003cdiv class=\"github-card-wrapper\"\u003e\n    \u003ca id=\"github-90606694e29f4997f206c39461257736\" target=\"_blank\" href=\"https://github.com/cpp-linter/cpp-linter-action\" class=\"cursor-pointer\"\u003e\n      \u003cdiv\n        class=\"w-full md:w-auto p-0 m-0 border border-neutral-200 dark:border-neutral-700 border rounded-md shadow-2xl\"\u003e\u003cdiv class=\"w-full md:w-auto pt-3 p-5\"\u003e\n          \u003cdiv class=\"flex items-center\"\u003e\n            \u003cspan class=\"text-2xl text-neutral-800 dark:text-neutral mr-[10px]\"\u003e\n              \u003cspan class=\"relative block icon\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 496 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/\u003e\u003c/svg\u003e\n\u003c/span\u003e\n            \u003c/span\u003e\n            \u003cdiv\n              id=\"github-90606694e29f4997f206c39461257736-full_name\"\n              class=\"m-0 font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral\"\u003e\n              cpp-linter/cpp-linter-action\n            \u003c/div\u003e\n          \u003c/div\u003e\n\n          \u003cp id=\"github-90606694e29f4997f206c39461257736-description\" class=\"m-0 mt-2 text-md text-neutral-800 dark:text-neutral\"\u003e\n            A Github Action for linting C/C++ code integrating clang-tidy and clang-format to collect feedback provided in the form of file-annotations, thread-comments, workflow step-summary, and Pull Request reviews.\n          \u003c/p\u003e","title":"About","type":"page"},{"content":"有幸赶上了公司的政策变化，我有 12 周的陪产假来做全职奶爸，照顾家人的同时希望挤出时间来学习，毕竟在职期间很有有机会能有近 3 个月的假期。\n照顾孩子兼顾学习真不是一件轻松的事情，我尽力兼顾了两者，做了如下的流水账记录。\n计划 # 我知道 12 周会很快过去，就在已经快要过去了 2 周时我决定有计划的来完成一些任务，比如：\n完成《代码整洁之道》、《重构》以及《动手学习深度学习这三本书》的阅读和豆瓣评论 为 pre-commit 写一个 clang-format 和 clang-tidy 的 cpp-linter-hooks 完成每个月 15 节英语课以及 3~4 的体育锻炼（游泳和足球） 找一个可以作为长期业余参与的开源项目，例如 pytest，tox，pypa。 也就是从休假的第 2 周开始，我开始记录每周的完成的小任务。\n周报 # 第 11~12 周（8.15 - 8.28）- 最后两周\n时间过得太快了，不知不觉就是假期的最后两周了。\nValidate VMs 更新关于 setuptools_scm 的使用。 在 cpp-linter Org 中做一些项目的修改和代码评审 第 10 周（8.8 - 8.14）- 最后三周\nValidate VMs 总结关于 setuptools_scm 的使用。 第 9 周（8.1 - 8.7）- 时间过得真快，转眼就到了陪产假的最后 4 周了\nDraft 一篇关于参与开源的文章 做了一些工作，以及 Troubleshooting support；参与开源项目和 Code Review。 游泳以及完成《重构》书评 第 8 周（7.25 - 7.31）\n做 cpp-linter 里的一些项目的更新，做 Artifactory 的迁移和测试。 这周计划做的事情世纪没完成几样，比如写文章和看书。 游泳 第 7 周（7.18 - 7.24）\n发现 Python 真是入门容易学好难\u0026hellip; 初步学了一下 tox 和 mypy，会之后的 project 中尝试使用。 重构了代码，本打算并把 Code Coverage 写到 100 %，但没实现，pytest 还需要继续学。 在琢磨一个有意思的可以作为长期业余时间来做的项目，目前有个模糊的雏形，先试试看 《重构》书没怎么读，周六游泳可以继续 第 6 周（7.11 - 7.17）\n完成了 cpp-linter-hooks 功能的开发并把它迁移到 cpp-linter org 下面。 创建了 .github 仓库，对于 org 这是一个很神奇的残酷，玩法很多，还在陆续探索中。 终于把读 Code Clean 的书评交了，还需继续读完成任务。 周五发了一篇公众号文章，是之前写的，整理终于发出了，这是 3 个月以来的第一次更新。 周日去游一次泳。 第 5 周（7.4 - 7.10）\n上周主要是抽空写 clang-tools-pip 和 cpp-linter-hooks 这两个功能，目前完成大概 70%，预计本周可以基本结束。 工作上也花了点时间，修复了之前写的 pipeline 的几个问题 上周开始读《重构》了，但没多少时间花在读书上，没读几页。一致想更新公号文章，可惜挺花时间的 上周日游泳也没游，因为脖子坏了，可能是喂奶低头造成的 :( 第 3 - 4 周：\n《代码整洁之道》 P56 - P130 在实现 cpp-linter-hooks 之前需要实现 install clang-tools with pip, 因此我创建了 clang-tools-pip 去市游泳馆游了一次泳，第二次本已约好但临时有事取消了 第 2 周：\n《代码整洁之道》 P26 - P56 创建了 cpp-linter-hooks 仓库，学习别人的代码 计划本周末和乔教练去游泳 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-06-13","externalUrl":null,"permalink":"/posts/vacation-weekly-report/","section":"Posts","summary":"本文记录了我在陪产假期间的学习和工作安排，包括阅读书籍、参与开源项目、体育锻炼等，分享了如何在照顾家庭的同时保持学习和成长。","title":"12 周的陪产假我一刻都没闲着","type":"posts"},{"content":"For a long time, many programmers have encountered various inconveniences in developing on Windows:\nFor example, setting up a development environment isn\u0026rsquo;t as simple as entering a single command to install various commands and packages like on Linux and Mac. Therefore, some programmers switched to Mac for development, while others simply used Linux as their development machine. Only those who had to use Windows as their development environment remained on Windows, making do.\nUntil the arrival of WSL, or more accurately, WSL2.\nThe combination of WSL + VS Code + Docker Desktop, these three musketeers, has made developing on Windows a truly enjoyable experience for me.\nWhat is WSL # WSL stands for Windows Subsystem for Linux. It\u0026rsquo;s a feature of the Windows 10 operating system that allows you to run a Linux file system, Linux command-line tools, and GUI applications directly on Windows, running alongside traditional Windows desktop and applications.\nThe minimum required version of WSL is Windows 10 version 1903 or later.\nWSL is specifically designed for developers who need to use Linux, such as web developers, those working on open-source projects, and developers who need to deploy to Linux server environments.\nWSL is suitable for those who prefer using Bash, common Linux tools (sed, awk, etc.), and Linux-first frameworks (Ruby, Python, etc.), while also enjoying using Windows as their productivity tool.\nLet\u0026rsquo;s take a look at the advantages of WSL compared to virtual machines.\nAdvantages of Using WSL # Compared to a full virtual machine, WSL requires fewer resources (CPU, memory, and storage). You can use Windows and Linux simultaneously and access your Windows files from Linux, providing a better interactive experience. Most importantly, using WSL combined with VS Code + Docker provides the perfect Linux experience while also offering the productivity of Windows. This is something that virtual machines or Linux operating systems alone cannot achieve. Macs can, but not everyone is suitable for Macs. Now let\u0026rsquo;s talk about how to install WSL and use it with VS Code + Docker.\nInstalling WSL # wsl --install This command will enable the required optional components, download the latest Linux kernel, set WSL 2 as your default, and install a Linux distribution for you (Ubuntu by default).\n# View the list of available distributions C:\\Users\\xshen\u0026gt;wsl --list --online The following is a list of valid distributions that can be installed. Install using \u0026#39;wsl --install -d \u0026lt;Distro\u0026gt;\u0026#39;. NAME FRIENDLY NAME Ubuntu Ubuntu Debian Debian GNU/Linux kali-linux Kali Linux Rolling openSUSE-42 openSUSE Leap 42 SLES-12 SUSE Linux Enterprise Server v12 Ubuntu-16.04 Ubuntu 16.04 LTS Ubuntu-18.04 Ubuntu 18.04 LTS Ubuntu-20.04 Ubuntu 20.04 LTS To install other distributions, such as Debian:\nwsl --install -d Debian For more details, please refer to the official documentation\nWSL + VS Code Demonstration # The following uses Ubuntu as an example to demonstrate downloading code and opening the code directory using VS Code.\nI have already opened the installed Ubuntu operating system via WSL.\nFirst, download the code:\nubuntu@CN-L-2680:~$ git clone https://github.com/cue-lang/cue.git --depth 1 Cloning into \u0026#39;cue\u0026#39;... remote: Enumerating objects: 1833, done. remote: Counting objects: 100% (1833/1833), done. remote: Compressing objects: 100% (1502/1502), done. remote: Total 1833 (delta 238), reused 1161 (delta 148), pack-reused 0 Receiving objects: 100% (1833/1833), 1.53 MiB | 5.39 MiB/s, done. Resolving deltas: 100% (238/238), done. Then go to the downloaded code directory and enter code .\nubuntu@CN-L-2680:~$ cd cue/ ubuntu@CN-L-2680:~/cue$ code . # The VS Code Server will only be installed the first time Installing VS Code Server for x64 (dfd34e8260c270da74b5c2d86d61aee4b6d56977) Downloading: 100% Unpacking: 100% Unpacked 2341 files and folders to /home/ubuntu/.vscode-server/bin/dfd34e8260c270da74b5c2d86d61aee4b6d56977. The first time, it will automatically download and install the VS Code Server. After installation, it will automatically launch the VS Code on your local machine and open the code directory on Ubuntu. The whole process is very smooth.\nAfter that, you can use the command-line apt-get command in VS Code to install any software you need. It\u0026rsquo;s really awesome!\nYou need to install the Microsoft-made Remote - WSL extension on your local VS Code; Additionally, if you need to use Docker in WSL, you need to pre-install Docker Desktop on Windows.\nPlease indicate the author and source when reprinting this article. Please do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2022-04-19","externalUrl":null,"permalink":"/en/posts/wsl/","section":"Posts","summary":"This article introduces how to develop on Windows using WSL, VS Code, and Docker Desktop. It provides detailed installation and configuration steps, as well as the advantages and experience of using these tools.","title":"Developing on Windows Just Got Awesome using WSL + VS Code + Docker Desktop - Worth a Try","type":"posts"},{"content":"","date":"2022-04-19","externalUrl":null,"permalink":"/en/tags/vscode/","section":"Tags","summary":"","title":"VSCode","type":"tags"},{"content":"","date":"2022-04-19","externalUrl":null,"permalink":"/en/tags/windows/","section":"Tags","summary":"","title":"Windows","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/en/tags/containerd/","section":"Tags","summary":"","title":"Containerd","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/en/tags/cri/","section":"Tags","summary":"","title":"CRI","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/en/tags/cri-o/","section":"Tags","summary":"","title":"CRI-O","type":"tags"},{"content":"Since Docker ignited the explosive growth of container usage, more and more tools and standards have emerged to help manage and utilize this containerization technology. Simultaneously, this has led to a plethora of confusing terminology.\nFor example, Docker, containerd, CRI, CRI-O, OCI, and runc. This article will introduce these terms you\u0026rsquo;ve heard of but may not understand, and explain how the container ecosystem works together.\nThe Container Ecosystem # The container ecosystem is composed of many exciting technologies, a large amount of specialized terminology, and large companies vying for dominance.\nFortunately, these companies occasionally find common ground, cooperating to agree on standards that improve the ecosystem\u0026rsquo;s interoperability across different platforms and operating systems, reducing reliance on any single company or project.\nThis diagram shows how Docker, Kubernetes, CRI, OCI, containerd, and runc fit together in this ecosystem.\nThe workflow is simple:\nTools like Docker and Kubernetes call a container runtime (CRI) such as containerd or CRI-O when running a container. The container runtime handles the actual work of creating, running, and destroying containers. Docker uses containerd as its runtime; Kubernetes supports multiple container runtimes, including containerd and CRI-O. These container runtimes adhere to the OCI specification and use runc to interact with the operating system kernel to create and run containers. Let\u0026rsquo;s introduce the terms and specifications mentioned in the diagram.\nDocker # Let\u0026rsquo;s start with the familiar Docker, as it\u0026rsquo;s the most popular tool for managing containers. For many, the name \u0026ldquo;Docker\u0026rdquo; is synonymous with \u0026ldquo;container.\u0026rdquo;\nDocker initiated the container revolution, creating a user-friendly tool for handling containers, also called Docker. The key points to understand here are:\nDocker is not the only container contender. Containers are no longer inextricably linked to the name Docker. In the current landscape of container tools, Docker is just one among many. Other notable container tools include: Podman, LXC, containerd, and Buildah.\nTherefore, believing containers are solely about Docker is inaccurate and incomplete.\nDocker Components # Docker simplifies building container images, pulling images from Docker Hub, and creating, starting, and managing containers. In reality, when you run a container with Docker, it\u0026rsquo;s actually run through the Docker daemon, containerd, and runc.\nTo achieve this, Docker comprises these projects (among others, but these are the major ones):\ndocker-cli: A command-line interface (CLI) for interacting with commands like docker pull, build, run, and exec. containerd: A daemon that manages and runs containers. It pushes and pulls images, manages storage and networking, and supervises container operation. runc: A low-level container runtime (the actual component that creates and runs containers). It includes libcontainer, a Go-based native implementation for creating containers. Docker Images # Many refer to Docker images, which are actually images packaged in the Open Container Initiative (OCI) format.\nTherefore, if you pull an image from Docker Hub or another registry, you should be able to use it with Docker commands, on a Kubernetes cluster, with the Podman tool, or with any other tool supporting the OCI image format specification.\nDockershim # Kubernetes included a component called dockershim to support Docker. Because Docker predates Kubernetes and didn\u0026rsquo;t implement CRI, dockershim was necessary to integrate Docker into Kubernetes. As containerization became an industry standard, the Kubernetes project added support for additional runtimes, supporting container execution via the Container Runtime Interface (CRI). Thus, dockershim became an anomaly in the Kubernetes project. The dependence on Docker and dockershim permeated various tools and projects within the Cloud Native Computing Foundation (CNCF) ecosystem, resulting in fragile code.\nIn April 2022, dockershim was completely removed from Kubernetes 1.24. Kubernetes discontinued direct Docker support, opting to utilize only container runtimes implementing its Container Runtime Interface. This likely means using containerd or CRI-O. This doesn\u0026rsquo;t imply Kubernetes cannot run Docker-formatted containers. containerd and CRI-O can both run Docker-formatted (actually OCI-formatted) images; they simply don\u0026rsquo;t require the docker commands or the Docker daemon.\nContainer Runtime Interface (CRI) # The CRI (Container Runtime Interface) is a Kubernetes API for controlling different runtimes used to create and manage containers. It makes Kubernetes more adaptable to various container runtimes. It\u0026rsquo;s a plugin interface, meaning any compliant container runtime can be used by Kubernetes.\nThe Kubernetes project doesn\u0026rsquo;t need to manually add support for each runtime. The CRI API describes how Kubernetes interacts with each runtime; the runtime determines how to actually manage containers, provided it adheres to the CRI API.\nYou can use containerd or CRI-O to run your containers because both runtimes implement the CRI specification.\ncontainerd # containerd is a high-level container runtime from Docker that implements the CRI specification. It was separated from the Docker project and later donated to the Cloud Native Computing Foundation (CNCF) to provide the container community with a foundation for creating new container solutions.\nDocker itself internally uses containerd; it\u0026rsquo;s installed when you install Docker.\ncontainerd implements the Kubernetes Container Runtime Interface (CRI) via its CRI plugin. It manages the entire container lifecycle, from image transfer and storage to container execution, monitoring, and networking.\nCRI-O # CRI-O is another high-level container runtime that implements the Container Runtime Interface (CRI) and can use OCI (Open Container Initiative)-compliant runtimes. It\u0026rsquo;s an alternative to containerd.\nCRI-O originated from RedHat, IBM, Intel, SUSE, Hyper, and others. It was created from scratch as a container runtime for Kubernetes, providing the ability to start, stop, and restart containers, similar to containerd.\nOpen Container Initiative (OCI) # The Open Container Initiative (OCI) is a group of technology companies aiming to create open industry standards around container images and runtimes. They maintain specifications for the container image format and how containers should run.\nThe idea behind OCI is that you can choose different runtimes that comply with the specification, each with different underlying implementations.\nFor instance, you might have an OCI-compliant runtime for your Linux hosts and another for your Windows hosts. This is the advantage of having a standard that can be implemented by many different projects. This \u0026ldquo;one standard, multiple implementations\u0026rdquo; approach is widely used, from Bluetooth devices to Java APIs.\nrunc # runc is a lightweight, universal runtime for containers. It adheres to the OCI specification and is the lowest-level component implementing the OCI interface; it interacts with the kernel to create and run containers.\nrunc provides all low-level functionality for containers, interacting with existing low-level Linux features like namespaces and cgroups, using these to create and run container processes.\nSeveral alternatives to runc exist:\ncrun: A container runtime written in C (in contrast, runc is written in Go). kata-runtime from the Katacontainers project, which implements the OCI specification as separate lightweight virtual machines (hardware virtualization). Google\u0026rsquo;s gVisor, which creates containers with their own kernel. It implements OCI in its runtime, called runsc. runc is a tool for running containers on Linux, meaning it can run on Linux, bare metal, or within a virtual machine.\nOn Windows, it\u0026rsquo;s slightly different. The equivalent of runc is Microsoft\u0026rsquo;s Host Compute Service (HCS), which includes a tool called runhcs, itself a fork of runc, also implementing the Open Container Initiative specification.\nSummary # This article demonstrates that Docker is just a small part of the container ecosystem. A set of open standards allows for interchangeable implementations.\nThis is why standards like CRI and OCI exist, along with projects like containerd, runc, and CRI-O.\nNow you understand the intriguing and slightly complex world of containers. Next time, avoid saying you\u0026rsquo;re using \u0026ldquo;Docker containers\u0026rdquo;! :)\nReferences # The differences between Docker, containerd, CRI-O and runc\n","date":"2022-03-29","externalUrl":null,"permalink":"/en/posts/container-ecosystem/","section":"Posts","summary":"This article introduces the key components and standards in the container ecosystem, such as Docker, containerd, CRI, CRI-O, OCI, and runc, explaining their relationships and how they work together.","title":"Docker, containerd, CRI, CRI-O, OCI, runc Explained and How They Work Together","type":"posts"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/en/tags/runc/","section":"Tags","summary":"","title":"Runc","type":"tags"},{"content":" Introduction # In organizations, using LDAP login very common way for users to log in with their credentials.\nHow to configure LDAP # Preparation: Installed LDAP Jenkins plugin\nAbout how to configure it, you can refer to Jenkins LDAP Plugin documentation https://plugins.jenkins.io/ldap/\nThis is my LDAP configuration just for testing.\nCan not login with LDAP? # Sometimes, for some reason, there is a problem with your organization\u0026rsquo;s LDAP server and you can\u0026rsquo;t log in to Jenkins using LDAP, but you need to use Jenkins now.\nYou can disable LDAP authentication by changing config.xml.\n# Login, cd to jenkins server folder $ cd /var/lib/jenkins/ # Highly rememend you to backup config.xml before making any change !!! # If you don\u0026#39;t backup config.xml, you\u0026#39;ll lost your LDAP configration after reboot service. $ cp config.xml config.xml.bak # Modify config.xml from \u0026lt;useSecurity\u0026gt;true\u0026lt;/useSecurity\u0026gt; # To \u0026lt;useSecurity\u0026gt;false\u0026lt;/useSecurity\u0026gt; # Restart Jenkins server sudo service jenkins restart Then you can log into the Jenkins server again.\nOnce your organization\u0026rsquo;s LDAP works again, you can replace config.xml with your backup config.xml file. Then your users can continue to log in via LDAP.\n","date":"2022-03-15","externalUrl":null,"permalink":"/en/posts/jenkins-ldap-configuration/","section":"Posts","summary":"This article explains how to enable and configure LDAP authentication in Jenkins, including how to disable it temporarily if needed.","title":"How to enable, configure and disable Jenkins LDAP","type":"posts"},{"content":"","date":"2022-03-15","externalUrl":null,"permalink":"/en/tags/ldap/","section":"Tags","summary":"","title":"LDAP","type":"tags"},{"content":"","date":"2022-03-09","externalUrl":null,"permalink":"/en/tags/fork/","section":"Tags","summary":"","title":"Fork","type":"tags"},{"content":" Background # Developers, and even companies, may encounter the following problems:\nA repository was initially forked, and subsequently underwent significant modifications, diverging from the parent repository in both functionality and programming language. Because it\u0026rsquo;s a forked repository, every Pull Request defaults to the parent repository\u0026rsquo;s branch, leading to accidental PRs to the parent repository. Contributors have contributed to and used the forked repository, but their contributions and downstream usage are not visible, hindering project growth. Due to these issues, developers may consider separating from the parent repository. However, GitHub currently doesn\u0026rsquo;t provide an Unfork/Detach function.\nWhile deleting and recreating the project achieves separation, it results in the loss of crucial information such as Issues, Wikis, and Pull Requests.\nUnforking is fundamentally different from leveraging Apache SkyWalking through a certain engine under a certain section. It\u0026rsquo;s more akin to the divergence of Hudson and Jenkins.\nSolution # After investigation and testing, the most viable solution is to contact GitHub Support. The specific steps are as follows:\nOpen this link: https://support.github.com/contact?tags=rr-forks Select your account or organization, then enter \u0026ldquo;unfork\u0026rdquo; in the Subject field. A virtual assistant will automatically appear; select the virtual assistant. Follow the virtual assistant\u0026rsquo;s prompts and select the appropriate answers (partial screenshot below). The conversation will be automatically transcribed. Send the request and wait for Support to process it (it shouldn\u0026rsquo;t take too long). It\u0026rsquo;s important to note that if your repository has been forked by others and you want to retain the fork history of your child repository after separating from the parent repository, you should select \u0026ldquo;Bring the child forks with the repository\u0026rdquo;.\nAlternatively, using commands like git clone --bare and git push --mirror preserves the complete Git history, but not Issues, Wikis, or Pull Requests.\nHopefully, this helps those who need it.\nReferences # Delete fork dependency of a GitHub repository Unfork a Github fork without deleting Please indicate the author and source when reprinting this article. Do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2022-03-09","externalUrl":null,"permalink":"/en/posts/unfork-github-repo/","section":"Posts","summary":"This article describes how to separate a forked repository from its parent repository using GitHub Support, avoiding data loss from deletion and reconstruction, and helping developers better manage forked repositories.","title":"Reliably Unforking a GitHub Repository Without Deletion and Reconstruction","type":"posts"},{"content":"","date":"2022-03-06","externalUrl":null,"permalink":"/tags/groovy/","section":"标签","summary":"","title":"Groovy","type":"tags"},{"content":"在使用 Jenkins 和 Groovy 越久，我就这样的疑问：\nGroovy 到底是什么语言？ Groovy 有哪些特性？ Groovy 和 Java 有什么区别？ Groovy 和 Java 如何选择？ Groovy 在整个开发语言中占什么位置？要不要学？\n本篇我的学习结果的分享，希望也能帮助你解答以上的这些问题。\n什么是 Groovy # Apache Groovy 是一种强大的、可选类型的动态语言，具有静态类型和静态编译功能，适用于 Java 平台，旨在通过简洁、熟悉且易于学习的语法提高开发人员的工作效率。 它与任何 Java 程序顺利集成，并立即为你的应用程序提供强大的功能，包括脚本功能、特定领域语言创作、运行时和编译时元编程和函数式编程。\nGroovy 的特性 # 翻译官方的说法，Groovy 有以下六大特性。\n平坦的学习曲线 - 简洁、易读且富有表现力的语法，Java 开发人员易于学习 强大的功能 - 闭包、构建器、运行时和编译时元编程、函数式编程、类型推断和静态编译 流畅的 Java 集成 - 与 Java 和任何第三方库无缝、透明地集成和互操作 领域特定语言 - 灵活可延展的语法，先进的集成和定制机制，在你的应用程序中集成可读的业务规则 充满活力和丰富的生态系统 - Web 开发、响应式应用程序、并发/异步/并行库、测试框架、构建工具、代码分析、GUI 构建 脚本和测试胶水 - 非常适合编写简洁和可维护的测试，以及所有构建和自动化任务 Groovy 和 Java 的区别 # Groovy 是一种编程语言，也支持脚本语言；Java 是一种面向对象的编程语言。 Groovy 支持多方法，运行方法的选择将在运行时选择；Java 提供多方法的声明，在编译时而不是运行时选择。 Groovy 中，自动资源管理机制是不存在的，静态的、匿名的内部类；Java 从 Java7 版本开始就提供了自动资源管理，在内部静态类或匿名类方面占上风。 Groovy 中，有一些函数式编程特性，如 Lambda 函数，函数式接口；而 Java 从 JDK 8 版本开始就有 Lambda 函数、函数式接口和许多其他的流和并行操作功能。 Groovy 可以用单引号或双引号格式定义和声明字符串和字符字面；Java 只有双引号格式来声明和定义字符串字面或字符字面。 Groovy 中所有东西都是一个对象，并且只使用对象。因此，不存在自动装箱或拆箱的概念，也不存在基元的转换；相反，Java 有基元数据类型和 Wrapper 类，可以显式或隐式地进行自动装箱和自动拆箱。 Groovy 中，数据类型的自动拓宽和缩小有很多宽广的范围，有很多转换；而Java在数据类型的缩小或拓宽方面有限制。 Groovy 对其所有类型的类成员或数据都有一个默认的访问修饰符；而Java的默认访问级别是包级，取决于类成员的类型。 Groovy 在其类中自动生成 getters 和 setter 来访问和修改类的成员；而在 Java 中，它们必须在类中明确提到访问修饰符。 Groovy 有 Groovy beans；而Java有Java beans。 Groovy 也被称为 Java 的超集，因为 Java 程序可以在 Groovy 环境中运行。反过来并不一定。 Groovy 在定义类型时有更简单的语法，只需使用 def 来声明一个变量；Java有不同类型的类型名称来声明变量或类的任何方法或成员。 Groovy 不要求任何主方法或方法的入口点来运行类或任何程序；而 Java 则要求类中的 main 方法来运行程序。 Groovy 和 Java 如何选择 # 如果可扩展性和性能至关重要并且公司开发 Web 应用程序，当然是 Java。比如电商、银行、金融、国防、医疗保健等领域的大公司都会选择 Java，因为它经过时间证明，通常用于开发复杂的企业项目。 Groovy 可以用来编排一些 Pipeline，自动化，测试任务，因为它既是编程语言也是一种出色的脚本语言，功能强大且易于学习。 Groovy 目前流行排名 # 我们从这张图看到 2022 年 Groovy 语言的排行有一个非常大的下滑，从之前的排名 12 直接跌倒了 20。\n从数据上 2022 年 2 月 Groovy 有小幅减少（从 0.76，%0.74%），但这不是主要原因，主要是很可能是因为 TIOBE index 从 Alexa 更换为 Similarweb 网络流量引擎导致的波动。\n这些语言的排名上升包括：Assembly language，Go，Swift, MATLAB, Delphi/Object Pascal, Classic Visual Basic, Objective-C 的增长抢占了 Groovy 原有的位置。\n另外从 Groovy 的流行历史来看，它目前还是有很多人在使用的。开发者或许不会把它当作第一语言，但作为脚本语言学习一下还是可以的。\n对于使用 Jenkins Shared Libraries 的 DevOps 工程师，需要学习 Groovy。\n参考 # Groovy vs Java: https://flyoutsourcing.com/blog/groovy-vs.-java-what-suits-your-needs.html Differences with Java：https://groovy-lang.org/differences.html TIOBE index：https://www.tiobe.com/tiobe-index/ ","date":"2022-03-06","externalUrl":null,"permalink":"/posts/groovy/","section":"Posts","summary":"Groovy 是一种强大的动态语言，适用于 Java 平台，本文介绍了 Groovy 的特性、与 Java 的区别以及在 Jenkins 中的应用场景。","title":"在 Jenkins 上用了这么久的 Groovy，是时候认识一下它了","type":"posts"},{"content":"","date":"2022-03-02","externalUrl":null,"permalink":"/en/tags/blackduck/","section":"Tags","summary":"","title":"BlackDuck","type":"tags"},{"content":" Details # Failure: PIP - Pip Inspector The Pip Inspector tree parse failed to produce output. Overall Status: FAILURE_DETECTOR - Detect had one or more detector failures while extracting dependencies. For more output please click to expand.\n👉 Click to see more output 👈 [main] --- ======== Detect Issues ======== [main] --- [main] --- DETECTORS: [main] --- Detector Issue [main] --- /workdir/test [main] --- Failure: PIP - Pip Inspector [main] --- The Pip Inspector tree parse failed to produce output. [main] --- [main] --- ======== Detect Result ======== [main] --- [main] --- Black Duck Project BOM: https://org.blackducksoftware.com/api/projects/246c8952-7cb8-40e9-9987-35f7d4602ae1/versions/e1cb4204-42d0-4445-8675-978df62b150d/components [main] --- [main] --- ======== Detect Status ======== [main] --- [main] --- GIT: SUCCESS [main] --- PIP: FAILURE [main] --- [main] --- Signature scan / Snippet scan on /workdir/test: SUCCESS [main] --- Overall Status: FAILURE_DETECTOR - Detect had one or more detector failures while extracting dependencies. Check that all projects build and your environment is configured correctly. [main] --- [main] --- If you need help troubleshooting this problem, generate a diagnostic zip file by adding \u0026#39;-d\u0026#39; to the command line, and provide it to Synopsys Technical Support. See \u0026#39;Diagnostic Mode\u0026#39; in the Detect documentation for more information. [main] --- [main] --- =============================== [main] --- [main] --- Detect duration: 00h 00m 54s 951ms [main] --- Exiting with code 5 - FAILURE_DETECTOR ENVIRONMENT:\nProduct: synopsys-detect-7.11.1.jar Others: OpenJDK 11, Python 3.6 and Python 2.7.5 Root cause # More output of this run, I see it used python (which is python2) not python3, so run pip-inspector.py failed.\nDEBUG [main-Executable_Stream_Thread] --- Python 2.7.5 ... [main] --- Running executable \u0026gt;/usr/bin/python /home/****/blackduck/runs/2022-03-01-07-45-05-986/shared/pip/pip-inspector.py --projectname=test Solution # Link python to python3, it works in my case.\nFor example\n# save python to other name sudo mv /usr/bin/python /usr/bin/python.old # link python3 to python sudo ln -s /usr/bin/python3 /usr/bin/python Then try to run bash \u0026lt;(curl -s -L https://detect.synopsys.com/detect7.sh) again, my test commands:\nbash \u0026lt;(curl -s -L https://detect.synopsys.com/detect7.sh) --blackduck.url=https://org.blackducksoftware.com --blackduck.api.token=MmMwMjdlOTctMT --detect.project.name=HUB --detect.project.version.name=TEST_v1.1.1 --detect.source.path=/workdir/test --logging.level.com.synopsys.integration=DEBUG --blackduck.trust.cert=TRUE --detect.tools.excluded=POLARIS --detect.blackduck.signature.scanner.snippet.matching=SNIPPET_MATCHING If you want to use Docker to do Blackduck scan, you can create a Docker image. like this\nFROM openjdk:11 # Set DETECT version you need, if it\u0026#39;s empty download the latest version. # https://sig-repo.synopsys.com/artifactory/bds-integrations-release/com/synopsys/integration/synopsys-detect ENV DETECT_LATEST_RELEASE_VERSION=\u0026#34;\u0026#34; RUN apt-get update \\ \u0026amp;\u0026amp; apt-get upgrade -y \\ \u0026amp;\u0026amp; apt-get install -y \\ git \\ python \\ pip \\ \u0026amp;\u0026amp; apt-get autoremove \\ \u0026amp;\u0026amp; apt-get clean RUN curl -sSOL https://detect.synopsys.com/detect7.sh \u0026amp;\u0026amp; bash detect7.sh --help \\ \u0026amp;\u0026amp; rm -rf /usr/bin/python \\ \u0026amp;\u0026amp; ln -s /usr/bin/python3 /usr/bin/python WORKDIR /src Hope this help.\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-03-02","externalUrl":null,"permalink":"/en/posts/blackduck-troubleshooting/","section":"Posts","summary":"This article explains how to resolve the \u0026ldquo;The Pip Inspector tree parse failed to produce output\u0026rdquo; error in Black Duck Detect, including the root cause and solution.","title":"Resolved - The Pip Inspector tree parse failed to produce output","type":"posts"},{"content":"DevOps 是 IT 界最近几年的一个热门话题，而且还会越来越热。\n最近有幸和一位做传播咨询的读者朋友交流关于 2022 年最值得关注的 DevOps 趋势以及一些问题和回答，分享给大家。\n行业趋势 # 趋势一：转向无服务器计算 # 无服务器计算是一种新兴趋势，实际上已经存在了十多年。企业购买无服务器框架需要一段时间，主要是因为对行业支持和对投资回报的担忧。\n无服务器具有许多越来越难以忽视的优势，主要的两个最大好处是效率和可靠性。没有基础设施管理的负担，企业可以将资源集中在正重要的事项上。此外，无服务器还降低了传统框架可能出现的潜在维护问题的风险。\n无服务器提供固有的可扩展性和可靠性并自动化开发人员不喜欢的日常操作任务，2022 年无服务器计算会经历下一次发展。\n趋势二：微服务架构增长 # 随着无服务器计算在 2022 年的发展，微服务也将如此。\n微服务架构是将单体应用分化为小的独立单元，或服务，从而为大型团队提供了更大的灵活性。它有以下优势：\n为企业提供比单体应用程序更好的可扩展性和敏捷性 开发人员可以使用他们熟悉的编程语言和工具，消除传统应用程序开发的局限 开发人员能够在不破坏整个代码库的情况下部署小的特性或功能 DevOps 团队可以根据业务需求来扩展每个应用部分，而不是一次性扩展整个应用 出现问题微服务可以轻松控制问题，而不会中断整个应用程序 当然也必须认识到微服务的一个弊端，如果实施不佳可能导致严重问题，包括数据丢失、可靠性差和安全风险。\n趋势三：Kubernetes 成为基础架构 # Kubernetes，也称 K8s，是容器编排开源平台，它能够与容器组交互，同时管理更多集群。除了容器管理，还提供安全、网络和存储服务，自我监控，节点和容器的健康状况检查。它可以处理从虚拟机集群管理到负载平衡等所有方方面面，提高生产力，简化 DevOps 开发、测试和部署流程。\n根据 Flexera 的 2021 年云计算状况报告，48% 的企业使用 Kubernetes，另有 25% 的企业计划使用它。另外 53% 的组织使用 Docker，21% 的组织计划使用。\n趋势四：DevSecOps 成为重要组成部分 # 安全性正在成为 DevOps 领域的另一个日益关注的问题。\n为了避免网络攻击，许多大型企业正在将安全性集成到他们的 DevOps 流程中。从 DevOps 到 DevSecOps 的转变预计在 2022 会有更多公司在软件开发生命周期的早期加入安全控制。 这使 DevOps 团队能够在开发阶段持续监控和修复安全缺陷，从而提高交付速度和质量。DevSecOps 正在成为许多公司组织结构图的重要组成部分。\n行业问答 # 问题一: DevOps 整个目前行业头部本土和国际玩家有哪些（GitLab)？ # 以我所在的外企而言通常是选择国际玩家，以最常用的代码管理和项目管理的工具为例：\n上了年头的外企大公司通常在使用 Atlassian 家的 Jira 和 Bitbucket。船大难掉头，选择 GitLab，GitHub 这样一站式的 DevOps 迁移成本很高，需要有足够的理由才可能换工具。 对于年轻的公司，GitLab 和 GitHub 都是很好的选择。GitLab 在企业内部建立私服居多；GitHub 也提供企业版私服，但对于开源项目而言 GitHub 依然是代码托管的首选。 其他用到的付费级 DevOps 工具还包括 Synopsys (Polaris, Blackduck)，Jfrog (Artifactory)，SonarQube 等。\n问题二: 行业目前有哪些重点趋势？比如安全这块，是不是目前行业关注度比较高？有哪些工具？ # 安全领域的关注度在逐年升高，尤其在外企很注重安全这块，他们愿意花钱来购买安全扫描工具来扫描代码，甚至还会要求所有的发布的产品代码中不能有高危漏洞。\n一些常用的工具包括：静态代码扫描，比如 Polaris, Veracode, Snyk, SonarQube, PVS-Studio；代码组成分析，比如 Blackduck，X-Ray 等等。\n问题三：企业在选择 DevOps 平台时主要考虑的因素有哪些？比如数据库安全，公司成熟度，海外知名度，等等 # 我认为主要考虑公司的知名度，其次产品的知名度，如果是开源产品会着重关注 GitHub 上的 Contributors 数量，它更能代表社区的活跃度，其次是 Fork 和 Star 数量。\n问题四：目前 DevOps 处于哪个阶段? 未来的发展机会是在哪里？ # DevOps 市场目前处在相对成熟的阶段，每个细分领域都有很多工具可以选择。未来基础设施会更多的向容器云方向发展。\n具有创新的 DevOps 产品依然会很有市场，像是 GitLab，HashiCorp 等公司的产品，他们在短短十年内成为世界级的软件公司。\n问题五：有哪些主流或平时重点关注的行业媒体号或自媒体公众号？ # 会经常看一些 DevOps 相关的以及 InfoQ，阿里、腾讯、美团等技术公众号。\n还会看 YouTube 上一些 DevOps 的个人及公司频道：TechWorld with Nana, CloudBeesTV，CNCF，DevOps Paradox，DevOps Toolkit 等。\n问题六：除了公众号外，你平时会上哪些行业社区？比如说 GitHub 或 CSDN？ # 最常看的是 GitHub 以及 GitHub Trending 来看最近受关注的项目。还会\n社区会定期去看 DEV Community, Medium, InfoQ 。会看知乎上一些话题下的精华，很少看 CSDN，懂得都懂。\n参考 # Top DevOps Trends to Watch in 2022 DevOps Trends To Look Out for in 2022 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-02-24","externalUrl":null,"permalink":"/posts/devops-trends-2022/","section":"Posts","summary":"本文介绍了2022年DevOps领域的主要趋势，包括无服务器计算、微服务架构、Kubernetes的普及以及DevSecOps的兴起，并回答了一些关于DevOps行业的问题。","title":"2022 年最值得关注的 DevOps 趋势和问答","type":"posts"},{"content":"在写博客和公众号这件事上，不知不觉已经是我的第五个年头了，没想过能这么久。\n借此分享一下这些年我的职业线路的变化，以及写博客\u0026amp;公众号有什么收获，算是自己过去的一个总结，如果能有点共鸣和帮助就更好了。\n从QA到DEV到DEVOPS # 最早关注我公众号读者朋友大概都是因为软件测试而结缘的。是的，我做了近 10 的软件测试工作，先后在 SIMcom、东软、京东商城、外企从事过功能\u0026amp;自动化\u0026amp;性能测试工作。\n从功能测试入行开始，我慢慢地感受到编程不是开发的独门武功，它也是测试工程师的必备技能，只有具备良好的编码能力，才能去做自动化、Unittest、以及测试开发等工作。\n当我做了自动化测试工程师，我又发现相对于“发现”问题，“解决”问题更令我愉悦。我开始梦想有机会能去做开发，这样不但可以提高自己的编程能力，另外开发、测试都懂也能为自己今后的职业发展找到更多可能性。\n最终是因为有这样的机会+自己的主动+编码过得去，我从测试转到了开发。起初的艰难和压力都是我工作近 10 年来前所未有的，白天看代码、晚上看代码、周末看代码\u0026hellip; 天天如此。经过了半年多的努力，才终于上岸，可以做 C/C++ 项目的 Bugfix 了。\n也正是因为有开发、自动化、持续集成的经验，在团队需要一名 Build/Release 工程师的时候，我知道这就是我最适合的岗位，负责产品的自动化构建、发布、基础设施建设、CI/CD 以及提高研发效能的相关开发工作。\n就这样我从 QA 到 DEV 到 DEVOPS。公众号的更名记录也记录了我的职业路线变化：\n2019年07月28日 “软件测试与开发”改名“DevOps攻城狮” 2018年12月29日 “DevQA”改名“软件测试与开发” 2018年12月26日 “软件测试QA”改名“DevQA” 2017年08月01日 注册“软件测试QA” 写作五年有哪些收获 # 写作是一项长期收益远超短期收益的事情。\n对于绝大多数人在短期内几乎不会有什么实质性的收益，还会花费大量的业余时间，妥妥的是用爱在发电。从金钱角度来衡量这件事，这是一件投入和产出完全不成比例的事情，很难坚持。\n如果从长期来看，坚持写作一定会带来价值的，我总结有以五个方面的好处：\n好记性不如烂笔头 - 当我们弄明白了一个技术难题，虽然当时明白了，但如果没记录下来，很有可能以后遇到同样的问题又不知道该如何解决。 让别人听懂才是真的懂 - 有时候对于一个问题我们认为自己明白了，当分享给别人的时候，才发现其中有的逻辑说不通，因此不得不继续思考并彻底搞清楚。 打造学习飞轮 - 当你坚持分享并有人关注到你并与你互动的时候，你就会有动力继续分享，学习新的知识然后再分享，一旦学习的飞轮造好了，坚持下去就变得容易。 间接收益 - 但凡坚持写点东西，对于以后找工作都或多或少会有些帮助，至少说明你是一个爱学习的人。如果你的分享让同行、未来你的面试官觉得很不错，很可能会给你带来一次新的工作机会。 直接收益 - 直接利益包括平台流量和广告收益、以及卖专栏、做咨询等。这要求就很高了，不但需要会自媒体运营，还有要超强的输出功力，这背后就是比别人更多的付出。 2017 年的时候我没想那么多，只是觉得自己也可以写点东西，就在 2017 年 7 月 6 日通过 GitHub Page 建立了自己的个人博客\n内容有了，复制过来也不费电，还能了解下公众号怎么玩的，然后就在同年 8 月开通了微信公众号；后来想看看小程序是怎么玩的，然后在 2020 年五一假期为我的博客创建了微信小程序(DevOps攻城狮)\n对于我来说花了这么多的业余时间来写作，说说有哪些具体的收获。\n知道如何在 GitHub 上建站、发布博客，把 GitHub 变成最常访问的网站之一 知道 Hexo 博客如何集成 Disqus, Google Analytics, Google Adsense, etc，并做了很多个改进 知道如何使用和集成 Github Actions、Travis、SonarQube 等工具 知道如何运营一个公众号；知道如何创建、发布一个微信小程序 参与开源项目，在开源项目中学习编码、开阔眼界和最佳实践 有同事说 ta 读到了我的文章，找到并关注了我的微信公众号，这让我很荣幸 收到过咨询，还有咨询者的感谢红包，能够帮到别人并收到正反馈让我非常开心 不敢想还能收到大出版社的邀请写一本技术书籍，因工作忙以及还有更重要的知识要学，主动放弃了 \u0026hellip; \u0026hellip; 这些收获中，我觉得最大的收获是打造学习飞轮，养成分享习惯。\n最好的时间是十年前，其次是现在。日拱一卒，功不唐捐，持续做对的事情，其他的就交给时间。\n—— 2022 年 2 月 20 日，凌晨更新。\n相关推荐阅读 # 做了9年测试，我为何转开发？ 从测试到开发的五个月 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-02-21","externalUrl":null,"permalink":"/misc/from-qa-to-dev-to-devops/","section":"Miscs","summary":"从软件测试到开发再到DevOps，码字五年有哪些收获？分享我的职业发展和写作经验。","title":"从QA到DEV到DEVOPS，码字五年有哪些收获","type":"misc"},{"content":"","date":"2022-02-14","externalUrl":null,"permalink":"/en/tags/vagrant/","section":"Tags","summary":"","title":"Vagrant","type":"tags"},{"content":"For an introduction to Vagrant, please refer to the previous article: What is Vagrant? The Difference Between Vagrant and VirtualBox\nWhat is Vagrant # For an introduction to Vagrant, please refer to the previous article: What is Vagrant? The Difference Between Vagrant and VirtualBox\nVagrant vs. Docker # One of the most frequently asked questions about Vagrant is: What\u0026rsquo;s the difference between Vagrant and Docker?\nDirectly comparing Vagrant and Docker without considering the context is inappropriate. In some simple scenarios, their functions overlap, but in many more scenarios, they are not interchangeable.\nSo, when should you use Vagrant, and when should you use Docker?\nTherefore, if you only want to manage virtual machines, you should use Vagrant; if you want to quickly develop and deploy applications, you should use Docker.\nLet\u0026rsquo;s discuss why in more detail.\nVagrant is a VM management tool, or orchestration tool; Docker is a tool used to build, run, and manage containers. The question actually boils down to the difference between a virtual machine (VM) and a container (Container).\nLet\u0026rsquo;s use a set of images from the internet to illustrate the differences between a physical machine (Host), a virtual machine (VM), and a container (Container).\nPhysical Machine (Host)\nVirtual Machine (VM)\nContainer (Container)\nFrom the images, we can more easily understand the differences between a virtual machine (VM) and a container (Container):\nFeature Virtual Machine Container Isolation Level Operating System Level Process Level Isolation Strategy Hypervisor CGROUPS System Resources 5 - 15% 0 - 5% Startup Time Minutes Seconds Image Storage GB MB Summary: Differences in Use Cases for Vagrant and Docker\nVagrant is designed to manage virtual machines, while Docker is designed to manage application environments.\nVagrant is more suitable for development and testing, solving environment consistency issues; Docker is more suitable for rapid development and deployment, and CI/CD.\nFinally, both Vagrant and Docker have a large number of community-contributed \u0026ldquo;Boxes\u0026rdquo; and \u0026ldquo;Images\u0026rdquo; available.\nWelcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo; - Focusing on DevOps knowledge sharing.\n","date":"2022-02-14","externalUrl":null,"permalink":"/en/posts/vagrant-vs-docker/","section":"Posts","summary":"This article compares Vagrant and Docker, analyzing their respective use cases and advantages to help readers choose the right tool for managing virtual machines or containers.","title":"Vagrant vs. Docker —— Which One Should You Choose?","type":"posts"},{"content":"","date":"2022-02-14","externalUrl":null,"permalink":"/en/tags/virtualbox/","section":"Tags","summary":"","title":"VirtualBox","type":"tags"},{"content":" What is Vagrant # Vagrant is an open-source software product used to easily build and maintain virtual software development environments.\nFor example, it can build development environments based on providers such as VirtualBox, VMware, KVM, Hyper-V, AWS, and even Docker. It improves development efficiency by simplifying the configuration management of virtualized software.\nVagrant is developed in Ruby, but its ecosystem supports development in several other languages.\nSimply put, Vagrant is an encapsulation layer over traditional virtual machines, allowing you to use virtual development environments more conveniently.\nVagrant\u0026rsquo;s History # Vagrant was initially launched as a personal project by Mitchell Hashimoto in January 2010.\nThe first version of Vagrant was released in March 2010. In October 2010, Engine Yard announced that they would sponsor the Vagrant project.\nThe first stable version of Vagrant, Vagrant 1.0, was released in March 2012, exactly two years after the original release.\nIn November of the same year, Mitchell founded HashiCorp to support full-time development of Vagrant. Vagrant remains open-source software, and HashiCorp is committed to creating commercial versions and providing professional support and training for Vagrant.\nNow HashiCorp has become a world-leading open-source company. Through a series of products, including Vagrant, Packer (packaging), Nomad (deployment), Terraform (cloud environment configuration), Vault (access management), and Consul (monitoring), it has redefined the entire DevOps process from end to end.\nVagrant initially supported VirtualBox, and version 1.1 added support for other virtualization software (such as VMware and KVM), as well as support for server environments such as Amazon EC2. Starting with version 1.6, Vagrant natively supported Docker containers, which can replace fully virtualized operating systems in some cases.\nHow to Use Vagrant # Prerequisites for using Vagrant:\nInstall Vagrant. Download Vagrant Install VirtualBox Once both are ready, you can create and use your virtual machine via the command line.\nFor example, you need an Ubuntu 18.04 LTS 64-bit virtual machine. More virtual machines can be searched for on the Box website, which is similar to Docker Hub, where users can download and upload various Vagrant Boxes.\nYou only need to execute a few simple commands to complete startup, login, logout, and destruction.\nInitialize Vagrant\nvagrant init hashicorp/bionic64 Start the virtual machine. This should take a few tens of seconds (the first time will take longer to download the image, depending on your internet speed).\nvagrant up Log in to your virtual machine, and you can then use your created Ubuntu virtual machine.\nvagrant ssh When you\u0026rsquo;re done, execute logout to log out.\nDifferences Between Vagrant and Traditional Virtual Machine Software # Vagrant is much more convenient than traditional virtual machine usage. Let\u0026rsquo;s look at how to create a virtual machine using the traditional method.\nTaking VirtualBox as an example, assuming you have already installed VirtualBox, the steps to create a virtual machine using the traditional method are as follows:\nFirst, download the corresponding ISO file. Then, load the ISO using VirtualBox or VMware. Finally, configure the CPU, memory, disk, network, user, etc., and wait for the installation to complete.\nThis method is very cumbersome to configure, requiring step-by-step operations. These configuration steps often require documentation to ensure that an \u0026ldquo;identical\u0026rdquo; virtual development environment can be created later.\nBy comparison, you should now have a general understanding of how Vagrant is used and some of the differences between it and traditional virtual machine usage.\nSummary # The advantages of Vagrant over traditional virtual machine usage: it provides easy-to-configure, reproducible, and portable work environments, thereby improving productivity and flexibility.\nVagrant can be said to be the easiest and fastest way to create and manage virtualized environments!\nIt is able to be so convenient because it stands on the shoulders of giants (VirtualBox, VMware, AWS, OpenStack, or other providers), and then uses tools such as Shell scripts, Ansible, Chef, and Puppet to automatically install and configure software on the virtual machine.\nThe next article will introduce the differences between Vagrant and Docker.\n","date":"2022-02-11","externalUrl":null,"permalink":"/en/posts/vagrant/","section":"Posts","summary":"This article introduces the concept and history of Vagrant, and how to use Vagrant to create and manage virtual machines, emphasizing the advantages of Vagrant over traditional virtual machines.","title":"What is Vagrant? Differences Between Vagrant and VirtualBox","type":"posts"},{"content":"","date":"2022-01-18","externalUrl":null,"permalink":"/en/tags/go/","section":"Tags","summary":"","title":"Go","type":"tags"},{"content":"Go is an open-source programming language that makes it easy to build simple, reliable, and efficient software.\nGo or Golang # Let\u0026rsquo;s start with a question most people might overlook: Is Google\u0026rsquo;s open-source programming language called Go or Golang? Or both? Three seconds to think\u0026hellip;\nGoogle says: It\u0026rsquo;s called Go. The reason some people call it Golang is that the previous Go language website was golang.org (because go.org was already taken), so some people mixed up Golang and Go.\nNow, entering golang.org will redirect you to go.dev, which is a way to officially clarify the name.\nAdvantages of Go # The official website describes Go as follows:\nGo is suitable for building large-scale, reliable, and efficient software quickly. Go is an open-source programming language backed by Google. Easy to learn and get started with. Built-in concurrency and a powerful standard library. A constantly evolving ecosystem of partners, communities, and tools. Today, Go is used in a variety of applications:\nGo is popular in cloud-based or server-side applications. Cloud infrastructure. Many of today\u0026rsquo;s most popular infrastructure tools are written in Go, such as Kubernetes, Docker, and Prometheus. Many command-line tools are written in Go. DevOps and web reliability automation often use Go. Go is also used in the fields of artificial intelligence and data science. It\u0026rsquo;s also used in microcontroller programming, robotics, and game development. This is why Go is becoming increasingly popular.\nIt was these advantages, and the need to write a CLI for work, that led me to learn Go.\nGo\u0026rsquo;s Ranking # Go\u0026rsquo;s popularity in China is quite high. Let\u0026rsquo;s take a look at Go\u0026rsquo;s current ranking.\nThis is the TIOBE January 2022 top 20 programming languages ranking. Go is ranked 13th, up one position from last year.\nCompared to Python, C, Java, C++, and C#, which are in the top five, do you think Go can catch up?\nFrom my observations of non-cloud companies and colleagues, most developers use C/C++, Java, C#, and Python. Therefore, I think this ranking is quite expected.\nShould Beginners Learn Python or Go? # Python has been around for over 30 years, but its popularity continues to grow. Python is an excellent object-oriented language, and you can also use a functional programming style to write code. Among all programming languages, you might not find one used by more non-programmers than Python.\nIts flexibility is one reason Python is so popular. It\u0026rsquo;s often used for scripting, web development, data science, teaching programming to children, creating animations, and more. So how does Go compare to Python?\nBoth Python and Go have simple syntax. Both Python and Go are easy for beginners to get started with and relatively easy to learn (Python is relatively easier). Python often dominates the data science field; Go is well-suited for systems programming. Go is much faster than Python in terms of program execution speed. As a high-level language, Python has a wider range of libraries and a larger community built around it. Go is ideal for handling large concurrent applications and supports concurrency—the ability to run multiple programs/tasks simultaneously. Python does not. Today, Python and Go are two of the most popular and convenient programming languages. For beginners, should they learn Python or Go?\nIf you\u0026rsquo;re a complete beginner, it\u0026rsquo;s recommended to learn Python first. Compared to Go, Python is easier to learn. If you\u0026rsquo;re a test engineer and want to learn a programming language, learn Python. Most automation testing positions require Python proficiency. If you\u0026rsquo;re a software developer or DevOps engineer, you should learn both. \u0026ldquo;Kids make choices, adults take both.\u0026rdquo; How to Learn Go # Read documentation or watch videos, but most importantly, get your hands dirty!!\nI first watched Go language video tutorials between 2010 and 2020, but because I didn\u0026rsquo;t do much coding, I remained in a state of knowing only a little.\nFor newcomers learning any programming language, tutorials only teach you about 30%. To truly learn, you must practice personally, otherwise, it will be: \u0026ldquo;Looks easy, but impossible to write\u0026rdquo;.\nChoose a direction and start coding immediately.\nMy direction was to write a CLI tool. Although Go\u0026rsquo;s built-in Flag package can be used to write CLI commands, after looking at many CLI projects developed using Go, I noticed that most of them didn\u0026rsquo;t use the built-in Flag package, but mostly used spf13/cobra or urfave/cli.\nThis is a list of projects using cobra here, including well-known projects like Kubernetes, Hugo, Docker, and the GitHub CLI. As for urfave/cli, I saw Jfrog CLI using it, but I didn\u0026rsquo;t see a list of other well-known projects using urfave/cli like cobra. For beginners like me, the most important thing is to start immediately, so you don\u0026rsquo;t need to spend too much time choosing a framework. Cobra has so many excellent projects backing it up, just use it. The most important thing is to start coding as soon as possible. In the coding process, choose top-tier projects that also use this framework as a reference. This can help us write better code by reading other people\u0026rsquo;s code. Don\u0026rsquo;t just Ctrl + C and Ctrl + V.\nFinally, here are some other excellent projects when developing CLIs:\ngithub.com/AlecAivazis/survey/v2 - Build interactive command lines in the terminal. github.com/enescakir/emoji - Emoji library, supports emoji output in the terminal. github.com/mgutz/ansi - Create ANSI color strings. ","date":"2022-01-18","externalUrl":null,"permalink":"/en/posts/what-is-go/","section":"Posts","summary":"This article introduces the basic concepts, advantages, and ranking of the Go programming language. It also guides beginners on choosing between learning Python or Go, providing practical learning suggestions and resources.","title":"What is Go? Advantages, Current Status, and Choosing Between Python and Go for Beginners","type":"posts"},{"content":"","date":"2022-01-12","externalUrl":null,"permalink":"/tags/dokerfile/","section":"标签","summary":"","title":"Dokerfile","type":"tags"},{"content":"本篇分享在编写 Dockerfiles 和使用 Docker 时应遵循的一些最佳实践。篇幅较长，建议先收藏慢慢看，保证看完会很有收获。\n文章目录 # Dockerfile 最佳实践\n使用多阶段的构建 调整 Dockerfile 命令的顺序 使用小型 Docker 基础镜像 尽量减少层的数量 使用无特权的容器 优先选择 COPY 而不是 ADD 将 Python 包缓存到 Docker 主机上 每个容器只运行一个进程 优先选择数组而不是字符串语法 理解 ENTRYPOINT 和 CMD 之间的区别 添加健康检查 HEALTHCHECK Docker 镜像最佳实践\nDocker 镜像的版本 不要在镜像中存储密钥 使用 .dockerignore 文件 检查和扫描你的 Docker 文件和镜像 签署和验证镜像 Dockerfile 最佳实践 # 1. 使用多阶段的构建 # 利用多阶段构建的优势来创建更精简、更安全的Docker镜像。多阶段 Docker 构建(multi-stage builds)允许你将你的 Dockerfile 分成几个阶段。\n例如，你可以有一个阶段用于编译和构建你的应用程序，然后可以复制到后续阶段。由于只有最后一个阶段被用来创建镜像，与构建应用程序相关的依赖关系和工具就会被丢弃，因此可以留下一个精简的、模块化的、可用于生产的镜像。\nWeb 开发示例：\n# 临时阶段 FROM python:3.9-slim as builder WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y --no-install-recommends gcc COPY requirements.txt . RUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt # 最终阶段 FROM python:3.9-slim WORKDIR /app COPY --from=builder /app/wheels /wheels COPY --from=builder /app/requirements.txt . RUN pip install --no-cache /wheels/* 在这个例子中，GCC 编译器在安装某些 Python 包时是必需的，所以我们添加了一个临时的、构建时的阶段来处理构建阶段。\n由于最终的运行时映像不包含 GCC，所以它更轻，也更安全。镜像大小比较：\nREPOSITORY TAG IMAGE ID CREATED SIZE docker-single latest 8d6b6a4d7fb6 16 seconds ago 259MB docker-multi latest 813c2fa9b114 3 minutes ago 156MB 再来看一个例子：\n# 临时阶段 FROM python:3.9 as builder RUN pip wheel --no-cache-dir --no-deps --wheel-dir /wheels jupyter pandas # 最终阶段 FROM python:3.9-slim WORKDIR /notebooks COPY --from=builder /wheels /wheels RUN pip install --no-cache /wheels/* 镜像大小比较：\nREPOSITORY TAG IMAGE ID CREATED SIZE ds-multi latest b4195deac742 2 minutes ago 357MB ds-single latest 7c23c43aeda6 6 minutes ago 969MB 总之，多阶段构建可以减少你的生产镜像的大小，帮助你节省时间和金钱。此外，这将简化你的生产容器。由于尺寸较小和简单，相对会有较小的攻击面。\n2. 调整 Dockerfile 命令的顺序 # 密切注意你的 Dockerfile 命令的顺序，以利用层缓存。\nDocker 在一个特定的 Docker 文件中缓存每个步骤（或层），以加快后续的构建。当一个步骤发生变化时，不仅该步骤，而且所有后续步骤的缓存都将被废止。\n例如：\nFROM python:3.9-slim WORKDIR /app COPY sample.py . COPY requirements.txt . RUN pip install -r /requirements.txt 在这个 Dockerfile 中，我们在安装需求之前复制了应用程序的代码。现在，每次我们改变 sample.py 时，构建都会重新安装软件包。这是非常低效的，特别是在使用 Docker 容器作为开发环境时。因此，把经常变化的文件放在 Dockerfile 的末尾是很关键的。\n你也可以通过使用 .dockerignore 文件来排除不必要的文件，使其不被添加到 Docker 构建环境和最终镜像中，从而帮助防止不必要的缓存失效。更多信息后面会提到。\n因此，在上面的 Dockerfile 中，你应该把 COPY sample.py . 命令移到底部，如下所示：\nFROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r /requirements.txt COPY sample.py . 注意。\n总是把可能发生变化的层放在 Dockerfile 中尽可能的低。 将多个 RUN apt-get update，RUN apt-get install 等命令结合到一起执行。(这也有助于减少镜像的大小，后面会很快就会提到这一点)。 如果你想关闭某个 Docker 构建的缓存，可以添加 --no-cache=True 标志。 3. 使用小型 Docker 基础镜像 # 较小的 Docker 镜像更具有模块化和安全性。较小的 Docker 基础镜像在构建、推送和拉动镜像的速度较小，它们也往往更安全，因为它们只包括运行应用程序所需的必要库和系统依赖。\n你应该使用哪个 Docker 基础镜像？这个没有一个固定的答案，它这取决于你要做什么。下面是 Python 的各种 Docker 基础镜像的大小比较。\nREPOSITORY TAG IMAGE ID CREATED SIZE python 3.9.6-alpine3.14 f773016f760e 3 days ago 45.1MB python 3.9.6-slim 907fc13ca8e7 3 days ago 115MB python 3.9.6-slim-buster 907fc13ca8e7 3 days ago 115MB python 3.9.6 cba42c28d9b8 3 days ago 886MB python 3.9.6-buster cba42c28d9b8 3 days ago 886MB 虽然基于 Alpine Linux 的 Alpine flavor 是最小的，但如果你找不到可以与之配合的编译二进制文件，往往会导致构建时间的增加。因此，你最终可能不得不自己构建二进制文件，这可能会增加镜像的大小（取决于所需的系统级依赖）和构建时间（由于必须从源头编译）。\n关于为什么最好不要使用基于 Alpine 的基础镜像，请参考适用于 Python 应用程序的最佳 Docker 基础映像 和 使用 Alpine 可以使 Python Docker 构建速度慢 50 倍 了解更多关于为什么最好避免使用基于 Alpine 的基础镜像。\n归根结底，这都是关于平衡的问题。如果有疑问，从 *-slim flavor 开始，特别是在开发模式下，因为你正在构建你的应用程序。你想避免在添加新的 Python 包时不得不不断地更新 Dockerfile 以安装必要的系统级依赖。当你为生产强化你的应用程序和 Dockerfile 时，你可能想探索使用 Alpine 来完成多阶段构建的最终镜像。\n另外，别忘了定期更新你的基础镜像，以提高安全性和性能。当一个基础镜像的新版本发布时，例如：3.9.6-slim \u0026ndash;\u0026gt; 3.9.7-slim，你应该拉出新的镜像并更新你正在运行的容器以获得所有最新的安全补丁。\n4. 尽量减少层的数量 # 尽量把 RUN、COPY 和 ADD 命令结合起来使用，因为它们会创建层。每一层都会增加镜像的大小，因为它们是被缓存的。因此，随着层数的增加，镜像大小也会增加。\n你可以用 docker history 命令来测试一下。\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE dockerfile latest 180f98132d02 51 seconds ago 259MB docker history 180f98132d02 IMAGE CREATED CREATED BY SIZE COMMENT 180f98132d02 58 seconds ago COPY . . # buildkit 6.71kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 58 seconds ago RUN /bin/sh -c pip install -r requirements.t… 35.5MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago COPY requirements.txt . # buildkit 58B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago WORKDIR /app ... 请注意尺寸。只有 RUN、COPY 和 ADD 命令增加了镜像的尺寸，你可以尽可能地通过合并命令来减少镜像的大小。比如：\nRUN apt-get update RUN apt-get install -y gcc 可以合并成一个 RUN 命令：\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y gcc 因此，创建一个单层而不是两个，这就减少了最终镜像的大小。虽然减少层数是个好主意，但更重要的是，这本身不是一个目标，而是减少镜像大小和构建时间的一个副作用。换句话说呢，与其试图优化每一条命令，你更应该关注前面的三种做法！！！\n多阶段构建 Dockerfile命令的顺序 以及使用一个小的基础镜像。 注意 # RUN、COPY 和 ADD 都会创建图层 每个图层都包含与前一个图层的差异 图层会增加最终镜像的大小 提示 # 合并相关命令 在创建过程中执行 RUN 步骤中删除不必要的文件 尽量减少运行 apt-get upgrade 的次数，因为它将所有软件包升级到最新版本。 对于多阶段的构建，不要太担心过度优化临时阶段的命令 最后，为了便于阅读，建议将多行参数按字母数字排序。\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ git \\ gcc \\ matplotlib \\ pillow \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* 5. 使用无特权的容器 # 默认情况下，Docker 在容器内以 root 身份运行容器进程。然而，这是一个糟糕的做法，因为在容器内以 root 身份运行的进程在 Docker 主机中也是以 root 身份运行。\n因此，如果攻击者获得了对容器的访问权，他们就可以获得所有的 root 权限，并可以对 Docker 主机进行一些攻击，例如：\n将敏感信息从主机的文件系统复制到容器中 执行远程命令 为了防止这种情况，确保以非 root 用户运行容器进程。\nRUN addgroup --system app \u0026amp;\u0026amp; adduser --system --group app USER app 你可以更进一步，删除 shell 权限，确保没有主目录。\nRUN addgroup --gid 1001 --system app \u0026amp;\u0026amp; \\ adduser --no-create-home --shell /bin/false --disabled-password --uid 1001 --system --group app USER app 验证\ndocker run -i sample id uid=1001(app) gid=1001(app) groups=1001(app) 在这里，容器内的应用程序在一个非 root 用户下运行。然而，请记住，Docker 守护进程和容器本身仍然是以 root 权限运行的。\n请务必查看以非根用户身份运行 Docker 守护进程，以获得以非根用户身份运行守护进程和容器的帮助。\n6. 优先选择 COPY 而不是 ADD # 除非你确定你需要 ADD 所带来的额外功能，否则请使用 COPY。\n那么 COPY 和 ADD 的区别是什么？\n首先，这两个命令都允许你从一个特定的位置复制文件到 Docker 镜像中。\nADD \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; COPY \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; 虽然它们看起来作用相同，但 ADD 有一些额外的功能。\nCOPY 用于将本地文件或目录从 Docker 主机复制到镜像上。 ADD 可以用于同样的事情，也可以用于下载外部文件。另外，如果你使用压缩文件（tar、gzip、bzip2等）作为 参数，ADD 会自动将内容解压到指定位置。 # 将主机上的本地文件复制到目的地 COPY /source/path /destination/path ADD /source/path /destination/path # 下载外部文件并复制到目的地 ADD http://external.file/url /destination/path # 复制和提取本地压缩文件 ADD source.file.tar.gz /destination/path 最后 COPY 在语义上比 ADD 更加明确和更容易理解。\n7. 缓存安装包到 Docker 主机上 # 当一个需求文件被改变时，镜像需要被重建以安装新的包。先前的步骤将被缓存，正如在最小化层数中提到的。在重建镜像时下载所有的包会导致大量的网络活动，并需要大量的时间。每次重建都要占用同等的时间来下载不同构建中的通用包。\n以 Python 为例，你可以通过将 pip 缓存目录映射到主机上的一个目录来避免这种情况。所以对于每次重建，缓存的版本会持续存在，这可以提高构建速度。\n在 Docker 运行中添加一个卷，作为 -v $HOME/.cache/pip-docker/:/root/.cache/pip 或者作为 Docker Compose 文件中的映射。\n上面介绍的目录只供参考，要确保你映射的是 cache 目录，而不是 site-packages（内置包所在的位置）。\n将缓存从 docker 镜像中移到主机上可以为你节省最终镜像的空间。\n# 忽略 ... COPY requirements.txt . RUN --mount=type=cache,target=/root/.cache/pip \\ pip install -r requirements.txt # 忽略 ... 8. 每个容器只运行一个进程 # 为什么建议每个容器只运行一个进程？\n让我们假设你的应用程序栈由两个 Web 服务器和一个数据库组成。虽然你可以很容易地从一个容器中运行所有三个，但你应该在一个单独的容器中运行每个服务，以便更容易重复使用和扩展每个单独的服务。\n扩展性 - 由于每个服务都在一个单独的容器中，你可以根据需要水平地扩展你的一个网络服务器来处理更多的流量。 可重用性 - 也许你有另一个服务需要一个容器化的数据库，你可以简单地重复使用同一个数据库容器，而不需要带着两个不必要的服务。 日志 - 耦合容器会让日志变得更加复杂。（我们将在本文后面进一步详细讨论这个问题） 可移植性和可预测性 - 当容器有较少的部分在工作时，制作安全补丁或调试问题就会容易得多。 9. 优先选择数组而不是字符串语法 # 你可以在你的 Dockerfiles 中以数组（exec）或字符串（shell）格式\n在 Dockerfile 中，你可以以数组（exec）或字符串（shell）格式来使用 CMD 和 ENTRYPOINT 命令\n# 数组（exec） CMD [\u0026#34;gunicorn\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;-k\u0026#34;, \u0026#34;uvicorn.workers.UvicornWorker\u0026#34;, \u0026#34;main:app\u0026#34;] # 字符串（shell） CMD \u0026#34;gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app\u0026#34; 两者都是正确的，并且实现了几乎相同的事情；但是，你应该尽可能地使用 exec 格式。\n以下来自 Docker的官方文档内容：\n确保你在 Dockerfile 中使用 CMD 和 ENTRYPOINT 的 exec 形式。 例如，使用 [\u0026quot;program\u0026quot;, \u0026quot;arg1\u0026quot;, \u0026quot;arg2\u0026quot;] 而不是 \u0026quot;program arg1 arg2\u0026quot;。使用字符串形式会导致 Docker 使用 bash 运行你的进程，而 bash 并不能正确处理信号。Compose 总是使用 JSON 形式，所以不用担心如果你在你的 Compose 文件中覆盖了命令或入口。 因此，由于大多数 shell 不处理对子进程的信号，如果你使用 shell 格式，CTRL-C（产生 SIGTERM）可能不会停止一个子进程。\n例子:\nFROM ubuntu:18.04 # BAD: 字符串（shell）格式 ENTRYPOINT top -d # GOOD: 数组（exec）格式 ENTRYPOINT [\u0026#34;top\u0026#34;, \u0026#34;-d\u0026#34;] 这两种情况执行效果一样。但请注意，在字符串（shell）格式的情况下，CTRL-C 不会杀死这个进程。相反，你会看到 ^C^C^C^C^C^C^C^C^C^C。\n另一个注意事项是，字符串（shell）格式携带的是 shell 的 PID，而不是进程本身。\n# 数组格式 root@18d8fd3fd4d2:/app# ps ax PID TTY STAT TIME COMMAND 1 ? Ss 0:00 python manage.py runserver 0.0.0.0:8000 7 ? Sl 0:02 /usr/local/bin/python manage.py runserver 0.0.0.0:8000 25 pts/0 Ss 0:00 bash 356 pts/0 R+ 0:00 ps ax # 字符串格式 root@ede24a5ef536:/app# ps ax PID TTY STAT TIME COMMAND 1 ? Ss 0:00 /bin/sh -c python manage.py runserver 0.0.0.0:8000 8 ? S 0:00 python manage.py runserver 0.0.0.0:8000 9 ? Sl 0:01 /usr/local/bin/python manage.py runserver 0.0.0.0:8000 13 pts/0 Ss 0:00 bash 342 pts/0 R+ 0:00 ps ax 10. 了解 ENTRYPOINT 和 CMD 之间的区别 # 我应该使用 ENTRYPOINT 还是 CMD 来运行容器进程？有两种方法可以在容器中运行命令。\nCMD [\u0026#34;gunicorn\u0026#34;, \u0026#34;config.wsgi\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:8000\u0026#34;] # 和 ENTRYPOINT [\u0026#34;gunicorn\u0026#34;, \u0026#34;config.wsgi\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:8000\u0026#34;] 两者本质上做的是同一件事：用 Gunicorn 服务器在 config.wsgi 启动应用程序，并将其绑定到 0.0.0.0:8000。\nCMD 很容易被重写。如果你运行 docker run \u0026lt;image_name\u0026gt; uvicorn config.asgi，上述 CMD 就会被新的参数所取代。\n例如，uvicorn config.asgi。而要覆盖 ENTRYPOINT 命令，必须指定 --entrypoint 选项。\ndocker run --entrypoint uvicorn config.asgi \u0026lt;image_name\u0026gt; 在这里，很明显，我们正在覆盖入口点。所以，建议使用 ENTRYPOINT 而不是 CMD，以防止意外地覆盖命令。\n它们也可以一起使用。比如说\nENTRYPOINT [\u0026#34;gunicorn\u0026#34;, \u0026#34;config.wsgi\u0026#34;, \u0026#34;-w\u0026#34;] CMD [\u0026#34;4\u0026#34;] 当像这样一起使用时，为启动容器所运行的命令就变成了：\ngunicorn config.wsgi -w 4 如上所述，CMD 很容易被重写。因此，CMD 可以被用来向 ENTRYPOINT 命令传递参数。比如很容易更改 workers 的数量，就像这样：\ndocker run \u0026lt;image_name\u0026gt; 6 这样就将有 6 个 Gunicorn workers 启动容器，而不是默认的 4 个。\n11. 添加健康检查 HEALTHCHECK # 使用 HEALTHCHECK 来确定容器中运行的进程是否不仅已启动并正在运行，而且是“健康”的。\nDocker 公开了一个 API 来检查容器中运行的进程的状态，它提供的信息不仅仅是进程是否“正在运行”，因为“运行”涵盖了“它正在运行”、“仍在启动”、甚至“陷入某种无限循环错误状态”。你可以通过 HEALTHCHECK 指令与此 API 交互。\n例如，如果你正在提供 Web 应用程序，则可以使用以下内容来确定 / 端点是否已启动并可以处理服务请求：\nHEALTHCHECK CMD curl --fail http://localhost:8000 || exit 1 如果你运行 docker ps，你可以看到 HEALTHCHECK 的状态。\n健康的示例\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 09c2eb4970d4 healthcheck \u0026#34;python manage.py ru…\u0026#34; 10 seconds ago Up 8 seconds (health: starting) 0.0.0.0:8000-\u0026gt;8000/tcp, :::8000-\u0026gt;8000/tcp xenodochial_clarke 不健康的示例\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 09c2eb4970d4 healthcheck \u0026#34;python manage.py ru…\u0026#34; About a minute ago Up About a minute (unhealthy) 0.0.0.0:8000-\u0026gt;8000/tcp, :::8000-\u0026gt;8000/tcp xenodochial_clarke 你可以更进一步，设置一个仅用于健康检查的自定义端点，然后配置 HEALTHCHECK 以针对返回的数据进行测试。\n例如，如果端点返回 {\u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot;} 的 JSON 响应，你可以指示 HEALTHCHECK 验证响应正文。\n以下是使用 docker inspect 查看运行状况检查状态的方法：\n这里省略了部分输出。\n❯ docker inspect --format \u0026#34;{{json .State.Health }}\u0026#34; ab94f2ac7889 { \u0026#34;Status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;FailingStreak\u0026#34;: 0, \u0026#34;Log\u0026#34;: [ { \u0026#34;Start\u0026#34;: \u0026#34;2021-09-28T15:22:57.5764644Z\u0026#34;, \u0026#34;End\u0026#34;: \u0026#34;2021-09-28T15:22:57.7825527Z\u0026#34;, \u0026#34;ExitCode\u0026#34;: 0, \u0026#34;Output\u0026#34;: \u0026#34;...\u0026#34; 你还可以向 Docker Compose 文件添加运行状况检查：\nversion: \u0026#34;3.8\u0026#34; services: web: build: . ports: - \u0026#39;8000:8000\u0026#39; healthcheck: test: curl --fail http://localhost:8000 || exit 1 interval: 10s timeout: 10s start_period: 10s retries: 3 选项：\ntest：要测试的命令。 interval：要测试的间隔 - 即，测试每 x 时间单位。 timeout：等待响应的最长时间。 start_period：何时开始健康检查。它可以在容器准备就绪之前执行其他任务时使用，例如运行迁移。 retries：在将测试指定为失败之前的最大重试次数。 如果你使用的是 Docker Swarm 以外的编排工具（比如 Kubernetes 或 AWS ECS），它们很可能有自己的内部系统来处理健康检查。在添加 HEALTHCHECK 指令之前，请参阅特定工具的文档。\nDocker 镜像最佳实践 # 1. Docker 镜像版本 # 只要有可能，就要避免使用 latest 标签的镜像。\n如果你依赖 latest 标签（这并不是一个真正的 \u0026ldquo;标签\u0026rdquo;，因为当镜像没有明确的标签时，它是默认应用的），你无法根据镜像标签来判断你的代码正在运行哪个版本。\n如果你想回滚就变得很困难，并且很容易被覆盖（无论是意外还是恶意的）。标签，就像你的基础设施和部署，应该是不可改变的。\n所以无论你如何对待你的内部镜像，都不应该对基本镜像使用 latest 标签，因为你可能会无意中把一个带有破坏性变化的新版本部署到生产中。\n对于内部镜像，应使用描述性的标签，以便更容易分辨哪个版本的代码正在运行，处理回滚，并避免命名冲突。例如，你可以使用以下描述符来组成一个标签。\n时间戳 Docker 镜像 ID Git 提交哈希值 语义版本 (Semantic version) 关于更多的选择，也可以参考 Stack Overflow 问题 \u0026ldquo;Properly Versioning Docker Images\u0026rdquo; 中的这个答案。\n比如说\ndocker build -t web-prod-b25a262-1.0.0 . 在这里，我们用下面的内容来形成标签\n项目名称：web 环境名称: prod Git commit short hash: b25a262 (通过命令 git rev-parse --short HEAD 来获得) 语义学版本：1.0.0 选择一个标签方案并与之保持一致是至关重要的。由于提交哈希值（commit hashes）可以很容易地将镜像标签与代码联系起来，建议将它们纳入你的标签方案。\n2. 不要在镜像中存储机密信息 # Secrets 是敏感的信息，如密码、数据库凭证、SSH密钥、令牌和 TLS 证书等。这些信息不应该在没有加密的情况下被放入你的镜像中，因为未经授权的用户如果获得了镜像的访问权，只需要检查这些层就可以提取密钥。\n因此不要在 Docker 文件中添加明文的密钥，尤其是当你把镜像推送到像 Docker Hub 这样的公共仓库！！\nFROM python:3.9-slim ENV DATABASE_PASSWORD \u0026#34;SuperSecretSauce\u0026#34; 相反，它们应该通过以下方式注入\n环境变量（在运行时) 构建时参数（在构建时) 协调工具，如 Docker Swarm（通过 Docker secrets）或 Kubernetes（通过 Kubernetes secrets）。 此外，你还可以通过在你的 .dockerignore 文件中添加常见的密钥文件和文件夹来帮助防止密钥的泄露。\n**/.env **/.aws **/.ssh 最后，要明确哪些文件会被复制到镜像中，而不是递归地复制所有文件。\n# 不好的做法 COPY . . # 好的做法 COPY ./app.py . 明确的做法也有助于限制缓存的破坏。\n环境变量 # 你可以通过环境变量来传递密钥，但它们会在所有子进程、链接的容器和日志以及 docker inspect 中可见。要更新它们也很困难。\ndocker run --detach --env \u0026#34;DATABASE_PASSWORD=SuperSecretSauce\u0026#34; python：3.9-slim b25a262f870eb0fdbf03c666e7fcf18f9664314b79ad58bc7618ea3445e39239 docker inspect --format=\u0026#39;{{range .Config.Env}}{{println .}}{{end}}\u0026#39; b25a262f870eb0fdbf03c666e7fcf18f9664314b79ad58bc7618ea3445e39239 DATABASE_PASSWORD=SuperSecretSauce PATH=/usr/local/bin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin LANG=C.UTF-8 GPG_KEY=E3FF2839C048B25C084DEBE9B26995E310250568 python_version=3.9.7 python_pip_version=21.2.4 python_setuptools_version=57.5.0 python_get_pip_url=https://github.com/pypa/get-pip/raw/c20b0cfd643cd4a19246ccf204e2997af70f6b21/public/get-pip.py PYTHON_GET_PIP_SHA256=fa6f3fb93cce234cd4e8dd2beb54a51ab9c247653b52855a48dd44e6b21ff28b 这是最直接的密钥管理方法。虽然它不是最安全的，但它会让诚实的人保持诚实，因为它提供了一个薄薄的保护层，有助于使密钥不被好奇的游荡的眼睛发现。\n使用共享卷传递密钥是一个更好的解决方案，但它们应该被加密，通过 Vault 或 AWS密钥管理服务（KMS），因为它们被保存到磁盘。\n构建时参数 # 你可以在构建时使用构建时参数来传递密钥，但这些密钥对于那些可以通过 docker 历史访问镜像的人来说是可见的。\n例子\nFROM python:3.9-slim ARG DATABASE_PASSWORD 构建\ndocker build --build-arg \u0026#34;DATABASE_PASSWORD=SuperSecretSauce\u0026#34; . 如果你只需要临时使用密钥作为构建的一部分。例如，用于克隆私有 repo 或下载私有软件包的 SSH 密钥。你应该使用多阶段构建，因为构建者的历史会被临时阶段忽略。\n# 临时阶段 FROM python:3.9-slim as builder # 密钥参数 arg ssh_private_key # 安装 git RUN apt-get update \u0026amp;\u0026amp; （运行 apt-get update）。 apt-get install -y --no-install-recommends git # 使用 ssh 密钥来克隆 repo RUN mkdir -p /root/.ssh/ \u0026amp;\u0026amp; \\\\ echo \u0026#34;${PRIVATE_SSH_KEY}\u0026#34; \u0026gt; /root/.ssh/id_rsa RUN touch /root/.ssh/known_hosts \u0026amp; \u0026amp; ssh-keyscan bitbucket.org \u0026gt;\u0026gt; /root/.ssh/known_hosts RUN git clone git@github.com:testdrivenio/not-real.git # 最后阶段 FROM python:3.9-slim 工作目录 /app # 从临时镜像中复制版本库 COPY --from=builder /your-repo /app/your-repo 多阶段构建只保留了最终镜像的历史。你可以把这个功能用于你的应用程序需要的永久密钥，比如数据库凭证。\n你也可以使用 docker build 中新的 --secret 选项来向 Docker 镜像传递密钥，这些密钥不会被存储在镜像中。\n# \u0026#34;docker_is_awesome\u0026#34; \u0026gt; secrets.txt FROM alpine # 从默认的密钥位置显示密钥。 RUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret 这将装载 secrets.txt 文件中的密钥。\n构建镜像\ndocker build --no-cache --progress=plain --secret id=mysecret,src=secrets.txt . # 输出 ... #4 [1/2] FROM docker.io/library/alpine #4 sha256:665ba8b2cdc0cb0200e2a42a6b3c0f8f684089f4cd1b81494fbb9805879120f7 #4 缓存的 #5 [2/2] RUN --mount=type=secret,id=mysecret cat /run/secrets/myecret #5 sha256:75601a522ebe80ada66dedd9dd86772ca932d30d7e1b11bba94c04aa55c237de #5 0.635 docker_is_awesome#5 DONE 0.7s #6 导出到镜像 最后，检查历史记录，看看密钥是否泄露了。\n❯ docker history 49574a19241c IMAGE CREATED CREATED BY SIZE COMMENT 49574a19241c 5 minutes ago CMD [\u0026#34;/bin/sh\u0026#34;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 5 minutes ago RUN /bin/sh -c cat /run/secrets/mysecret # b… 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 4 weeks ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/sh\u0026#34;] 0B \u0026lt;missing\u0026gt; 4 weeks ago /bin/sh -c #(nop) ADD file:aad4290d27580cc1a… 5.6MB Docker 密钥 # 如果你正在使用 Docker Swarm，你可以用 Docker secrets 来管理密钥。\n例如，启动 Docker Swarm 模式。\ndocker swarm init 创建一个 docker 密钥。\necho \u0026#34;supersecretpassword\u0026#34; | docker secret create postgres_password - qdqmbpizeef0lfhyttxqfbty0 docker secret ls ID NAME DRIVER CREATED UPDATED qdqmbpizeef0lfhyttxqfbty0 postgres_password 4 seconds ago 4 seconds ago 当一个容器被赋予上述密钥的访问权时，它将挂载在 /run/secrets/postgres_password。这个文件将包含明文的密钥的实际值。\n使用其他的编排工具？\n使用 AWS Secrets Manager 的密钥与 Kubernetes 的密钥 DigitalOcean Kubernetes - 保护 DigitalOcean Kubernetes 集群的推荐步骤 Google Kubernetes引擎 - 与其他产品一起使用密钥管理器 Nomad - Vault 集成和检索动态密钥 3. 使用 .dockerignore 文件 # 之前已经提到过几次使用 .dockerignore 文件。这个文件用来指定你不希望被添加到发送给 Docker 守护进程的初始构建上下文中的文件和文件夹，后者将构建你的镜像。换句话说，你可以用它来定义你需要的构建环境。\n当一个 Docker 镜像被构建时，整个 Docker 上下文 - 即你的项目的根在 COPY 或 ADD 命令执行之前就被发送给了 Docker 守护进程。\n这可能是相当费资源，尤其是当你的项目中有许多依赖关系、大量的数据文件或构建工件时。\n另外，当 Docker CLI 和守护程序不在同一台机器上。比如守护进程是在远程机器上执行的，你就更应该注意构建环境的大小了。\n你应该在 .dockerignore 文件中添加什么？\n临时文件和文件夹 构建日志 本地 secrets 本地开发文件，如 docker-compose.yml 版本控制文件夹，如 \u0026ldquo;.git\u0026rdquo;、\u0026quot;.hg\u0026quot; 和 \u0026ldquo;.vscode\u0026rdquo; 等 例子：\n**/.git **/.gitignore **/.vscode **/coverage **/.env **/.aws **/.ssh Dockerfile README.md docker-compose.yml **/.DS_Store **/venv **/env 总之，结构合理的 .dockerignore 可以帮助\n减少 Docker 镜像的大小 加快构建过程 防止不必要的缓存失效 防止泄密 4. 检查并扫描你的 Dockerfile 和镜像 # Linting 是检查源代码中是否存在可能导致潜在缺陷的编程和风格错误以及不良做法的过程。就像编程语言一样，静态文件也可以被 lint。特别是对于你的 Dockerfile，linter 可以帮助确保它们的可维护性、避免弃用语法并遵守最佳实践。整理镜像应该是 CI 管道的标准部分。\nHadolint 是最流行的 Dockerfile linter：\nhadolint Dockerfile Dockerfile:1 DL3006 warning: Always tag the version of an image explicitly Dockerfile:7 DL3042 warning: Avoid the use of cache directory with pip. Use `pip install --no-cache-dir \u0026lt;package\u0026gt;` Dockerfile:9 DL3059 info: Multiple consecutive `RUN` instructions. Consider consolidation. Dockerfile:17 DL3025 warning: Use arguments JSON notation for CMD and ENTRYPOINT arguments 这是 Hadolint 一个在线的链接 https://hadolint.github.io/hadolint/ 也可以安装 VS Code 插件\n你可以将 Dockerfile 与扫描镜像和容器的漏洞结合使用。\n以下是一些有影响力的镜像扫描工具：\nSnyk 是 Docker 本地漏洞扫描的独家提供商。你可以使用 docker scan CLI 命令来扫描镜像。 Trivy 可用于扫描容器镜像、文件系统、git 存储库和其他配置文件。 Clair 是一个开源项目，用于对应用程序容器中的漏洞进行静态分析。 Anchore 是一个开源项目，为容器镜像的检查、分析和认证提供集中式服务。 总而言之，对你的 Dockerfile 和镜像进行 lint 和扫描，来发现任何偏离最佳实践的潜在问题。\n5. 签名和验证镜像 # 你怎么知道用于运行生产代码的镜像没有被篡改？\n篡改可以通过中间人（MITM）攻击或注册表被完全破坏来实现。Docker 内容信任（DCT）可以对来自远程注册中心的 Docker 镜像进行签名和验证。\n为了验证镜像的完整性和真实性，请设置以下环境变量。\nDOCKER_CONTENT_TRUST=1 现在，如果你试图拉一个没有被签名的镜像，你会收到以下错误。\nError: remote trust data does not exist for docker.io/namespace/unsigned-image: notary.docker.io does not have trust data for docker.io/namespace/unsigned-image 你可以从使用 Docker 内容信任签署镜像文档中了解签署镜像的情况。\n当从 Docker Hub下 载镜像时，确保使用官方镜像或来自可信来源的经过验证的镜像。较大的团队应该使用他们自己的内部私有容器仓库\n6. 设置内存和 CPU 的限制 # 限制 Docker 容器的内存使用是一个好主意，特别是当你在一台机器上运行多个容器时。这可以防止任何一个容器使用所有可用的内存，从而削弱其他容器的功能。\n限制内存使用的最简单方法是在 Docker cli 中使用 --memory 和 --cpu 选项。\ndocker run --cpus=2 -m 512m nginx 上述命令将容器的使用限制在 2 个 CPU 和 512 兆的内存。\n你可以在 Docker Compose 文件中做同样的事情，像这样。\nversion: \u0026#34;3.9\u0026#34; services: redis: image: redis:alpine deploy: resources: limits: cpus: 2 memory: 512M reservations: cpus: 1 memory: 256M 请注意 reservations 字段。它是用来设置软限制的，当主机的内存或CPU资源不足时，它就会优先考虑。\n其他相关资源\n带有内存、CPU和GPU的运行时选项：https://docs.docker.com/config/containers/resource_constraints/ Docker Compose 的资源限制：https://docs.docker.com/compose/compose-file/compose-file-v3/#resources 总结 # 以上就是本文介绍的 17 条最佳实践，掌握这些最佳实践一定会让你的 Dockerfile 和 Docker Image 变得精简，干净，和安全。\n本文出自 Docker Best Practices for Python Developers。\n欢迎关注微信公众号「DevOps攻城狮」- 专注于DevOps领域知识分享。\n","date":"2022-01-12","externalUrl":null,"permalink":"/posts/docker-best-practice/","section":"Posts","summary":"本文分享了在编写 Dockerfiles 和使用 Docker 时应遵循的一些最佳实践，包括多阶段构建、镜像优化、安全性等方面的建议。","title":"你一定要了解这 17 条 Docker 最佳实践！","type":"posts"},{"content":"工作十几年用过了不少显示器，从最初的 17寸，到后来的 23寸、27寸、32寸、再到现在的 34 寸，根据我自己的使用体验，来个主观推荐：\n第一名，一个34寸曲面显示器 第二名，一个27寸 + 一个23寸的双屏组合 第三名，一个32寸 + 一个23寸的双屏组合 第三名，两个 23 寸的双屏组合（并列第三名）\n以上这些屏幕推荐购买 2K 及以上的分辨率，1080p 的分辨率不推荐。\n下面我就按照时间轴说说我用过的那些显示器。\n我用过的显示器 # 双屏 23寸 1080p # 五年前公司配置的是两个 23 寸的 1080p 显示器，还有显示器支架，这在当时是非常好用的组合了，对工作效率的提升确实很有帮助。\n（真不知道那些不肯在显示器上花钱的公司是怎么想的，换个大屏显示器是花最少的钱提高程序员效率的最有效办法了）\n放在现在 1080p 的分辨率已经不推荐了，建议购买 2K 以及以上分辨率的显示屏。\n单屏 27寸 2K # 在家办公和学习苦于家里的办公桌大小有限，放置两个 23 寸显示器没那么多空间，因此我打算购买一个 27 寸显示器。 但分辨率是继续选择 1080p 还是 2K 呢？\n当时我看好的 Dell 一款 2K 分辨率的显示器，就是价格有点贵了（两千多），于是我就在网上买了另外一款 1080p 的 27 寸显示器。\n当我拿到这款 1080p 的 27 寸显示器，能明显的看到它的颗粒感，一旦注意到了就没法再忽略它了。最后退而求其次，花了不到 1500 选择了优派的一款 27 寸 2K 显示器。\n这一用就是三年，直到今年搬家了，我有了更大的办公桌，27 寸的 2K 显示器在大小上已经不太能满足我了，我就开始搜罗新的升级目标。\n双屏 27寸 2K # 我开始考虑是购买一个 34 寸显示器还是两个 27 寸 2K 显示器？\n最后处于成本的考虑，我打算在闲鱼上收一台同款的 27 寸 2K 显示器组成双屏。但因为货源确实很少，加上一本不出外地，迟迟没有买到，最后不得不考虑购买两个新的 27 寸 2K 显示器。我再次在购买的两块 27 寸的 2K 显示器，还在闲鱼上买的显示器支架全都到货了，就等快乐的组装了。\n可当我把两个显示器都摆在桌子上，我后悔了，比我想象中的要长，一米六的桌子都快盛不下它们了，另外可能是我支架没调好的原因，这两个显示器摆放在一起的时候中间有一条上宽下窄的缝 \u0026hellip; \u0026hellip;\n曲面单屏 34寸 2K # 没有尝试过真的不知道到底什么才是适合自己的。现在我终于知道了\u0026hellip; 目前最适合我的就是 34寸 显示曲面屏。\n为什么是曲面的呢？\n我目前公司的显示器组合是 32寸2K + 23寸1080p，这个32寸显示器就是直面屏幕的，在看屏幕边缘的内容时没有曲面屏幕来的舒服。\n回到34寸曲面屏，市场中种类繁多，如果不想踩坑（折腾），建议直接买小米34寸曲面显示器。双十一钱购买的好像不足 1900。\n最后 # 以上就是我用过的显示器的简单分享，比较主流但也比较有限。比如我没用过超过 2K 分辨率以上的显示器，也没用过更大尺寸和三屏以上的组合。\n最后还是希望这个分享对你选购显示器有一点点帮助。\n","date":"2021-12-21","externalUrl":null,"permalink":"/posts/choose-monitor/","section":"Posts","summary":"本文分享了个人在选择显示器时的经验和建议，包括不同尺寸、分辨率和屏幕组合的优缺点，以及如何根据工作需求选择最合适的显示器。","title":"2022年序员如何选择显示器？1080p还是2K? 单屏还是多屏？","type":"posts"},{"content":"","date":"2021-12-21","externalUrl":null,"permalink":"/tags/monitor/","section":"标签","summary":"","title":"Monitor","type":"tags"},{"content":"","date":"2021-12-07","externalUrl":null,"permalink":"/tags/badge/","section":"标签","summary":"","title":"Badge","type":"tags"},{"content":"","date":"2021-12-07","externalUrl":null,"permalink":"/tags/cd/","section":"标签","summary":"","title":"CD","type":"tags"},{"content":" 问题 # 在一个组织内，不同的团队之间可能会有不同的维度来评估 CI/CD 的成熟度。这使得对衡量每个团队的 CI/CD 的表现变得困难。\n如何快速评估哪些项目遵循最佳实践？如何更容易地构建高质量的安全软件？组织内需要建立一个由团队成员一起讨论出来的最佳实践来帮助团队建立明确的努力方向。\n如何评估 # 这里我参考了开源项目 CII 最佳实践徽章计划，这是 Linux 基金会 (LF) 发起的一个开源项目。它提供一系列自由/开源软件 (FLOSS) 项目的最佳实践方法。参照这些最佳实践标准的项目可以进行自认证, 以获得核心基础设施促进会(CII)徽章。要做到这点无需任何费用，你的项目可以使用 Web 应用（BadgeApp) 来证明是如何符合这些实践标准的以及其详细状况。\n这些最佳实践标准可以用来：\n鼓励项目遵循最佳实践。 帮助新的项目找到那些它们要遵循的最佳实践 帮助用户了解哪些项目遵循了最佳实践（这样用户可以更倾向于选择此类项目）。 最佳实践包含以下五个标准：基本，变更控制，报告，质量，安全，分析。\n更多关于标准的细分可以参考 CII 中文文档 或 CII 英文文档。\n已经很多知名的项目比如 Kubernetes, Node.js 等在使用这个最佳实践徽章计划\n如果你的项目在 GitHub 上或是你可以按照上述的徽章计划进行评估，就可以使用它来评估你项目的最佳实践，并可以在项目主页的 README 上显示徽章结果。\n定制最佳实践标准 # 如果上述项目不能满足你的评估要求，结合我的实践，制定了如下“最佳实践标准”并分配了相应的成熟度徽章，供参考。\n计算规则 # 每个最佳实践标准都有分数，通常一般的标准是10分，重要的标准是20分 带有🔰的最佳实践标准表示“一定要有” 带有👍的最佳实践标准表示“应当有” 每个项目的最佳实践标准分数之和落在的区间获得对应的徽章 徽章分数对照表 # 徽章 分数 描述 🚩WIP \u0026lt; 100 小于100分获得 🚩Work In Progress 徽章 ✔️PASSING = 100 等于100分获得 ✔️PASSING 徽章 🥈SILVER \u0026gt; 100 \u0026amp;\u0026amp; \u0026lt;= 150 大于100，小于等于150分获得🥈银牌徽章 🥇GOLD \u0026gt; 150 大于等于150分获得🥇金牌徽章 注：这个分数区间可调整。\n最佳实践标准和分数 # 类别 最佳实践标准 分数 描述 基本 🔰构建任何分支 20 Jenkins：支持任何分支构建 🔰构建任何PR 20 Jenkins：支持对任何 Pull Request 在 Merge 之前进行构建 🔰上传制品 10 Jenkins：构建产物上传到制品仓库保存 👍容器化构建 10 推荐使用容器化技术实现Pipeline 质量 🔰自动化测试 20 Jenkins：支持触发冒烟/单元/回归测试 👍性能测试 10 Jenkins：支持触发性能测试 👍代码覆盖率收集 10 Jenkins：支持获得代码覆盖率 安全 🔰漏洞扫描 10 Jenkins：支持触发漏洞扫描 🔰License扫描 10 Jenkins：支持触发证书扫描 分析 👍Code Lint 10 Jenkins：支持对PR进行代码格式检查 👍静态代码分析 10 Jenkins：支持对PR进行静态代码分析 👍动态代码分析 10 Jenkins：支持对PR进行动态代码分析 报告 🔰Email或Slack通知 10 支持通过Email或Slack等方式通知 注：以Jenkins为例。\n最终的结果 # No Repository Name 实现的最佳实践标准 徽章 1 project-a 🔰构建任何分支🔰构建任何PR🔰上传制品🔰自动化测试🔰Email或Slack通知 🚩WIP 2 project-b 🔰构建任何分支🔰构建任何PR🔰上传制品🔰自动化测试🔰漏洞扫描🔰License扫描🔰Email或Slack通知 ✔️PASSING 3 project-c 🔰构建任何分支🔰构建任何PR🔰上传制品👍容器化构建🔰自动化测试🔰漏洞扫描🔰License扫描🔰Email或Slack通知 🥈SILVER 4 project-d 🔰构建任何分支🔰构建任何PR🔰上传制品👍容器化构建🔰自动化测试👍性能测试👍代码覆盖率收集🔰漏洞扫描🔰License扫描👍Code Lint👍静态代码分析👍动态代码分析🔰Email或Slack通知 🥇GOLD Q\u0026amp;A # Q: 为什么使用徽章而不是分数？\nA: 使用徽章能更好的帮助团队朝着目标而不是分数努力。\nQ: 建立最佳实践标准还有哪些帮助？\nA: 团队之间容易进行技术共享，更容易地构建高质量的安全软件，保持团队之间在统一的高水准。\n","date":"2021-12-07","externalUrl":null,"permalink":"/posts/cicd-assessment/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 的 generic-webhook-trigger 插件来实时获取 Bitbucket 仓库的事件信息，如 Pull Request ID 等。","title":"组织内如何评估 CI/CD 成熟度","type":"posts"},{"content":"","date":"2021-11-09","externalUrl":null,"permalink":"/tags/actions/","section":"标签","summary":"","title":"Actions","type":"tags"},{"content":"最近实现了一个很有意思的 Workflow，就是通过 GitHub Actions 自动将每次最新发布的文章自动同步到我的 GitHub 首页。\n就像这样在首页显示最近发布的博客文章。\n要实现这样的工作流需要了解以下这几点：\n需要创建一个与 GitHub 同名的个人仓库，这个仓库的 README.md 信息会显示在首页 通过 GitHub Actions 自动获取博客的最新文章并更新 README.md 只有当有新的文章发布的时候才触发自动获取、更新文章 GitHub Action GitHub 同名的个人仓库是一个特殊仓库，即创建一个与你的 GitHub 账号同名的仓库，添加的 README.md 会在 GitHub 个人主页显示。\n举个例子：如果你的 GitHub 名叫 GeBiLaoWang，那么当你创建一个叫 GeBiLaoWang 的 Git 仓库，添加 README.md 后就会在主页显示。\n针对这个功能 GitHub 上有很多丰富多彩的个人介绍（如下）。更多灵感可以参看这个链接：https://awesomegithubprofile.tech/\n自动获取文章并更新 README.md # 在 GitHub 上有很多开发者为 GitHub Actions 开发新的小功能。我这里用到一个开源项目叫 blog-post-workflow，它可以通过 RSS（订阅源）来获取到博客的最新文章。\n它不但支持 RSS 还支持获取 StackOverflow 以及 Youtube 视频等资源。\n我只需要在 GitHub 同名的仓库下添加一个这样的 Workflow YML .github/workflows/blog-post-workflow.yml 即可。\nname: Latest blog post workflow on: schedule: - cron: \u0026#39;* 2 * * *\u0026#39; workflow_dispatch: jobs: update-readme-with-blog: name: Update this repo\u0026#39;s README with latest blog posts runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: gautamkrishnar/blog-post-workflow@master with: # 我的博客 RSS 链接 feed_list: \u0026#34;https://shenxianpeng.github.io/atom.xml\u0026#34; # 获取最新 10 篇文章 max_post_count: 10 刚开始我需要让这个 Workflow 能工作即可。因此用的定时触发，即就是每天早上两点就自动获取一次最新文章并更新这个特殊仓库 README.md。\n这个做法还可以，但不够节省资源也不够完美。最好的做法是：只有当有新文章发布时才触发上面的 Workflow 更新 README.md。这就需要有一个 Webhook 当检测到有文章更新时自动触发这里的 Workflow。\n触发另一个 GitHub Action # GitHub Actions 提供了一个 Webhook 事件叫做 repository_dispatch 可以来做这件事。\n它的原理：使用 GitHub API 来触发一个 Webhook 事件，这个事件叫做 repository_dispatch，这个事件里的类型是可以自定义的，并且在要被触发的 workflow 里需要使用 repository_dispatch 事件。\n即：在存放博客文章的仓库里要有一个 Workflow 通过发送 repository_dispatch 事件触发特殊仓库中的 Workflow 来更新 README.md。\n这里我定义事件类型名叫 special_repository，它只接受来自 GitHub API repository_dispatch 事件。\n再次调整上面的 .github/workflows/blog-post-workflow.yml 文件如下：\n# special_repository.yml name: Latest blog post workflow on: repository_dispatch: # 这里的类型是可以自定义的，我将它起名为：special_repository types: [special_repository] workflow_dispatch: jobs: update-readme-with-blog: name: Update this repo\u0026#39;s README with latest blog posts runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: gautamkrishnar/blog-post-workflow@master with: feed_list: \u0026#34;https://shenxianpeng.github.io/atom.xml\u0026#34; max_post_count: 10 接受事件的 Workflow 修改好了。如何发送类型为 special_repository 的 repository_dispatch 事件呢？我这里通过 curl 直接调用 API 来完成。\ncurl -XPOST -u \u0026#34;${{ secrets.PAT_USERNAME}}:${{secrets.PAT_TOKEN}}\u0026#34; \\ -H \u0026#34;Accept: application/vnd.github.everest-preview+json\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; https://api.github.com/repos/shenxianpeng/shenxianpeng/dispatches \\ --data \u0026#39;{\u0026#34;event_type\u0026#34;: \u0026#34;special_repository\u0026#34;}\u0026#39; 最后，发送事件 Workflow YML .github/workflows/send-dispatch.yml 如下:\nname: Tigger special repository on: push: # 当 master 分支有变更的时候触发 workflow branches: - master workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: Send repository dispatch event run: | curl -XPOST -u \u0026#34;${{ secrets.PAT_USERNAME}}:${{secrets.PAT_TOKEN}}\u0026#34; \\ -H \u0026#34;Accept: application/vnd.github.everest-preview+json\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; https://api.github.com/repos/shenxianpeng/shenxianpeng/dispatches \\ --data \u0026#39;{\u0026#34;event_type\u0026#34;: \u0026#34;special_repository\u0026#34;}\u0026#39; 注：PAT_USERNAME 和 PAT_TOKEN 需要在当前的仓库【设置 -\u0026gt; Secrets】里进行添加，这里就不具体介绍了，需要可以自行搜索。\n以上就是通过 GitHub Actions 实现当博客有新发布的文章后自动更新 GitHub 首页的所有内容了。\n如果还有什么有意思的玩法欢迎评论区里分享一下吧。\n","date":"2021-11-09","externalUrl":null,"permalink":"/posts/github-special-repository/","section":"Posts","summary":"本文介绍了如何使用 GitHub Actions 自动将发布的博客文章更新到 GitHub 个人主页，提升个人主页的动态性和可读性。","title":"GitHub Actions 还能这么玩？自动将发布的博客文章更新到 GitHub 个人主页","type":"posts"},{"content":" Preface # The 2021-22 World Quality Report (WQR), a collaborative effort by Micro Focus, Capgemini, and Sogeti, is the only report analyzing software quality and testing trends globally.\nThis report surveyed 1750 executives and professionals. This includes everyone from top management to QA test managers and quality engineers, spanning 10 industries across 32 countries.\nThe World Quality Report (WQR) is a unique global study, and this year\u0026rsquo;s survey highlights the impact of evolving pandemic-influenced application demands in newly deployed methods, as well as the adoption of QA into Agile and DevOps practices, and the continued rise of AI.\nFollowing test-focused software quality reports like this helps us quickly understand the current state and trends in the software testing industry.\nFive Key Themes # A key message from the WQR: In the continuing COVID-19 pandemic, we\u0026rsquo;ve seen the convergence of digital transformation and the real-time adoption of Agile and DevOps practices. Furthermore, QA is emerging as a leader in adopting Agile and DevOps practices, providing teams with the tools and processes to foster quality throughout the Software Development Life Cycle (SDLC).\nThe WQR highlights five specific themes around key findings and trends:\nImpact of the COVID-19 pandemic on QA organizations and software testing Real-time convergence of digital transformation with DevOps and Agile adoption and the increasingly crucial role of QA Geographically dispersed teams focusing on business outcomes when deploying applications across environments Artificial Intelligence (AI) enhancing Agile and DevOps to cultivate a growing culture of quality accountability across all teams Using AI-driven continuous testing and quality management tools to address customer experience priorities and rapidly changing pandemic-influenced requirements Key Findings and Trends # 1. Impact of the COVID-19 Pandemic on QA Organizations and Software Testing # The COVID-19 pandemic had a direct and tangible impact on almost all aspects of business, including QA. However, many QA organizations were able to adapt to the realities of new hybrid work environments and transition to the new reality of working in distributed teams. This was likely possible because the trend towards hybrid distributed teams was already growing, and the pandemic simply accelerated this trend.\nCustomer Experience is King # The COVID-19 pandemic refocused attention on customers and their experiences. The top-rated aspects this year were:\nEnhanced customer experience, chosen by 63% of respondents Followed by enhanced security (62%) Then higher responsiveness to business needs (61%) And higher-quality software solutions (61%) From Guardian to Quality Champion # Testing and QA goals have also been reordered in the past year. Last year, guardians of business outcomes and quality were clear leaders, while this year, the support rate between these metrics has narrowed.\nQuality guardians, quality speed, and quality enablement within teams led at 62% Business assurance, digital wellbeing, and automation came in second at 61% QA teams are evolving from guardians of quality to champions of quality. QA teams are becoming vibrant leaders in organizational quality initiatives, empowering everyone on the team to achieve quality while contributing to business outcomes and growth.\n2. Real-time Convergence of Digital Transformation with DevOps and Agile Adoption and the Increasingly Crucial Role of QA # Driving Digital Transformation # This year, digital transformation initiatives aligned with pandemic requirements. Before the pandemic began, Agile and DevOps were a growing trend. During the pandemic, we began to see QA now playing a key role in organizations\u0026rsquo; adoption of Agile and DevOps, blurring the lines between development and testing, while creating a blended approach to quality.\nLet\u0026rsquo;s look at the drivers of digital transformation: Improved productivity and efficiency led at 47%; followed by improved service/product quality at 46%; then speed, better agility and flexibility; customer experience directly behind speed; closely followed by cost reduction and creating innovative opportunities; competitive differentiation was last.\nBecause competitive differentiation seems more like a side benefit of digital transformation, whereas digital transformation itself helps improve efficiency, quality, speed, and overall better customer experience.\nThe Growing Role of QA in DevOps and Agile Adoption – Guided by Business Priorities # This year, we saw a significant shift in business requirements, becoming more important than the demands of the technology stack. Compared to last year, the number of participants weighting the tech stack decreased by 16 percentage points, replaced by the largest increase in:\nBusiness priorities, now ranked number one, increased by 11 percentage points compared to last year Also significantly increased compared to last year is culture/agility, up 21 percentage points 3. Geographically Dispersed Teams Focusing on Business Outcomes When Deploying Applications Across Environments # Last year\u0026rsquo;s survey, conducted at the start of the global pandemic, showed signs of the transformations needed to meet business objectives, and the new requirements of remote work and digital transformation. With this year\u0026rsquo;s survey, we see that digital transformation continues, even while keeping pace with pandemic-influenced new work requirements. This accelerated company plans to migrate workloads to the cloud, partly due to planned digital transformation initiatives, and the rapid shift to remote work, fueling a need for increased security.\nDue to the pandemic-influenced workplace, the top-rated focus was on remote access to testing systems and test environments (using SaaS and cloud). Supporting this remote access were secondary factors based on remote, including better team collaboration tools. To support the quality of modern applications, the test environments themselves must also be modernized. This year, we saw:\nOrganizations increasingly satisfied with using cloud and containers to modernize test environments (highest satisfaction) Followed by improved booking and management of test environments (+16) Then providing visibility (+22) Lastly cost efficiency (+18) 4. Artificial Intelligence (AI) Enhancing Agile and DevOps to Cultivate a Growing Culture of Quality Accountability Across All Teams # Artificial intelligence continues to transform how test automation is built, and how testing is executed. We are seeing increasing confidence in the level of AI-based testing within organizations, with almost half of the respondents stating that they already possess the repository of test execution data needed for AI and ML, and that they are willing to act on the intelligence provided by their AI and ML platforms.\nThis year, we also asked respondents to predict their likelihood of leveraging a range of approaches to accelerate and optimize testing in Agile and DevOps environments. Compared to the same period last year:\nAutomated quality gates integrating testing into CI/CD pipelines (+5) Implementing intelligent and automated dashboards to achieve continuous quality monitoring saw the largest increase (+9) The newly added this year, using AI to optimize test cases ranked second overall, only behind shifting testing left. 5. Using AI-driven Continuous Testing and Quality Management Tools to Address Customer Experience Priorities and Rapidly Changing Pandemic-Influenced Requirements # This year, we asked respondents about the benefits of test automation:\nFirst, all items showed a year-over-year downward trend compared to last year, showcasing the challenges faced when working in hybrid and distributed teams. Such as better defect detection, shorter test cycles, lower overall security risks, better test coverage, lower testing costs, and control over the transparency of testing are all evident benefits. AI/ML is the fourth highest benefit, also proving its potential and value. Key Recommendations # QA Orchestration in Agile and DevOps # Focus on what matters most: customer experience and business objectives, meeting both with efficiency and speed. Simultaneously, adopt an engineering mindset within your teams, and embrace multi-skilling and upskilling. A new trend rapidly becoming the new normal is the SDET (Software Development Engineer in Test). Invest in insights, especially real-time insights across your entire QA and testing function. Focus on developing intelligent dashboards with real-time KPIs, from short-term tactical plans to long-term planning and strategic direction.\nIntelligent Test Automation # Standardize the use of test automation in QA by adopting an automation-first approach to software quality delivery. Expand automation across the E2E lifecycle, incorporating automation into all QA activities.\nArtificial Intelligence and Machine Learning # Drive the use of AI – don’t be driven by it. AI and ML promise exponential improvements, but use AI as a tool, not a replacement for the business decisions you’re making. For example, use AI to illuminate what to do and when to do it. It helps not just identify failures but also helps identify the reasons those failures are happening. Furthermore, focus AI on what matters most, identifying the most challenging areas of quality in software delivery. If you haven’t incorporated AI into quality yet, now is the best time to start.\nTest Environment Management (TEM) and Test Data Management (TDM) # Cloud adoption is continuing to show steady and consistent growth, but be mindful to ensure that the future doesn’t overshadow present needs. The key to successfully adopting cloud is to ensure integrity with legacy applications. Additionally, data analytics is now a key aspect of a test data management framework.\nSecurity and Smart Industries # Remote connectivity demands appropriate consideration of security and resilience for testing and QA organizations. Invest in innovation, in your labs, and in your teams. Whether you start with a POC to prove feasibility, securing management support is key to implementing change.\nSummary # The changes I learned from reading the entire WQR report:\nGuided by business priorities. Compared to last year, the weighting of the tech stack decreased by 16%, replaced by business priorities. Then culture, agility increased by 21% Highest satisfaction rating for modernizing test environments using cloud and containers Automated quality gates integrating testing into CI/CD pipelines (+5%) Implementing intelligent and automated dashboards to achieve continuous quality monitoring saw the largest increase (+9%) Using AI to optimize test cases ranked second overall, only behind shifting testing left Artificial intelligence continues to transform how automation is built and how testing is executed. Almost half of the respondents stated that they possess the repository of test execution data needed for AI/ML, and that they are willing to act on what AI/ML provides. Follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo; and reply with \u0026ldquo;WQR\u0026rdquo; to download the complete version of the \u0026ldquo;2021-22 World Quality Report (WQR)\u0026rdquo;.\n","date":"2021-11-06","externalUrl":null,"permalink":"/en/posts/world-quality-report/","section":"Posts","summary":"This article presents the key findings and trends from the 2021-22 World Quality Report (WQR), highlighting the impact of the COVID-19 pandemic on software quality and testing, and the crucial role of QA in Agile and DevOps.","title":"2021-22 World Quality Report (WQR)","type":"posts"},{"content":"","date":"2021-11-06","externalUrl":null,"permalink":"/en/tags/quality/","section":"Tags","summary":"","title":"Quality","type":"tags"},{"content":"","date":"2021-11-06","externalUrl":null,"permalink":"/en/tags/report/","section":"Tags","summary":"","title":"Report","type":"tags"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/en/tags/coverity/","section":"Tags","summary":"","title":"Coverity","type":"tags"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/en/tags/polaris/","section":"Tags","summary":"","title":"Polaris","type":"tags"},{"content":" This might be the most detailed article about Polaris available in Chinese.\nWhat is Polaris? # Polaris - A SaaS platform for managed Static Application Security Testing (SAST) tools. It\u0026rsquo;s a web site used to categorize and remediate vulnerabilities and run reports. SAST - A tool that analyzes source code or build artifacts to find security vulnerabilities. It\u0026rsquo;s a crucial step in ensuring security throughout the Software Development Life Cycle (SDLC). Coverity - Coverity is the original Static Application Security Testing (SAST) tool provided by Synopsys. Polaris is the SaaS version of Coverity. Synopsys - The company that develops Polaris and other software scanning tools, such as BlackDuck.\nWhich Languages Does Polaris Support? # C/C++ C# Java JavaScript TypeScript PHP Python Fortran Swift ...and more Polaris SaaS Platform # Usually, if your organization has introduced the Polaris SaaS service, you will have the following URL available: https://organization.polaris.synopsys.com\nAfter logging in, you can create projects for your Git repositories.\nRecommendation: Name the project the same as the Git repository.\nHow Does Polaris Perform Vulnerability Scanning? # Polaris Installation # Before performing a Polaris scan, you need to download and install polaris.\nIf your Polaris server URL is: POLARIS_SERVER_URL=https://organization.polaris.synopsys.com\nThe download link is: $POLARIS_SERVER_URL/api/tools/polaris_cli-linux64.zip\nThen unzip the downloaded polaris_cli-linux64.zip to your local machine and add its bin directory to your PATH.\nPolaris YAML Configuration File # Before scanning, you need to create a YAML file for your project. The default configuration file name is polaris.yml, located in the project root directory. If you want to specify a different configuration file name, you can use the -c option in the polaris command.\nRun polaris setup in the project root directory to generate a generic polaris.yml file.\nRun polaris configure to verify that your file is syntactically correct and that polaris has no problems.\nCapture - Capture # A YAML configuration file can contain three types of Capture:\nBuild - Run build commands and then analyze the results. Filesystem - For interpreted languages, provide the project type and a list of extensions to analyze. Buildless - For some languages that use dependency managers, such as Maven. Languages Build Options C, C++, ObjectiveC, Objective C++,Go, Scala, Swift Use Build capture PHP, Python, Ruby Use Buildless or Filesystem capture C#, Visual Basic. Use Build capture for more accurate results; use Buildless capture for simplicity Java Use Build capture for more accurate results; use Buildless capture for simplicity JavaScript,TypeScript Use Filesystem capture; use Buildless capture for simplicity Analyze - Analyze # If you are scanning C/C++ code, you should include this analysis section to make full use of Polaris\u0026rsquo;s scanning capabilities:\nanalyze: mode: central coverity: cov-analyze: [\u0026#34;--security\u0026#34;,\u0026#34;--concurrency\u0026#34;] Polaris YAML Example Files # Example 1: A C/C++ project\nversion: \u0026#34;1\u0026#34; project: name: test-cplus-demo branch: ${scm.git.branch} revision: name: ${scm.git.commit} date: ${scm.git.commit.date} capture: build: cleanCommands: - shell: [make, -f, GNUmakefile, clean] buildCommands: - shell: [make, -f, GNUmakefile] analyze: mode: central install: coverity: version: default serverUrl: https://organization.polaris.synopsys.com Example 2: A Java project\nversion: \u0026#34;1\u0026#34; project: name: test-java-demo branch: ${scm.git.branch} revision: name: ${scm.git.commit} date: ${scm.git.commit.date} capture: build: cleanCommands: - shell: [gradle, -b, build.gradle, --no-daemon, clean] buildCommands: - shell: [gradle, -b, build.gradle, --no-daemon, shadowJar] fileSystem: ears: extensions: [ear] files: - directory: ${project.projectDir} java: files: - directory: ${project.projectDir} javascript: files: - directory: client-vscode - excludeRegex: node_modules|bower_components|vendor python: files: - directory: ${project.projectDir} wars: extensions: [war] files: - directory: ${project.projectDir} analyze: mode: central install: coverity: version: default serverUrl: https://organization.polaris.synopsys.com Example 3: A CSharp project\nversion: \u0026#34;1\u0026#34; project: name: test-ssharp-demo branch: ${scm.git.branch} revision: name: ${scm.git.commit} date: ${scm.git.commit.date} capture: build: buildCommands: # If the build process is complex, you can write a script and then call it. - shell: [\u0026#39;script\\polaris.bat\u0026#39;] # Skip some files you don\u0026#39;t want to scan. skipFiles: - \u0026#34;*.java\u0026#34; - \u0026#34;*.text\u0026#34; - \u0026#34;*.js\u0026#34; analyze: mode: central install: coverity: version: default serverUrl: https://organization.polaris.synopsys.com For more details on writing polaris.yml, please refer to the official Polaris documentation: https://sig-docs.synopsys.com/polaris/topics/c_conf-overview.html\nExecuting Analysis # Use the following command to perform Polaris analysis:\npolaris -c polaris.yml analyze -w --coverity-ignore-capture-failure --coverity-ignore-capture-failure - Ignore Coverity capture failures. Run polaris help analyze to see more information about analysis commands.\nPolaris Analysis Results # If the Polaris analysis is successful, you will see a success message in the console as follows:\n[INFO] [1zb99xsu] Coverity job completed successfully! [INFO] [1zb99xsu] Coverity - analyze phase took 4m 36.526s. Analysis Completed. Coverity analysis { \u0026#34;JobId\u0026#34;: \u0026#34;mlkik4esb961p0dtq8i6m7pm14\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Success\u0026#34; } Job issue summary { \u0026#34;IssuesBySeverity\u0026#34;: { \u0026#34;Critical\u0026#34;: 0, \u0026#34;High\u0026#34;: 250, \u0026#34;Medium\u0026#34;: 359, \u0026#34;Low\u0026#34;: 81 }, \u0026#34;Total\u0026#34;: 690, \u0026#34;NewIssues\u0026#34;: 0, \u0026#34;ClosedIssues\u0026#34;: 0, \u0026#34;SummaryUrl\u0026#34;: \u0026#34;https://organization.polaris.synopsys.com/projects/bb079756-194e-4645-9121-5131493a0c93/branches/d567c376-4d5d-4941-8733-aa27bb2f5f5b\u0026#34; } This shows a total of 690 vulnerabilities found, and how many of each severity level. Specific vulnerability information needs to be viewed by logging into the Polaris SaaS platform.\nClicking the link in SummaryUrl will directly jump to the Polaris scan results for that project.\n","date":"2021-10-24","externalUrl":null,"permalink":"/en/posts/what-is-polaris/","section":"Posts","summary":"This article introduces the basic concepts of Polaris, the supported programming languages, how to use the SaaS platform, and how to configure and run Polaris for static code analysis. It also provides example YAML configuration files and how to view the analysis results.","title":"Polaris - Static Code Analysis","type":"posts"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/en/tags/security/","section":"Tags","summary":"","title":"Security","type":"tags"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/en/tags/static/","section":"Tags","summary":"","title":"Static","type":"tags"},{"content":"不管是对于 Git 的初学者还是经常使用 Git 的码农们，在日常工作中难免会有遇到有的命令一时想不起来。不妨将下面总结的一些 Git 常用命令及技巧收藏或打印出来，以备需要的时候可以很快找到。\ngit config # # 检查 git 配置 git config -l # 设置你的 git 提交 username 和 email # 例如：对于公司里项目 git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your_email@organization.com\u0026#34; # 例如：对于个人的 GitHub 项目 git config user.name \u0026#34;Your Name\u0026#34; git config user.email \u0026#34;your_emailn@gmail.com\u0026#34; # 使用 HTTP/HTTPS 避免每次输入密码 git config --global credential.helper cache git init # # 初始化一个仓库 git init git add # # 将文件添加到暂存区 git add file_name # 将所有文件添加到暂存区 git add . # 仅将某些文件添加到暂存区, 例如:仅添加所有以 \u0026#39;test*\u0026#39; 开头的文件 git add test* git status # # 检查仓库状态 git status git commit # # 提交更改 git commit # 提交带有消息的更改 git commit -m \u0026#34;This is a commit message\u0026#34; git log # # 查看提交历史 git log # 查看提交历史和显示相应的修改 git log -p # 显示提交历史统计 git log --stat # 显示特定的提交 git show commit_id # 以图形方式显示当前分支的提交信息 git log --graph --oneline # 以图形方式显示所有分支的提交信息 git log --graph --oneline --all # 获取远程仓库的当前提交日志 git log origin/master git diff # # 在使用 diff 提交之前所做的更改 git diff git diff some_file.js git diff --staged git rm # # 删除跟踪文件 git rm file_name git mv # # 重命名文件 git mv old_file_name new_file_name git checkout # # 切换分支 git checkout branch_name # 还原未暂存的更改 git checkout file_name git reset # # 还原暂存区的更改 git reset HEAD file_name git reset HEAD -p git commit --amend # # 修改最近的提交信息 git commit --amend # 修改最近的提交信息为：New commit message git commit --amend -m \u0026#34;New commit message\u0026#34; git revert # # 回滚最后一次提交 git revert HEAD # 回滚指定一次提交 git revert commit_id git branch # # 创建分支 git branch branch_name # 创建分支并切到该分支 git checkout -b branch_name # 显示当前分支 git branch # 显示所有分支 git branch -a # 检查当前正在跟踪的远程分支 git branch -r # 删除分支 git branch -d branch_name git merge # # 将 branch_name 合并到当分支 git merge branch_name # 中止合并 git merge --abort git pull # # 从远程仓库拉取更改 git pull git fetch # # 获取远程仓库更改 git fetch git push # # 推送更改到远程仓库 git push # 推送一个新分支到远程仓库 git push -u origin branch_name # 删除远程仓库分支 git push --delete origin branch_name git remote # # 添加远程仓库 git add remote https://repository_name.com # 查看远程仓库 git remote -v # 查看远程仓库的更多信息 git remote show origin Git技巧和窍门 # 清理已合并分支 # 清理已经合并的本地分支\ngit branch --merged master | grep -v \u0026#34;master\u0026#34; | xargs -n 1 git branch -d .gitignore # 指明 Git 应该忽略的故意不跟踪的文件的文件，比如 .gitignore 如下\n# 忽略 .vscode 目录 .vscode/ # 忽略 build 目录 build/ # 忽略文件 output.log .gitattributes # 关于 .gitattributes 请参考\nhttps://www.git-scm.com/docs/gitattributes https://docs.github.com/en/get-started/getting-started-with-git/configuring-git-to-handle-line-endings ","date":"2021-10-23","externalUrl":null,"permalink":"/posts/git-cheatsheet/","section":"Posts","summary":"本文总结了 Git 的常用命令和技巧，帮助开发者快速查找和使用 Git 命令，提高工作效率。","title":"Git 常用命令备忘录","type":"posts"},{"content":"","date":"2021-09-18","externalUrl":null,"permalink":"/en/tags/gradle/","section":"Tags","summary":"","title":"Gradle","type":"tags"},{"content":"After you have set up the SonarQube instance, you will need to integrate SonarQube with project.\nBecause I used the community edition version, it doesn\u0026rsquo;t support the C/C++ project, so I only demo how to integrate with Maven, Gradle, and Others.\nFor example, the demo project name and ID in SonarQube are both test-demo, and I build with Jenkins.\nBuild with Maven # Add the following to your pom.xml file:\n\u0026lt;properties\u0026gt; \u0026lt;sonar.projectKey\u0026gt;test-demo\u0026lt;/sonar.projectKey\u0026gt; \u0026lt;/properties\u0026gt; Add the following code to your Jenkinsfile:\nstage(\u0026#39;SonarQube Analysis\u0026#39;) { def mvn = tool \u0026#39;Default Maven\u0026#39;; withSonarQubeEnv() { sh \u0026#34;${mvn}/bin/mvn sonar:sonar\u0026#34; } } Build with Gradle # Add the following to your build.gradle file:\nplugins { id \u0026#34;org.sonarqube\u0026#34; version \u0026#34;3.3\u0026#34; } sonarqube { properties { property \u0026#34;sonar.projectKey\u0026#34;, \u0026#34;test-demo\u0026#34; } } Add the following code to your Jenkinsfile:\nstage(\u0026#39;SonarQube Analysis\u0026#39;) { withSonarQubeEnv() { sh \u0026#34;./gradlew sonarqube\u0026#34; } } Build with Other(for JS, TS, Python, \u0026hellip;) # Create a sonar-project.properties file in your repository and paste the following code:\nsonar.projectKey=test-demo Add the following code to your Jenkinsfile:\nstage(\u0026#39;SonarQube Analysis\u0026#39;) { def scannerHome = tool \u0026#39;SonarScanner\u0026#39;; withSonarQubeEnv() { sh \u0026#34;${scannerHome}/bin/sonar-scanner\u0026#34; } } More about how to integrate with SonarQube, please visit your SonarQube instance documentation: http://localhost:9000/documentation\n","date":"2021-09-18","externalUrl":null,"permalink":"/en/posts/sonarqube-integration/","section":"Posts","summary":"This article explains how to integrate SonarQube Community Edition with Maven, Gradle, and other projects, including the necessary configurations and Jenkins pipeline setup.","title":"How does SonarQube Community Edition integrate with the project","type":"posts"},{"content":"","date":"2021-09-18","externalUrl":null,"permalink":"/en/tags/sonarqube/","section":"Tags","summary":"","title":"SonarQube","type":"tags"},{"content":"","date":"2021-09-07","externalUrl":null,"permalink":"/en/tags/lcov/","section":"Tags","summary":"","title":"Lcov","type":"tags"},{"content":"","date":"2021-09-07","externalUrl":null,"permalink":"/en/tags/perl/","section":"Tags","summary":"","title":"Perl","type":"tags"},{"content":"When execute command: lcov --capture --directory . --no-external --output-file coverage.info to generate code coverage report, I encountered the following error:\n$ lcov --capture --directory . --no-external --output-file coverage.info Capturing coverage data from . Can\u0026#39;t locate JSON/PP.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/local/bin/geninfo line 63. BEGIN failed--compilation aborted at /usr/local/bin/geninfo line 63. sh-4.2$ perl -MCPAN -e \u0026#39;install JSON\u0026#39; Can\u0026#39;t locate CPAN.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .). BEGIN failed--compilation aborted. Can\u0026rsquo;t locate CPAN.pm # fixed this problem \u0026ldquo;Can\u0026rsquo;t locate CPAN.pm\u0026rdquo; by running the command yum install perl-CPAN\nsh-4.2$ sudo perl -MCPAN -e \u0026#39;install JSON\u0026#39; [sudo] password for sxp: Can\u0026#39;t locate CPAN.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .). BEGIN failed--compilation aborted. sh-4.2$ sudo yum install perl-CPAN Then run sudo perl -MCPAN -e 'install JSON' again, it works.\nCan\u0026rsquo;t locate JSON/PP.pm # fixed this problem by copying backportPP.pm to the PP.pm file.\n$ cd /usr/local/share/perl5/JSON $ ls backportPP backportPP.pm $ cp backportPP.pm PP.pm Can\u0026rsquo;t locate Module/Load.pm # bash-4.2$ geninfo --version Can\u0026#39;t locate Module/Load.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/local/bin/geninfo line 63. BEGIN failed--compilation aborted at /usr/local/bin/geninfo line 63. bash-4.2$ Install perl-Module-Load-Conditional can resolved.\nsudo yum install perl-Module-Load-Conditional Can\u0026rsquo;t locate Capture/Tiny.pm in @INC # sh-4.2$ lcov --version Can\u0026#39;t locate Capture/Tiny.pm in @INC (@INC contains: /usr/local/bin/../lib /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/local/bin/../lib/lcovutil.pm line 13. BEGIN failed--compilation aborted at /usr/local/bin/../lib/lcovutil.pm line 13. Compilation failed in require at /usr/local/bin/lcov line 104. BEGIN failed--compilation aborted at /usr/local/bin/lcov line 104. Fixed with following command\nperl -MCPAN -e \u0026#39;install Capture::Tiny\u0026#39; Then run lcov --version back to work.\nsh-4.2$ lcov --version lcov: LCOV version v1.16-16-g038c2ca Can\u0026rsquo;t locate DateTime.pm # $ genhtml --help Can\u0026#39;t locate DateTime.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/local/bin/genhtml line 87. BEGIN failed--compilation aborted at /usr/local/bin/genhtml line 87. Need to install the perl module DateTime, On Centos7 run\nsudo yum install 'perl(DateTime)'\nBut this still doesn\u0026rsquo;t work for me.\nRun geninfo command failed # Capturing coverage data from . Compress::Raw::Zlib version 2.201 required--this is only version 2.061 at /usr/local/share/perl5/IO/Uncompress/RawInflate.pm line 8. BEGIN failed--compilation aborted at /usr/local/share/perl5/IO/Uncompress/RawInflate.pm line 8. Compilation failed in require at /usr/local/share/perl5/IO/Uncompress/Gunzip.pm line 12. BEGIN failed--compilation aborted at /usr/local/share/perl5/IO/Uncompress/Gunzip.pm line 12. Compilation failed in require at /usr/local/bin/geninfo line 62. BEGIN failed--compilation aborted at /usr/local/bin/geninfo line 62. sh-4.2$ Install package Compress::Raw::Zlib fixed.\nperl -MCPAN -e \u0026#39;install Compress::Raw::Zlib\u0026#39; ","date":"2021-09-07","externalUrl":null,"permalink":"/en/posts/lcov-error/","section":"Posts","summary":"This article explains how to resolve the \u0026ldquo;Can\u0026rsquo;t locate JSON/PP.pm in @INC \u0026hellip;\u0026rdquo; error when running lcov, including installing missing Perl modules.","title":"Run lcov failed \"Can't locate JSON/PP.pm in @INC ...\"","type":"posts"},{"content":"","date":"2021-08-05","externalUrl":null,"permalink":"/en/tags/postgresql/","section":"Tags","summary":"","title":"PostgreSQL","type":"tags"},{"content":" Backgroud # In my opinion, SonarQube is not a very easy setup DevOps tool to compare with Jenkins, Artifactory. You can\u0026rsquo;t just run some script under the bin folder to let the server boot up.\nYou must have an installed database, configuration LDAP in the config file, etc.\nSo I\u0026rsquo;d like to document some important steps for myself, like setup LDAP or PostgreSQL when I install SonarQube of v9.0.1. It would be better if it can help others.\nPrerequisite and Download # Need to be installed JRE/JDK 11 on the running machine.\nHere is the prerequisites overview: https://docs.sonarqube.org/latest/requirements/requirements/\nDownload SonarQube: https://www.sonarqube.org/downloads/\ncd sonarqube/ ls wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-9.0.1.46107.zip unzip sonarqube-9.0.1.46107.zip cd sonarqube-9.0.1.46107/bin/linux-x86-64 sh sonar.sh console Change Java version # I installed SonarQube on CentOS 7 machine, the Java version is OpenJDK 1.8.0_242 by default, but the prerequisite shows at least need JDK 11. There is also JDK 11 available on my machine, so I just need to change the Java version.\nI recommend using the alternatives command change Java version，refer as following:\n$ java -version openjdk version \u0026#34;1.8.0_242\u0026#34; OpenJDK Runtime Environment (build 1.8.0_242-b08) OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode) $ alternatives --config java There are 3 programs which provide \u0026#39;java\u0026#39;. Selection Command ----------------------------------------------- 1 java-1.7.0-openjdk.x86_64 (/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.251-2.6.21.1.el7.x86_64/jre/bin/java) *+ 2 java-1.8.0-openjdk.x86_64 (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-1.el7.x86_64/jre/bin/java) 3 java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.12.0.7-0.el7_9.x86_64/bin/java) Enter to keep the current selection[+], or type selection number: 3 $ java -version openjdk version \u0026#34;11.0.12\u0026#34; 2021-07-20 LTS OpenJDK Runtime Environment 18.9 (build 11.0.12+7-LTS) OpenJDK 64-Bit Server VM 18.9 (build 11.0.12+7-LTS, mixed mode, sharing) Install Database # SonarQube needs you to have installed a database. It supports several database engines, like Microsoft SQL Server, Oracle, and PostgreSQL. Since PostgreSQL is open source, light, and easy to install, so I choose PostgreSQL as its database.\nHow to download and install PostgreSQL please see this page: https://www.postgresql.org/download/linux/redhat/\nTroubleshooting # 1. How to establish a connection with SonarQube and PostgreSQL # Please refer to the sonar.properties file at the end of this post.\n2. How to setup LDAP for users to log in # sonar.security.realm=LDAP ldap.url=ldap://den.exmaple-org:389 ldap.bindDn=user@exmaple-org.com ldap.bindPassword=mypassword ldap.authentication=simple ldap.user.baseDn=DC=exmaple-org,DC=com ldap.user.request=(\u0026amp;(objectClass=user)(sAMAccountName={login})) ldap.user.realNameAttribute=cn ldap.user.emailAttribute=email 3. How to fix LDAP login SonarQube is very slowly # Comment out ldap.followReferrals=false in sonar.properties file would be help.\nRelated post: https://community.sonarsource.com/t/ldap-login-takes-2-minutes-the-first-time/1573/7\n4. How to fix \u0026lsquo;Could not resolve 11 file paths in lcov.info\u0026rsquo; # I want to display Javascript code coverage result in SonarQube, so I added sonar.javascript.lcov.reportPaths=coverage/lcov.info to the sonar-project.properties\nBut when I run sonar-scanner.bat in the command line, the code coverage result can not show in sonar. I noticed the following error from the output:\nINFO: Analysing [C:\\workspace\\xvm-ide\\client\\coverage\\lcov.info] WARN: Could not resolve 11 file paths in [C:\\workspace\\xvm-ide\\client\\coverage\\lcov.info] There are some posts related to this problem, for example, https://github.com/kulshekhar/ts-jest/issues/542, but no one works in my case.\n# here is an example error path in lcov.info ..\\src\\auto-group\\groupView.ts Finally, I have to use the sed command to remove ..\\ in front of the paths before running sonar-scanner.bat, then the problem was solved.\nsed -i \u0026#39;s/\\..\\\\//g\u0026#39; lcov.info Please comment if you can solve the problem with changing options in the tsconfig.json file.\n4. How to output to more logs # To output more logs, change sonar.log.level=INFO to sonar.log.level=DEBUG in below.\nNote: all above changes of sonar.properties need to restart the SonarQube instance to take effect.\nFinal sonar.properties # For the sonar.properties file, please see below or link\n# DATABASE # # IMPORTANT: # - The embedded H2 database is used by default. It is recommended for tests but not for # production use. Supported databases are Oracle, PostgreSQL and Microsoft SQLServer. # - Changes to database connection URL (sonar.jdbc.url) can affect SonarSource licensed products. # User credentials. # Permissions to create tables, indices and triggers must be granted to JDBC user. # The schema must be created first. sonar.jdbc.username=sonarqube sonar.jdbc.password=mypassword #----- PostgreSQL 9.6 or greater # By default the schema named \u0026#34;public\u0026#34; is used. It can be overridden with the parameter \u0026#34;currentSchema\u0026#34;. sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube # Binding IP address. For servers with more than one IP address, this property specifies which # address will be used for listening on the specified ports. # By default, ports will be used on all IP addresses associated with the server. sonar.web.host=10.118.245.19 # Web context. When set, it must start with forward slash (for example /sonarqube). # The default value is root context (empty value). sonar.web.context= # TCP port for incoming HTTP connections. Default value is 9000. sonar.web.port=9000 # LDAP CONFIGURATION # Follow or not referrals. See http://docs.oracle.com/javase/jndi/tutorial/ldap/referral/jndi.html (default: true) ldap.followReferrals=false # Enable the LDAP feature sonar.security.realm=LDAP # Set to true when connecting to a LDAP server using a case-insensitive setup. # sonar.authenticator.downcase=true # URL of the LDAP server. Note that if you are using ldaps, then you should install the server certificate into the Java truststore. ldap.url=ldap://den.exmaple-org:389 # Bind DN is the username of an LDAP user to connect (or bind) with. Leave this blank for anonymous access to the LDAP directory (optional) ldap.bindDn=user@exmaple-org.com # Bind Password is the password of the user to connect with. Leave this blank for anonymous access to the LDAP directory (optional) ldap.bindPassword=mypassword # Possible values: simple | CRAM-MD5 | DIGEST-MD5 | GSSAPI See http://java.sun.com/products/jndi/tutorial/ldap/security/auth.html (default: simple) ldap.authentication=simple # USER MAPPING # Distinguished Name (DN) of the root node in LDAP from which to search for users (mandatory) ldap.user.baseDn=DC=exmaple-org,DC=com # LDAP user request. (default: (\u0026amp;(objectClass=inetOrgPerson)(uid={login})) ) ldap.user.request=(\u0026amp;(objectClass=user)(sAMAccountName={login})) # Attribute in LDAP defining the user’s real name. (default: cn) ldap.user.realNameAttribute=cn # Attribute in LDAP defining the user’s email. (default: mail) ldap.user.emailAttribute=email sonar.search.javaAdditionalOpts=-Dbootstrap.system_call_filter=false # Global level of logs (applies to all 4 processes). # Supported values are INFO (default), DEBUG and TRACE sonar.log.level=INFO # Paths to persistent data files (embedded database and search index) and temporary files. # Can be absolute or relative to installation directory. # Defaults are respectively \u0026lt;installation home\u0026gt;/data and \u0026lt;installation home\u0026gt;/temp sonar.path.data=/var/sonarqube/data sonar.path.temp=/var/sonarqube/temp ","date":"2021-08-05","externalUrl":null,"permalink":"/en/posts/sonarqube-setup/","section":"Posts","summary":"This article documents the steps to install SonarQube, configure LDAP, and set up PostgreSQL as the database. It also includes troubleshooting tips for common issues encountered during setup.","title":"SonarQube installation and troubleshootings","type":"posts"},{"content":"","date":"2021-07-27","externalUrl":null,"permalink":"/en/tags/coverage/","section":"Tags","summary":"","title":"Coverage","type":"tags"},{"content":"","date":"2021-07-27","externalUrl":null,"permalink":"/en/tags/gcov/","section":"Tags","summary":"","title":"Gcov","type":"tags"},{"content":" Problem # When we introduced Gocv to build my project for code coverage, I encountered the following error message:\nerror 1 # g++ -m64 -z muldefs -L/lib64 -L/usr/lib64 -lglib-2.0 -m64 -DUV_64PORT -DU2_64_BUILD -fPIC -g DU_starter.o NFA_msghandle.o NFA_svr_exit.o du_err_printf.o -L/workspace/code/myproject/src/home/x64debug/bin/ -L/workspace/code/myproject/src/home/x64debug/bin/lib/ -lundata -lutcallc_nfasvr -Wl,-rpath=/workspace/code/myproject/src/home/x64debug/bin/ -Wl,-rpath=/.dulibs28 -Wl,--enable-new-dtags -L/.dulibs28 -lodbc -lm -lncurses -lrt -lcrypt -lgdbm -ldl -lpam -lpthread -ldl -lglib-2.0 -lstdc++ -lnsl -lrt -lgcov -o /workspace/code/myproject/src/home/x64debug/objs/du/share/dutsvr /usr/bin/ld: /workspace/code/myproject/src/home/x64debug/objs/du/share/dutsvr: hidden symbol `__gcov_init\u0026#39; in /usr/lib/gcc/x86_64-redhat-linux/4.8.5/libgcov.a(_gcov.o) is referenced by DSO error 2 # It may also be such an error\n/home/p7539c/cutest/CuTest.c:379: undefined reference to `__gcov_init\u0026#39; CuTest.o:(.data+0x184): undefined reference to `__gcov_merge_add\u0026#39; Positioning problem # Let\u0026rsquo;s take the error 1.\nFrom the error message, I noticed -lundata -lutcallc_nfasvr are all the linked libraries (-llibrary)\nI checked libraries undata and utcallc_nfasvr one by one, and found it displayed U __gcov_init and U means undefined symbols.\nUse the find command to search the library and the nm command to list symbols in the library.\n-sh-4.2$ find -name *utcallc_nfasvr* ./bin/libutcallc_nfasvr.so ./objs/du/work/libutcallc_nfasvr.so -sh-4.2$ nm ./bin/libutcallc_nfasvr.so | grep __gcov_init U __gcov_init How to fix # In my case, I just added the following code LIB_1_LIBS := -lgcov to allow the utcallc_nfasvr library to call gcov.\nLIB_1 := utcallc_nfasvr # added below code to my makefile LIB_1_LIBS := -lgcov Rebuild, the error is gone, then checked library, it displayed t __gcov_init this time, it means symbol value exists not hidden.\n-sh-4.2$ nm ./bin/libutcallc_nfasvr.so | grep __gcov_init t __gcov_init Or in your case may build a shared library like so, similarly, just add the compile parameter -lgcov\ng++ -shared -o libMyLib.so src_a.o src_b.o src_c.o -lgcov Summary # I have encountered the following problems many times\nundefined reference to `__gcov_init\u0026#39; undefined reference to `__gcov_merge_add\u0026#39; `hidden symbol `__gcov_init\u0026#39; in /usr/lib/gcc/x86_64-redhat-linux/4.8.5/libgcov.a(_gcov.o) is referenced by DSO` Each time I can fix it by adding -glcov then recompile. the error has gone after rebuild. (you use the nm command to double-check whether the symbol has been added successfully.)\nHopes it can help you.\n","date":"2021-07-27","externalUrl":null,"permalink":"/en/posts/how-to-fix-gcov-hidden-symbol/","section":"Posts","summary":"This article explains how to resolve the \u0026ldquo;hidden symbol `__gcov_init\u0026rsquo; in ../libgcov.a(_gcov.o) is referenced by DSO\u0026rdquo; error when building a project with Gcov, including how to ensure symbols are not hidden.","title":"How to fix \"hidden symbol `__gcov_init' in ../libgcov.a(_gcov.o) is referenced by DSO\"","type":"posts"},{"content":" Backgorud # When you want to add build status to your Bitbucket the specific commit of a branch when you start a build from the branch\nWhen the build status is wrong, you want to update it manually. for example, update build status from FAILED to SUCCESSFUL\nYou can call Bitbucket REST API to do these.\nCode snippet # Below is the code snippet to update Bitbucket build status with REST API in the shell script.\nThe code on GitHub Gist: https://gist.github.com/shenxianpeng/bd5eddc5fb39e54110afb8e2e7a6c4fb\nClick Read More to view the code here.\n#!/bin/sh username=your-bitbucket-user password=your-bitbucket-password commit_id=\u0026#39;57587d7d4892bc4ef2c4375028c19b27921e2485\u0026#39; # build_result has 3 status: SUCCESSFUL, FAILED, INPROGRESS build_result=\u0026#39;SUCCESSFUL\u0026#39; description=\u0026#39;Manully update bitbucket status\u0026#39; build_name=\u0026#39;test #1\u0026#39; build_url=http://localhost:8080/job/test/ bitbucket_rest_api=\u0026#39;https://myorg.bitbucket.com/rest/build-status/latest/commits\u0026#39; gen_post_data() { cat \u0026lt;\u0026lt;EOF { \u0026#34;state\u0026#34;: \u0026#34;$build_result\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;$commit_id\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;$build_name\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;$build_url\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;$description\u0026#34; } EOF } echo \u0026#34;$(gen_post_data)\u0026#34; curl -u $username:$password \\ -H \u0026#34;Accept: application/json\u0026#34; \\ -H \u0026#34;Content-Type:application/json\u0026#34; \\ -X POST $bitbucket_rest_api/$commit_id --data \u0026#34;$(gen_post_data)\u0026#34; if [ $? -ne 0 ] then echo \u0026#34;$0: Update bitbucket build status failed.\u0026#34; exit 1 else echo \u0026#34;$0: Update bitbucket build status success.\u0026#34; exit 0 fi And the screenshot of the final update result\n","date":"2021-07-25","externalUrl":null,"permalink":"/en/posts/bitbucket-update-build-status/","section":"Posts","summary":"How to add or update the build status of a specific commit in Bitbucket using its REST API. It includes a shell script example for updating the build status and provides context on when to use this functionality.","title":"Add or update Bitbucket build status with REST API","type":"posts"},{"content":"","date":"2021-07-25","externalUrl":null,"permalink":"/en/tags/shell/","section":"Tags","summary":"","title":"Shell","type":"tags"},{"content":"This article briefly introduces: What is code coverage? Why measure code coverage? Code coverage metrics, working principles, mainstream code coverage tools, and why not to overestimate code coverage metrics.\nWhat is Code Coverage? # Code coverage is a measure of the amount of code executed during the entire testing process. It measures which statements in the source code have been executed during testing and which have not.\nWhy Measure Code Coverage? # As we all know, testing can improve the quality and predictability of software releases. But do you know how effective your unit tests or even your functional tests are in actually testing the code? Are more tests needed?\nThese are the questions that code coverage can attempt to answer. In short, we need to measure code coverage for the following reasons:\nUnderstand the effectiveness of our test cases on the source code Understand whether we have done enough testing Maintain test quality throughout the software lifecycle Note: Code coverage is not a panacea. Coverage measurement cannot replace good code review and excellent programming practices.\nGenerally, we should adopt a reasonable coverage target and strive for uniform coverage across all modules, rather than just looking at whether the final number is high enough to be satisfactory.\nFor example: Suppose the code coverage is very high in some modules, but there are not enough test cases covering some key modules. Even though the code coverage is high, it does not mean that the product quality is high.\nTypes of Code Coverage Metrics # Code coverage tools typically use one or more standards to determine whether your code has been executed after being subjected to automated testing. Common metrics seen in coverage reports include:\nFunction Coverage: What percentage of defined functions have been called Statement Coverage: What percentage of statements in the program have been executed Branch Coverage: What percentage of branches in control structures (e.g., if statements) have been executed Condition Coverage: What percentage of Boolean sub-expressions have been tested as true and false Line Coverage: What percentage of lines of source code have been tested How Code Coverage Works # Code coverage measurement primarily uses three methods:\n1. Source Code Instrumentation # Instrumentation statements are added to the source code, and the code is compiled using a normal compiler toolchain to generate an instrumented assembly. This is what we commonly call instrumentation; Gcov belongs to this category of code coverage tools.\n2. Runtime Instrumentation # This method collects information from the runtime environment while the code is executing to determine coverage information. In my understanding, the principles of JaCoCo and Coverage tools belong to this category.\n3. Intermediate Code Instrumentation # New bytecode is added to instrument compiled class files, generating a new instrumented class. To be honest, I Googled many articles and didn\u0026rsquo;t find a definitive statement on which tools belong to this category.\nUnderstanding the basic principles of these tools, combined with existing test cases, helps in correctly selecting code coverage tools. For example:\nIf the product\u0026rsquo;s source code only has E2E (end-to-end) test cases, usually only the first type of tool can be selected, i.e., an executable file compiled through instrumentation, followed by testing and result collection. If the product\u0026rsquo;s source code has unit test cases, usually the second type of tool is selected, i.e., runtime collection. This type of tool has high execution efficiency and is easy to integrate continuously. Current Mainstream Code Coverage Tools # There are many code coverage tools. Below are code coverage tools I\u0026rsquo;ve used for different programming languages. When selecting tools, try to choose those that are open-source, popular (active), and easy to use.\nProgramming Language Code Coverage Tool C/C++ Gcov Java JaCoCo JavaScript Istanbul Python Coverage.py Golang cover Don\u0026rsquo;t Overestimate Code Coverage Metrics # Code coverage is not a panacea; it only tells us which code has not been \u0026ldquo;executed\u0026rdquo; by test cases. A high percentage of code coverage does not equal high-quality and effective testing.\nFirst, high code coverage is not enough to measure effective testing. Instead, code coverage more accurately gives a measure of the extent to which code has not been tested. This means that if our code coverage metric is low, then we can be sure that important parts of the code have not been tested; however, the converse is not necessarily true. High code coverage does not fully indicate that our code has been adequately tested.\nSecond, 100% code coverage should not be one of our explicit goals. This is because there is always a trade-off between achieving 100% code coverage and actually testing important code. While it is possible to test all code, the value of testing is also likely to diminish as you approach this limit, considering the tendency to write more meaningless tests to meet coverage requirements.\nBorrowing a quote from Martin Fowler\u0026rsquo;s article on Test Coverage:\nCode coverage is a useful tool for finding untested parts of your codebase, but it\u0026rsquo;s not much use as a number telling you how good your tests are.\nReferences # https://www.lambdatest.com/blog/code-coverage-vs-test-coverage/ https://www.atlassian.com/continuous-delivery/software-testing/code-coverage https://www.thoughtworks.com/insights/blog/are-test-coverage-metrics-overrated\n","date":"2021-07-14","externalUrl":null,"permalink":"/en/posts/code-coverage/","section":"Posts","summary":"This article briefly introduces the concept, importance, common metrics, working principle, and mainstream tools of code coverage, emphasizing that code coverage metrics should not be over-relied upon.","title":"About Code Coverage","type":"posts"},{"content":"This article shares how to use Gcov and LCOV to metrics code coverage for C/C++ projects. If you want to know how Gcov works, or you need to metrics code coverage for C/C++ projects later, I hope this article is useful to you.\nProblems # The problem I\u0026rsquo;m having: A C/C++ project from decades ago has no unit tests, only regression tests, but you want to know what code is tested by regression tests? Which code is untested? What is the code coverage? Where do I need to improve automated test cases in the future?\nCan code coverage be measured without unit tests? Yes.\nCode coverage tools for C/C++ # There are some tools on the market that can measure the code coverage of black-box testing, such as Squish Coco, Bullseye, etc. Their principle is to insert instrumentation when build product.\nI\u0026rsquo;ve done some research on Squish Coco, because of some unresolved compilation issues that I didn\u0026rsquo;t buy a license for this expensive tool.\nWhen I investigated code coverage again, I found out that GCC has a built-in code coverage tool called Gcov.\nPrerequisites # For those who want to use Gcov, to illustrate how it works, I have prepared a sample program that requires GCC and LCOV to be installed before running the program.\nIf you don\u0026rsquo;t have an environment or don\u0026rsquo;t want to install it, you can check out this example repository\nNote: The source code is under the master branch master, and code coverage result html under branch coverage.\n# This is the version of GCC and lcov on my test environment. sh-4.2$ gcc --version gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39) Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. sh-4.2$ lcov -v lcov: LCOV version 1.14 How Gcov works # Gcov workflow diagram\nThere are three main steps:\nAdding special compilation options to the GCC compilation to generate the executable, and *.gcno. Running (testing) the generated executable, which generates the *.gcda data file. With *.gcno and *.gcda, generate the gcov file from the source code, and finally generate the code coverage report. Here\u0026rsquo;s how each of these steps is done exactly.\n1. Compile # The first step is to compile. The parameters and files used for compilation are already written in the makefile.\nmake build Click to see the output of the make command sh-4.2$ make build gcc -fPIC -fprofile-arcs -ftest-coverage -c -Wall -Werror main.c gcc -fPIC -fprofile-arcs -ftest-coverage -c -Wall -Werror foo.c gcc -fPIC -fprofile-arcs -ftest-coverage -o main main.o foo.o As you can see from the output, this program is compiled with two compile options -fprofile-arcs and -ftest-coverage. After successful compilation, not only the main and .o files are generated, but also two .gcno files are generated.\nThe .gcno record file is generated after adding the GCC compile option -ftest-coverage, which contains information for reconstructing the base block map and assigning source line numbers to blocks during the compilation process.\n2. Running the executable # After compilation, the executable main is generated, which is run (tested) as follows\n./main Click to see the output when running main sh-4.2$ ./main Start calling foo() ... when num is equal to 1... when num is equal to 2... When main is run, the results are recorded in the .gcda data file, and if you look in the current directory, you can see that two .gcda files have been generated.\n$ ls foo.c foo.gcda foo.gcno foo.h foo.o img main main.c main.gcda main.gcno main.o makefile README.md .gcda record data files are generated because the program is compiled with the -fprofile-arcs option introduced. It contains arc transition counts, value distribution counts, and some summary information.\n3. Generating reports # make report Click to see the output of the generated report sh-4.2$ make report gcov main.c foo.c File \u0026#39;main.c\u0026#39; Lines executed:100.00% of 5 Creating \u0026#39;main.c.gcov\u0026#39; File \u0026#39;foo.c\u0026#39; Lines executed:85.71% of 7 Creating \u0026#39;foo.c.gcov\u0026#39; Lines executed:91.67% of 12 lcov --capture --directory . --output-file coverage.info Capturing coverage data from . Found gcov version: 4.8.5 Scanning . for .gcda files ... Found 2 data files in . Processing foo.gcda geninfo: WARNING: cannot find an entry for main.c.gcov in .gcno file, skipping file! Processing main.gcda Finished .info-file creation genhtml coverage.info --output-directory out Reading data file coverage.info Found 2 entries. Found common filename prefix \u0026#34;/workspace/coco\u0026#34; Writing .css and .png files. Generating output. Processing file main.c Processing file foo.c Writing directory view page. Overall coverage rate: lines......: 91.7% (11 of 12 lines) functions..: 100.0% (2 of 2 functions) Executing make report to generate an HTML report actually performs two main steps behind this command.\nWith the .gcno and .gcda files generated at compile and run time, execute the command gcov main.c foo.c to generate the .gcov code coverage file.\nWith the code coverage .gcov file, generate a visual code coverage report via LCOV.\nThe steps to generate the HTML result report are as follows.\n# 1. Generate the coverage.info data file lcov --capture --directory . --output-file coverage.info # 2. Generate a report from this data file genhtml coverage.info --output-directory out Delete all generated files # All the generated files can be removed by executing make clean command.\nClick to see the output of the make clean command sh-4.2$ make clean rm -rf main *.o *.so *.gcno *.gcda *.gcov coverage.info out Code coverage report # The home page is displayed in a directory structure\nAfter entering the directory, the source files in that directory are displayed\nThe blue color indicates that these statements are overwritten\nRed indicates statements that are not overridden\nLCOV supports statement, function, and branch coverage metrics.\nSide notes:\nThere is another tool for generating HTML reports called gcovr, developed in Python, whose reports are displayed slightly differently from LCOV. For example, LCOV displays it in a directory structure, while gcovr displays it in a file path, which is always the same as the code structure, so I prefer to use the former.\n","date":"2021-07-11","externalUrl":null,"permalink":"/en/posts/gcov-example/","section":"Posts","summary":"This article shares how to use Gcov and LCOV to metrics code coverage for C/C++ projects. It explains the steps to compile, run tests, and generate coverage reports, including example commands and expected outputs.","title":"Code coverage testing of C/C++ projects using Gcov and LCOV","type":"posts"},{"content":"I\u0026rsquo;ve run into some situations when the build fails, perhaps because some processes don\u0026rsquo;t finish, and even setting a timeout doesn\u0026rsquo;t make the Jenkins job fail.\nSo, to fix this problem, I used try .. catch and error to make my Jenkins job failed, hopes this also helps you.\nPlease see the following example:\npipeline { agent none stages { stage(\u0026#39;Hello\u0026#39;) { steps { script { try { timeout(time: 1, unit: \u0026#39;SECONDS\u0026#39;) { echo \u0026#34;timeout step\u0026#34; sleep 2 } } catch(err) { // timeout reached println err echo \u0026#39;Time out reached.\u0026#39; error \u0026#39;build timeout failed\u0026#39; } } } } } } Here is the output log\n00:00:01.326 [Pipeline] Start of Pipeline 00:00:01.475 [Pipeline] stage 00:00:01.478 [Pipeline] { (Hello) 00:00:01.516 [Pipeline] script 00:00:01.521 [Pipeline] { 00:00:01.534 [Pipeline] timeout 00:00:01.534 Timeout set to expire in 1 sec 00:00:01.537 [Pipeline] { 00:00:01.547 [Pipeline] echo 00:00:01.548 timeout step 00:00:01.555 [Pipeline] sleep 00:00:01.558 Sleeping for 2 sec 00:00:02.535 Cancelling nested steps due to timeout 00:00:02.546 [Pipeline] } 00:00:02.610 [Pipeline] // timeout 00:00:02.619 [Pipeline] echo 00:00:02.621 org.jenkinsci.plugins.workflow.steps.FlowInterruptedException 00:00:02.625 [Pipeline] echo 00:00:02.627 Time out reached. 00:00:02.630 [Pipeline] error 00:00:02.638 [Pipeline] } 00:00:02.656 [Pipeline] // script 00:00:02.668 [Pipeline] } 00:00:02.681 [Pipeline] // stage 00:00:02.696 [Pipeline] End of Pipeline 00:00:02.709 ERROR: build timeout failed 00:00:02.710 Finished: FAILURE ","date":"2021-06-24","externalUrl":null,"permalink":"/en/posts/jenkins-timeout/","section":"Posts","summary":"This article explains how to handle Jenkins job timeouts effectively by using \u003ccode\u003etry\u003c/code\u003e and \u003ccode\u003ecatch\u003c/code\u003e blocks to ensure the job fails correctly when a timeout occurs.","title":"How to make Jenkins job fail after timeout? (Resolved)","type":"posts"},{"content":" 前言 # 本篇记录两个在做 Jenkins 与 AIX 做持续集成得时候遇到的 Git clone 代码失败的问题，并已解决，分享出来或许能有所帮助。\nDependent module /usr/lib/libldap.a(libldap-2.4.so.2) could not be loaded. 通过 SSH 进行 git clone 出现 Authentication failed 问题1：Dependent module /usr/lib/libldap.a(libldap-2.4.so.2) could not be loaded # Jenkins 通过 HTTPS 来 checkout 代码的时候，出现了如下错误：\n[2021-06-20T14:50:25.166Z] ERROR: Error cloning remote repo \u0026#39;origin\u0026#39; [2021-06-20T14:50:25.166Z] hudson.plugins.git.GitException: Command \u0026#34;git fetch --tags --force --progress --depth=1 -- https://git.company.com/scm/vas/db.git +refs/heads/*:refs/remotes/origin/*\u0026#34; returned status code 128: [2021-06-20T14:50:25.166Z] stdout: [2021-06-20T14:50:25.166Z] stderr: exec(): 0509-036 Cannot load program /opt/freeware/libexec64/git-core/git-remote-https because of the following errors: [2021-06-20T14:50:25.166Z] 0509-150 Dependent module /usr/lib/libldap.a(libldap-2.4.so.2) could not be loaded. [2021-06-20T14:50:25.166Z] 0509-153 File /usr/lib/libldap.a is not an archive or [2021-06-20T14:50:25.166Z] the file could not be read properly. [2021-06-20T14:50:25.166Z] 0509-026 System error: Cannot run a file that does not have a valid format. [2021-06-20T14:50:25.166Z] [2021-06-20T14:50:25.166Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:2450) [2021-06-20T14:50:25.166Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandWithCredentials(CliGitAPIImpl.java:2051) [2021-06-20T14:50:25.166Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.access$500(CliGitAPIImpl.java:84) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$1.execute(CliGitAPIImpl.java:573) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$2.execute(CliGitAPIImpl.java:802) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:161) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:154) .......................... [2021-06-20T14:50:25.167Z] Suppressed: hudson.remoting.Channel$CallSiteStackTrace: Remote call to aix-devasbld-01 [2021-06-20T14:50:25.167Z] at hudson.remoting.Channel.attachCallSiteStackTrace(Channel.java:1800) .......................... [2021-06-20T14:50:25.168Z] at java.lang.Thread.run(Thread.java:748) [2021-06-20T15:21:20.525Z] Cloning repository https://git.company.com/scm/vas/db.git 但是直接在虚拟机上通过命令 git clone https://git.company.com/scm/vas/db.git ，可以成功下载，没有出现任何问题。\n如果将 LIBPATH 设置为 LIBPATH=/usr/lib 就能重现上面的错误，这说明通过 Jenkins 下载代码的时候它是去 /usr/lib/ 下面找 libldap.a 如果将变量 LIBPATH 设置为空 export LIBPATH= 或 unset LIBPATH，执行 git clone https://... 就正常了。 尝试在 Jenkins 启动 agent 的时候修改 LIBPATH 变量设置为空，但都不能解决这个问题，不明白为什么不行！？\n那就看看 /usr/lib/libldap.a 是什么问题了。\n# ldd 的时候发现这个静态库有问题 $ ldd /usr/lib/libldap.a /usr/lib/libldap.a needs: /opt/IBM/ldap/V6.4/lib/libibmldapdbg.a /usr/lib/threads/libc.a(shr.o) Cannot find libpthreads.a(shr_xpg5.o) /opt/IBM/ldap/V6.4/lib/libidsldapiconv.a Cannot find libpthreads.a(shr_xpg5.o) Cannot find libc_r.a(shr.o) /unix /usr/lib/libcrypt.a(shr.o) Cannot find libpthreads.a(shr_xpg5.o) Cannot find libc_r.a(shr.o) # 可以看到它链接到是 IBM LDAP $ ls -l /usr/lib/libldap.a lrwxrwxrwx 1 root system 35 Jun 10 2020 /usr/lib/libldap.a -\u0026gt; /opt/IBM/ldap/V6.4/lib/libidsldap.a # 再看看同样的 libldap.a 在 /opt/freeware/lib/ 是没问题的 $ ldd /opt/freeware/lib/libldap.a ldd: /opt/freeware/lib/libldap.a: File is an archive. $ ls -l /opt/freeware/lib/libldap.a lrwxrwxrwx 1 root system 13 May 27 2020 /opt/freeware/lib/libldap.a -\u0026gt; libldap-2.4.a 问题1：解决办法 # # 尝试替换 # 先将 libldap.a 重名为 libldap.a.old（不删除以防需要恢复） $ sudo mv /usr/lib/libldap.a /usr/lib/libldap.a.old # 重新链接 $ sudo ln -s /opt/freeware/lib/libldap.a /usr/lib/libldap.a $ ls -l /usr/lib/libldap.a lrwxrwxrwx 1 root system 27 Oct 31 23:27 /usr/lib/libldap.a -\u0026gt; /opt/freeware/lib/libldap.a 重新链接完成后，重新连接 AIX agent，再次执行 Jenkins job 来 clone 代码，成功了！\n问题2：通过 SSH 进行 git clone 出现 Authentication failed # 由于 AIX 7.1-TL4-SP1 即将 End of Service Pack Support，因此需要升级。但是升级到 AIX 7.1-TL5-SP6 后无法通过 SSH 下载代码。\n$ git clone ssh://git@git.company.com:7999/vas/db.git Cloning into \u0026#39;db\u0026#39;... Authentication failed. fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 像这样的错误，在使用 Git SSH 方式来 clone 代码经常会遇到，通常都是没有设置 public key。只要执行 ssh-keygen -t rsa -C your@email.com 生成 id_rsa keys，然后将 id_rsa.pub 的值添加到 GitHub/Bitbucket/GitLab 的 public key 中一般就能解决。\n但这次不一样，尽管已经设置了 public key，但错误依旧存在。奇快的是之前 AIX 7.1-TL4-SP1 是好用的，升级到 AIX 7.1-TL5-SP6 就不好用了呢？\n使用命令 ssh -vvv \u0026lt;git-url\u0026gt; 来看看他们在请求 git 服务器时候 debug 信息。\n# AIX 7.1-TL4-SP1 bash-4.3$ oslevel -s 7100-04-01-1543 bash-4.3$ ssh -vvv git.company.com OpenSSH_6.0p1, OpenSSL 1.0.1e 11 Feb 2013 debug1: Reading configuration data /etc/ssh/ssh_config debug1: Failed dlopen: /usr/krb5/lib/libkrb5.a(libkrb5.a.so): 0509-022 Cannot load module /usr/krb5/lib/libkrb5.a(libkrb5.a.so). 0509-026 System error: A file or directory in the path name does not exist. debug1: Error loading Kerberos, disabling Kerberos auth. ....... ....... ssh_exchange_identification: read: Connection reset by peer # New machine AIX 7.1-TL5-SP6 $ oslevel -s 7100-05-06-2015 $ ssh -vvv git.company.com OpenSSH_7.5p1, OpenSSL 1.0.2t 10 Sep 2019 debug1: Reading configuration data /etc/ssh/ssh_config debug1: Failed dlopen: /usr/krb5/lib/libkrb5.a(libkrb5.a.so): 0509-022 Cannot load module /usr/krb5/lib/libkrb5.a(libkrb5.a.so). 0509-026 System error: A file or directory in the path name does not exist. debug1: Error loading Kerberos, disabling Kerberos auth. ....... ....... ssh_exchange_identification: read: Connection reset by peer 可以看到的差别是 OpenSSH 的版本不同，可能是因此导致的，根据这个推测很快就找到了类似的问题和答案（Stackoverflow 链接)\n问题2：解决办法 # 在 ~/.ssh/config 文件里添加选项 AllowPKCS12keystoreAutoOpen no\n但问题又来了，这个选项是 AIX 上的一个定制选项，在 Linux 上是没有的。\n这会导致同一个域账户在 AIX 通过 SSH 可以 git clone 成功，但在 Linux 上 git clone 会失败。\n# Linux 上不识别改选项 stderr: /home/****/.ssh/config: line 1: Bad configuration option: allowpkcs12keystoreautoopen /home/****/.ssh/config: terminating, 1 bad configuration options fatal: Could not read from remote repository. 如果 config 文件可以支持条件选项就好了，即当为 AIX 是添加选项 AllowPKCS12keystoreAutoOpen no，其他系统则没有该选项。可惜 config 并不支持。 如果能单独的设置当前 AIX 的 ssh config 文件就好了。尝试将 /etc/ssh/ssh_config 文件修改如下，重启服务，再次通过 SSH clone，成功~！ Host * AllowPKCS12keystoreAutoOpen no # ForwardAgent no # ForwardX11 no # RhostsRSAAuthentication no # RSAAuthentication yes # PasswordAuthentication yes # HostbasedAuthentication no # GSSAPIAuthentication no # GSSAPIDelegateCredentials no # GSSAPIKeyExchange no # GSSAPITrustDNS no # ....省略 ","date":"2021-06-20","externalUrl":null,"permalink":"/posts/git-clone-failed-on-aix/","section":"Posts","summary":"本文记录了在 AIX 上使用 Jenkins 进行 Git Clone 时遇到的两个问题及其解决方法，包括依赖库加载失败和 SSH 认证失败。","title":"解决在 AIX 上 Git Clone 失败的两个问题","type":"posts"},{"content":"Recently, when downloading code from Bitbucket using AIX 7.1, I encountered this error:\nfatal: write error: A file cannot be larger than the value set by ulimit.\n$ git clone -b dev https://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@git.company.com/scm/vmcc/opensrc.git --depth 1 Cloning into \u0026#39;opensrc\u0026#39;... remote: Counting objects: 2390, done. remote: Compressing objects: 100% (1546/1546), done. fatal: write error: A file cannot be larger than the value set by ulimit. fatal: index-pack failed On AIX 7.3, I encountered this error:\nfatal: fetch-pack: invalid index-pack output\n$ git clone -b dev https://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@git.company.com/scm/vmcc/opensrc.git --depth 1 Cloning into \u0026#39;opensrc\u0026#39;... remote: Counting objects: 2390, done. remote: Compressing objects: 100% (1546/1546), done. fatal: write error: File too large68), 1012.13 MiB | 15.38 MiB/s fatal: fetch-pack: invalid index-pack output This is because the files in this repository are too large, exceeding the AIX limit on user file resource usage.\nThis can be checked using ulimit -a. More information about the ulimit command can be found at ulimit Command\n$ ulimit -a time(seconds) unlimited file(blocks) 2097151 data(kbytes) unlimited stack(kbytes) 32768 memory(kbytes) 32768 coredump(blocks) 2097151 nofiles(descriptors) 2000 threads(per process) unlimited processes(per user) unlimited We can see that file has an upper limit of 2097151. Changing this to unlimited should solve the problem.\nThe limits file /etc/security/limits is accessible by the root user (ordinary users do not have access permissions).\n# The following is part of the content of this file default: fsize = 2097151 core = 2097151 cpu = -1 data = -1 rss = 65536 stack = 65536 nofiles = 2000 Changing fsize = 2097151 to fsize = -1 removes the file size limit. After making this change, log in again for the changes to take effect.\nExecuting ulimit -a again shows that the change has taken effect.\n$ ulimit -a time(seconds) unlimited file(blocks) unlimited data(kbytes) unlimited stack(kbytes) 32768 memory(kbytes) 32768 coredump(blocks) 2097151 nofiles(descriptors) 2000 threads(per process) unlimited processes(per user) unlimited Now file(blocks) is unlimited. Let\u0026rsquo;s try git clone again.\ngit clone -b dev https://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@git.company.com/scm/vmcc/opensrc.git --depth 1 Cloning into \u0026#39;opensrc\u0026#39;... remote: Counting objects: 2390, done. remote: Compressing objects: 100% (1546/1546), done. remote: Total 2390 (delta 763), reused 2369 (delta 763) Receiving objects: 100% (2390/2390), 3.80 GiB | 3.92 MiB/s, done. Resolving deltas: 100% (763/763), done. Checking out files: 100% (3065/3065), done. Success!\n","date":"2021-06-17","externalUrl":null,"permalink":"/en/posts/aix-ulimit/","section":"Posts","summary":"Resolving Git large repository download failures on AIX due to file size limits by modifying ulimit settings.","title":"Resolving Git Large Repository Download Failures on AIX by Removing File Resource Limits","type":"posts"},{"content":"","date":"2021-06-17","externalUrl":null,"permalink":"/en/tags/ulimit/","section":"Tags","summary":"","title":"Ulimit","type":"tags"},{"content":"最近在我使用 Artifactory Enterprise 遇到了上传制品非常缓慢的问题，在经过与 IT，Artifactory 管理员一起合作终于解决这个问题，在此分享一下这个问题的解决过程。\n如果你也遇到类似或许有所帮助。\n问题描述 # 最近发现通过 Jenkins 往 Artifactory 里上传制品的时候偶尔出现上传非常缓慢的情况，尤其是当一个 Jenkins stage 里有多次上传，往往会在第二次上传的时候出现传输速度极为缓慢（KB/s ）。\n问题排查和解决 # 我的构建环境和 Jenkins 都没有任何改动，所有的构建任务都出现了上传缓慢的情况，为了排除可能是使用 Artifactory plugin 的导致原因，通过 curl 命令来进行上传测试，也同样上传速度经常很慢。\n那么问题就在 Artifactory 上面。\n是 Artifactory 最近升级了？ 还是 Artifactory 最近修改了什么设置？ 也许是 Artifactory 服务器的问题？ 在跟 Artifactory 管理员进行了沟通之后，排除了以上 1，2 的可能。为了彻底排除是 Artifactory 的问题，通过 scp 进行拷贝的时候同样也出现了传输速度非常慢的情况，这问题就出现在网络上了。\n这样需要 IT 来帮助排查网络问题了，最终 IT 建议更换网卡进行尝试（因为他们之前有遇到类似的情况），但这种情况会有短暂的网络中断，不过最终还是得到了管理者的同意。\n幸运的是在更换网卡之后，Jenkins 往 Artifactory 传输制品的速度恢复了正常。\n总结 # 处理次事件的一点点小小的总结：\n由于这个问题涉及到几个团队，为了能够快速推进，此时明确说明问题，推测要有理有据，以及该问题导致了什么样的严重后果（比如影响发布）才能让相关人重视起来，否则大家都等着，没人回来解决问题。\n当 Artifactory 管理推荐使用其他数据中心 instance，建议他们先尝试更换网卡；如果问题没有得到解决，在同一个数据中心创建另外一台服务器。如果问题还在，此时再考虑迁移到其他数据中心instance。这大大减少了作为用户去试错所带来的额外工作量。\n","date":"2021-06-16","externalUrl":null,"permalink":"/posts/artifactory-slow-upload/","section":"Posts","summary":"在使用 JFrog Artifactory 上传制品时遇到速度缓慢和上传失败的问题，经过排查和解决，分享经验和教训。","title":"关于 Artifactory 上传制品变得非常缓慢，偶尔失败的问题分享","type":"posts"},{"content":"","date":"2021-06-07","externalUrl":null,"permalink":"/en/tags/eslint/","section":"Tags","summary":"","title":"ESlint","type":"tags"},{"content":" I\u0026rsquo;m just documenting to myself that it was solved by following.\nWhen I want to integrate the ESlint report with Jenkins. I encourage a problem\nThat is eslint-report.html display different with it on my local machine, and I also log to Jenkins server and grab the eslint-report.html to local, it works well.\nI used HTML Publisher plugin to display the HTML report, but only the ESlint HTML report has problems other report work well, so I guess this problem may be caused by Jenkins.\nFinally, I find it. (Stackoverflow URL)\nFollow the below steps for solution # Open the Jenkin home page. Go to Manage Jenkins. Now go to Script Console. And in that console paste the below statement and click on Run. System.setProperty(\u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;\u0026#34;) After that, it will load CSS and JS. According to Jenkins\u0026rsquo;s new Content Security Policy and I saw No frames allowed.\nThat is exactly the error I get on chrome by right-clicking on Elements.\n","date":"2021-06-07","externalUrl":null,"permalink":"/en/posts/jenkins-eslint/","section":"Posts","summary":"This article explains how to resolve the issue of ESlint HTML report not displaying correctly in Jenkins jobs due to Content Security Policy restrictions, including the steps to configure Jenkins to allow the report to load properly.","title":"Resolved problem that ESlint HTML report is not displayed correctly in Jenkins job","type":"posts"},{"content":"在使用 Git 提交代码之前，建议做以下这些设置。\n叫指南有点夸张，因为它在有些情况下下不适用，比如你已经有了 .gitattributes 或 .editorconfig 等文件，那么有些设置就不用做了。\n因此暂且叫他指北吧，它通常情况下还是很有用的。\n废话不多说，看看都需要哪些设置吧。\n1. 配置 name 和 email # # 注意，你需要将下面示例中我的 name 和 email 换成你自己的 $ git config --global user.name \u0026#34;shenxianpeng\u0026#34; $ git config --global user.email \u0026#34;xianpeng.shen@gmail.com\u0026#34; 对于，我还推荐你设置头像，这样方便同事间的快速识别。\n当你不设置头像的时候，只有把鼠标放到头像上才知道 Pull Request 的 Reviewers 是谁（来自于Bitubkcet）。\n2. 设置 core.autocrlf=false # 为了防止 CRLF(windows) 和 LF(UNIX/Linux/Mac) 的转换问题。为了避免在使用 Git 提交代码时出现历史被掩盖的问题，强烈建议每个使用 Git 的人执行以下命令\n$ git config --global core.autocrlf false # 检查并查看是否输出 \u0026#34;core.autocrlf=false\u0026#34;，这意味着命令设置成功。 $ git config --list 如果你的项目底下已经有了 .gitattributes 或 .editorconfig 文件，通常这些文件里面都有放置 CRLF 和 LF 的转换问题的设置项。\n这时候你就不必特意执行命令 git config --global core.autocrlf false\n3. 编写有规范的提交 # 我在之前的文章里分享过关于如何设置提交信息规范，请参看《Git提交信息和分支创建规范》。\n4. 提交历史的压缩 # 比如你修改一个 bug，假设你通过 3 次提交到你的个人分支才把它改好。这时候你提 Pull Request 就会显示有三个提交。\n如果提交历史不进行压缩，这个 PR 被合并到主分支后，以后别人看到你这个 bug 的修复就是去这三个 commits 里去一一查看，进行对比，才能知道到底修改了什么。\n压缩提交历史就是将三次提交压缩成一次提交。\n可以通过 git rebase 命令进行 commit 的压缩，比如将最近三次提交压缩成一次可以执行\ngit rebase -i HEAD~3 5. 删除已经 merge 的分支 # 有些 SCM，比如 Bitbucket 不支持默认勾选 Delete source branch after merging，这个问题终于在 Bitbucket 7.3 版本修复了。详见 BSERV-9254 和 BSERV-3272 （2013年创建的）。\n注意在合并代码时勾选删除源分支这一选项，否则会造成大量的开发分支留在 Git 仓库下。\n如果还需要哪些设置这里没有提到的，欢迎补充。\n","date":"2021-05-14","externalUrl":null,"permalink":"/posts/git-guidelines/","section":"Posts","summary":"本文介绍了在使用 Git 提交代码之前需要进行的一些常见设置，包括配置用户名和邮箱、处理换行符、编写规范的提交信息等，帮助开发者更好地管理代码版本。","title":"Git 常见设置指北","type":"posts"},{"content":" Why need branching naming convention # To better manage the branches on Git(I sued Bitbucket), integration with CI tool, Artifactory, and automation will be more simple and clear.\nFor example, good unified partition naming can help the team easily find and integrate without special processing. Therefore, you should unify the partition naming rules for all repositories.\nBranches naming convention # main branch naming # In general, the main\u0026rsquo;s branch names most like master or main.\nDevelopment branch naming # I would name my development branch just called develop.\nBugfix and feature branches naming # For Bitbucket, it has default types of branches for use, like bugfix/, feature/. So my bugfix, feature combine with the Jira key together, such as bugfix/ABC-1234 or feature/ABC-2345.\nHotfix and release branches naming # For hotfix and release, my naming convention always like release/1.1.0, hotfix/1.1.0.HF1.\nOther branches # Maybe your Jira task ticket you don\u0026rsquo;t want to make it in bugfix or feature, you can name it to start with task, so the branch name is task/ABC-3456.\nIf you have to provide diagnostic build to custom, you can name your branch diag/ABC-5678.\nSummary # Anyway, having a unified branch naming convention is very important for implement CI/CD and your whole team.\n","date":"2021-05-13","externalUrl":null,"permalink":"/en/posts/git-branching-strategy/","section":"Posts","summary":"This article introduces the conventional branch naming specification, including the purpose of branch names, key points, and basic rules for naming branches in Git. It also provides examples of branch prefixes and their meanings.","title":"Branch Naming Convention","type":"posts"},{"content":" Problem # When you do CI with JFrog Artifactory when you want to download the entire folder artifacts, but maybe your IT doesn\u0026rsquo;t enable this function, whatever some seasons.\nYou can try the below JFrog Artifactory API to know if you\u0026rsquo;re using Artifactory whether allowed to download the entire folder artifacts.\njust visit this API URL: https://den-artifactory.company.com/artifactory/api/archive/download/team-generic-release-den/project/abc/main/?archiveType=zip\nYou will see an error message returned if the Artifactory is not allowed to download the entire folder.\n{ \u0026#34;errors\u0026#34;: [ { \u0026#34;status\u0026#34;: 403, \u0026#34;message\u0026#34;: \u0026#34;Download Folder functionality is disabled.\u0026#34; } ] } More details about the API could find here Retrieve Folder or Repository Archive\nWorkaround # So to be enabled to download entire folder artifacts, I found other JFrog Artifactory APIs provide a workaround.\nHow to download the entire folder artifacts programmatically? this post will show you how to use other Artifactory REST API to get a workaround.\n1. Get All Artifacts Created in Date Range # API URL: Artifacts Created in Date Range\nThis is the snippet code I use this API\n# download.sh USERNAME=$1 PASSWORD=$2 REPO=$3 # which day ago do you want to download N_DAY_AGO=$4 # today START_TIME=$(($(date --date=\u0026#34;$N_DAY_AGO days ago\u0026#34; +%s%N)/1000000)) END_TIME=$(($(date +%s%N)/1000000)) ARTIFACTORY=https://den-artifactory.company.com/artifactory if [ ! -x \u0026#34;`which sha1sum`\u0026#34; ]; then echo \u0026#34;You need to have the \u0026#39;sha1sum\u0026#39; command in your path.\u0026#34;; exit 1; fi RESULTS=`curl -s -X GET -u $USERNAME:$PASSWORD \u0026#34;$ARTIFACTORY/api/search/creation?from=$START_TIME\u0026amp;to=$END_TIME\u0026amp;repos=$REPO\u0026#34; | grep uri | awk \u0026#39;{print $3}\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed -r \u0026#39;s/^.{1}//\u0026#39;` echo $RESULTS for RESULT in $RESULTS ; do echo \u0026#34;fetching path from $RESULT\u0026#34; PATH_TO_FILE=`curl -s -X GET -u $USERNAME:$PASSWORD $RESULT | grep downloadUri | awk \u0026#39;{print $3}\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed -r \u0026#39;s/^.{1}//\u0026#39;` echo \u0026#34;download file path $PATH_TO_FILE\u0026#34; curl -u $USERNAME:$PASSWORD -O $PATH_TO_FILE done Then you just use this as: sh download.sh ${USERNAME} ${PASSWORD} ${REPO_PATH} ${N_DAY_AGO}\n2. Get all artifacts matching the given Ant path pattern # More about this API see: Pattern Search\nTake an example screenshot of pattern search:\nThen you can use Shell, Python language to get the file path from the response, then use curl -u $USERNAME:$PASSWORD -O $PATH_TO_FILE command to download the file one by one.\nIf you have better solutions, suggestions, or questions, you can leave a comment.\n","date":"2021-05-04","externalUrl":null,"permalink":"/en/posts/artifactory-api-search/","section":"Posts","summary":"This article explains how to download the entire folder artifacts from JFrog Artifactory when the \u0026ldquo;Download Folder functionality is disabled\u0026rdquo;. It provides a workaround using Artifactory REST APIs to retrieve and download artifacts programmatically.","title":"How to download the entire folder artifacts when Artifactory \"Download Folder functionality is disabled\"?","type":"posts"},{"content":"","date":"2021-05-04","externalUrl":null,"permalink":"/en/tags/jfrog/","section":"Tags","summary":"","title":"JFrog","type":"tags"},{"content":" What\u0026rsquo;s the problem? # Today I am having a problem where the Windows installer I created is not installing, and the following Windows installer box pops up.\nBut it works well in the previous build, and I didn\u0026rsquo;t make any code changes. It is strange, actually fix this problem is very easy but not easy to find.\nHow to fix it? # In my case, I just remove the space from my build folder naming. I have made follow mistakes:\nMy previous build name is v2.2.2.3500-da121sa-Developer, but for this build, I named it to v2.2.2.3500-32jkjdk - Developer\nHow to find the solution? # This problem takes me several hours until I google this article which inspired me.\nJust like the above article, if I try to use the command line msiexec.exe other-commands ..., compare with the works installer will also quick to find the root cause.\nI realized it immediately I should try to remove the spaces from the build folder\u0026hellip; Wow, the installer back to work.\nIf this happens to you, I hope it also works for you, and leave a comment if it works for you.\n","date":"2021-04-22","externalUrl":null,"permalink":"/en/posts/why-windows-installer-pop-up/","section":"Posts","summary":"This article explains a common issue with Windows installers where a pop-up appears unexpectedly, and how to resolve it by correcting the build folder naming convention.","title":"Why Windows Installer pop up? (Resolved)","type":"posts"},{"content":"","date":"2021-04-06","externalUrl":null,"permalink":"/tags/jacoco/","section":"标签","summary":"","title":"JaCoCo","type":"tags"},{"content":"本文适用的是 Gradle 来构建和适用 JaCoCo。\n分别介绍了 build.gradle 的文件配置，执行测试和生成报告，报告参数说明，以及如何忽略指定的包或类从而影响测试覆盖率的结果。\nbuild.gradle 文件配置 # 比如使用 gradle 来管理的项目可以在 build.gradle 里添加如下代码\nplugins { id \u0026#39;jacoco\u0026#39; } jacoco { toolVersion = \u0026#34;0.8.5\u0026#34; } test { useJUnitPlatform() exclude \u0026#39;**/**IgnoreTest.class\u0026#39; // 如果有 test case 不通过，如有必要可以通过这样忽略掉 finalizedBy jacocoTestReport // report is always generated after tests run } jacocoTestReport { dependsOn test // tests are required to run before generating the report reports { xml.enabled true csv.enabled false html.destination file(\u0026#34;${buildDir}/reports/jacoco\u0026#34;) } } 执行测试，生成代码覆盖率报告 # 然后执行 gradle test 就可以了。之后可以可以在 build\\reports\\jacoco 目录下找到报告了。\n重点是如何分析报告。打开 index.html，报告显示如下：\n报告参数说明 # Coverage Counters（覆盖计数器） # JaCoCo 使用一组不同的计数器来计算覆盖率指标，所有这些计数器都来自于 Java 类文件中包含的信息，这些信息基本上是 Java 字节码指令和嵌入类文件中的调试信息。这种方法可以在没有源代码的情况下，对应用程序进行有效的即时检测和分析。在大多数情况下，收集到的信息可以映射到源代码，并可视化到行级粒度。\n这种方法也有一定的局限性，就是类文件必须与调试信息一起编译，以计算行级覆盖率并提供源码高亮。但不是所有的 Java 语言结构都可以直接编译成相应的字节码。在这种情况下，Java 编译器会创建所谓的合成代码，有时会导致意外的代码覆盖率结果。\nInstructions (C0 Coverage) - 指令（C0覆盖率） # 最小的单位 JaCoCo 计数是单个 Java 字节码指令，指令覆盖率提供了关于被执行或遗漏的代码量的信息，这个指标完全独立于源码格式化，即使在类文件中没有调试信息的情况下也始终可用。\nBranches (C1 Coverage) - 分支（C1覆盖率） # JaCoCo 还计算所有 if 和 switch 语句的分支覆盖率，这个指标计算一个方法中此类分支的总数，并确定执行或遗漏的分支数量。即使在类文件中没有调试信息的情况下，分支覆盖率总是可用的。但请注意在这个计数器定义的上下文中异常处理不被认为是分支。\n如果类文件没有编译调试信息，决策点可以被映射到源行并相应地高亮显示。\n没有覆盖。行中没有分支被执行（红菱形 部分覆盖。仅执行了该线的部分分支（黄钻 全覆盖。线路中的所有分支都已执行（绿色菱形） Cyclomatic Complexity - 环形复杂度 # JaCoCo 还计算了每个非抽象方法的循环复杂度并总结了类、包和组的复杂度。循环复杂度是指在（线性）组合中，能够产生通过一个方法的所有可能路径的最小路径数。 因此复杂度值可以作为完全覆盖某个软件的单元测试用例数量的指示，即使在类文件中没有调试信息的情况下，也可以计算出复杂度数字。\n循环复杂度v(G)的正式定义是基于将方法的控制流图表示为一个有向图。\nv(G) = E - N + 2\n其中E为边数，N为节点数。JaCoCo根据分支数(B)和决策点数(D)计算方法的循环复杂度，其等价公式如下。\nv(G) = B - D + 1\n根据每个分支的覆盖状态，JaCoCo还计算每个方法的覆盖和遗漏复杂度。遗漏的复杂度再次表明了完全覆盖一个模块所缺少的测试用例数量。请注意，由于JaCoCo不考虑异常处理作为分支，尝试/捕获块也不会增加复杂性。\nLines - 行 # 对于所有已经编译过调试信息的类文件，可以计算出各个行的覆盖率信息。当至少有一条分配给该行的指令被执行时，就认为该源行已被执行。\n由于单行通常会编译成多条字节码指令，源码高亮显示每行包含源码的三种不同状态。\nNo coverage: 该行没有指令被执行（红色背景）。 部分覆盖。该行中只有部分指令被执行（黄色背景）。 全覆盖。该行的所有指令都已执行（绿色背景）。 根据源码的格式，一个源码的一行可能涉及多个方法或多个类。因此，方法的行数不能简单地相加来获得包含类的总行数。同样的道理也适用于一个源文件中多个类的行数。JaCoCo根据实际的源代码行数来计算类和源代码文件的行数。\nMethod - 方法 # 每个非抽象方法至少包含一条指令。当至少有一条指令被执行时，一个方法就被认为是被执行的。由于 JaCoCo 工作在字节代码层面，构造函数和静态初始化器也被算作方法，其中一些方法在 Java 源代码中可能没有直接的对应关系，比如隐式的，因此生成了默认的构造函数或常量的初始化器。\nClasses - 类 # 当一个类中至少有一个方法被执行时，该类被认为是被执行的。请注意，JaCoCo 认为构造函数和静态初始化器都是方法。由于 Java 接口类型可能包含静态初始化器，这种接口也被视为可执行类。\n覆盖率的计算原文\n从代码覆盖率报告中忽略指定的包或代码 # 对于有些包和代码可能不属于你的项目，但也被统计在内，可以通修改在 build.gradle 将指定的代码或是包从 JaCoCo 报告中忽略掉。如下：\n// 省略部分代码 jacocoTestReport { dependsOn test // tests are required to run before generating the report reports { xml.enabled true csv.enabled false html.destination file(\u0026#34;${buildDir}/reports/jacoco\u0026#34;) } afterEvaluate { classDirectories.setFrom(files(classDirectories.files.collect { fileTree(dir: it, exclude: [ \u0026#39;com/vmware/antlr4c3/**\u0026#39;]) \u0026#39;com/vmware/antlr4c3/**\u0026#39;, \u0026#39;com/basic/parser/BasicParser*\u0026#39; ]) })) } } ","date":"2021-04-06","externalUrl":null,"permalink":"/posts/jacoco-imp/","section":"Posts","summary":"本文介绍了 JaCoCo 的使用方法，包括 Gradle 配置、执行测试生成报告、报告参数说明以及如何忽略指定的包或类影响测试覆盖率结果。","title":"JaCoCo 代码覆盖率实践分享","type":"posts"},{"content":"Python, needless to say, is one of the easiest dynamic programming languages to learn. Its cross-platform compatibility, readability, ease of writing, and rich packages make it one of the most commonly used languages among DevOps/testing/development engineers.\nMany have used it to accomplish a lot of work, but do you simply stop at functional implementation and ignore writing more concise and elegant Pythonic code?\nWhen I first started using Python, I didn\u0026rsquo;t know the word Pythonic. Years ago, a senior programmer mentioned during my training that some code in a project wasn\u0026rsquo;t Pythonic enough and needed refactoring. From the context, I understood it to mean: the Python code wasn\u0026rsquo;t written in the Pythonic way.\nWhat is Pythonic # Making full use of Python\u0026rsquo;s features to produce clear, concise, and maintainable code. Pythonic means that the code is not only syntactically correct but also follows the conventions of the Python community and uses the language as intended.\nExamples # Here\u0026rsquo;s a snippet of code from a C/C++ programmer:\nint a = 1; int b = 100; int total_sum = 0; while (b \u0026gt;= a) { total_sum += a; a++; } Without learning Python programming patterns, converting the above code to Python might look like this:\na = 1 b = 100 total_sum = 0 while b \u0026gt;= a: total_sum += a a += 1 A Pythonic way to write this would be:\ntotal_sum = sum(range(1, 101)) Here\u0026rsquo;s another common example. A for loop in Java might be written like this:\nfor(int index=0; index \u0026lt; items.length; index++) { items[index].performAction(); } In Python, a cleaner approach would be:\nfor item in items: item.perform_action() Or even a generator expression:\n(item.some_attribute for item in items) Essentially, when someone says something isn\u0026rsquo;t pythonic, they\u0026rsquo;re saying the code could be rewritten in a way that\u0026rsquo;s more aligned with Python\u0026rsquo;s coding style. Also, get familiar with Python\u0026rsquo;s Built-in Functions instead of reinventing the wheel.\nThe \u0026ldquo;Official Introduction\u0026rdquo; to Pythonic # In fact, an introduction to Pythonic is secretly \u0026ldquo;hidden\u0026rdquo; in the Python command line. Just open the Python console and type import this, and you\u0026rsquo;ll see:\nC:\\Users\\xshen\u0026gt;python Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32 Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren\u0026#39;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you\u0026#39;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it\u0026#39;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let\u0026#39;s do more of those! \u0026gt;\u0026gt;\u0026gt; A direct translation is: Tim Peters\u0026rsquo; \u0026ldquo;The Zen of Python\u0026rdquo;\nBeautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren\u0026#39;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you\u0026#39;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it\u0026#39;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let\u0026#39;s do more of those! Have you grasped the concept of Pythonic?\n","date":"2021-03-28","externalUrl":null,"permalink":"/en/posts/pythonic/","section":"Posts","summary":"This article introduces the concept of Pythonic code and demonstrates through examples how to write more concise and elegant Python code, helping developers improve code quality and readability.","title":"Is Your Python Code Pythonic Enough?","type":"posts"},{"content":"","date":"2021-03-28","externalUrl":null,"permalink":"/en/tags/pythonic/","section":"Tags","summary":"","title":"Pythonic","type":"tags"},{"content":" Problem # When you use Jenkins multibranch pipeline, you may want to have different default parameters settings for defferent branches build.\nFor example:\nFor develop/hotfix/release branches, except regular build, you also want to do some code analyzes, like code scanning, etc. For other branches, like feature/bugfix or Pull Request that you just want to do a regular build.\nSo you need to have dynamic parameter settings for your multibranch pipeline job.\nSolution # So for these cases, how to deal with Jenkins multibranch pipeline. Here are some code snippet that is works well in my Jenkinsfile.\ndef polarisValue = false def blackduckValue = false if (env.BRANCH_NAME.startsWith(\u0026#34;develop\u0026#34;) || env.BRANCH_NAME.startsWith(\u0026#34;hotfix\u0026#34;) || env.BRANCH_NAME.startsWith(\u0026#34;release\u0026#34;)) { polarisValue = true blackduckValue = true } pipeline { agent { node { label \u0026#39;gradle\u0026#39; } } parameters { booleanParam defaultValue: polarisValue, name: \u0026#39;Polaris\u0026#39;, summary: \u0026#39;Uncheck to disable Polaris\u0026#39; booleanParam defaultValue: blackduckValue, name: \u0026#39;BlackDuck\u0026#39;, summary: \u0026#39;Uncheck to disable BD scan\u0026#39; } stages { // ... } } ","date":"2021-03-24","externalUrl":null,"permalink":"/en/posts/jenkins-dynamic-default-parameters/","section":"Posts","summary":"This article explains how to set different default parameters for different branches in Jenkins multibranch pipelines, allowing for dynamic configuration based on the branch being built.","title":"Different branches have different default parameters in Jenkins","type":"posts"},{"content":"","date":"2021-03-20","externalUrl":null,"permalink":"/en/tags/codereview/","section":"Tags","summary":"","title":"CodeReview","type":"tags"},{"content":" Background # Code review is the process of having someone else examine your code. Its purpose is to ensure that the overall code health of the codebase continuously improves over time.\nThere\u0026rsquo;s a Chinese proverb: \u0026ldquo;Three people walking together, one is bound to be my teacher.\u0026rdquo;\nThe same is true for code review:\nOthers\u0026rsquo; reviews may offer different perspectives and suggestions; Everyone makes mistakes, and having an extra pair of eyes reduces the chance of errors. Therefore, code review is the most important check before merging your code into the main branch. It\u0026rsquo;s also an excellent and low-cost way to find bugs in software.\nGiven the importance and obvious benefits of code review, it\u0026rsquo;s often heard that code review is difficult to implement and ineffective in teams. Where does the problem lie?\nFrom my observation, there are two main reasons:\nFirst, reading other people\u0026rsquo;s code takes time, often requiring the code submitter to explain the business logic to the reviewer, thus consuming time for both parties; Second, if the code reviewer is overworked and stressed and doesn\u0026rsquo;t have time, it\u0026rsquo;s easy to lead to inadequate execution and going through the motions;\nHow can we conduct code reviews more effectively? Let\u0026rsquo;s first look at how large companies do it. Google\u0026rsquo;s article on code review provides specific principles.\nGoogle\u0026rsquo;s Code Review Principles # When conducting a code review, ensure that:\nThe code is well-designed The functionality is helpful to the code\u0026rsquo;s users Any UI changes are sensible and look good Any concurrent programming is done safely The code is no more complex than necessary The developer hasn\u0026rsquo;t implemented things they might need in the future The code has appropriate unit tests The tests are well-designed The developer has used clear names for everything The comments are clear, useful, and primarily explain why rather than what The code is properly documented The code follows our style guide Ensure you examine every line of code you are asked to review, look at the context, ensure you are improving code health, and praise the developer for good work.\nOriginal: https://google.github.io/eng-practices/review/reviewer/looking-for.html\nImplementation of Code Review Principles # To better implement code review, it\u0026rsquo;s necessary to first establish principles. You can adapt, remove, or add to the above principles based on your actual situation;\nSecond, technical leadership should actively promote the unified code review principles to developers;\nThird, the specific rules in the principles should be incorporated into the Pull Request process as much as possible through process control and automated checks.\nAlso, remember to be peaceful as a Reviewer!!! When reviewing code, avoid giving suggestions with a \u0026ldquo;teaching\u0026rdquo; tone, as this can easily backfire.\nHere are some incomplete practices for reference.\nProcess Control # Preventing any code without review from entering the main branch # Using Bitbucket as an example. GitHub and GitLab have similar settings.\nTurn on the option Prevent changes without a pull request in the branch permission settings. Of course, if necessary, you can add exceptions to this option, allowing the added person to commit code without a Pull Request.\nEnable the Minimum approvals option in Merge Check. For example, set Number of approvals = 1, so at least one Reviewer needs to click the Approve button to allow merging.\nAutomated Checks # Verify compilation and testing through the CI pipeline # Establish an automated build and test pipeline so that builds, tests, and checks can be automatically performed when creating a Pull Request. Jenkins\u0026rsquo; Multi-branch pipeline can meet this need.\nTurn on the Minimum successful builds option in Bitbucket\u0026rsquo;s Merge Check to verify build/test results, preventing any code that hasn\u0026rsquo;t passed build and tests from merging into the main branch.\nAdditionally, you can achieve this by writing your own tools or integrating other CI tools for checks, such as:\nAnalyze the commit history of the Pull Request to suggest Reviewers; Use Lint tools to check coding standards; Use REST APIs to check if commits need to be compressed to ensure a clear commit history; Use SonarQube to check the Quality Gate, etc. Implementing automated checks can help Reviewers focus their review efforts on the specific implementation of the code, leaving other aspects to the tools.\nConclusion # The effectiveness of code review is positively correlated with whether a team has a good technical atmosphere or whether there is technical leadership and influential senior engineers.\nIf most engineers in a team are high-quality, they will enjoy writing excellent code (or nitpicking). Conversely, if the team doesn\u0026rsquo;t value standards and only pursues short-term performance, it will only lead to accumulating more and more technical debt and products becoming increasingly worse. Welcome to leave comments and share your opinions or suggestions.\n","date":"2021-03-20","externalUrl":null,"permalink":"/en/posts/code-review/","section":"Posts","summary":"This article introduces Google\u0026rsquo;s code review principles and shares practical experience on how to effectively implement code review in a team, including process control and automated checks.","title":"Thoughts and Practices Based on Google's Code Review Principles","type":"posts"},{"content":"Today, when I tried to upgrade my team\u0026rsquo;s Jenkins server from Jenkins 2.235.1 to Jenkins 2.263.3, I met a problem that can not launch the Windows agent.\n[2021-01-29 23:50:40] [windows-agents] Connecting to xxx.xxx.xxx.xxx Checking if Java exists java -version returned 11.0.2. [2021-01-29 23:50:40] [windows-agents] Installing the Jenkins agent service [2021-01-29 23:50:40] [windows-agents] Copying jenkins-agent.exe ERROR: Unexpected error in launching an agent. This is probably a bug in Jenkins Also: java.lang.Throwable: launched here at hudson.slaves.SlaveComputer._connect(SlaveComputer.java:286) at hudson.model.Computer.connect(Computer.java:435) at hudson.slaves.SlaveComputer.doLaunchSlaveAgent(SlaveComputer.java:790) ... ... at java.lang.Thread.run(Thread.java:748) java.lang.NullPointerException at hudson.os.windows.ManagedWindowsServiceLauncher.launch(ManagedWindowsServiceLauncher.java:298) This issue had been raised in the Jenkins Jira project: JENKINS-63198 and JENKINS-63198\nThere is also a Windows Support Updates guide here that mentioned this problem.\nFinally, I fixed this problem by the following steps:\nUpdate windows-slaves-plugin to the lastest version 1.7 (fixes for Jenkins 2.248+) Then the error should be like this\n[2021-01-30 23:53:40] [windows-agents] Connecting to xxx.xxx.xxx.xxx Checking if Java exists java -version returned 11.0.2. [2021-01-30 23:53:47] [windows-agents] Copying jenkins-agent.xml [2021-01-30 23:53:48] [windows-agents] Copying agent.jar [2021-01-30 23:53:48] [windows-agents] Starting the service ERROR: Unexpected error in launching an agent. This is probably a bug in Jenkins org.jinterop.dcom.common.JIException: Unknown Failure at org.jvnet.hudson.wmi.Win32Service$Implementation.start(Win32Service.java:149) Caused: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedMethodAccessor219.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.kohsuke.jinterop.JInteropInvocationHandler.invoke(JInteropInvocationHandler.java:140) Also: java.lang.Throwable: launched here Then change jenkins-agent.exe.config file. remove or comment out this line \u0026lt;supportedRuntime version=\u0026quot;v2.0.50727\u0026quot; /\u0026gt; as below also do this for jenkins-slave.exe.config in case it also exists.\n\u0026lt;configuration\u0026gt; \u0026lt;runtime\u0026gt; \u0026lt;!-- see http://support.microsoft.com/kb/936707 --\u0026gt; \u0026lt;generatePublisherEvidence enabled=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;/runtime\u0026gt; \u0026lt;startup\u0026gt; \u0026lt;!-- this can be hosted either on .NET 2.0 or 4.0 --\u0026gt; \u0026lt;!-- \u0026lt;supportedRuntime version=\u0026#34;v2.0.50727\u0026#34; /\u0026gt; --\u0026gt; \u0026lt;supportedRuntime version=\u0026#34;v4.0\u0026#34; /\u0026gt; \u0026lt;/startup\u0026gt; \u0026lt;/configuration\u0026gt; Then try to Launch agent. If it still does not work and has this error message \u0026ldquo;.NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service\u0026rdquo;, you need to upgrade your .NET Framework.\nHere is a link for update .NET Framework.\nHopefully, this could help you to fix connect the issue of the Windows agent. Let me know in case of any questions.\n","date":"2021-02-11","externalUrl":null,"permalink":"/en/posts/jenkins-windows-agent-cannot-start/","section":"Posts","summary":"This article explains how to resolve the issue of Windows agents not starting after upgrading Jenkins, including the necessary steps to update the Windows Slaves plugin and modify configuration files.","title":"Jenkins upgrade issue \"Windows agents won't start\" workaround","type":"posts"},{"content":"DevOps 实际上是什么意思？🤔\nDevOps 是一种软件开发方法，涉及持续开发，持续测试，持续集成，部署和监视。这一系列过程跨越了传统上孤立的开发和运营团队，DevOps 试图消除它们之间的障碍。\n因此，DevOps 工程师基本上与 Development 和 Operations 团队合作。它是这两个主要部分之间的链接。\n概念与工具 # DevOps 包括诸如构建自动化、CI/CD、基础架构即代码等概念，并且有许多工具可以实现这些概念。由于这些工具数量众多，因此可能会造成混乱和压倒性的结果。\n最重要的是要了解概念，并为每个类别的学习找一种特定的工具。例如，当你已经知道什么是 CI/CD 并知道如何使用 Jenkins 时，也将很容易学习同类型的其他替代工具。\n接下来让就来看看学习 DevOps 需要掌握哪些技能。\n1）软件开发的概念 # 作为一名 DevOps 工程师，你不会直接对应用程序进行编程，但是当你与开发团队紧密合作以改善和自动化他们的任务时，你需要了解以下概念：\n开发人员的工作方式 他们正在使用哪个 git 工作流程 如何配置应用程序 自动化测试 2）操作系统 # 作为 DevOps 工程师，你负责准备在操作系统上部署应用程序的所需要的基础结构环境。并且由于大多数服务器是 Linux 服务器，因此你需要了解 Linux 操作系统，并善于使用命令行，所以你需要知道：\n基本的 Shell 命令 Linux 文件系统 管理服务器的基础知识 SSH 密钥管理 在服务器上安装不同的工具 3）网络与安全 # 你还需要了解网络和安全性的基础知识才能配置基础架构，例如：\n配置防火墙以保护应用程序 了解 IP 地址，端口和 DNS 的工作方式 负载均衡器 代理服务器 HTTP/HTTPS 但是，要在 DevOps 和 IT Operations 之间划清界线，你不是系统管理员。因此，在这里不需要高级知识，理解和了解基本知识就够了。IT 方面是这些 SysAdmins，Networking 或 Security Engineers 人的专长。\n4）容器化 # 随着容器成为新标准，你可能会将应用程序作为容器运行，这意味着你需要大致了解：\n虚拟化的概念 容器的概念 学习哪个工具？ Docker - 当今最受欢迎的容器技术 5）持续集成和部署 # 在 DevOps 中，所有代码更改（例如开发人员的新功能和错误修复）都应集成到现有应用程序中，并以自动化方式连续地部署到最终用户。因此，建立完整的 CI/CD 管道是 DevOps 工程师的主要任务和职责。\n在完成功能或错误修正后，应自动触发在 CI 服务器（例如 Jenkins ）上运行的管道，该管道：\n运行测试 打包应用程序 构建 Docker 镜像 将 Docker Image 推送到工件存储库，最后 将新版本部署到服务器（可以是开发，测试或生产服务器） 因此，你需要在此处学习技能：\n设置 CI/CD 服务器 构建工具和程序包管理器工具以执行测试并打包应用程序 配置工件存储库（例如 Nexus，Artifactory） 当然，可以集成更多的步骤，但是此流程代表 CI/CD 管道的核心，并且是 DevOps 任务和职责的核心。\n学习哪个工具？Jenkins 是最受欢迎的人之一。其他：Bamboo，Gitlab，TeamCity，CircleCI，TravisCI。\n6）云提供商 # 如今，许多公司正在使用云上的虚拟基础架构，而不是管理自己的基础架构。这些是基础架构即服务（IaaS）平台，可提供一系列服务，例如备份，安全性，负载平衡等。\n因此，你需要学习云平台的服务。例如。对于 AWS，你应该了解以下基本知识：\nIAM 服务-管理用户和权限 VPC 服务-你的专用网络 EC2 服务-虚拟服务器 AWS 提供了更多的服务，但是你只需要了解你实际需要的服务即可。例如，当 K8s 集群在 AWS 上运行时，你还需要学习 EKS 服务。 AWS 是功能最强大，使用最广泛的一种，但也是最困难的一种。\n学习哪个工具？AWS 是最受欢迎的一种。其他热门：Azure，Google Cloud，阿里云，腾讯云。\n7）容器编排 # 如前所述，容器已被广泛使用，在大公司中，成百上千个容器正在多台服务器上运行，这意味着需要以某种方式管理这些容器。\n为此目的，有一些容器编排工具，而最受欢迎的是 Kubernetes。因此，你需要学习：\nKubernetes 如何工作 管理和管理 Kubernetes 集群 并在其中部署应用程序 学习哪个工具？Kubernetes - 最受欢迎，没有之一。\n8）监视和日志管理 # 软件投入生产后，对其进行监视以跟踪性能，发现基础结构以及应用程序中的问题非常重要。因此，作为 DevOps 工程师的职责之一是：\n设置软件监控 设置基础架构监控，例如用于你的 Kubernetes 集群和底层服务器。 学习哪个工具？Prometheus, Grafana\u0026hellip;\n9）基础设施即代码 # 手动创建和维护基础架构非常耗时且容易出错，尤其是当你需要复制基础架构时，例如用于开发，测试和生产环境。\n在 DevOps 中，希望尽可能地自动化，那就是将“基础结构即代码（Infrastructure as Configuration）”引入其中。因此使用 IaC ，我们将使用代码来创建和配置基础结构，你需要了解两种 IaC 方式：\n基础设施配置 配置管理 使用这些工具，可以轻松地复制和恢复基础结构。因此，你应该在每个类别中都知道一种工具，以使自己的工作更有效率，并改善与同事的协作。\n学习哪个工具？\n基础架构设置：Terraform 是最受欢迎的一种。 配置管理：Ansible，Puppet，Chef。\n10）脚本语言 # 作为 DevOps 工程师就常见的工作就是编写脚本和小型的应用程序以自动化任务。为了能够做到这一点，你需要了解一种脚本或编程语言。\n这可能是特定于操作系统的脚本语言，例如 bash 或 Powershell。\n还需要掌握一种独立于操作系统的语言，例如 Python 或 Go。这些语言功能更强大，更灵活。如果你善于使用其中之一，它将使你在就业市场上更具价值。\n学习哪个工具？Python：目前是最需要的一个，它易于学习，易于阅读并且具有许多可用的库。其他：Go，NodeJS，Ruby。\n11）版本控制 # 上述所有这些自动化逻辑都作为代码编写，使用版本控制工具（例如Git）来管理这些代码和配置文件。\n学习哪个工具？Git - 最受欢迎和广泛使用，没有之一。\nDevOps Roadmap [2021] - How to become a DevOps Engineer\n","date":"2021-01-21","externalUrl":null,"permalink":"/posts/devops-roadmap-2021/","section":"Posts","summary":"本文介绍了成为DevOps工程师所需的技能和工具，涵盖软件开发、操作系统、网络安全、容器化、持续集成与部署等方面的知识。","title":"2021年DevOps工程师的学习路线","type":"posts"},{"content":"DevOps 已经走了很长一段路，毫无疑问，它将在今年继续发光。由于许多公司都在寻求有关数字化转型的最佳实践，因此重要的是要了解领导者认为行业发展的方向。从这个意义上讲，以下文章是 DevOps 领导者对 DevOps 趋势的回应的集合，需要在 2021 年关注。\n让我们看看他们每个人对来年的 DevOps 有何评价。\n迁移到微服务将成为必须 —— Wipro Limited 首席 DevOps 工程师 “从整体迁移到微服务和容器化架构对于所有公司的数字化转型之旅都是必不可少的，它不再是一个选择或选项。这就是Kubernetes 的采用率将上升的地方，当组织迁移到多重云时，Terraform 将成为实现基础架构自动化的最终选择。”\nHybrid将成为部署规范 —— JFrog 开发人员关系 VP “2020年将加速远程工作，加快向云的迁移，并将 DevOps 从最佳实践转变为每个业务的重要组成部分。随着我们进入2021年，该行业将在多个方面拥抱Hybrid。首先，企业将完全采用混合型劳动力，将远程工作和现场团队协作的优势相结合。 其次，商业模式将变得混合，例如将虚拟规模与本地网络合并的会议。最终，随着公司对堆栈进行现代化以利用云原生技术的优势，混合将成为部署规范，但要意识到并非所有事物都可以迁移到外部。2021年的赢家将是拥抱业务，模型和产品混合的公司。”\nDataOps将蓬勃发展 - 乐天高级 DevOps 工程师 “DataOps 肯定会在 2021 年蓬勃发展，COVID 可能会在其中发挥作用。由于 COVID 和 WFH 的情况，数字内容的消费量猛增，这要求自动缩放和自我修复系统的自动化达到新水平，以满足增长和需求。\n到目前为止，DevOps 设置的系统仅用于日志记录，监视和警报（ELK/EFK 堆栈，Prometheus/Grafana/Alertmanager等）。现在，DevOps 现在应该加强并使用可用数据和指标来 产生有价值的见解，学习并应用机器学习模型来预测事件或中断，开发从数据中学习自身并预测能力的自动化以改进预算计划。许多人已经开始对此部分调用 MLOps/AIOps。”\n弹性测试将成为主流 —— Neotys 产品负责人 从我的角度来看，可观察性，性能测试和弹性测试之间的交叉点将成为主流。 随着 AWS 和 Google 等 WW 领导者最近发布的 Ops 问题，以及各个领域的数字化转型都在加速发展，市场将逐渐意识到，公共或私有云形式提供的无限可扩展性是不够的。” - Neotys 产品负责人 Patrick Wolf\nGitOps 将成为常态 —— 梅西百货的首席架构师 “一个“构建，拥有，拥有”的开发过程需要开发人员知道和理解的工具。GitOps 是 DevOps 如何使用开发人员工具来驱动操作的名称。\nGitOps 是一种进行持续交付的方法。 更具体地说，它是用于构建统一部署，监视和管理的 Cloud Native 应用程序的操作模型。 它通过将 Git 用作声明性基础结构和应用程序的真实来源来工作。 当在 Git 中推送和批准提交时，自动化的 CI/CD 管道将对你的基础架构进行更改。它还利用差异工具将实际生产状态与源代码控制下的生产状态进行比较，并在出现差异时提醒你。GitOps 的最终目标是加快开发速度，以便你的团队可以安全可靠地对 Kubernetes 中运行的复杂应用程序进行更改和更新。” -梅西百货（Macy\u0026rsquo;s）首席建筑师 Soumen Sarkar\n将会有更多的迁移到无服务器 —— ADP Lifion 的站点 SRE 经理 “2021 年将是注视更多迁移到无服务器的一年。如果容器和业务流程是 Z 代.. 那么无服务器的实时负载将是 Gen+ .. 仅在你使用时使用和付款可能看起来是一样的.. 但请考虑运行基于 k8s pod 的微服务，以便在需要时在无服务器上运行相同的服务。” - ADP Lifion 网站可靠性工程经理 Shivaramakrishnan G\nNoOps 出现 —— ClickIT Smart Technologies 的 CEO “我希望出现更多托管服务，并减少我们的 DevOps 运营并减少客户的运营支出。更多无服务器应用程序，更多无服务器服务，例如 Aurora 无服务器，Fargate，Amazon S3 和无服务器静态网站。数据中心中的 Amazon ECS/EKS（新版本 re：invent 2020）以及云管理服务，可让你减少数据中心的维护和开发。同样，将更多云原生的原理和功能移植到数据中心，例如。亲戚。” - ClickIT Smart Technologies 首席执行官 Alfonso Valdes\nBizDevOps 将大放异彩 —— Petco 的 DevOps 经理 “在架构和公司层次结构方面朝着成本优化的方向发展-随着业务的发展，DevOps 的价值不断提高。\n专注于灵活的，云原生的架构和工具，这些功能一旦具备了“大佬”的能力，就可以打包成小型企业使用的包装（Snowflake 或 Hazelcast 与 Oracle/Teradata）\nFaaS 才刚刚起步（无服务器，Lambda 等）- 运营问题正在得到解决，人们正在意识到潜力。”\n基础设施即代码（IaC）的地位将更高 —— 沃尔沃高级解决方案架构师 “基础架构即代码（IaC）：云中 DevOps 的核心原则。你的基础架构本地或云中的服务器，网络和存储设备（定义为代码）。这使公司可以自动化并简化其基础架构。 IaC 还提供了一个简单的基础结构版本控制系统，该系统可让团队在发生灾难性故障时回退到“有效的最后配置”。这意味着可以快速恢复并减少停机时间。”\n自动化和混乱工程变得非常重要 —— 直布罗陀印度开发中心的集团开发经理 “一切都是自动化的-构建，部署，测试，基础架构和发布。\n具有所需质量门的生产线。更快，可重复，可自定义和可靠的自动化是任何项目成功的关键。混沌工程-在当今混合基础设施世界中非常关键的方面。系统行为和客户体验紧密结合在一起，你越早对其进行测试，就越能为客户提供更好的体验。”\n云原生方法将被标准化 —— Ben Sapp “由于云空间已经真正地发展起来（最近十年左右），并且容器化已成为规范，所以一切都非常标准化，几乎就像大型机时代一样。\n当然，会有趋势和赚钱的机会。但是我不知道下一个大破坏者是什么。现在的一切基本上都与五年前的最佳做法相同，但更加可靠。我想越来越多的人将继续从宠物转移到牛身上，剩下诸如 Ansible 和 puppet 之类的工具仅用于打包程序和云 init 来构建容器主机。\nimo 是软件开发的黄金时代。 DevOps 和云原生方法已经实现了许多目标。管道，托管，存储，负载平衡……这些都在 5 分钟之内解决了。”\n安全将成为重中之重 —— CloudSkiff “从 DevSecOps 角度绝对跟踪基础设施中不受控制的变化。作为代码的基础架构很棒，但是有太多可移动的部分：代码库，状态文件，实际云状态。事情倾向于漂移。这些更改可能有多种原因：从开发人员通过 Web 控制台创建或更新基础架构而不告知任何人，到云提供商方面不受控制的更新。处理基础架构漂移与代码库之间的挑战可能会充满挑战。” - CloudSkiff\nChaos Engineering 将变得越来越重要 —— International Technology Ventures 的 CTO “在更多组织中的 DevOps 规划讨论中，混沌工程将变得越来越重要（且更常见）。大多数组织通常不执行混沌工程学（Chaos Engineering），即在生产中对软件系统进行实验以建立对系统抵御动荡和意外情况能力的信心。\n如果我们在传统的五个成熟度模型框架内考虑 DevOps，那么 Chaos Engineering 将是第 4 或第 5 级学科，将包含在 DevOps 实践的保护范围内。正如将单独的测试/质量保证小组的传统角色纳入 DevOops 的学科一样，Chaos Engineering 也应如此。”\n更关注即时日志以快速验证成功或失败 —— ADESA 平台稳定性总监 “在后期部署中使用日志来验证发布是否成功，或存在严重错误。人们需要建立的最大联系是定义手动流程，然后实现自动化的巨大飞跃。一键部署，即时日志可快速验证成功或失败，然后触发回滚。随之而来的是复杂性以及跨服务依赖性，是否可以回滚某些内容，或者是否需要对其他服务进行进一步测试。想象一下 100 种微服务（又称管道，甚至还有 100 个容器）。\n作为一项，我总是庆祝成功的回滚，因为它不会对服务产生影响，而且是成功的。” -ADESA平台稳定性总监Craig Schultz\nDevSecOps 将成为 DevOps 的默认部分 —— JFrog 的 DevOps 架构师 DevSecOps 的 “Sec” 部分将越来越成为软件开发生命周期中不可或缺的一部分，真正的安全性 “向左移动” 方法将成为新的规范，CI/CD 管道中的专用安全性步骤将更少从开发人员的 IDE 到依赖关系和静态代码分析，安全和自动识别和采取措施将是所有流程步骤的一部分。在没有适当（自动？）解决这些问题的情况下，不会发布软件组件。真正的安全问题是免费软件。”\n希望你喜欢我们对 DevOps 趋势的专家综述，并在 2021 年关注。如果你认为这里缺少应考虑的内容，请在评论中分享你的观点。\n原文 15 DevOps Trends to Expect in 2021 的翻译。\n","date":"2021-01-21","externalUrl":null,"permalink":"/posts/devops-trends-2021/","section":"Posts","summary":"本文介绍了 2021 年 DevOps 领域的主要趋势，包括微服务架构、无服务器计算、Kubernetes 的普及以及 DevSecOps 的兴起。","title":"预测 2021 年的 DevOps 趋势","type":"posts"},{"content":" This is just a note to myself about the difference between Jenkins result and currentResult.\nDeclarative Pipeline # Here is a test code from this ticket JENKINS-46325\npipeline { agent any stages { stage (\u0026#39;Init\u0026#39;) { steps { echo \u0026#34;Init result: ${currentBuild.result}\u0026#34; echo \u0026#34;Init currentResult: ${currentBuild.currentResult}\u0026#34; } post { always { echo \u0026#34;Post-Init result: ${currentBuild.result}\u0026#34; echo \u0026#34;Post-Init currentResult: ${currentBuild.currentResult}\u0026#34; } } } stage (\u0026#39;Build\u0026#39;) { steps { echo \u0026#34;During Build result: ${currentBuild.result}\u0026#34; echo \u0026#34;During Build currentResult: ${currentBuild.currentResult}\u0026#34; sh \u0026#39;exit 1\u0026#39; } post { always { echo \u0026#34;Post-Build result: ${currentBuild.result}\u0026#34; echo \u0026#34;Post-Build currentResult: ${currentBuild.currentResult}\u0026#34; } } } } post { always { echo \u0026#34;Pipeline result: ${currentBuild.result}\u0026#34; echo \u0026#34;Pipeline currentResult: ${currentBuild.currentResult}\u0026#34; } } } Output\nInit result: null Init currentResult: SUCCESS Post-Init result: null Post-Init currentResult: SUCCESS During Build result: null During Build currentResult: SUCCESS [test-pipeline] Running shell script + exit 1 Post-Build result: FAILURE Post-Build currentResult: FAILURE Pipeline result: FAILURE Pipeline currentResult: FAILURE ERROR: script returned exit code 1 Finished: FAILURE Scripted Pipeline # Here is a test code from cloudbees support case\nExample that forces a failure then prints result:\nnode { try { // do something that fails sh \u0026#34;exit 1\u0026#34; currentBuild.result = \u0026#39;SUCCESS\u0026#39; } catch (Exception err) { currentBuild.result = \u0026#39;FAILURE\u0026#39; } echo \u0026#34;RESULT: ${currentBuild.result}\u0026#34; } Output:\nStarted by user anonymous [Pipeline] Allocate node : Start Running on master in /tmp/example/workspace/test [Pipeline] node { [Pipeline] sh [test] Running shell script + exit 1 [Pipeline] echo RESULT: FAILURE [Pipeline] } //node [Pipeline] Allocate node : End [Pipeline] End of Pipeline Finished: FAILURE Example that doesn’t fail then prints result:\nnode { try { // do something that does not fail echo \u0026#34;Im not going to fail\u0026#34; currentBuild.result = \u0026#39;SUCCESS\u0026#39; } catch (Exception err) { currentBuild.result = \u0026#39;FAILURE\u0026#39; } echo \u0026#34;RESULT: ${currentBuild.result}\u0026#34; } Output:\nStarted by user anonymous [Pipeline] Allocate node : Start Running on master in /tmp/example/workspace/test [Pipeline] node { [Pipeline] echo Im not going to fail [Pipeline] echo RESULT: SUCCESS [Pipeline] } //node [Pipeline] Allocate node : End [Pipeline] End of Pipeline Finished: SUCCESS ","date":"2021-01-14","externalUrl":null,"permalink":"/en/posts/jenkinsresult-vs-currentresult/","section":"Posts","summary":"This article explains the difference between \u003ccode\u003eresult\u003c/code\u003e and \u003ccode\u003ecurrentResult\u003c/code\u003e in Jenkins pipelines, including examples of how they behave in both declarative and scripted pipelines.","title":"What's the difference between result and currentResult in Jenkins?","type":"posts"},{"content":"Recently our Bamboo server has an error when connecting to the Windows build system.\nSome errors like: Can not connect to the host XXXX 22 port.\nI log into the build machine find port 22 with the below command\nnetstat -aon|findstr \u0026#34;22\u0026#34; but not found port 22 is inbound.\nInbound port 22 # There\u0026rsquo;s a lot of articles will tell you how to open ports on Windows. see this one\nhttps://www.windowscentral.com/how-open-port-windows-firewall\nBut still not works for me # My problem is when I inbound port 22, execute the above command still can\u0026rsquo;t see the port 22 is listening.\nSo I install the Win32-OpenSSH, this will lanch two services, then port 22 is listening.\nHere are the wiki page about download and installation https://github.com/PowerShell/Win32-OpenSSH/wiki/Install-Win32-OpenSSH\n","date":"2021-01-12","externalUrl":null,"permalink":"/en/posts/how-to-open-port-22/","section":"Posts","summary":"This article explains how to open port 22 on Windows and ensure it is listening, which is necessary for SSH connections. It includes steps to install OpenSSH and configure the firewall.","title":"How to open port 22 and make it listening on Windows","type":"posts"},{"content":"","date":"2021-01-12","externalUrl":null,"permalink":"/en/tags/openssh/","section":"Tags","summary":"","title":"OpenSSH","type":"tags"},{"content":"As I have management our team\u0026rsquo;s git repositories for more than two years, and as my daily work using Bitbucket, so I\u0026rsquo;ll take it as an example.\nHere are some settings recommend you to enable.\nSet Reject Force Push Set Branch Prevent deletion Set tags for each hotfix/GA releases Merge Check -\u0026gt; Minimum approvals (1) Yet Another Commit Checker Reject Force Push # This is the first setting I highly recommend you/your team to open it. if not, when someone using the git push -f command to the git repository, you may lost commits if his local code is old then remote repository.\nyou have to recover the lost commits manually, I have heard 3 times around me. so enable the hook: Reject Force Push ASAP!\nSet Branch Prevent deletion # If some branch is very important, you don\u0026rsquo;t want to lost it, set Branch Prevent deletion ASAP!\nSet tags for each hotfix/GA releases # For each hotfix/GA release, highly recommend create tags after release.\nMerge Check # Pull Request is a very good workflow process for each team. in case of commits were merged directly without review, we enable Minimum approvals (1).\nSo, every branch wants to be merged to the main branch MUST add a reviewer (not allow yourself) and the review must click the Approve button otherwise the merge button is disabled.\nYet Another Commit Checker # This is a very powerful feature. it helps to standardize commit messages and create a branch.\nMore details about this tool you can refer to this introduction\nI have a Chinese article to describe how to use Yet Another Commit Checker implement. if you interest it, you can see the post here\n","date":"2021-01-12","externalUrl":null,"permalink":"/en/posts/git-repository-settings/","section":"Posts","summary":"Provides a list of recommended settings for Bitbucket and GitHub repositories, including enabling force push rejection, branch protection, tag management, merge checks, and commit message standards.","title":"These settings in Bitbucket/GitHub recommends enable","type":"posts"},{"content":"This post just a note to myself and it works on my environment. it has not been widely tested.\nEnable git sparse-checkout # Just in my case, I cloned a git repo exists problems on the Windows platform with some folder, in order to work on the Windows platform we get a work around solution as following:\nCase 1: when you not already cloned a repository\nmkdir git-src cd git-src git init git config core.sparseCheckout true echo \u0026#34;/assets/\u0026#34; \u0026gt;\u0026gt; .git/info/sparse-checkout git remote add origin git@github.com:shenxianpeng/shenxianpeng.git git fetch git checkout master Case 2: when you already cloned a repository\ncd git-src git config core.sparseCheckout true echo \u0026#34;/assets/\u0026#34; \u0026gt;\u0026gt; .git/info/sparse-checkout rm -rf \u0026lt;other-file/folder-you-dont-need\u0026gt; git checkout Disable git sparse-checkout # git config core.sparseCheckout false git read-tree --empty git reset --hard ","date":"2021-01-11","externalUrl":null,"permalink":"/en/posts/git-sparse-checkout/","section":"Posts","summary":"This article explains how to enable and disable git sparse-checkout, including examples of how to configure sparse-checkout for specific directories and how to reset it.","title":"git sparse-checkout enable and disable","type":"posts"},{"content":"","date":"2021-01-06","externalUrl":null,"permalink":"/en/tags/codesign/","section":"Tags","summary":"","title":"CodeSign","type":"tags"},{"content":"Many programmers may have encountered issues with the Verisign Timestamp server, http://timestamp.verisign.com/scripts/timstamp.dll, becoming unavailable when performing code signing, especially at the start of the new year. The following error may have appeared:\nThe reason is that the default timestamp server for code signing is no longer accessible.\nA response on Stack Overflow post provided an answer from Verisign Support:\nTheir authentication services were sold to Symantec, and the current service provider is Digicert. This server has been deprecated.\nThey suggest contacting Digicert or finding free timestamp servers online.\nThis was a user\u0026rsquo;s response; I didn\u0026rsquo;t find an official response online, so I decided to send an email to formally confirm. I received a reply shortly after:\nSimilar to the previous reply: Years ago, Verisign\u0026rsquo;s authentication and certificate business was sold to Symantec and has now transitioned to Digicert. You will need to work with your current provider for support or an updated timestamp URL. Please visit http://www.digicert.com for more information.\nNow that\u0026rsquo;s confirmed, let\u0026rsquo;s confidently proceed with changing to a new timestamp server.\nI found that Digicert\u0026rsquo;s timestamp server is http://timestamp.digicert.com. After switching to the new timestamp server, the digital signature process resumed normally.\nIn addition to the Digicert URL above, the following URLs can be used as replacements:\nhttp://timestamp.comodoca.com/authenticode http://timestamp.globalsign.com/scripts/timestamp.dll http://tsa.starfieldtech.com However, I didn\u0026rsquo;t choose any of these; I opted for the official timestamp service and kept it as a backup. What if the \u0026ldquo;official\u0026rdquo; provider gets sold again someday?\n","date":"2021-01-06","externalUrl":null,"permalink":"/en/posts/verisign-server-not-working/","section":"Posts","summary":"This article describes how to resolve issues with the unavailable Verisign timestamp server, providing alternative timestamp server addresses to help developers successfully complete code signing.","title":"Resolving the Unavailable Code Sign Default Timestamp Server http://timestamp.verisign.com/scripts/timstamp.dll","type":"posts"},{"content":"","date":"2021-01-06","externalUrl":null,"permalink":"/en/tags/timestamp/","section":"Tags","summary":"","title":"Timestamp","type":"tags"},{"content":" 前言 # 自 2020 年因疫情开始，越来越多的 IT 公司都因不得不在家办公从而彻底转为 WFH（Work From Home） 公司，因此对于 IT 从业者来说，工作机会今后将会是全球性的。\n如果你有意想进入一个跨国公司工作，想与世界各地的人在一起工作，那么就不能仅仅的关注国内的这些大厂，要将眼光放眼到全世界，看看这些耳熟能详的公司对于工程师的职位要求有哪些。\n今天就先来看看 DevOps 岗位的需求是什么样的，了解这些，一来可以帮助我们在2021 年树立学习方向，而来如果你有意向去这些公司，了解并提早做准备才能有机会获取你想要的岗位。\n由于这些职位的介绍和要求会很长，因此我就先说结论。\n主要技能 # 国外很多公司他们使用的云服务商主要是 AWS，因此熟悉和使用 AWS 熟练使用 DevOps 工具，如 Jenkins, Ansible（Chef）, Git 等 Docker 和 Kubernetes 是每个想从事 DevOps 需要掌握的 熟悉操作系统，至少要 Linux 大多数都是要求 Python 很熟练，高级一些的岗位会要求熟悉 Go, Java 语言 最后，乐于学习，积极主动，具有创造性思维是每个 DevOps 最重要的特质，因此新的技术和工具层出不穷，我们需要保持和新进同行 具体的职位要求细节，请看后面的职位介绍吧 \u0026hellip;\nZOOM：DevOps Engineer # 工作地点：San Jose, CA\n岗位链接 https://www.linkedin.com/jobs/view/2337488435\nZOOM 不用过多介绍了，2020 年因为疫情，业务极具增长的一家视频会议的公司。也多次被推荐为最佳雇主，以及最佳的工作场所。\n岗位职责\n设计、部署、监控 ZOOM 平台服务 提升 ZOOM 平台服务从规划到上线的全生命周期策略 与跨职能干系人紧密合作，分析和解决复杂的生产问题 构建弹性和可扩展的服务基础设施，以适应基于区域的数据中心 优化当前CI/CD流程，简化服务器配置和部署的自动化工作 支持测试自动化和部署策略，以优化服务性能，确保产品质量 职位要求\n本科/硕士(CS或相关专业优先) 至少 5 年 DevOps 或 SRE 经验 对 AWS 基础架构(如DynamoDB, S3, Nginx, CloudWatch)，Linux 批处理命令，ELK 堆栈和容器编排(如 K8s, Docker)有深入的了解 熟练使用 Jenkins, Ansible 和 Git 仓库 能够监控，调试和自动化日常任务 熟悉云基础设施技术和基于云的测试自动化解决方案 乐于学习，积极主动，具有创造性思维 苹果：DevOps Engineer (CI/CD) # 地点：Cupertino, CA\n岗位链接：https://www.linkedin.com/jobs/view/2346233023/?eBP=JOB_SEARCH_ORGANIC\u0026amp;recommendedFlavor=COMPANY_RECRUIT\u0026amp;refId=RA%2BGKQ6QNrbwK1PHj3opBQ%3D%3D\u0026amp;trackingId=%2FCHeWxW%2FHm%2BULb2XD5Yyyg%3D%3D\u0026amp;trk=flagship3_search_srp_jobs\n我们的团队为苹果的应用程序运行 CI/C D管道，支持全球成千上万的开发者。我们对不断改进软件开发生命周期的方式充满热情，并为大规模工程问题重新发明前沿解决方案开辟边界。作为团队的一员，你将开发应用程序和微服务来构建和改进我们的下一代 CI/CD 管道。\n职位要求\n精通 Python 编程 有 Unix/Linux 平台工作经验 熟练使用 DevOps 工具，如 Chef, Docker, Kubernetes 具有软件开发过程的经验，如构建、单元测试、代码分析、发布过程和代码覆盖 有 CI/CD 流程和平台经验，如 Jenkins 较强的分析和解决问题的能力 优秀的书面和口头沟通能力，能够与大型开发团队合作 工作内容\n开发和维护应用开发团队的 CI/CD 流程 跨团队合作，改进产品的构建、集成和发布流程 开发和维护应用服务 CI/CD 管道的服务和集成 维护和管理包含 Linux/Unix/macOS 系统的动态构建场 能够参与下班后的轮班工作 教育和经验\n计算机科学或同等学历 其他要求\n有使用 Django/Flask 开发基于 Python 的微服务的经验 熟悉GitHub开发流程 有 Jenkins 管理和扩展的经验 具备扩展 CI/CD 系统和微服务的经验 有 Xcode 和开发 iOS, macOS 和其他苹果平台应用的经验 Oracle：Software Developer 4 for DevOps # 工作地点：Pleasanton, CA 职位链接：https://www.linkedin.com/jobs/view/2351849053/?eBP=JOB_SEARCH_ORGANIC\u0026amp;recommendedFlavor=COMPANY_RECRUIT\u0026amp;refId=RA%2BGKQ6QNrbwK1PHj3opBQ%3D%3D\u0026amp;trackingId=XQXcRFZiVeqSGJApqI4Grw%3D%3D\u0026amp;trk=flagship3_search_srp_jobs\n作为 Oracle Analytics Cloud 团队的开发人员，你将有机会在广泛分布的多租户云环境中构建和运行一套大规模集成云服务。你将需要在分布式系统方面有丰富的经验，熟练地解决大型数据系统中的实际挑战，对如何构建具有弹性，高可用性，可扩展性的解决方案有深刻的了解，并且需要具有可靠的设计，开发和交付回溯能力，终端系统。\n职位要求\n计算机科学，计算机工程学士学位或更高学位，或具有8年以上应用经验的同等学历 关于 Oracle Cloud Infrastructure 的动手经验–用户/策略管理，创建资源，配置和部署软件，自动化端到端到端供应 精通 Python 编程技巧 精通关系数据库，擅长 SQL 有使用容器技术的经验，尤其是 Docker 在 Linux 环境中的经验 有配置 Git 等源代码系统的经验 成为 CI/CD 和开发最佳实践的拥护者，尤其是在自动化和测试方面 具有构建高性能，弹性，可扩展性和精心设计的系统的经验 良好的沟通能力，能够清楚地阐明工程设计 敏捷软件开发经验 所需技能：\n熟悉 Kubernetes，Mesos 等（编排） 身份管理，安全性，网络等 NVIDIA DevOps Engineer # 工作地点：Santa Clara, CA 职位链接：https://www.linkedin.com/jobs/view/2249840303/?alternateChannel=search\u0026amp;refId=Y17t502dpy%2FAU43SIQPRIA%3D%3D\n工作内容\n为多个内部服务开发和支持暂存和生产环境 与全球各地的各个团队进行协调，以促进和改进 CI/CD 的实践 使用内部 Kubernetes 和商业云帮助构建服务 你将在团队中灵活地做出技术决策 岗位要求\n计算机工程，计算机科学或相关技术学科的理学学士学位或同等工作经验 6年以上经验 出色的脚本语言编程和调试技能 成熟的 Linux 系统管理经验（强烈建议使用 CentOS 和 Ubuntu） 对 CI/CD管道和工具有很好的了解（GitLab 或类似的工具） 数据库管理和性能调优经验 具有容器（Docker，Kubernetes）Web服务（SOAP/REST）和可扩展存储（HDFS/Ceph）的经验 从人群中脱颖而出的方法\n你具有 Python 编程方面的专业知识 熟悉 Windows 系统管理是一个巨大的优势 你曾经使用过云服务（AWS，Azure等） Cisco CX Cloud - Senior DevOps Release Engineer # 工作地点：San Jose, CA 职位链接：https://www.linkedin.com/jobs/view/2310027987/?alternateChannel=search\u0026amp;refId=COCpYBYNZm9w483i3SERXg%3D%3D\u0026amp;trackingId=FpZJedsrgIhNxlm961X75Q%3D%3D\n工作范围\n你可以跨 CX E\u0026amp;PI 和更广泛的客户体验（CX）组织进行协作，以启用我们的DevOps Release转换功能 你将以思想和实践的开发人员的身份开展工作，他不仅会认识并建立我们的组织实力，还将为团队带来新的观点和想法 与多元化的包容性软件工程师团队合作，在发布过程中实现端到端的DevOps，以确保加速的吞吐量和系统可靠性 与安全，应用程序和基础架构团队合作，从开发，测试，阶段，预生产和生产环境中检测简化的变更生命周期 运用你的经验将基础架构实现为代码，转换发布管道，并在高性能 DevOps 管道上以 NoOps 的心态部署到生产中 促进纪律严明的方法以确保部署的可预测性和质量 基准化和优化关键运营指标，确保我们符合运营SLA 测试并控制安全性，可靠性，可伸缩性和性能标准 积极寻求持续改进和学习的机会 岗位要求\n至少 10 年以上的设计，架构，启用和执行 DevOps 管道 以诚信，信任和透明的态度进行道德领导 具有在面向敏捷 DevOps 的团队和文化中担任高级主管的经验，并使用现代框架，技术，将基础架构用作代码工具的 DevOps 实践 将自动进行配置管理（使用Ansible等工具） 精通 DevOps，Blue/Green 和 Canary 部署以及云工程中的最佳实践 已经证明了微服务，作为代码的基础架构，监视和日志记录方面的专业知识 将编写代码，列出在 AWS 上运行的框架和架构 与跨职能团队合作时，你在复杂产品体系结构的持续集成和连续部署方面拥有深厚的专业知识 具有出色的能力，可以为支持全球客户群的云原生 SaaS 应用程序实现高可用性，灾难恢复，监视和警报，自动化以及持续的高性能 具有 Terraform 和 CloudFormation 模板的专业知识 在代码管道解决方案方面拥有深厚的专业知识 使用 Docker 和 Kubernetes 管理容器 使用 Jenkins 等平台管理构建管道 具有出色的组织和人际关系技巧，可以促进协作；在多功能矩阵管理环境中可以很好地工作 应该能够为 AWS 编写 Terraform 模块 已经展示了领域的专业知识，可以使用 SaaS 或消费者云软件公司的 DevOps 团队使用AWS之类的技术执行发布管道转换 在整合复杂的，跨公司的流程和信息策略方面拥有丰富的经验，包括技术规划和执行以及策略制定和维护 对临时性工作负载具有丰富的经验 体验 Kubernetes 有 Atlantis 知识者优先+ 在 DevOps 执行或工程职位上有10年以上的经验 具有丰富的经验，可在云体系结构上实现发布管道，并符合代码和临时工作负载 将基础架构作为代码和将配置作为代码技术的丰富经验 使用 Terraform，Kubernetes 和 Docker 等工具构建，扩展和保护应用程序云基础架构的丰富经验 在 GitHub 中管理多个代码库的丰富经验 在大型公共云中构建 Cloud Native 应用程序的丰富经验 具有实施可观察性，应用程序监视和日志聚合解决方案的经验 与跨职能团队合作并指导跨部门团队提供激情和经验，以提供受 DevOps 启发的解决方案 为大型数据管道开发高度可扩展的云原生架构 将异构构建，测试，部署和发布活动转换为在AWS上运行的同类企业级 DevOps 实施 交付和创作持续集成构建和部署自动化方面的丰富经验，例如 CI/CD 管道对 CloudFormation 模板，Ansible 和类似框架等解决方案的丰富经验 与应用程序和基础架构部署相关的 AWS 平台的丰富经验 能够在 Python，Go和 Java 中流畅开发 ","date":"2021-01-03","externalUrl":null,"permalink":"/posts/2021-devops-job-requirement/","section":"Posts","summary":"了解国外 IT 公司对 DevOps 工程师的技能要求，帮助你在 2021 年树立学习方向，获取理想岗位。","title":"2021 年国外 IT 公司对于 DevOps 工程师的要求有哪些？","type":"posts"},{"content":"时间飞快，马上就迎来了 2021 年，又到了总结自己这一年来的所有的经历和收获的时候了。\n2020 是非比寻常的一年，对于每个人不论是生活还是工作，都因为新冠病毒被影响着。\n但生活总归要继续。刷微博、抖音各种社交媒体是一天；工作学习也是一天。我是那种一段时间感觉没进步就特别恐慌的人，做完后者总会让我感觉更安心一些。\n回顾 2020 # 关于工作 # 对于 2020 年，我一直努力想要去做到最好，年尾回头看看，还算满意吧。\n在年底写下工作工作总结的时候，发现自己这一年确实做了很多工作。除了做好构建和发布的本职工作之外，做了很多改进流程和提高效率工作，捡一些重要的来说：\n通过 Jenkins Shared Libraries 和 Multi-branch Pipeline 完成了无人值守的自动化构建，为团队和我节省了大量的时间；同时也通过验证 PR 构建和测试，提高签入代码的稳定和质量。\n输出文章 每个 Jenkins 用户都应该知道这三个最佳实践\n推动团队迁移到企业级的 Artifactory 来下载和存储构建，从而改进 CI/CD 的效率和无人值守的能力。为更好的自动化下载、安装、测试等提供方便。\n输出文章 写给那些想使用 JFrog Artifactory 管理制品的人\n使用 Python 来做了很多关于 Jira, BitBucket 的集成，通过自动化提高效率。另外还发布了一个 Python 项目 UOPY。\n输出文章 在 GitHub 上发布一个 Python 项目需要注意哪些\n落地了 Git 提交信息和分支创建规范，统一了 Git 提交信息和分支创建规范。\n输出文章 程序员自我修养之Git提交信息和分支创建规范\n学习和使用了一些新工具，比如 Ansible playbook、ELK、JaCoCo 等并应用到项目中。\n输出文章 初识 Ansible，JaCoCo 实践\n作为 DevOps/软件工程师还需要有良好的表达能力，否则你做的东西再好，但无法很清晰的跟同事及领导分享出来也是事倍功半。尤其像我在外企，还需要用英文去做分享。\n另外，我想通过坚持阅读和实践，让自己在技术上有比较大的提高。有一段时间我坚持的比较好，完成了好几本技术书籍的阅读，但好的习惯就怕被打破，一旦被打破就很难坚持了。之前一般都是中午午休的时间会阅读半个小时以上的技术书，后来常常因为中午要去新房盯装修，加上每天要做的事情很多（工作、学英语、阅读、装修）的时候，很多时候往往是事情做了，但效果并没有达到预期。\n分享上 # 2020 年博客上一共更新了 41 篇文章，在公众号『DevOps攻城狮』上面一共更新了 26 篇文章。\n这个数量和质量跟一些其他的技术公众号真的没法比，但对于我个人：一个不以写作为生、只写原创文章的攻城狮，只要能不间断的输出，达到这个数量我已经比较满意了。\n而且在年初有一个清华大学的编辑老师找到我，想让我出一本书，这真是让我受宠若惊。我知道自己几斤几两，但能够受到邀请，并寄给我合同，就已经是对我写作的最大鼓励了。\n当时打算写一本《Jenkins 2实践指南》，如果签了合同，我就需要投入至少一年的业余时间死磕自己才有可能完成。\n最后考虑到自己当时的状况：优先做好工作；其次还有比Jenkins更重要的技术我需要去学习；另外周末要装修等一系列的事情\u0026hellip;我最终放弃了这个机会。\n我知道自己还处在输入比输出更加迫切的阶段，我想只要心中的小火苗还在，但行好事，莫问前程。\n工作 flag # 对于个人年终总结，不仅可以回顾自己过去的一年做了哪些，哪些没做好，有什么可以改进的，还能通过回顾和总结为新年立一个 flag。\n2021 我希望自己能更加专注，利用好时间，更加有效的工作和学习。\n除了做好工作，最重要的是提高英语口语和深入学习技术，其次才是输出。\n光有 flag 不行，还要落到具体的行动上。\n提高英语口语 # 🚩 2021 继续坚持至少每天半小时口语打卡练习；另外每周一次 50 分钟的与外教的口语交流。希望能够脱稿进行英文分享，同时能通过英语托业考试。\n深入学习技术 # 我现在学习技术的方式有这么几种：直接去官网阅读英文文档；如果读完还是没搞明白，就去 Linkedin Learning 或是 Udemy 上去搜相应的技术，不但学习了技术，同时也练习了英文听力；如果需要深入系统的去学习一门编程语言以及一些底层技术，会去读一些经典技术书籍。\n🚩 2021 年坚持每天也至少半个小时来阅读技术，一个月至少读完一本技术书籍并且有输出。\n生活 flag # 生活上希望家人和朋友都能身体健康，生活幸福。\n🚩 财务稳定，最好能增长；自己能够健康生活，减重 10 斤。\n最后，不时的来回顾自己 2021 年的 flag，时不时的看看自己是否偏离了最初的方向。2021 我来了！\n过去的年终总结 # 2019 年终总结 2018 从测试到开发的五个月\n","date":"2020-12-31","externalUrl":null,"permalink":"/misc/2020-summary/","section":"Miscs","summary":"\u003cp\u003e时间飞快，马上就迎来了 2021 年，又到了总结自己这一年来的所有的经历和收获的时候了。\u003c/p\u003e","title":"2020 年终总结","type":"misc"},{"content":"","date":"2020-11-24","externalUrl":null,"permalink":"/en/tags/backup/","section":"Tags","summary":"","title":"Backup","type":"tags"},{"content":"I believe most of you have used the Jenkins configuration as code principle, so you build/test/release process will be described in code.\nIt looks like all is good, but not all of the configuration is in code, same of the Jenkins system configuration is store in the Jenkins service, so we also need to backup Jenkins in case of any disaster.\nThere are two ways to backup Jenkins, one used Jenkins plugin, the other is create shell scripts.\nUsing plug-in backup # I used the ThinBackup plugin, here is my thinBackup configuration.\nbackup to a folder which user jenkins has permission to write. this is important.\nPreviously I backup Jenkins to a mount folder, but it doesn\u0026rsquo;t work recently. so I use user jenkins to log in on the Jenkins server and found my jenkins can\u0026rsquo;t access the directory, but I personal user can.\nI will daily backup my Jenkins server, from Monday to Saturday.\nFo me max number of backup sets is 3, because every backup archive more than 400 MB.\nOthers check boxs\nBackup build results Backup \u0026lsquo;userContent\u0026rsquo; folder Backup next build number file Backup plugins archives Move old backups to ZIP files Using shell script backup # Here is a popular repository and code for your reference.\nrepository: https://github.com/sue445/jenkins-backup-script gist: https://gist.github.com/abayer/527063a4519f205efc74 ","date":"2020-11-24","externalUrl":null,"permalink":"/en/posts/jenkins-backup/","section":"Posts","summary":"This article explains how to backup Jenkins using the ThinBackup plugin and shell scripts, ensuring that your Jenkins configuration and build data are safely stored.","title":"How to backup Jenkins","type":"posts"},{"content":"想要对 Java 项目进行代码覆盖率的测试，很容易就找到 JaCoCo 这个开源代码覆盖率分析工具是众多工具中最后欢迎的哪一个。\n本篇仅仅是在学习 JaCoCo 时对其实现设计文档 https://www.jacoco.org/trunk/doc/implementation.html 的粗略翻译。\n实现设计(Implementation Design) # 这是实现设计决策的一个无序列表，每个主题都试图遵循这样的结构:\n问题陈述 建议的解决方案 选择和讨论 覆盖率分析机制(Coverage Analysis Mechanism) # 覆盖率信息必须在运行时收集。为此，JaCoCo 创建原始类定义的插装版本，插装过程发生在加载类期间使用一个叫做 Java agents 动态地完成。\n有几种收集覆盖率信息的不同方法。每种方法都有不同的实现技术。下面的图表给出了 JaCoCo 使用的技术的概述:\n字节码插装非常快，可以用纯 Java 实现，并且可以与每个 Java VM 一起工作。可以将带有 Java 代理钩子的动态插装添加到 JVM 中，而无需对目标应用程序进行任何修改。\nJava 代理钩子至少需要 1.5 个 JVMs。用调试信息(行号)编译的类文件允许突出显示源代码。不幸的是，一些 Java 语言结构被编译成字节代码，从而产生意外的突出显示结果，特别是在使用隐式生成的代码时（如缺省构造函数或 finally 语句的控制结构）。\n覆盖 Agent 隔离(Coverage Agent Isolation) # Java 代理由应用程序类装入器装入，因此代理的类与应用程序类生活在相同的名称空间中，这可能会导致冲突，特别是与第三方库 ASM。因此，JoCoCo 构建将所有代理类移动到一个唯一的包中。\nJaCoCo 构建将包含在 jacocoagent.jar 中的所有类重命名为具有 org.jacoco.agent.rt_\u0026lt;randomid\u0026gt; 前缀，包括所需的 ASM 库类。标识符是从一个随机数创建的，由于代理不提供任何 API，因此没有人会受到此重命名的影响，这个技巧还允许使用 JaCoCo 验证 JaCoCo 测试。\n最低的Java版本(Minimal Java Version) # JaCoCo 需要 Java 1.5 及以上版本。\nJava 1.5 VMs 提供了用于动态插装的 Java 代理机制。使用 Java 1.5 语言级别进行编码和测试比使用旧版本更有效、更少出错——而且更有趣。JaCoCo 仍然允许运行针对这些编译的 Java 代码。\n字节码操纵(Byte Code Manipulation) # 插装需要修改和生成 Java 字节码的机制。JaCoCo 在内部使用 ASM 库来实现这个目的。\n实现 Java 字节码规范将是一项广泛且容易出错的任务。因此，应该使用现有的库。ASM库是轻量级的，易于使用，在内存和 CPU 使用方面非常高效，它被积极地维护并包含为一个巨大的回归测试套件，它的简化 BSD 许可证得到了 Eclipse 基金会的批准，可以与 EPL 产品一起使用。\nJava类的身份(Java Class Identity) # 在运行时加载的每个类都需要一个唯一的标识来关联覆盖率数据，JaCoCo 通过原始类定义的 CRC64 哈希代码创建这样的标识。\n在多类加载器环境中，类的纯名称不能明确地标识类。例如，OSGi 允许在相同的虚拟机中加载相同类的不同版本。在复杂的部署场景中，测试目标的实际版本可能与当前开发版本不同。代码覆盖率报告应该保证所呈现的数字是从有效的测试目标中提取出来的。类定义的散列代码允许区分类和类的版本。CRC64 哈希计算简单而快速，结果得到一个小的64位标识符。\n类加载器可能加载相同的类定义，这将导致 Java 运行时系统产生不同的类。对于覆盖率分析来说，这种区别应该是不相关的。类定义可能会被其他基于插装的技术(例如 AspectJ)改变。在这种情况下，哈希码将改变，标识将丢失。另一方面，基于被改变的类的代码覆盖率分析将会产生意想不到的结果。CRC64 代码可能会产生所谓的冲突，即为两个不同的类创建相同的哈希代码。尽管 CRC64 在密码学上并不强，而且很容易计算碰撞示例，但对于常规类文件，碰撞概率非常低。\n覆盖运行时依赖(Coverage Runtime Dependency) # 插装代码通常依赖于负责收集和存储执行数据的覆盖运行时。JaCoCo 只在生成的插装代码中使用 JRE 类型。\n在使用自己的类加载机制的框架中，使运行时库对所有插装类可用可能是一项痛苦或不可能完成的任务。自 Java 1.6 java.lang.instrument.Instrumentation。插装有一个扩展引导带加载器的API。因为我们的最低目标是 Java 1.5，所以 JaCoCo 只通过官方的 JRE API 类型来解耦插装类和覆盖运行时。插装的类通过 Object.equals(Object) 方法与运行时通信。插装类可以使用以下代码检索其探测数组实例。注意，只使用 JRE APIs:\nObject access = ... // Retrieve instance Object[] args = new Object[3]; args[0] = Long.valueOf(8060044182221863588); // class id args[1] = \u0026#34;com/example/MyClass\u0026#34;; // class name args[2] = Integer.valueOf(24); // probe count access.equals(args); boolean[] probes = (boolean[]) args[0]; 最棘手的部分发生在第 1 行，上面的代码片段中没有显示必须获得通过 equals() 方法提供对覆盖运行时访问的对象实例。到目前为止，已经实施和测试了不同的方法:\nSystemPropertiesRuntime: 这种方法将对象实例存储在系统属性下。这个解决方案打破了系统属性必须只包含 java.lang.String 的约定。字符串值，因此会在依赖于此定义的应用程序(如Ant)中造成麻烦。 LoggerRuntime: 这里我们使用共享的 java.util.logging.Logger。并通过日志参数数组而不是 equals() 方法进行通信。覆盖运行时注册一个自定义处理程序来接收参数数组。这种方法可能会破坏安装自己日志管理器的环境(例如Glassfish)。 ModifiedSystemClassRuntime: 这种方法通过插装将公共静态字段添加到现有的 JRE 类中。与上面的其他方法不同，此方法仅适用于活动 Java 代理的环境。 InjectedClassRuntime：这个方法使用 Java 9 中引入的 java.lang.invoke.MethodHandles.Lookup.defineClass 定义了一个新类。 从 0.8.3 版本开始，在 JRE 9 或更高版本上运行时，JaCoCo Java 代理实现使用 InjectedClassRuntime 在引导类装入器中定义新类，否则使用ModifiedSystemClassRuntime 向现有 JRE 类添加字段。从版本 0.8.0 开始，字段被添加到类 java.lang.UnknownError 中。version 0.5.0 - 0.7.9 向类 java.util.UUID 中添加了字段，与其他代理发生冲突的可能性较大。\n内存使用(Memory Usage) # 对于具有数千类或数十万行代码的大型项目，覆盖率分析应该是可能的。为了允许合理的内存使用，覆盖率分析是基于流模式和“深度优先”遍历的。\n一个庞大的覆盖率报告的完整数据树太大了，无法适合合理的堆内存配置。因此，覆盖率分析和报告生成被实现为“深度优先”遍历。也就是说，在任何时间点，工作记忆中只需要保存以下数据:\n当前正在处理的单个类。 这个类的所有父类(包、组)的汇总信息。 Java元素标识符(Java Element Identifiers) # Java 语言和 Java VM 对Java 元素使用不同的字符串表示格式。例如，Java 中的类型引用读起来像 java.lang.Object。对象，VM 引用的类型与 Ljava/lang/Object 相同。JaCoCo API 仅基于VM标识符。\n直接使用 VM 标识符不会在运行时造成任何转换开销。有几种基于 Java VM 的编程语言可能使用不同的符号。因此，特定的转换应该只在用户界面级别发生，例如在报表生成期间。\nJaCoCo实现的模块化(Modularization of the JaCoCo implementation) # JaCoCo 是在提供不同功能的几个模块中实现的。这些模块是作为带有适当清单文件的 OSGi 包提供的。但是它不依赖于 OSGi 本身。\n使用 OSGi bundle 允许在开发时和运行时在 OSGi 容器中定义良好的依赖关系。由于对 OSGi 没有依赖关系，捆绑包也可以像普通的 JAR 文件一样使用。\n","date":"2020-11-17","externalUrl":null,"permalink":"/posts/jacoco/","section":"Posts","summary":"介绍 JaCoCo 的实现设计，包括覆盖率分析机制、Java 版本要求、字节码操纵、内存使用等方面的内容。","title":"JaCoCo 实现原理 (JaCoCo Implementation Design)","type":"posts"},{"content":"最近在思考如何将团队里的所有的虚拟机都很好的管理并监控起来，但是由于我们的虚拟机的操作系统繁多，包括 Windows, Linux, AIX, HP-UX, Solaris SPARC 和 Solaris x86. 到底选择哪种方式来管理比较好呢？这需要结合具体场景来考虑。\nAnsible 和其他工具的对比 # 这里有一个关于 Chef，Puppet，Ansible 和 Saltstack 的对比文章\nhttps://www.edureka.co/blog/chef-vs-puppet-vs-ansible-vs-saltstack/\n选择合适的工具 # 仅管理 Windows 和 Linux # 如果你的虚拟机没有这么多平台，只是 Windows 和 Linux，假如你已经有了 VMware vSphere 来管理了，那么可以通过 VMware vSphere API 来查看这些机器的状态。\n这里是 VMware 官方的 API Library 供使用：\nVMware vSphere API Python Bindings Go library for the VMware vSphere API 管理多个操作系统 # 如果你和我的情况一下，想监控很多个操作操作系统，那么就只能通过 ssh 来登录到每一台机器上去查看，比如执行 uptime 等命令。可以写 shell 脚本来完成这些登录、检测等操作。\n另外就是使用 Ansible 的 Playbook。Playbook 里描述了你要做的操作，这是一个权衡，学习 Ansible 的 Playbook 需要花些时间的。\n如果想了解下 Ansible 那么可以试试 Ansible Playbook。以下是我使用 Ansible 做了一些练习。\nPlaybook结构 # +- vars | +- vars.yml | +- ... +- hosts # save all hosts you want to monitor +- run.yml # ansible executable file Playbook具体代码 # vars/vars.yml\n--- # system ip: \u0026#34;{{ ansible_default_ipv4[\u0026#39;address\u0026#39;] }}\u0026#34; host_name: \u0026#34;{{ ansible_hostname }}\u0026#34; os: \u0026#34;{{ ansible_distribution }}\u0026#34; version: \u0026#34;{{ ansible_distribution_version }}\u0026#34; total_mb: \u0026#34;{{ ansible_memtotal_mb }}\u0026#34; vcpus: \u0026#34;{{ ansible_processor_vcpus }}\u0026#34; hosts\n[unix-vm] aix ansible_host=walbld01.dev.company.com ansible_user=test ansible_become_pass=test hp-ux ansible_host=walbld04.dev.company.com ansible_user=test ansible_become_pass=test linux ansible_host=walbld05.dev.company.com ansible_user=test ansible_become_pass=test [win-vm] win-bld02 ansible_host=walbld02.dev.company.com ansible_user=Administrator ansible_password=admin ansible_port=5985 ansible_connection=winrm ansible_winrm_server_cert_validation=ignore [other-vm] solaris ansible_host=walbld07.dev.company.com ansible_user=test ansible_become_pass=test win-udb03 ansible_host=walbld03.dev.company.com ansible_user=administrator ansible_become_pass=admin run.yml\n--- # this playbook is simple test - name: \u0026#34;get unix build machine info\u0026#34; hosts: unix-vm gather_facts: True tasks: - name: get uname, hostname and uptime shell: \u0026#34;uname \u0026amp;\u0026amp; hostname \u0026amp;\u0026amp; uptime\u0026#34; register: output - debug: var=output[\u0026#39;stdout_lines\u0026#39;] - name: \u0026#34;get windows build machine os info\u0026#34; hosts: win-vm gather_facts: True tasks: - debug: var=hostvars[\u0026#39;win-bld02\u0026#39;].ansible_facts.hostname - debug: var=hostvars[\u0026#39;win-bld02\u0026#39;].ansible_distribution 如何执行 # 首先需要安装了 ansible，然后执行\n# run with playbook ansible-playbook -i hosts run.yml 注：上面的代码是脱敏过的，需要根据你的环境进行调整才能执行成功。\nAnsible TroubleShotting # \u0026quot;msg\u0026quot;: \u0026quot;winrm or requests is not installed: No module named winrm\u0026quot;\nNeed install pywinrm on your master server.\n\u0026ldquo;msg\u0026rdquo;: \u0026ldquo;plaintext: auth method plaintext requires a password\u0026rdquo;\nwhen I run ansible mywin -i hosts -m win_ping -vvv, I notice the output used Python2.7, so I install pywinrm with command sudo pip2 install pywinrm, then my problem was resolved.\nmywin | UNREACHABLE! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;plaintext: auth method plaintext requires a password\u0026#34;, \u0026#34;unreachable\u0026#34;: true } Result: You should be using ansible_password and not ansible_pass. link\n","date":"2020-10-28","externalUrl":null,"permalink":"/posts/ansible-practice/","section":"Posts","summary":"通过 Ansible 实践，探索如何高效管理和监控多种操作系统的虚拟机。","title":"Ansible 实践","type":"posts"},{"content":"I have been using Artifactory for continuous integration for some time now, and I have gained some experience and insights into enterprise-level Artifactory. I hope this sharing will help those who are just getting started with this tool to understand what Artifactory is, what it can do, why you should choose it, and what to pay attention to during its use.\nWhat is Artifactory # In short: Artifactory is a tool for storing artifacts. Currently, Artifactory is a very influential and powerful tool.\nAdvantages of Artifactory # Your team may already have its own way of managing artifacts, such as FTP. What can Artifactory bring? Let\u0026rsquo;s take a look at its advantages.\nNote: The following advantages are introduced for the JFrog Artifactory Enterprise Edition. The open-source edition, the OSS version, does not have the following rich features.\nAdvantage 1: It\u0026rsquo;s a Universal Repository Manager # JFrog Artifactory Enterprise Edition fully supports repository managers for all major package formats. It can not only manage binary files but also manage dependencies of packages in almost all languages on the market, as shown in the figure below:\nTherefore, using Artifactory allows you to store all binary files and packages in one place.\nAdvantage 2: Better Integration with CI Tools # It supports all mainstream CI tools (as shown in the figure below) and captures detailed build environment information during deployment to achieve fully reproducible builds.\nIn addition, it provides rich REST APIs, so any operation on the GUI page can be programmatically completed through code, facilitating CI/CD implementation.\nAdvantage 3: Powerful Search Functionality # If your builds are stored on FTP and you want to find a specific artifact from a large number of artifacts without knowing its name, it can be very difficult.\nArtifactory provides powerful search capabilities. You can search by name with regular expressions; you can also search by file checksum; and you can quickly search by properties, as shown in the following examples:\nExample 1: Search by Name # If you want to find a build artifact from a specific commit, such as the commit hash a422912, you can directly enter *a422912* and press Enter to quickly find it from numerous artifacts, such as Demo_Linux_bin_a422912.zip\nExample 2: Search by Property # For example, to find all builds with the property release.status set to released, you can search like this.\nExample 3: Search by Checksum # If you only know the file\u0026rsquo;s checksum, you can also search by it. For example, calculate the file\u0026rsquo;s checksum using sha1sum:\n$ sha1sum test.zip ad62c72fb097fc4aa7723e1fc72b08a6ebcacfd1 *test.zip Advantage 4: Artifact Lifecycle Management # By defining repositories with different maturity levels and using the Artifactory Promote function, you can move artifacts to different maturity repositories and better manage and maintain the artifact lifecycle through metadata properties.\nIn addition to these advantages, Artifactory has many more features that I won\u0026rsquo;t go into detail here.\nMore features can be found on the JFrog Artifactory official website: https://jfrog.com/artifactory/features/\nNext, through a demo, we will introduce how to use Artifactory and some best practices to avoid detours.\nArtifactory Homepage Introduction # Top of the Page # You can see that this Artifactory has served over 5000 artifacts. You can also see the current version number of Artifactory and the latest version.\nMiddle of the Page, From Left to Right # The far left is the search function, which allows you to easily find artifacts through a variety of search criteria. Then there is some user manual, video, REST API documentation, etc.\nIn the middle is Set Me Up, which allows you to select and filter the repositories you want to operate on. Clicking on a specific repository will pop up detailed instructions on how to use it.\nOn the far right, it displays the recently deployed builds and the most downloaded artifacts (95 represents the number of downloads).\nBottom of the Page # At the bottom are some integrated tools and technical user documentation related to Artifactory, making it easy to quickly find the most authoritative technical documentation during integration.\nPractices and Workflow # Setting Up Watched Repositories # In the Set Me Up section on the homepage, you can see that we have many repositories. However, among the many repositories, most members are only interested in some of them. You can then only focus on some repositories. Add a like, and then click the like button to list only your favorite Artifact Repositories.\nRepository Permissions and Retention Policies # Repository (maturity) Retention Policy Personal Account Service Account Admin dev Usually not cleaned read/write read/write all int One week or a few days read read/write all stage Never cleaned read read/write all release Never cleaned read read/write all The table makes it easy to understand the permission settings and retention policies. This is suitable for most situations, but not necessarily for all enterprise situations.\nArtifactory Repository Naming Convention # In this list of repositories, you can see that certain naming conventions are followed. Here, the JFrog Artifactory officially recommended naming convention is followed. It is strongly recommended that you do the same. It consists of four parts:\n\u0026lt;team\u0026gt;-\u0026lt;technology\u0026gt;-\u0026lt;maturity\u0026gt;-\u0026lt;locator\u0026gt;\nIn the figure, the team has been anonymized; let\u0026rsquo;s call it team1. Then comes the technology; there are many options, such as generic, Docker, Maven, NPM, etc. I used generic because our product is a binary file compiled from C/C++, which belongs to the generic category. Next is maturity. A repository usually consists of four levels of maturity, from low to high, which are dev, int, stage, and release. Finally, it indicates the location of the artifact. For example, a multinational company may have Artifactory instances in different regions to ensure upload/download speed and other needs. The den in the figure is an abbreviation for the location of the current Artifactory. Understanding the Workflow from Build Generation to Release # dev stands for development. This repository has read/write permissions for all product members, who can upload libraries or other binary files.\nint stands for integration. For example, artifacts successfully built from Jenkins will first be placed in this repository. If the build fails, it will not be uploaded to Artifactory.\nstage represents the pre-release repository. Artifacts that have passed Unit Test/Smoke Test will be Promoted to this repository for further testing, such as manual testing.\nrelease Artifacts that have passed testing will be Promoted to this repository.\nTo better manage the Artifactory directory and artifact lifecycle, I recommend standardizing branch naming and adding properties to artifacts at different stages.\n1. Standardized Branch Naming for Clear Artifactory Directories # For example, a product is called ART, its Git repository is also called ART, and it has a branch called feature/ART-1234.\nThe environment variables in the Jenkins Pipeline are set as follows:\nenvironment { INT_REPO_PATH = \u0026#34;team1-generic-int-den/ART/${BRANCH_NAME}/${BUILD_NUMBER}/\u0026#34; } Let\u0026rsquo;s see how this branch build flows.\nAfter the first successful build of this branch through Jenkins, it will first be placed under the directory team1-generic-int-den/ART/feature/ART-1234/1/. If a second build is successful, the artifact directory will be: team1-generic-int-den/ART/feature/ART-1234/2/, and so on.\nTo better manage the directories under the repository, it is recommended that the team agree on branch naming conventions in advance so that all builds of the same type of branch will appear under the same directory.\nFor branch naming conventions, see this article: Programmer\u0026rsquo;s Self-Cultivation: Git Commit Message and Branch Creation Specification\nIf you also want to put Pull Request Builds on Artifactory, it is recommended to set it up as follows:\nenvironment { PR_INT_REPO_PATH = \u0026#34;team1-generic-int-den/ART/PRs/${BRANCH_NAME}/${BUILD_NUMBER}/\u0026#34; } In this way, all successful Pull Request Builds will be placed under the PRs directory, which is convenient for searching and management.\n2. Adding Different Properties at Different Stages # If the above builds pass some quality checkpoints, such as unit tests, automated tests, and SonaQube scans, it is recommended to add different properties, such as:\nunit.test.status=passed automated.test.status=passed sonaqube.scan.status=passed\nThen, based on the above status, promote the artifacts that meet the conditions from the int repository to the stage repository. Test engineers go to the stage repository to get the build and perform testing. After passing the test, add the corresponding property status to the artifact, such as adding manual.test.status=passed in the Property.\nThen, the release pipeline goes to the stage repository to find the build that meets all the conditions for release.\nunit.test.status=passed automated.test.status=passed sonaqube.scan.status=passed manual.test.status=passed\nAfter successful release, promote the build from the stage repository to the release repository and add the property release.status=released. This completes the release.\nConclusion # In software delivery, quality trustworthiness and security trustworthiness are two important criteria for evaluating the reliability of a version. In this process, like using a funnel, the build is screened layer by layer, from the int repository to the stage repository, and finally to the release repository, completing the artifact release. By using Artifactory to create a single source of truth for artifact management, it paves the way for continuous software delivery.\nPrevious Related Articles # Introduction to JFrog Artifactory Artifactory and Jenkins Integration Solving the Problem of Jenkins Artifactory Plugin Failing to Upload Artifacts to HTTPS Artifactory Only on AIX ","date":"2020-10-04","externalUrl":null,"permalink":"/en/posts/what-is-artifactory/","section":"Posts","summary":"This article introduces the concepts, advantages, working principles, and best practices of JFrog Artifactory, helping readers understand how to use Artifactory to manage software artifacts.","title":"For Those Who Want to Use JFrog Artifactory to Manage Artifacts","type":"posts"},{"content":" Why Establish Conventions # The old saying goes, \u0026ldquo;No rules, no circle.\u0026rdquo; In team collaborative development, everyone writes commit messages when submitting code, but without conventions, everyone will have their own writing style. Therefore, when reviewing the git log, you often see a variety of styles, which is very unfavorable for reading and maintenance.\nLet\u0026rsquo;s look at the comparison between no conventions and conventions through the following two examples, and what benefits conventions can bring.\nCommit Messages—Unconventional vs. Conventional # From this commit message, you don\u0026rsquo;t know what he modified or the intention of the modification.\nThis is the Angular commit message, which follows Conventional Commits.\nThis is the most widely used Git commit message convention in the industry, and many projects are already using it. If your project has not yet established a Git commit message convention, it is recommended to copy or refer to this convention.\nFor a team, when many people work together on a project, establishing a commit message convention in advance is very helpful for the long-term development of the project and the subsequent addition and maintenance of personnel.\nThe benefits are summarized as follows:\nHelps others better understand your change intentions, making it easier to contribute/modify code. Structured commit messages facilitate the identification of automation scripts and CI/CD. Provides the ability to automatically generate CHANGELOGs. Finally, this also reflects a programmer\u0026rsquo;s self-cultivation. Branch Creation—Unconventional vs. Conventional # If there is no convention for creating branches and no restrictions are imposed, many branches will look like this: ABC-1234-Test, ABC-2345-demo, Hotfix-ABC-3456, Release-1.0, or even worse. When there are many branches, it will appear chaotic and inconvenient to search.\nIf branch creation conventions are established, for example, the branches above must start with a type through a Hook during creation, then the newly created branches will look like this: bugfix/ABC-1234, feature/ABC-2345, hotfix/ABC-3456, release/1.0. This not only helps with retrieval but also allows others to understand the purpose of the branch through the type and facilitates the development of subsequent CI/CD pipelines.\nHow to Solve Convention Issues # We should proceed from two aspects:\nFirst, establish commit message and branch creation conventions for the team, letting team members understand and follow the conventions. Then, when submitting code or creating branches, use Git Hooks to prevent unconventional submissions to the remote repository. Establishing Git Commit Message Conventions # The most effective way to establish reasonable conventions is to refer to whether there are common conventions in the software industry. Currently, the most widely used convention in the industry is Conventional Commits, which is used by many projects, including Angular.\nYou can formulate conventions suitable for your team based on the above conventions, for example:\nJIRA-1234 feat: support for async execution ^-------^ ^--^: ^-------------------------^ | | | | | +--\u0026gt; Summary in present tense. | | | +--\u0026gt; Type: feat, fix, docs, style, refactor, perf, test or chore. | +--\u0026gt; Jira ticket number Type Must be one of the following: feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing or correcting existing tests chore: Changes to the build process, .gitignore or auxiliary tools and libraries such as documentation generation, etc. Setting Up Git Hooks # Here, Bitbucket is used as an example. Other Git tools such as GitHub and Gitlab have similar functions.\nBitbucket uses the Yet Another Commit Checker free plugin.\nFirst, enable Yet Another Commit Checker.\nThen, let\u0026rsquo;s introduce some commonly used settings of Yet Another Commit Checker one by one.\n1. Enable Require Valid JIRA Issue(s) # Enabling this function automatically verifies the existence of a Jira issue number during commit message submission through a Hook. If not, the submission fails. This forces the association of the commit message with the Jira issue number when submitting code.\n2. Commit Message Regex # For example, setting a simple regular expression like [A-Z\\-0-9]+ .* requires that the Jira issue number must start with this format ABCD-1234, and there must be a space between the description and the Jira issue number.\nWith the above settings, the commit message will be limited to the following format:\nABCD-1234 Balabala...... For example, this more complex regular expression:\n^[A-Z-0-9]+ .*(?\u0026lt;type\u0026gt;chore|ci|docs|feat|fix|perf|refactor|revert|style|test|Bld|¯\\\\_\\(ツ\\)_\\/¯)(?\u0026lt;scope\u0026gt;\\(\\w+\\)?((?=:\\s)|(?=!:\\s)))?(?\u0026lt;breaking\u0026gt;!)?(?\u0026lt;subject\u0026gt;:\\s.*)?|^(?\u0026lt;merge\u0026gt;Merge.* \\w+)|^(?\u0026lt;revert\u0026gt;Revert.* \\w+) This regular expression not only limits the beginning to a JIRA issue number followed by a space, but also requires the type to be filled in the description information, and finally the description information. It also supports different descriptions for Merge or Revert.\nUse the following test cases to specifically understand what kind of commit message convention restrictions the above regular expression will produce.\n# Test cases that pass NV-1234 chore: change build progress DT-123456 docs: update xdemo usage QA-123 ci: update jenkins automatic backup CC-1234 feat: new fucntional about sync Merge branch master into develop Reverted: Revert support feature \u0026amp; bugfix branches build Merge pull request from develop to master # Test cases that fail NV-1234 build: update NV-1234 Chore: change progress DT-123456 Docs: update xdemo QA-123ci: update jenkins automatic backup CC-1234 Feat: new fucntional about sync DT-17734: 8.2.2 merge from CF1/2- Enhance PORT.STATUS DT-17636 fix AIX cord dump issue DT-18183 Fix the UDTHOME problem for secure telnet DT-18183 Add new condition to get UDTHOME DT-15567 code merge by Xianpeng Shen. Test results can also be found here: https://regex101.com/r/5m0SIJ/10.\nSuggestion: If you also want to set such a strict and complex regular expression in your Git repository, it is recommended to consider and test it thoroughly before officially putting it into your Git repository\u0026rsquo;s Hook settings.\n3. Commit Regex Error # This setting is used to prompt error messages. When a team member submits a message that does not conform to the specifications and the submission fails, a reasonable prompt message will be given, which helps to find the problem. For example, if the submission fails, the following information will be seen on the command line:\nCommit Message Specifications: \u0026lt;Jira-ticket-number\u0026gt; \u0026lt;type\u0026gt;: \u0026lt;Description\u0026gt; Example: ABC-1234 feat: Support for async execution 1. Between Jira ticket number and type MUST has one space. 2. Between type and description MUST has a colon and a space. Type MUST be one of the following and lowercase feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing or correcting existing tests chore: Changes to the build process, .gitignore or auxiliary tools and libraries such as documentation generation, etc. Based on this description information, the submitter can easily know what the correct convention looks like, and then modify their last commit message using the git commit --amend command.\n4. Branch Name Regex # This is a convention restriction for creating branches. After setting the corresponding regular expression, developers can only push branches that meet the conditions of the regular expression to the remote repository.\nFor example, this branch creation regular expression: ^(bugfix|feature|release|hotfix).*|(master)|(.*-dev)\nThis restricts all branches to start with bugfix, feature, release, hotfix, or types like v1.0-dev.\nYou can design a branch regular expression for your project based on the above regular expression.\n5. Branch Name Regex Error # This setting is to prompt for errors when pushing unconventional branches. Pre-setting the corresponding error prompt information helps users quickly find the reason for the push failure. For example, the following error message:\nBranches must begin with these types: bugfix/ feature/ release/ hotfix/ Tells the user that the branch must start with bugfix/, feature/, release/, hotfix/.\n6. Other Settings # There are also some other settings, such as the status of the associated Jira issue. This prevents developers from secretly submitting code to already closed Jira issues, which may cause untested code to enter the repository.\nThere are also Require Matching Committer Email and Require Matching Committer Name to limit developers to configure usernames and emails that match their login usernames and emails to standardize the usernames and emails displayed in commit information, and to facilitate the collection of subsequent data such as Git information statistics.\nReferences # Conventional Commits https://www.conventionalcommits.org/en/v1.0.0/ Angular Commit Guidelines: https://github.com/angular/angular.js/blob/master/DEVELOPERS.md#commits Projects Using Conventional Commits: https://www.conventionalcommits.org/en/v1.0.0/#projects-using-conventional-commits Yet Another Commit Checker: https://mohamicorp.atlassian.net/wiki/spaces/DOC/pages/1442119700/Yet+Another+Commit+Checker+YACC+for+Bitbucket\n","date":"2020-09-24","externalUrl":null,"permalink":"/en/posts/commit-messages-specification/","section":"Posts","summary":"This article introduces how to formulate and implement Git commit message and branch creation conventions to improve code quality and team collaboration efficiency.","title":"Programmer's Self-Cultivation — Git Commit Message and Branch Creation Conventions","type":"posts"},{"content":"This article introduces the important aspects to consider when publishing a Python project on GitHub for individuals or enterprises.\nConfiguring setup.py Publishing to PyPI Generating pydoc Choosing a Version Number Choosing a License Configuring setup.py # Packaging and publishing are accomplished by preparing a setup.py file. Assume your project directory structure is as follows:\ndemo ├── LICENSE ├── README.md ├── MANIFEST.in # Used to customize the contents of `dist/*.tar.gz` during packaging ├── demo │ └── __init__.py ├── setup.py ├── tests │ └── __init__.py │ └── __pycache__/ └── docs Using the packaging command python setup.py sdist bdist_wheel will generate two files, demo-1.0.0-py3-none-any.whl and demo-1.0.0.tar.gz, in the dist directory.\nThe .whl file is used for installation via pip install dist/demo-1.0.0-py3-none-any.whl, installing it to the ...\\Python38\\Lib\\site-packages\\demo directory.\n.tar.gz is an archive of the packaged source code. MANIFEST.in controls the contents of this file.\nThe following example shows how to use MANIFEST.in to customize the contents of dist/*.tar.gz. The MANIFEST.in file contents are as follows:\ninclude LICENSE include README.md include MANIFEST.in graft demo graft tests graft docs global-exclude __pycache__ global-exclude *.log global-exclude *.pyc Based on the above file contents, when using the command python setup.py sdist bdist_wheel to generate the demo-1.0.0.tar.gz file, it will include the LICENSE, README.md, and MANIFEST.in files, as well as all files in the demo, tests, and docs directories. Finally, it excludes all __pycache__, *.log, and *.pyc files.\nFor more information on the MANIFEST.in file syntax, see https://packaging.python.org/guides/using-manifest-in/\nOfficial examples and documentation are available at https://packaging.python.org/tutorials/packaging-projects/\nA Python sample project is available for reference at https://github.com/pypa/sampleproject\nCarefully reviewing the links above will fully satisfy the publishing requirements for most projects.\nPublishing to PyPI # As Python users know, external libraries can be downloaded using the following command. Python has a large number of third-party libraries; publishing open-source projects to PyPI makes them easily accessible to users.\npip install xxxx What is PyPI? # PyPI is short for The Python Package Index, a repository for finding, installing, and publishing Python packages.\nPyPI has two environments:\nTest environment TestPyPI Production environment PyPI Preparation # To familiarize yourself with the PyPI publishing tools and process, use the test environment TestPyPI. If you are already familiar with the PyPI publishing tools and process, you can directly use the production environment PyPI. TestPyPI and PyPI require separate registrations. That is, if you are registered in the production environment, you will also need to register to use the test environment. Note: The same account cannot be registered on both PyPI and TestPyPI simultaneously. Assuming your project is complete and ready to be published to PyPI, execute the following commands to publish your project:\nrm dist/* # Generate the source archive .tar.gz file and build file .whl file python setup.py sdist bdist_wheel # Use the following command to publish to TestPyPI twine upload --repository testpypi dist/* # Use the following command to publish to PyPI twine upload dist/* About pydoc # Python has a built-in doc feature called pydoc. Running python -m pydoc shows its options and functionality.\ncd docs python -m pydoc -w ..\\ # Generate all documentation Running python -m pydoc -b starts a local web page to access the documentation for all libraries in your ...\\Python38\\Lib\\site-packages\\ directory.\nHow can this local web documentation be accessed externally? GitHub\u0026rsquo;s built-in GitHub Pages feature makes it easy to provide an online URL.\nOpen your GitHub Python project settings -\u0026gt; find GitHub Pages -\u0026gt; select your branch and path for the Source, and save to immediately get a URL. For example:\nhttps://xxxxx.github.io/demo/ is your project homepage, displaying the README.md information https://xxxxx.github.io/demo/docs/demo.html is your project\u0026rsquo;s pydoc documentation About Version Number # For official releases, pay attention to version number selection.\nFor simple projects with low completeness, it is recommended to start with version 0.0.1. For projects with complete functionality and high completeness, you can start with version 1.0.0. For example, a project has four stages from preparation to official release: Alpha, Beta, Release Candidate, and Official Release. If the official release version number is 1.1.0, according to the following versioning specification:\nX.YaN # Alpha release X.YbN # Beta release X.YrcN # Release Candidate X.Y # Final release The Alpha, Beta, Release Candidate, and Final Release versions are as follows:\nAlpha release version number is 1.1.0a1, 1.1.0a1, 1.1.0aN... Beta release version number is 1.1.0b1, 1.1.0b1, 1.1.0bN... Release Candidate version number is 1.1.0rc1, 1.1.0rc2, 1.1.0rcN... Final release version number 1.1.0, 1.1.1, 1.1.N...\nPython\u0026rsquo;s official versioning and dependency specification document\nChoosing a License # For enterprise projects, the license is generally provided by the company\u0026rsquo;s legal team; publishers only need to format the license file (e.g., formatting the license.txt file to 70-80 characters per line).\nFor personal projects or to learn about open-source licenses, common software open-source licenses (listed in order of condition count):\nGNU AGPLv3 GNU GPLv3 GNU LGPLv3 Mozilla Public License 2.0 Apache License 2.0 MIT License Boost Software License 1.0 The Unlicense This article provides a reference: 《How to Choose an Open Source License for Your Github Repository》\nChoosing a License: https://choosealicense.com/licenses Choosing a License for GitHub repositories: https://github.com/github/choosealicense.com Choosing a License Appendix: https://choosealicense.com/appendix\n","date":"2020-09-13","externalUrl":null,"permalink":"/en/posts/how-to-release-python-project/","section":"Posts","summary":"This article introduces the important aspects to consider when publishing a Python project on GitHub, including project structure, dependency management, and version control.","title":"Publishing a Python Project on GitHub — Things to Note","type":"posts"},{"content":"","date":"2020-09-13","externalUrl":null,"permalink":"/en/tags/pypi/","section":"Tags","summary":"","title":"PyPI","type":"tags"},{"content":"","date":"2020-09-13","externalUrl":null,"permalink":"/en/tags/release/","section":"Tags","summary":"","title":"Release","type":"tags"},{"content":" Backgroup # If you want to release python project on PyPI, you must need to know about PyPI usage characteristics, then I did some test about pip install command.\nFor example: I have a Python project called demo-pip. and beta release would like 1.1.0.xxxx, offical release version is 1.1.0 to see if could success upgrade when using pip command.\nBase on the below test results, I summarized as follows:\nInstall a specific version of demo-pip from PyPI, with --upgrade option or not, they\u0026rsquo;ll all both success. Install the latest package version of demo-pip from PyPI that version is large than the locally installed package version, with --upgrade option installs successfully. without --upgrade option install failed. Install the latest package version of demo-pip from PyPI that version is less than the locally installed package version, with --upgrade option or not, install failed. 1.1.0.xxxx version naming is OK, but when the beta version is larger than 1.1.0, for example, the beta version is 1.1.0.1000, pip install with --upgrade not work when our official release version is 1.1.0. a.\tOne option is the official release version start from 1.1.0.1000, beta version starts from 1.1.0.0001, 1.1.0.0002… Or the beta version should be less than 1.1.0, maybe 1.0.0.xxxx b.\tAnother option is follow up python official versioning that is the best practice, then the beta release version will be 1.1.b1, 1.1.b2, 1.1.bN… (it passed No.5 test below) My Test Case # \u0026lt;No.\u0026gt; \u0026lt;Test Case Steps\u0026gt; \u0026lt;Test Output\u0026gt; \u0026lt;Test Results\u0026gt; 1 1.build and install demo-pip-1.0.5\n2.install from PyPI. on PyPI, the latest version is 1.0.4 C:\\workspace\\demo-pip\u0026gt;pip install dist\\demo-pip-1.0.5-py3-none-any.whl\nProcessing c:\\workspace\\demo-pip\\dist\\demo-pip-1.0.5-py3-none-any.whl\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.4\nUninstalling demo-pip-1.0.4:\nSuccessfully uninstalled demo-pip-1.0.4\nSuccessfully installed demo-pip-1.0.5\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nRequirement already up-to-date: demo-pip in c:\\program files\\python38\\lib\\site-packages (1.0.5) install with --upgrade option failed when the installed version number is less than the current version number 2 1.rebuild and install demo-pip-1.0.3\n2.install from PyPI again with --upgrade option C:\\workspace\\demo-pip\u0026gt;pip install dist\\demo-pip-1.0.3-py3-none-any.whl\nProcessing c:\\workspace\\demo-pip\\dist\\demo-pip-1.0.3-py3-none-any.whl\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.5\nUninstalling demo-pip-1.0.5:\nSuccessfully uninstalled demo-pip-1.0.5\nSuccessfully installed demo-pip-1.0.3\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nCollecting demo-pip\nDownloading https://test-files.pythonhosted.org/packages/41/c5/fe16fdc482927b2831c36f96d6e5a1c5b7a2a676ddc4c00c67a9ccf644e9/demo-pip-1.0.4-py3-none-any.whl (51 kB)\n|████████████████████████████████| 51 kB 362 kB/s\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.3\nUninstalling demo-pip-1.0.3:\nSuccessfully uninstalled demo-pip-1.0.3\nSuccessfully installed demo-pip-1.0.4 install with --upgrade option success from PyPI when install version number is larger than the current version number 3 1. create a new build demo-pip-1.0.3.1000\n2. install demo-pip-1.0.3.1000 with --upgrade option\n3. install demo-pip-1.0.3.1000, without --upgrade option C:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nCollecting demo-pip\nDownloading https://test-files.pythonhosted.org/packages/41/c5/fe16fdc482927b2831c36f96d6e5a1c5b7a2a676ddc4c00c67a9ccf644e9/demo-pip-1.0.4-py3-none-any.whl (51 kB)\n|████████████████████████████████| 51 kB 83 kB/s\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.3.1000\nUninstalling demo-pip-1.0.3.1000:\nSuccessfully uninstalled demo-pip-1.0.3.1000\nSuccessfully installed demo-pip-1.0.4\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nRequirement already satisfied: demo-pip in c:\\program files\\python38\\lib\\site-packages (1.0.3.1000) 1. install with --upgrade option success\n2. install without --upgrade option failed 4 1. create a new build demo-pip-1.0.4.1000\n2. install demo-pip-1.0.4.1000 with --upgrade option.\n3. install specific version of demo-pip C:\\workspace\\demo-pip\u0026gt;pip install dist\\demo-pip-1.0.4.1000-py3-none-any.whl\nProcessing c:\\workspace\\demo-pip\\dist\\demo-pip-1.0.4.1000-py3-none-any.whl\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.4\nUninstalling demo-pip-1.0.4:\nSuccessfully uninstalled demo-pip-1.0.4\nSuccessfully installed demo-pip-1.0.4.1000\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nRequirement already up-to-date: demo-pip in c:\\program files\\python38\\lib\\site-packages (1.0.4.1000)\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ demo-pip==1.0.4\nLooking in indexes: https://test.pypi.org/simple/\nCollecting demo-pip==1.0.4\nDownloading https://test-files.pythonhosted.org/packages/41/c5/fe16fdc482927b2831c36f96d6e5a1c5b7a2a676ddc4c00c67a9ccf644e9/demo-pip-1.0.4-py3-none-any.whl (51 kB)\n|████████████████████████████████| 51 kB 362 kB/s\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.4.1000\nUninstalling demo-pip-1.0.4.1000:\nSuccessfully uninstalled demo-pip-1.0.4.1000\nSuccessfully installed demo-pip-1.0 Install failed when the install version number is less than the currently installed version number\nif install a specific version of demo-pip with --upgrade option or not. will both works. 5 1. Follow up python official version naming for beta-version, create a new build demo-pip-1.0.b1\n2. install from PyPi without --upgrade option\n3. install from PyPi with --upgrade option C:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nRequirement already satisfied: demo-pip in c:\\program files\\python38\\lib\\site-packages (1.0b1)\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nCollecting demo-pip\nDownloading https://test-files.pythonhosted.org/packages/41/c5/fe16fdc482927b2831c36f96d6e5a1c5b7a2a676ddc4c00c67a9ccf644e9/demo-pip-1.0.4-py3-none-any.whl (51 kB)\n|████████████████████████████████| 51 kB 362 kB/s\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0b1\nUninstalling demo-pip-1.0b1:\nSuccessfully uninstalled demo-pip-1.0b1\nSuccessfully installed demo-pip-1.0.4 install successful with --upgrade option\nso it means 1.0.b1 version number is less than 1.0.4 version ","date":"2020-08-30","externalUrl":null,"permalink":"/en/posts/pip-install-and-versioning/","section":"Posts","summary":"Explains the behavior of pip install commands with different versioning schemes, including how to handle beta versions and the implications of using \u003ccode\u003e--upgrade\u003c/code\u003e with specific version numbers.","title":"About Python pip install and versioning","type":"posts"},{"content":"","date":"2020-08-17","externalUrl":null,"permalink":"/en/tags/jira/","section":"Tags","summary":"","title":"Jira","type":"tags"},{"content":" Backgroud # When you are using a server account for CI/CD, if you want to make the server account avatar to looks professional on Jira update but the server account may not allowed to log to Jira, so you can not update the avatar though GUI, you could use Jira REST API to do this.\nI assume you have an account called robot, here are the examples of how to update though REST API.\nExample in Python # import http.client conn = http.client.HTTPSConnection(\u0026#34;jira.your-company.com\u0026#34;) payload = \u0026#34;{\\r\\n\\t\\\u0026#34;id\\\u0026#34;: \\\u0026#34;24880\\\u0026#34;,\\r\\n\\t\\\u0026#34;isSelected\\\u0026#34;: false,\\r\\n\\t\\\u0026#34;isSystemAvatar\\\u0026#34;: true,\\r\\n\\t\\\u0026#34;urls\\\u0026#34;: {\\r\\n\\t\\t\\\u0026#34;16x16\\\u0026#34;: \\\u0026#34;https://jira.your-company.com/secure/useravatar?size=xsmall\u0026amp;avatarId=24880\\\u0026#34;,\\r\\n\\t\\t\\\u0026#34;24x24\\\u0026#34;: \\\u0026#34;https://jira.your-company.com/secure/useravatar?size=small\u0026amp;avatarId=24880\\\u0026#34;,\\r\\n\\t\\t\\\u0026#34;32x32\\\u0026#34;: \\\u0026#34;https://jira.your-company.com/secure/useravatar?size=medium\u0026amp;avatarId=24880\\\u0026#34;,\\r\\n\\t\\t\\\u0026#34;48x48\\\u0026#34;: \\\u0026#34;https://jira.your-company.com/secure/useravatar?avatarId=24880\\\u0026#34;}\\r\\n}\u0026#34; headers = { \u0026#39;content-type\u0026#39;: \u0026#34;application/json\u0026#34;, \u0026#39;authorization\u0026#39;: \u0026#34;Basic Ymx3bXY6SzhNcnk5ZGI=\u0026#34;, \u0026#39;cache-control\u0026#39;: \u0026#34;no-cache\u0026#34;, \u0026#39;postman-token\u0026#39;: \u0026#34;ecfc3260-9c9f-e80c-e3e8-d413f48dfbf4\u0026#34; } conn.request(\u0026#34;PUT\u0026#34;, \u0026#34;/rest/api/latest/user/avatar?username=robot\u0026#34;, payload, headers) res = conn.getresponse() data = res.read() print(data.decode(\u0026#34;utf-8\u0026#34;)) Example in Postman # # URL and Method is PUT https://jira.your-company.com/rest/api/latest/user/avatar?username=robot # Authorization # Type: Basic Auth # Username: server-account-username # Password: server-accoutn-password # Body { \u0026#34;id\u0026#34;: \u0026#34;24880\u0026#34;, \u0026#34;isSelected\u0026#34;: false, \u0026#34;isSystemAvatar\u0026#34;: true, \u0026#34;urls\u0026#34;: { \u0026#34;16x16\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=xsmall\u0026amp;avatarId=24880\u0026#34;, \u0026#34;24x24\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=small\u0026amp;avatarId=24880\u0026#34;, \u0026#34;32x32\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=medium\u0026amp;avatarId=24880\u0026#34;, \u0026#34;48x48\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?avatarId=24880\u0026#34;} } How to find the avatar id # You replace other avatar ids you like. Here is how to find you avatar id you want ","date":"2020-08-17","externalUrl":null,"permalink":"/en/posts/jira-update-account-avatar-with-rest-api/","section":"Posts","summary":"How to update the avatar of a Jira server account using the REST API, including examples in Python and Postman.","title":"Update Jira server account avatar with rest API","type":"posts"},{"content":" Problem # Sometimes my Windows server 2012 R2 has RDP connect problem below:\nThe remote session was disconnected because there are no Remote Desktop client access licenses available for this computer. Please contact the server administrator. How to Fix # You could log in to the vSphere Web Client if you have via console or have some other way to log in to the machine.\nOpen regedit.exe and navigate to\nSearch and Delete LicensingGracePeriod and LicensingGracePeriodExpirationWarningDays\nIf deletion failed, this failure message appears unable to delete all specified values, you need change permission. Refer to the related videos on YouTuBe.\nReboot the system if it is still doesn\u0026rsquo;t work.\nIn my case, every 90 to 120 days, the RDP end of grace period shows up, this is not the final solution. please let me know if you have a better solution.\nFinally, thanks to Bill K. shared with me the above solution.\n","date":"2020-08-10","externalUrl":null,"permalink":"/en/posts/rdp-problem/","section":"Posts","summary":"Fix the RDP connection issue on Windows Server 2012 R2, where the error message indicates that there are no Remote Desktop client access licenses available.","title":"Fixed \"Remote session was disconnected because there are no Remote Desktop client access licenses available\"","type":"posts"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/en/tags/daily/","section":"Tags","summary":"","title":"Daily","type":"tags"},{"content":" Starting August 7, 2020, I would like to start recording some daily/important things in English.\n3/15/2025 - I appreciate people who say great things about me behind my back # Today, one of my colleagues shared with me:\n\u0026ldquo;For example, Xianpeng does a lot of work, but he’s very modest, so some people might take advantage of that. You have to stand-up for yourself a bit.\u0026rdquo;\nI can’t thank him directly, as he’ll know who passed the message to me, but I truly appreciate him for saying these words.\n1/25/2025 - I have been Vilnius for 6 months # I have been living and working in Vilnius for six months now. I am satisfied with my decision to move here, the work and life are all great.\nThe only drawback is the weather during winter, as it can be quite gloomy. However, the summer season has been wonderful so far.\nI am looking forward to my upcoming trip to China, where I will be able to see my family and enjoy some sunshine. I will return soon.\n7/26/2024 # I have arrived LT last week and this week is my first week working in our Vilnius office.\nI feel very good to work here, onboarding was very smooth, the colleagues are nice, and I like the work environment in and out the office. Hope everything goes well for my family.\n6/25/2024 # Wow, I\u0026rsquo;m in the AUTHORS.txt of pip, see. I just made some commits of pip.\nOh, by the way, my family was approved last week. I\u0026rsquo;m waiting to pick it up in Japan.\n6/13/2024 - Blue Card Approved # It\u0026rsquo;s been 20 days since my visa interview and I finally know that my blue card is approved, but my familly still has to wait.\nI still have a lot of things to do such as normal work, KT, release, looking at appartment, confirming things, and so on.\nI hope my family will get approved soon so I can go to Japan and pick up our cards by the end of this month. Hope everthing goes well.\n5/18/2024 - Ready for Lithuania visa interview # Start May, I\u0026rsquo;m very busy, not only I need to work but also need to prepare for the visa application.\nFor work, I need to do KT, Q\u0026amp;A and our regular work. For visa, I need to apply Japan visa first and submit documents to Lithuania Embassy in Japan for some reason.\nHope everything goes well on Tuesday (5/21) 🙏.\n4/3/2024 - I accepted an offer position in Lithuania # After two months back and force, I finally get the offer in Lithuania.\nIt\u0026rsquo;s a big change for me and my family. I hope it\u0026rsquo;s a better choice without regrets.\n1/31/2024 - Grateful your recognitions # I want to thank my boss and leader. Thank them for for their recognition and wanting me to move to the next level. ❤️\nIt was very nice to talk with you. I have an honor to work alongside such talented people like yourself! Your presentation was awesome! \u0026ndash; from Pavel\n2023 is the busy year. In life, I\u0026rsquo;m the \u0026ldquo;daddy\u0026rdquo;; In work, I \u0026rsquo;m stickler for \u0026ldquo;best practices\u0026rdquo;, and after work, I\u0026rsquo;m a fan of \u0026ldquo;open source and writing\u0026rdquo;.\nOnward and Upward.\n12/02/2022 - I don\u0026rsquo;t have time # I don\u0026rsquo;t have time to contribute to open source I don\u0026rsquo;t have time to learn from Udemy or YouTuBe I don\u0026rsquo;t have time to update my blog and WeChat Account I don\u0026rsquo;t have time to play football after work\nMost of my time is at work, taking care of children, resting or sleeping.\nIf I want to do some of the above part-time things, just rest and sleep less :)\n06/13/2022 - Be thankful # It\u0026rsquo;s been over a month since I\u0026rsquo;ve written anything because my daughter was born and I\u0026rsquo;ve been busy all May.\nFirst, I worked remotely in the confinement center for nearly a month. After leaving the club, I started to take paternity leave.\nThe two of us are taking care of the children, which is still very busy for new parents.\nAside from being busy, thank goodness it all went smoothly.\n04/20/2022 - Bless my wife and baby # It\u0026rsquo;s been a tough April, wish my wife well and my baby well. Bless us ❤️\n12/25/2021 - I was surprised # Several days ago, A female financial colleague told me I did a great job sharing about CI/CD practices. (I posted the recording to slack channel of the China lab for all the engineers who had attended the meeting.)\nI was surprised that as a non-tech colleague she was also watched, and this was our first conversation. Thanks so much.\n11/24/2021 - Brother Ma told me that I am very good at CI/CD # Yesterday I gave a presentation to other teams in Dalian, it\u0026rsquo;s all about my practices in the DevOps. To be honest I’m not a good speaker.\nBrother Ma is a very very senior software engineer and I think he is also a full-stack engineer. He told me I am very good at CI/CD and he wants to learn from me in the next year.\nI\u0026rsquo;m very surprised that he gave me such high praise. I will keep on making more efforts in DevOps.\nThank you very much, Brother Ma.\n11/10/2021 - Record every time I was praised # I would record every time I was praised, so that when I failed on something, I will look back to see these praised to gain confidence.\nToday, I have a meeting with foreign colleagues, I say more fluent than ever, and my manage mentioned my English improved a lot. this is the second time he praised my English improved.\nNext week, I need to prepare to share related to CI/CD/DevOps best practice I did before with other colleagues, this time just need to use Chinese :)\n08/30/2021 - Bless my wife and me # Hopes our dream could come true this time. Bless us ❤️\n06/26/2021 - Mentor and mentee program # Since this month(June), I begin to have a mentor :) (WIP)\n05/26/2021 - Congrats to myself # This week I had two things that were worth remembering for me:\nThe first, when I had a meeting with our product family India team to share my CI/CD practice with Jenkins Yesterday. After the meeting, my boss told me that he was surprised with my English improvement. It gives more motivation to learn English.\nThe second, this year, my Build project Quality Gate goes into the next round. The first time was in 2019 that before the pandemic, we could go to our company\u0026rsquo;s HQ to attend five days Build event in the USA. For now, everyone can not trail overseas. I hope that the pandemic can go away ASAP, then maybe I can attend a face-to-face Build event.\n04/11/2021 - Just an update # I have moved to my new home for almost one month. It\u0026rsquo;s very happy to go to work only needs 15 minutes. I\u0026rsquo;m reading several very thick books, such as Code Complete and C++ Primer Plus books. Btw, I got promoted this month that I have worked here for six years. 02/18/2021 - The coach (Qiao Q) left Rocket and started his new career # I called Qiao a coach because he swims very well and tells me how to freestyle. He also does DevOps/Build-related works in Rocket, then we have a lot of things to talk about, so we have a good relationship.\nI a little upset when I heard that he told me he would leave Rocket. As we talk more, I understand him and support him in this decision.\nSince age and experience are no longer our advantages in Dev, the new challenges will help him get more opportunities in DevOps than he did in his job here, so he made a wise choice, and I hope he will have better development in the future.\n11/1/2020 - English and Decoration # Sunday, sunny.\nEnglish\nToday I clean English from App and have a test of the TOEIC, and the test score is 64, and my test results have never been more than 70. I hope I can keep testing every weekend so that to improve my English and hope next time the test will achieve about 70 :)\nThe House Decoration\nFrom October, my house had finished ceramic tile seam beauty, suspended ceiling, the laying off part of the marble, and now is whitewashing.\nLast Friday I paid the money for the custom kitchen and furniture, it\u0026rsquo;s more than 40,000 RMB, it\u0026rsquo;s over my budget in total. IKEA will ship furniture on Friday and install them next Saturday. Before that, I must prepare the kitchen appliances, so from Last Night (11.1) start to buy, the price is very well.\nSo in the next two weeks, the decoration\u0026rsquo;s progress very well, which was well worth the wait.\nCheers!\n8/30/2020 - About work and life # Sunday, sunny.\nI review my daily report so that I could remember what I\u0026rsquo;ve been doing for the past two weeks at work. I don\u0026rsquo;t want to talk about my regular work like build, release products, etc. I want to write down what I\u0026rsquo;ve learned in the past two weeks.\nAbout Work\nLearn how to show python project doc files online. you could create a GitHub Pages that is a GitHub build-in feature. then will have a website like https://\u0026lt;github-username\u0026gt;.github.io/\u0026lt;repo-name\u0026gt;.html\nLive demo - our team build project which analyzes Bitbucket git history and provides below features\nRecommend reviewer for Pull Request Recommend test cases for regression testing Recommend test scope for QA engineer Visualization based on data Enhancement currently our Artifactory usage, like provide move artifacts from INT(integration) repo to STAGE repo when build passed the smoke test.\nAbout Life\nFrom the last two weeks, my colleagues and I start swimming in the indoor pool every Friday night. we want to lose weight in these activities, but we always eat after swimming, this is not good for us to lose weight :(\nWe finally decide to use IKEA to decorate our house and completed the final design last Saturday. We finally started on Saturday, and they set up a simple start ceremony for me.\nThe first task was to look at tiles, and after all kinds of research and comparison, we finally ordered a big brand of tiles. Although it is more expensive, it\u0026rsquo;s still worth it.\n8/16/2020 - What is the core knowledge I need to learn # Sunday, cloudy. IKEA measures house then I go to the company to study.\nThis week I always thinking about what is my most important knowledge? Learn more about DevOps tools? or others?\nFinally, I come out with the two points I need to enhance, the ability to communicate and knowledge of the primordial.\nespecially is English communicate. I should set up some goals to archive, like the TOEIC exam. I need to use more time on reading such as Code Completed, C Primer books. OK, Keep doing both. 💪\n","date":"2020-08-07","externalUrl":null,"permalink":"/en/misc/daily/","section":"Miscs","summary":"Starting August 7, 2020, I would like to start recording some daily/important things in English.","title":"Daily Notes","type":"misc"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/en/misc/","section":"Miscs","summary":"","title":"Miscs","type":"misc"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/en/tags/notes/","section":"Tags","summary":"","title":"Notes","type":"tags"},{"content":" 背景 # 本篇讨论如何通过 Jenkins generic webhook trigger 插件来获取 Git 仓库事件（Events）。比如获取仓库的 Pull Request ID 等。\n使用过 Jenkins Multi-branch pipeline Job 的用户知道，这个 Job 类型的环境变量中可以得到 Pull Request 的相关信息如下\n为了获取这个变量需要创建这种类型的 Job，并且可能需要 clone 该仓库的代码，有点杀鸡宰牛的意思，看起来并不是一个特别恰当的办法。\n如何通过创建一个普通的 Jenkins Job 就能实时获取 Bitbucket 仓库以及 Pull Request 事件呢？通过以下功能和插件可以实现。\n配置 Bitucket 的 Webhook 通过 Jenkins generic-webhook-trigger 插件接收 Webhook 的 Event 事件 实现步骤 # 设置 Bitbucket Webhook # 在需要监听的 Bitbucket 仓库中创建一个 webhook，如下：\nName: test-demo URL: http://JENKINS_URL/generic-webhook-trigger/invoke?token=test-demo 备注：Bitbucket 中还有一个另外一个设置项，根据我的测试，该设置项 Post Webhooks 与上面的 Webhooks 都能实现本文要实现的功能。\n2. 配置 Jenkins Job # 想获取其他 Event 信息，比如 PR title, commit 等，请参考这个链接 bitbucket-server-pull-request.feature，按照上面的设置即可。\n这里的 token 值 test-demo 可以任意起名，但要与 Bitbucket event URL 中的 token 保持一致。\n测试 # 在 Jenkins Job pipeline 里添加了这个代码片段 echo pr_id is ${pr_id} 用来检查输出 Pull Request ID 是否如预期。\n然后在配置好的 Bitbucket 仓库下面创建一个 Pull Request\nJenkins Job 被 Pull Request Open 事件自动触发并执行了\n通过 Jenkins 的输出日志看到成功获取到了这个 Pull Request ID 值\n使用扩展 # 假如你有个程序，可以通过传入的 Pull Request ID 并借助 Bitbucket REST API 来获取并分析指定 Pull Request 的内容的。比如获取相关文件的历史记录，从而知道这些文件谁修改的最多以及这次修改涉及到了哪些 Jira 单号，从而做一些 Review 或是执行回归测试的推荐等等。\n有了这个 PR ID 就可以通过 Jenkins 来自动触发去执行你程序了。\n以上的这种方法适合不想或是不知道如何监听 Git 服务器（Bitbucket、GitHub 或是 GitLab 等）事件而需要要单独创建一个服务而准备的。如果你有什么很好的实践，期待你的留言分享。\n","date":"2020-08-07","externalUrl":null,"permalink":"/posts/bitbucket-pull-request-event/","section":"Posts","summary":"本文介绍如何使用 Jenkins 的 generic-webhook-trigger 插件来实时获取 Bitbucket 仓库的事件信息，如 Pull Request ID 等。","title":"通过 generic-webhook-trigger 插件实时获取 Bitbucket Repository Events","type":"posts"},{"content":" Jenkins Tip 3—Each installment describes a Jenkins tip using brief text and images.\nProblem # When using a Jenkins pipeline, if a Shell command returns a non-zero value (meaning the Shell command encountered an error), the Jenkins Job defaults to marking the current stage as failed. Therefore, the entire Job also fails.\nIn some cases, we want the Jenkins Job to show a successful status even if the Shell command fails and returns a non-zero value.\nFor example: A Shell command lists files starting with fail-list-. If such files exist, the user is notified; otherwise, no notification is sent.\nls -a fail-list-* By default, executing the above command causes the entire Job to fail.\nSolution # After some investigation, the following code snippet solved the problem.\nstage(\u0026#34;Send notification\u0026#34;) { steps { script { def fileExist = sh script: \u0026#34;ls -a fail-list-* \u0026gt; /dev/null 2\u0026gt;\u0026amp;1\u0026#34;, returnStatus: true if ( fileExist == 0 ) { // send email to user }else { // if not found fail-list-* file, make build status success. currentBuild.result = \u0026#39;SUCCESS\u0026#39; } } } } Analysis # When executing the Shell command, returnStatus: true is added. This returns and saves the status code, which is then compared to 0.\nIf it\u0026rsquo;s not equal to 0, and currentBuild.result = 'SUCCESS' is not added, the entire Jenkins Job will still be marked as failed. Adding it artificially ignores the error and sets the Job status to success.\n","date":"2020-07-22","externalUrl":null,"permalink":"/en/posts/jenkins-tips-3/","section":"Posts","summary":"How to handle non-zero return codes from Shell commands in a Jenkins Pipeline to ensure the job shows a successful status even if a command fails.","title":"Jenkins—Executing Shell Scripts—Handling Non-Zero Return Codes","type":"posts"},{"content":"When I upgrade Jenkins 2.176.3 to Jenkins 2.235.1, my Windows agent can not connect with master successfully and outcome this warning message \u0026ldquo;.NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service\u0026rdquo;.\nIt means I need to upgrade my Windows agent .NET Framework. Here are steps about how to upgrade .NET Framework to version 3.5.\nInstall .NET Framework 3.5 # Open Programs and Features\nSelect .NET Framework 3.5 Features (In my screenshot, it had already installed)\nThen try to reconnect the Jenkins agent, then it should back to work.\nInstall Jenkins agent service # If you can not found a Jenkins agent like me,\nYou can try these steps to install Jenkins agent\n# install Jenkins agent service cd c:\\\\jenkins .\\jenkins-agent.exe install net start jenkinsslave-C__agent # unstall Jenkins agent service sc delete jenkinsslave-C__agent Manual install .NET Framework 3.5 # Btw, if you could not install .NET Framework 3.5 successfully. you can try to install manually by this step\nManually download microsoft-windows-netfx3-ondemand-package.cab\nSpecify the path like below to install(note: the path is the directory where the file located)\nIn my case no need to reboot the Windows agent.\nHopefully, this also works for you. Let me know in case you have any comments.\n","date":"2020-07-16","externalUrl":null,"permalink":"/en/posts/jenkins-windows-agent-connect-problem/","section":"Posts","summary":"Resolve the issue of Jenkins Windows agents not connecting due to missing .NET Framework, including steps to install .NET Framework 3.5 and set up the Jenkins agent service.","title":"How to fix \".NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service\"","type":"posts"},{"content":"I am Xianpeng, a build engineer. Today, I am going to share with you three Jenkins Practice.\nI will talk about Configuration as code, followed up with shared libraries, and then Multi-Branch Pipeline in the end.\nConfiguration as Code # What is Configuration as Code?\nConfiguration as code (CAC) is an approach that managing configuration resources in a bitbucket repository\nWhat are the benefits? # First, Jenkins Job Transparency # To those who have experience with Bamboo jobs, you know how hard it is to grasp the logic from the GUI, this is especially true to people who don’t know the tools very well. So, when we migrated Bamboo jobs to Jenkins, we decided to use Configuration as code, because the code is more readable and very easy for engineers to understand the logic and flow.\nSecondly, Traceability # Another drawback of configuring Jenkins Jobs through GUI is that it cannot trace the history and see who did what. The ability to see who made changes is very very important for some critical Jenkins jobs, such as build jobs. With Configuration as code, we treat the Jenkins job code the same way as other application code, the benefits are not only on traceability-wise, but also the ability to roll-back to a specific version if needed.\nThirdly, Quick Recovery # Using Configuration as code has another benefit, which is the ability to quickly recover Jenkins\u0026rsquo;s job upon hardware issues. However, if Jenkins Job is configured through GUI, when the server that host the Jenkins corrupted, you might at risk of losing everything relates to Jenkins. So, from the business continuity perspective, it is also suggesting us to use configuration as code.\nJenkins Shared Libraries # Just like writing any application code, that we need to create functions, subroutines for reuse and sharing purpose. The same logic applies to the Jenkins configuration code. Functionalities such as sending emails, printing logs, deploying the build to FTP/Artifactory can be put into Jenkins Shared Libraries. Jenkins Shared Libraries is managed in Bitbucket.\nSo, let’s take a look, …\nxshen@localhost MINGW64 /c/workspace/cicd/src/org/devops (develop) $ ls -l total 28 -rw-r--r-- 1 xshen 1049089 5658 Jun 18 09:23 email.groovy -rw-r--r-- 1 xshen 1049089 898 Jun 13 20:05 git.groovy -rw-r--r-- 1 xshen 1049089 1184 Jun 8 12:10 opensrc.groovy -rw-r--r-- 1 xshen 1049089 1430 Jul 3 10:33 polaris.groovy -rw-r--r-- 1 xshen 1049089 2936 Jul 3 10:32 trigger.groovy drwxr-xr-x 1 xshen 1049089 0 Jun 8 12:10 utils/ -rw-r--r-- 1 xshen 1049089 787 May 12 13:24 utils.groovy As you can see, these groovy files are so-called shared libraries that complete works such as sending emails, git operations, updating opensource and so on. So, it is becoming very clear why we want to use shared libraries because it can reduce duplicate code.\nIt is also easier to maintain because instead of updating several places, we just need to update the shared libraries if any changes required. The last but not least, it encourages reuse and sharing cross teams. For example, the shared libraries I created are also used other team.\nMulti-branch pipeline # Next, Multi-branch pipeline. Some of you may have seen the same diagram like this.\nIn this picture, the pull request will trigger an automatic build, which is very helpful to engineers because their changes will not be merged to the main branch unless it passes the build test and smoke test.\nSo, I will share more detailed information here with you.\nThe thing works behind the scene is called Jenkins Multi-branch Pipeline. Before getting into the details, let’s first see what it looks like.\nNote: If your branch or Pull Request has been deleted, the branch or Pull Request will either be removed from the multi-branch Job or show a crossed-out status as shown above, this depending on your Jenkins setting.\nSo, as you can see from this page, there are multi Jenkins jobs. That is because for each bugfix or feature branch in Bitbucket, this multi-branch pipeline will automatically create a Jenkins job for them.\nSo, when developers complete their works, they can use these Jenkins jobs to create official build by themselves without the need of involving a build engineer. However, this was not the case in the past. At the time that we did not have these self-service jobs, developers always ask help from me, the build engineer to create a build for them. We have around twenty U2 developers in the team, you can image the efforts needed to satisfy these requirements.\nSo, I just covered the first benefit of this multi-branch pipeline, which creates a self-service for the team, save their time, save my time.\nAnother benefit of this self-service build and install is that our main branch will be more stable and save us from the time spent on investigating whose commit was problematic because only changes passed build, install and smoke test will be merged into the main branch.\nNow, you may wonder the value of this work, like how many issues have been discovered by this auto build and install test.\nTaking our current development as an example, there were about 30 pull requests merged last month, and six of them were found has built problems on some platforms.\nAs you all know, the cost of quality will be very low if we can find issues during the development phase, rather than being found by QA, Support or even customer.\nPlease comments in case you have any questions.\n","date":"2020-07-03","externalUrl":null,"permalink":"/en/posts/jenkins-best-practice/","section":"Posts","summary":"Discusses three best practices for Jenkins: Configuration as Code, Shared Libraries, and Multi-Branch Pipeline, highlighting their benefits in terms of transparency, traceability, and self-service builds.","title":"Jenkins Top 3 best practice","type":"posts"},{"content":" 《Jenkins Tips 2》 —— 每期用简短的图文描述一个 Jenkins 小技巧。\n问题 # 想要把 Linux 上不同的文本数据通过 Jenkins 发送邮件给不同的人。\n思路 # 想通过 Shell 先对数据进行处理，然后返回到 Jenkins pipeline 里，但只能得到 Shell 返回的字符串，因此需要在 Jenkinsfile 里把字符串处理成数组，然后通过一个 for 循环对数组中的值进行处理。\n以下是要处理的文本数据：\n# Example $ ls fail-list-user1.txt fail-list-user2.txt fail-list-user3.txt 要将以上文件通过 Jenkins 分别进行处理，得到用户 user1，user2，user3 然后发送邮件。\n解决 # 字符串截取 # 通过 Shell 表达式只过滤出 user1 user2 user3\n# list 所有以 fail-list 开头的文件，并赋给一个数组 l l=$(ls -a fail-list-*) for f in $l; do f=${f#fail-list-} # 使用#号截取左边字符 f=${f%.txt} # 使用%号截取右边字符 echo $f # 最终输出仅包含 user 的字符串 done 测试结果如下：\n$ ls fail-list-user1.txt fail-list-user2.txt fail-list-user3.txt $ l=$(ls -a fail-list-*) \u0026amp;\u0026amp; for f in $l; do f=${f#fail-list-}; f=${f%.txt}; echo $f ; done; user1 user2 user3 处理字符串为数组 # 以下在 Jenkinsfile 使用 groovy 将 Shell 返回的字符串处理成字符数组。\n// Jenkinsfile // 忽略 stage, steps 等其他无关步骤 ... scripts { // 将 Shell 返回字符串赋给 owners 这个变量。注意在 $ 前面需要加上 \\ 进行转义。 def owners = sh(script: \u0026#34;l=\\$(ls -a fail-list-*) \u0026amp;\u0026amp; for f in \\$l; do f=\\${f#fail-list-}; f=\\${f%.txt}; echo \\$f ; done;\u0026#34;, returnStdout:true).trim() // 查看 owners 数组是否为空，isEmpty() 是 groovy 内置方法。 if ( ! owners.isEmpty() ) { // 通过 .split() 对 owners string 进行分解，返回字符串数组。然后通过 .each() 对返回的字符串数组进行循环。 owners.split().each { owner -\u0026gt; // 打印最终的用户返回 println \u0026#34;owner is ${owner}\u0026#34; // 发送邮件，例子 email.SendEx([ \u0026#39;buildStatus\u0026#39; : currentBuild.currentResult, \u0026#39;buildExecutor\u0026#39;: \u0026#34;${owner}\u0026#34;, \u0026#39;attachment\u0026#39; : \u0026#34;fail-list-${owner}.txt\u0026#34; ]) } } } 最终完成了通过 Groovy 将 Shell 返回的字符串处理成字符数组，实现上述例子中对不同人进行邮件通知的需求。\n希望以上例子对你做其他类似需求的时候有所启示和帮助。\n","date":"2020-06-22","externalUrl":null,"permalink":"/posts/jenkins-tips-2/","section":"Posts","summary":"如何在 Jenkins Pipeline 中将 Shell 返回的字符串处理为字符数组，以便在后续步骤中进行处理和使用。","title":"将 Jenkins Shell 返回的字符串处理为字符数组","type":"posts"},{"content":" 《Jenkins Tips 1》 —— 每期用简短的图文描述一个 Jenkins 小技巧。\n问题 # 不希望 Shell 脚本因失败而中止 想一直运行 Shell 脚本并报告失败 解决 # 方法一 # 运行 Shell 时，你可以通过使用内置的 +e 选项来控制执行你的脚本错误。这可以禁用“非 0 退出”的默认行为。\n请参考如下四个示例中的测试 Shell 和测试结果 Console Output。\n示例一 # 执行的时候如果出现了返回值为非零（即命令执行失败）将会忽略错误，继续执行下面的脚本。\nset +e ls no-exit-file whoami 示例二 # 执行的时候如果出现了返回值为非零，整个脚本就会立即退出。\nset -e ls no-exit-file whoami 方法二 # 示例三 # 还有一种方式，如果不想停止失败的另一种方法是添加 || true 到你的命令结尾。\n# 做可能会失败，但并不关注失败的命令时 ls no-exit-file || true 示例四 # 如果要在失败时执行某些操作则添加 || \u0026lt;doSomethingOnFailure\u0026gt; 。\n# 做可能会失败的事情，并关注失败的命令 # 如果存在错误，则会创建变量 error 并将其设置为 true ls no-exit-file || error=true # 然后去判断 error 变量的值。如果为真，则退出 Shell if [ $error ] then exit -1 fi ","date":"2020-06-21","externalUrl":null,"permalink":"/posts/jenkins-tips-1/","section":"Posts","summary":"如何在 Jenkins 中使用 \u003ccode\u003eset +e\u003c/code\u003e 和 \u003ccode\u003eset -e\u003c/code\u003e 来控制 Shell 脚本的执行行为，以便在出现错误时不终止整个构建流程。","title":"忽略 Jenkins Shell 步骤中的故障","type":"posts"},{"content":" 背景 # 实现定期批量登录远程虚拟机然后进行一些指定的操作，还支持用户添加新的 hostname。\n需求分解 # 通过一个简单的 shell 脚本可实现定期进行 ssh 登录操作，但如何实现的更优雅一些就需要花点时间了，比如：\n定期自动执行 输出比较直观的登录测试结果 支持用户添加新的虚拟机 hostname 到检查列表中 执行完成后，通知用户等等 希望在不引入其他 Web 页面的情况下通过现有的工具 Jenkins 使用 Shell 脚本如何实现呢？\n写一个脚本去循环一个 list 里所有的 hostname，经考虑这个 list 最好是一个 file，这样方便后续处理。 这样当用户通过执行 Jenkins job 传入新的 hostname 时，使用新的 hostname 到 file 里进行 grep，查看是否已存在。 如果 grep 到，不添加；如果没有 grep 到，将这个 hostname 添加到 file 里。 将修改后的 file 添加到 git 仓库里，这样下次 Jenkins 的定时任务就会执行最近添加的 hostname 了。 实现重点 # 使用 expect。在使用 ssh 连接远程虚拟机的时候需要实现与远程连接时实现交互，例如：可以期待屏幕上的输出，然后进而进行相应的输入。在使用 expect 之前需要先安装，以 Redhat 的安装命令为例： sudo yum install expect 来进行安装。\n更多有关 expect 使用的可以参看这个连接：http://xstarcd.github.io/wiki/shell/expect.html\n使用了 Shell 数组。使用 Shell 读取文件数据，进行登录操作，将操作失败的记录到一个数组里，然后打印出来。\n在通过 Jenkins 提交新的 hostname 到 Git 仓库时，origin 的 URL 需要是 https://${USERNAME}:${PASSWORD}@git.company.com/scm/vmm.git 或 git@company.com:scm/vmm.git（需要提前在执行机器上生成了 id_rsa.pub）\n代码已经上传 GitHub 请参看 https://github.com/shenxianpeng/vmm.git\n最终效果 # 开始执行，提供输入新的 hostname # 执行完成，将执行结果归档以便查看 # 打开归档结果如下 # ##################################################### ######### VM login check via SSH results ############ ##################################################### # # # Compelted (success) 14/16 (total) login vm check. # # # # Below 2 host(s) login faied, need to check. # # # abc.company.com xyz.company.com # # ##################################################### 最后 # 现在技术的更新非常快，尤其作为 DevOps 工程师，各种工具层出不穷，想要每一样工具都掌握几乎是不可能的。\n只会工具不了解其背后的原理，等到新工具出现替换掉旧的工具，其实这些年是没有进步的。\n只有认真的把在工作中遇到的每个问题背后来龙去脉去搞懂，才能地基打的稳，这样不论工具怎么变，学习起来都会很快。\n掌握操作系统，Shell，以及一门擅长的编程语言之后再去学习那些工具，要不永远都是漂浮在空中。\n","date":"2020-06-13","externalUrl":null,"permalink":"/posts/vm-status-check-via-jenkins/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 实现定期批量登录远程虚拟机，并支持用户添加新的主机名称，提供了完整的实现代码和步骤。","title":"这也能用Jenkins？快速实现一个定期批量登录远程虚拟机并支持添加新的主机名称的Job","type":"posts"},{"content":" 本文对于同样在 AIX 遇到这个问题的人会非常有帮助。另外，不要被标题无聊到，解决问题的过程值得参考。\n分享一个花了两天时间才解决的一个问题：使用 Jenkins Artifactory 插件上传制品到 https 协议的企业级的 Artifactory 失败。该问题只在 AIX 平台上出现的，其他 Windows，Linux, Unix 均正常。\n前言 # 最近计划将之前使用的 Artifactory OSS（开源版）迁移到 Artifactory Enterprise（企业版）上。为什么要做迁移？这里有一个 Artifactory 对比的矩阵图 https://www.jfrog.com/confluence/display/JFROG/Artifactory+Comparison+Matrix\n简单来说，开源版缺少与 CI 工具集成时常用的 REST API 功能，比如以下常用功能\n设置保留策略(Retention)。设置上传的制品保留几天等，达到定期清理的目的。 提升(Promote)。通过自动化测试的制品会被提升到 stage（待测试）仓库，通过手工测试的提升到 release（发布）仓库。 设置属性(set properties)。对于通过不同阶段的制品通过 CI 集成进行属性的设置。 正好公司已经有企业版了，那就开始迁移吧。本以为会很顺利的完成，没想到唯独在 IBM 的 AIX 出现上传制品失败的问题。\n环境信息\nJenkins ver. 2.176.3 Artifactory Plugin 3.6.2 Enterprise Artifactory 6.9.060900900 AIX 7.1 \u0026amp;\u0026amp; java version 1.8.0 以下是去掉了无相关的信息的错误日志。\n[consumer_0] Deploying artifact: https://artifactory.company.com/artifactory/generic-int-den/database/develop/10/database2_cdrom_opt_AIX_24ec6f9.tar.Z Error occurred for request GET /artifactory/api/system/version HTTP/1.1: A system call received a parameter that is not valid. (Read failed). Error occurred for request PUT /artifactory/generic-int-den/database/develop/10/database2_cdrom_opt_AIX_24ec6f9.tar.Z;build.timestamp=1591170116591;build.name=develop;build.number=10 HTTP/1.1: A system call received a parameter that is not valid. (Read failed). Error occurred for request PUT /artifactory/generic-int-den/database/develop/10/database2_cdrom_opt_AIX_24ec6f9.tar.Z;build.timestamp=1591170116591;build.name=develop;build.number=10 HTTP/1.1: A system call received a parameter that is not valid. (Read failed). [consumer_0] An exception occurred during execution: java.lang.RuntimeException: java.net.SocketException: A system call received a parameter that is not valid. (Read failed) at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:44) at org.jfrog.build.extractor.producerConsumer.ConsumerRunnableBase.run(ConsumerRunnableBase.java:11) at java.lang.Thread.run(Thread.java:785) Caused by: java.net.SocketException: A system call received a parameter that is not valid. (Read failed) at java.net.SocketInputStream.socketRead(SocketInputStream.java:127) at java.net.SocketInputStream.read(SocketInputStream.java:182) at java.net.SocketInputStream.read(SocketInputStream.java:152) at com.ibm.jsse2.a.a(a.java:227) at com.ibm.jsse2.a.a(a.java:168) at com.ibm.jsse2.as.a(as.java:702) at com.ibm.jsse2.as.i(as.java:338) at com.ibm.jsse2.as.a(as.java:711) at com.ibm.jsse2.as.startHandshake(as.java:454) at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:436) at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384) at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:374) at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.ServiceUnavailableRetryExec.execute(ServiceUnavailableRetryExec.java:85) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) at org.jfrog.build.client.PreemptiveHttpClient.execute(PreemptiveHttpClient.java:89) at org.jfrog.build.client.ArtifactoryHttpClient.execute(ArtifactoryHttpClient.java:253) at org.jfrog.build.client.ArtifactoryHttpClient.upload(ArtifactoryHttpClient.java:249) at org.jfrog.build.extractor.clientConfiguration.client.ArtifactoryBuildInfoClient.uploadFile(ArtifactoryBuildInfoClient.java:692) at org.jfrog.build.extractor.clientConfiguration.client.ArtifactoryBuildInfoClient.doDeployArtifact(ArtifactoryBuildInfoClient.java:379) at org.jfrog.build.extractor.clientConfiguration.client.ArtifactoryBuildInfoClient.deployArtifact(ArtifactoryBuildInfoClient.java:367) at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:39) ... 2 more Failed uploading artifacts by spec 很奇怪会出现上述问题，从开源版的 Artifactory 迁移到企业版的 Artifactory，它们之间最直接的区别是使用了不同的传输协议，前者是 http 后者是 https。\nHTTPS 其实是有两部分组成：HTTP + SSL/TLS，也就是在 HTTP 上又加了一层处理加密信息的模块，因此更安全。\n本以为 Google 一下就能找到此类问题的解决办法，可惜这个问题在其他平台都没有，只有 AIX 上才有，肯定这个 AIX 有什么“过人之处”和其他 Linux/Unix 不一样。\n使用 curl 来替代 # 由于上述问题重现在需要重新构建，比较花时间，就先试试直接用 curl 命令来调用 Artifactory REST API 看看结果。\n做了以下测试，查看 Artifactory 的版本\ncurl https://artifactory.company.com/artifactory/api/system/version curl: (35) Unknown SSL protocol error in connection to artifactory.company.com:443 # 打开 -v 模式，输出更多信息 bash-4.3$ curl -v https://artifactory.company.com/artifactory/api/system/version * Trying 10.18.12.95... * Connected to artifactory.company.com (10.18.12.95) port 443 (#0) * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH * TLSv1.2 (OUT), TLS handshake, Client hello (1): * TLSv1.2 (IN), TLS handshake, Server hello (2): * NPN, negotiated HTTP1.1 * TLSv1.2 (IN), TLS handshake, Certificate (11): * TLSv1.2 (OUT), TLS alert, Server hello (2): * Unknown SSL protocol error in connection to artifactory.company.com:443 * Closing connection 0 curl: (35) Unknown SSL protocol error in connection to artifactory.company.com:443 果然也出错了，curl 也不行，可能就是执行 curl 命令的时候没有找到指定证书，查了 curl 的 help，有 --cacert 参数可以指定 cacert.pem 文件。\nbash-4.3$ curl --cacert /var/ssl/cacert.pem https://artifactory.company.com/artifactory/api/system/version { \u0026#34;version\u0026#34; : \u0026#34;6.9.0\u0026#34;, \u0026#34;revision\u0026#34; : \u0026#34;60900900\u0026#34;, \u0026#34;addons\u0026#34; : [ \u0026#34;build\u0026#34;, \u0026#34;docker\u0026#34;, \u0026#34;vagrant\u0026#34;, \u0026#34;replication\u0026#34;, \u0026#34;filestore\u0026#34;, \u0026#34;plugins\u0026#34;, \u0026#34;gems\u0026#34;, \u0026#34;composer\u0026#34;, \u0026#34;npm\u0026#34;, \u0026#34;bower\u0026#34;, \u0026#34;git-lfs\u0026#34;, \u0026#34;nuget\u0026#34;, \u0026#34;debian\u0026#34;, \u0026#34;opkg\u0026#34;, \u0026#34;rpm\u0026#34;, \u0026#34;cocoapods\u0026#34;, \u0026#34;conan\u0026#34;, \u0026#34;vcs\u0026#34;, \u0026#34;pypi\u0026#34;, \u0026#34;release-bundle\u0026#34;, \u0026#34;replicator\u0026#34;, \u0026#34;keys\u0026#34;, \u0026#34;chef\u0026#34;, \u0026#34;cran\u0026#34;, \u0026#34;go\u0026#34;, \u0026#34;helm\u0026#34;, \u0026#34;rest\u0026#34;, \u0026#34;conda\u0026#34;, \u0026#34;license\u0026#34;, \u0026#34;puppet\u0026#34;, \u0026#34;ldap\u0026#34;, \u0026#34;sso\u0026#34;, \u0026#34;layouts\u0026#34;, \u0026#34;properties\u0026#34;, \u0026#34;search\u0026#34;, \u0026#34;filtered-resources\u0026#34;, \u0026#34;p2\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;webstart\u0026#34;, \u0026#34;support\u0026#34;, \u0026#34;xray\u0026#34; ], \u0026#34;license\u0026#34; : \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; } 试了下成功了。\n到这里问题已经解决了，只要使用 curl 调用 Artifactory REST API 就能完成上传操作了。但我用的 Jenkins Artifactory Plugin，如果使用 curl 我需要把之前的代码重新再实现一遍，然后再测试，就为了 AIX 一个平台的问题，实在是“懒”的重新开始。本着这样懒惰的性格，还得继续解决 Jenkins 调用 agent 去执行上传失败的问题。\n最终解决办法 # 尝试设置 SSL_CERT_FILE 环境变量 # 想试试用上述的办法来解决 Jenkins 的问题。如果能有一个环境变量能设置指定 cacert.pem 文件的路径，那样在 Jenkins 调用 agent 执行上传时候就能找到证书，可能就能解决这个问题了。果然是有这样的环境变量的 SSL_CERT_FILE，设置如下\nset SSL_CERT_FILE=/var/ssl/cacert.pem 设置好环境变量之后，通过 curl 调用，再不需要使用 --cacert 参数了。这下看起来有戏了，带着喜悦的心情把这个环境变量加到 agent 机器上，设置如下：\n或者可以修改 agent 机器上的 /etc/environment 文件。\n结果经测试错误信息依旧，看来 Jenkins 执行的 remote.jar 进行上传时跟本地配置环境没有关联，看来需要从执行 remote.jar 着手，把相应的设置或是环境变量在启动 remote.jar 时传进去。\nJenkins 管理 agent 的原理是通过在 agent 上启动一个 remote.jar 实现的\n在启动 remote.jar 时设置环境变量 # java 的 -D 参数可以完成这一点。\n进行了大量的搜索和尝试，最终在 IBM 的官方找到了这篇文档 https://www.ibm.com/support/knowledgecenter/SSYKE2_8.0.0/com.ibm.java.security.component.80.doc/security-component/jsse2Docs/matchsslcontext_tls.html\n文档大意是，IBM SDK 系统属性 com.ibm.jsse2.overrideDefaultTLS=[true|false] 有 true 和 false 两个值，如果想要与 Oracle SSLContext.getInstance(\u0026quot;TLS\u0026quot;) 的行为相匹配，请将此属性设置为 true，默认值为 false。\n下表显示了系统属性对 SSLContext.getInstance(\u0026ldquo;TLS\u0026rdquo;) 的影响\nProperty value setting Protocol enabled false TLS V1.0 true TLS V1.0, V1.1, and V1.2 绝大多数的 Java 应用都是使用 Oracle 的 JDK 来开发的，这里要与 Oracle 的行为保持一致；另外 IBM 的 SDK 默认协议只有 TLS V1.0，而上面的 log 可以看到使用的 TLSv1.2 协议，因此需要将属性设置为 true。\n最终在 Jenkins 的 agent 配置里将 JVM Options 区域加上这句 -Dcom.ibm.jsse2.overrideDefaultTLS=true，断开连接，重新启动 agent，再次执行 Pipeline，成功的把 AIX 上的制品上传到 Artifactory 上了，问题解决了。\n总结 # 遇到问题并解决问题是一件非常爽的事，从中也学到了很多之前不曾了解过的知识，解决问题的过程比 Google 随便查查更让人印象深刻，再遇到此类问题可能就会举一反三了。\n另外，凡事如果觉得自己在短时间内没有头绪、自己搞不定的时候尽快寻求有经验的同事的帮助。感谢帮助我的同事们，没有他们的帮助和指导就不能这么快的解决问题。\n","date":"2020-06-03","externalUrl":null,"permalink":"/posts/java-net-socketexception-on-aix/","section":"Posts","summary":"本文介绍了在 AIX 上使用 Jenkins Artifactory 插件上传制品到 https 协议的 Artifactory 失败的问题及其解决方法，包括设置环境变量和调整 Java 系统属性。","title":"解决 Jenkins Artifactory Plugin 仅在 AIX 上传制品到 https 协议的 Artifactory 失败的问题","type":"posts"},{"content":"A popular saying in the programmer community is, \u0026ldquo;Programmers who can code can\u0026rsquo;t beat those who can make PPTs.\u0026rdquo; I remember a lyric from the song \u0026ldquo;Let Yourself Go\u0026rdquo; at New Oriental\u0026rsquo;s 2019 annual meeting that resonated with most programmers:\n\u0026ldquo;Working hard and tired, what\u0026rsquo;s the point of achievements? In the end, we can\u0026rsquo;t beat those who make PPTs.\u0026rdquo;\nFor a while, everyone seemed to agree with this statement, expressing their dissatisfaction and helplessness.\n(I) # Having worked in the field for over 10 years and witnessed various talented individuals online and at work, my understanding of \u0026ldquo;true ability\u0026rdquo; has evolved.\nInitially, I believed that those who were quiet, reserved, and technically proficient were true masters, while those who showed off their limited technical skills were merely showing off. However, when sharing technical insights with the team, I discovered that clearly explaining something to most people who don\u0026rsquo;t understand it is also a skill in itself.\nFirst, you need to fully understand the subject matter before explaining it to others. Second, you need to anticipate the questions others might ask and be able to answer them. Finally, you need to establish a clear narrative and present it logically. (II) # Recently, in Wu Jun\u0026rsquo;s \u0026ldquo;Reading and Writing\u0026rdquo; course on the \u0026ldquo;Get\u0026rdquo; app, he gave an example:\nA project team has three members. The first is highly skilled in professional work, doing the most important tasks and guiding others. The second excels at team organization, boosting morale and leading the team to overcome challenges. The third can clearly communicate the team\u0026rsquo;s work.\nIf the boss needs to promote one of them, who has the highest chance? Many would think the highly skilled professional. However, in reality, it\u0026rsquo;s often the third person. Why?\nLet\u0026rsquo;s imagine the three presenting a report:\nThe first person presents a lot of technical details, boring the leadership. The team\u0026rsquo;s work isn\u0026rsquo;t recognized, and no resources are secured. They might not be chosen for future presentations, or the project might be scrapped.\nIf the organizer presents, they come across as an administrator, peripheral to the project, giving directions without understanding the details. The presentation will be ineffective.\nIf the communicator presents, the team\u0026rsquo;s work is most likely to be recognized, and resources secured. Everyone benefits, leading to future presentations. Over time, they are perceived as the most impactful contributor.\nThis isn\u0026rsquo;t fiction. Look around your workplace; you\u0026rsquo;ll find similar situations. It\u0026rsquo;s a natural selection process within an organization. Those with good communication skills often give presentations and reports, gaining visibility and advancement opportunities.\n(III) # Many working in foreign companies notice that many executives are Indian. I\u0026rsquo;ve heard Indian colleagues present to leadership, and their presentations are far superior. Beyond English proficiency, their content and preparation stand out:\nExcellent PPTs using structured diagrams. Clear core message and focus. A coherent narrative that clearly explains the subject matter. You might find that you have everything they do, and even more, but they present better, leaving a strong impression on the audience, making them seem brilliant, while you just chuckle.\nAs programmers, improving technical skills is essential, but developing writing and communication skills will help build your brand and advance your career.\nIn Conclusion # I recently prepared an English PPT for a presentation to foreign leadership. My manager\u0026rsquo;s feedback was incredibly helpful, revealing significant differences between my presentation and theirs.\nFor example, consider explaining the use of Jenkins Shared Libraries:\nBefore: In the early stage of doing this work one year ago, I wrote many duplicate code such as sending emails, printing logs. Shared Libraries could help to solve this problem \u0026hellip;\nAfter: Just like writing any application code, that we need to create functions, subroutines for reuse and sharing purpose. The same logic applies to Jenkins configuration code \u0026hellip;\nThere\u0026rsquo;s no need to mention initial setbacks and mistakes. While honest, it doesn\u0026rsquo;t benefit you. Feedback included improvements to the opening, transitions between slides, and closing remarks.\nMy prejudice against PPTs has vanished, and I now consider creating effective PPTs a valuable skill.\nTime to get back to revising my PPT and practicing my delivery\u0026hellip;\n","date":"2020-05-30","externalUrl":null,"permalink":"/en/posts/programmers-read-and-write/","section":"Posts","summary":"This article explores the importance of writing skills for programmers, highlighting the role of communication and expression in career advancement, and sharing personal experiences and insights on writing.","title":"Beyond Coding - The Importance of Writing for Programmers","type":"posts"},{"content":"","date":"2020-05-30","externalUrl":null,"permalink":"/en/tags/others/","section":"Tags","summary":"","title":"Others","type":"tags"},{"content":"A common problem I encounter when developing Jenkins Declarative Pipelines is this: the modified Pipeline looks fine, but when it\u0026rsquo;s submitted to the code repository and a Jenkins build is performed, a syntax error is discovered. Then I have to modify, submit, and build again, potentially finding other unnoticed syntax issues.\nTo reduce the frequency of submitting to the code repository due to syntax errors, it would be helpful to perform basic syntax validation before submission to check for syntax errors in the current Pipeline.\nAfter investigation, I found that Jenkins itself provides a syntax check REST API that can be directly used to validate Declarative Pipelines. This method requires executing a long curl command, which seems cumbersome. It would be much better if this could be run directly within the IDE.\nVS Code, being the most popular IDE currently, indeed has relevant plugins.\nBelow are two methods for checking for syntax errors in Jenkinsfile files for Jenkins Declarative Pipelines. Both methods use the Jenkins REST API.\nNote:\nCurrently, only Declarative Pipelines support syntax validation; Scripted Pipelines do not.\nIf you use the Jenkins replay feature or develop Pipelines using the Jenkins web page, this problem does not exist.\nREST API # If your project uses Jenkins Shared Libraries, for easier use of the REST API, consider creating a linter.sh file in that repository and adding it to your .gitignore. This allows you to configure your username and password in the file without accidentally committing them to the Git repository.\nThe following is the linter.sh script content for reference.\n# How to use # sh linter.sh your-jenkinsfile-path # Replace with your Jenkins username username=admin # Replace with your Jenkins password password=admin # Replace with your Jenkins URL JENKINS_URL=http://localhost:8080/ PWD=`pwd` JENKINS_FILE=$1 curl --user $username:$password -X POST -F \u0026#34;jenkinsfile=\u0026lt;$PWD/$JENKINS_FILE\u0026#34; $JENKINS_URL/pipeline-model-converter/validate Let\u0026rsquo;s test the effect: sh linter.sh your-jenkinsfile-path\nExample 1\n$ sh linter.sh Jenkinsfile Errors encountered validating Jenkinsfile: WorkflowScript: 161: Expected a stage @ line 161, column 9. stages { ^ Example 2\nsh linter.sh Jenkinsfile Errors encountered validating Jenkinsfile: WorkflowScript: 60: Invalid condition \u0026#34;failed\u0026#34; - valid conditions are [always, changed, fixed, regression, aborted, success, unsuccessful, unstable, failure, notBuilt, cleanup] @ line 60, column 9. failed{ ^ # Change \u0026#34;failed\u0026#34; to \u0026#34;failure\u0026#34;, and execute again; it succeeds. sh linter.sh Jenkinsfile Jenkinsfile successfully validated. When the Pipeline is very long, it\u0026rsquo;s always difficult to find unmatched brackets or missing parentheses. This script allows you to check for problems before submitting.\nJenkinsfile successfully validated. Jenkins Pipeline Linter Connector Plugin # The second method is more universal; any Declarative Pipeline can use this plugin to check for syntax errors.\nInstalling the Plugin # Search for Jenkins Pipeline Linter Connector in the VSCode plugin marketplace.\nConfiguring the Plugin # Open File -\u0026gt; Preferences -\u0026gt; Settings -\u0026gt; Extensions, find Jenkins Pipeline Linter Connector, and configure it as follows.\nRunning the Plugin # Right-click -\u0026gt; Command Palette -\u0026gt; Validate Jenkinsfile\nOr\nUse the shortcut Shift + Alt + V\nExecution Result # Summary # If you use VSCode as your development tool, the Jenkins Pipeline Linter Connector plugin is recommended.\nFor Jenkins Shared Libraries repositories, consider creating a shell script to perform validation by executing the script.\nOf course, if you only use a simple Jenkinsfile, you can also write it on the Jenkins web Pipeline page, which has built-in syntax checking.\nIf you have other methods, please feel free to leave a comment and let me know.\n","date":"2020-05-23","externalUrl":null,"permalink":"/en/posts/jenkins-pipeline-linter-connector/","section":"Posts","summary":"This article introduces two methods to ensure there are no syntax errors before submitting a Jenkins Pipeline using the REST API for syntax validation and using the VSCode plugin for syntax checking.","title":"How to Ensure No Syntax Errors Before Submitting a Jenkins Pipeline","type":"posts"},{"content":" Record JMeter Scripts # use JMeter\u0026rsquo;s HTTP(S) Test Script Recorder, please refer to this official document https://jmeter.apache.org/usermanual/jmeter_proxy_step_by_step.html\nRunning JMeter Scripts # Debug scripts on JMeter in GUI Mode\nYou can debug your record scripts in GUI Mode until there are no errors\nrun test scripts in Non-GUI Mode(Command Line mode) recommend\njmeter -n -t ..\\extras\\Test.jmx -l Test.jtl Running JMeter Scripts on Jenkins # Need Tools # Jmeter - Web Request Load Testing Jmeter-plugins ServerAgent-2.2.1 - PerfMon Agent to use with Standard Set Test server # Two virtual machines\nSystem under test Jmeter execution machine, this server is also Jenkins server Implement # Develop test script # Record Scripts - use JMeter\u0026rsquo;s HTTP(S) Test Script Recorder, please refer to this official document https://jmeter.apache.org/usermanual/jmeter_proxy_step_by_step.html\nCreate Jenkins job for running JMeter scripts # Create a new item-\u0026gt;select Freestyle project\nAdd build step-\u0026gt;Execute Windows batch command\n//access to jenkins jobs workspace, empty the last test results cmd cd C:\\Users\\peter\\.jenkins\\jobs\\TEST-122 Upload large data\\workspace del /Q \u0026#34;jtl\u0026#34;\\* del /Q \u0026#34;PerfMon Metrics Collector\u0026#34;\\* Add build step-\u0026gt;Execute Windows batch command\n//add first run jmeter script command, if you want run others script you can continue to add \u0026#34;Execute Windows batch command\u0026#34; jmeter -n -t script/UploadLargeData-1.jmx -l jtl/UploadLargeData-1.jtl Configure build email - Configure System\n//Configure System, Extended E-mail Notification SMTP server: smtp.gmail.com //Job Configure, Enable \u0026#34;Editable Email Notification\u0026#34; Project Recipient List: xianpeng.shen@gmail.com Project Reply-To List: $DEFAULT_REPLYTO Content Type: HTML (text/html) Default Subject:$DEFAULT_SUBJECT Default Content: ${SCRIPT, template=\u0026#34;groovy-html.template\u0026#34;} //Advance setting Triggers: Always Send to Recipient List Generate test report # JMeter-\u0026gt;Add listener-\u0026gt;add jp@gc - PerfMon Metrics Collector, browse Test.jtl, click right key on graph Export to CSV\nAnalyze test results # Introduction test scenarios\nUsing 1, 5, 10, 20, 30, (50) users loading test, record every group user test results\nGlossary\nSample(label) - This indicates the number of virtual users per request. Average - It is the average time taken by all the samples to execute specific label Median - is a number which divides the samples into two equal halves. %_line - is the value below which 90, 95, 99% of the samples fall. Min - The shortest time taken by a sample for specific label. Max - The longest time taken by a sample for specific label. Error% - percentage of failed tests. Throughput - how many requests per second does your server handle. Larger is better. KB/Sec - it is the Throughput measured in Kilobytes per second. Example: Test results of each scenario shown in the following table\nUser # Samples Average Median 90% Line 95% LIne Min Max Error % Throughput Received Send KB/sec 1 31 348 345 452 517 773 5 773 0.00% 2.85215 2.5 5 155 1166 1164 1414 1602 1639 9 1821 0.00% 4.26445 3.73 10 310 2275 2299 2687 2954 3671 20 4104 0.00% 4.38547 3.84 20 620 4479 4620 5113 6152 6435 39 6571 0.00% 4.42826 3.88 30 930 6652 6899 7488 9552 10051 4 10060 0.00% 4.46776 3.91 Test results analysis chart\n","date":"2020-05-09","externalUrl":null,"permalink":"/en/posts/jmeter-performance-testing/","section":"Posts","summary":"A guide on using JMeter for performance testing, including recording scripts, running tests in GUI and non-GUI modes, and integrating with Jenkins for automated testing.","title":"How to use JMeter to do Performance Testing","type":"posts"},{"content":"","date":"2020-05-09","externalUrl":null,"permalink":"/en/tags/jmeter/","section":"Tags","summary":"","title":"JMeter","type":"tags"},{"content":" 背景 # 最近我们团队需要将一些示例和例子从内部的 Bitbucket 同步到 GitHub。我了解 GitHub 可以创建公共的或是私人的仓库，但我们需要保持以下两点\n只分享我们想给客户分享的内容 不改变当前的工作流程，即继续使用 Bitbucket 因此我们需要在 GitHub 上创建相应的仓库，然后将内部 Bitbucket 仓库中对应的 master 分支定期的通过 CI job 同步到 BitHub 上去。\n分支策略 # 首先，需要对 Bitbucket 进行分支权限设置\nmaster 分支只允许通过 Pull Request 来进行修改 Pull Request 默认的 reviewer 至少需要一人，并且只有同意状态才允许合并 其次，为了方便产品、售后等人员使用，简化分支策略如下\n从 master 分支上创建 feature 或是 bugfix 分支（取决于你的修改目的） 然后将你的更改提交到自己的 feature 或 bugfix 分支 在你自己的分支通过测试后，提交 Pull Request 到 master 分支 当 reviewer 同意状态，才能进行合并进入到 master 分支 Jenkins Pipeline # 基于这样的工作不是特别的频繁，也为了方便维护 Jenkins Pipeline 的简单和易于维护，我没有在需要同步的每个仓库里添加 Jenkinsfile 或在 Bitbucket 里添加 webhooks。有以下几点好处：\n只创建一个 Jenkins Job，用一个 Jenkinsfile 满足所有仓库的同步 减少了冗余的 Jenkinsfile 的代码，修改时只需更维护一个文件 不需要在每个仓库里添加一个 Jenkinsfile，更纯粹的展示示例，避免给非 IT 人员造成困扰 不足之处，不能通过 SCM 来触发构建，如果想通过 webhooks 来触发，有的公司需要申请权限来添加 webhooks 比较麻烦；另外可能无法区分从哪个仓库发来的请求，实现指定仓库的同步。\n因此如果不是特别频繁的需要同步，提供手动或是定时同步即可。\n// 这个 Jenkinsfile 是用来将 Bitbucket 仓库的 master 分支同步到 GitHub 仓库的 master 分支 // This Jenkinsfile is used to synchronize Bitbucket repositories master branches to GitHub repositories master branches. @Library(\u0026#39;jenkins-shared-library@develop\u0026#39;) _ def email = new org.cicd.email() pipeline { agent { label \u0026#34;main-slave\u0026#34; } parameters { booleanParam(defaultValue: false, name: \u0026#39;git-repo-win\u0026#39;, summary: \u0026#39;Sync internal git-repo-win master branch with external git-repo-win on GitHub\u0026#39;) booleanParam(defaultValue: true, name: \u0026#39;git-repo-lin\u0026#39;, summary: \u0026#39;Sync internal git-repo-lin master branch with external git-repo-lin on GitHub\u0026#39;) booleanParam(defaultValue: false, name: \u0026#39;git-repo-aix\u0026#39;, summary: \u0026#39;Sync internal git-repo-aix master branch with external git-repo-aix on GitHub\u0026#39;) booleanParam(defaultValue: false, name: \u0026#39;git-repo-sol\u0026#39;, summary: \u0026#39;Sync internal git-repo-sol master branch with external git-repo-sol on GitHub\u0026#39;) } options { timestamps() buildDiscarder(logRotator(numToKeepStr:\u0026#39;50\u0026#39;)) } stages { stage(\u0026#34;Synchronous master branch\u0026#34;){ steps{ script { try { params.each { key, value -\u0026gt; def repoName = \u0026#34;$key\u0026#34; if ( value == true) { echo \u0026#34;Start synchronizing $key Bitbucket repository.\u0026#34; sh \u0026#34;\u0026#34;\u0026#34; rm -rf ${repoName} return_status=0 git clone -b master ssh://git@git.your-company.com:7999/~xshen/${repoName}.git cd ${repoName} git config user.name \u0026#34;Sync Bot\u0026#34; git config user.email \u0026#34;bot@your-company.com\u0026#34; git remote add github git@github.com:shenxianpeng/${repoName}.git git push -u github master return_status=\u0026#34;\\$?\u0026#34; if [ \\$return_status -eq 0 ] ; then echo \u0026#34;Synchronize ${repoName} from Bitbucket to GitHub success.\u0026#34; cd .. rm -rf ${repoName} exit 0 else echo \u0026#34;Synchronize ${repoName} from Bitbucket to GitHub failed.\u0026#34; exit 1 fi\u0026#34;\u0026#34;\u0026#34; } else { echo \u0026#34;${repoName} parameter value is $value, skip it.\u0026#34; } } cleanWs() } catch (error) { echo \u0026#34;Some error occurs during synchronizing $key process.\u0026#34; } finally { email.Send(currentBuild.currentResult, env.CHANGE_AUTHOR_EMAIL) } } } } } } 以上的 Jenkinsfile 的主要关键点是这句 params.each { key, value -\u0026gt; }，可以通过对构建时选择参数的进行判断，如果构建时参数已勾选，则会执行同步脚本；否则跳过同步脚本，循环到下一个参数进行判断，这样就实现了可以对指定仓库进行同步。\nBackground # Recently our team need to share code from internal Bitbucket to external GitHub. I know GitHub can create private and public repository, but we have these points want to keep.\nonly share the code what we want to share not change current work process, continue use Bitbucket. So we have created corresponding repositories in the internal Bitbucket, and the master branches of these repositories will periodically synchronize with the master branches of corresponding repositories on GitHub via Jenkins job.\nBranch Strategy # Then the work process will be like\nCreate a feature or bugfix branch (it depends on the purpose of your modification).\nCommit changes to your feature/bugfix branch.\nPlease pass your feature/bugfix branch test first then create a Pull Request from your branch to master branch, at least one reviewer is required by default.\nAfter the reviewer approved, you or reviewer could merge the Pull Request, then the changes will be added to the master branch.\nTiming trigger CI job will sync code from internal repositories master branch to GitHub master branch by default. also support manual trigger.\nJenkins Job # Base on this work is not very frequency, so I want make the Jenkins job simple and easy to maintain, so I don\u0026rsquo;t create every Jenkinsfile for every Bitbucket repositories.\nPros\nOnly one Jenkinsfile for all Bitbucket repositories. Less duplicate code, less need to change when maintenance. Don\u0026rsquo;t need to add Jenkinsfile into very Bitbucket repositories. Cons\nCan not support SCM trigger, in my view this need add Jenkinsfile into repository. The main part for this Jenkinsfile is below, use this function params.each { key, value -\u0026gt; } can by passing in parameters when start Jenkins build.\n","date":"2020-05-05","externalUrl":null,"permalink":"/posts/sync-from-bitbucket-to-github/","section":"Posts","summary":"介绍如何通过 Jenkins 将 Bitbucket 仓库的 master 分支同步到 GitHub。","title":"如何将 Bitbucket 仓库同步到 GitHub","type":"posts"},{"content":" Background # I have set several multi-branch pipeline and it can support Bitbucket Pull Request build. So, when developer create a Pull Request on Bitbucket, Jenkins can auto-trigger PR build. but this jenkins-plugin may not very stable, it had not work two times and I actually don\u0026rsquo;t know why it does that. But I know the use Git webhook is a direct and hard approach could solve this problem. After my test, the answer is yes. it works as expect.\nPrinciple # By setting Webhook events, you can listen for git push, create Pull requests and other events, and automatically trigger Jenkins scan when these events occur, so that Jenkins can get the latest branch (or Pull Request) created (or deleted), and automatically build Jenkins Job.\nSetting # Webhook name: test-multibranch Webhook URL: http://localhost:8080/multibranch-webhook-trigger/invoke?token=test-multibranch Test connection: 200(green), it passed. Events: Repository: N/A Pull Request: Opened, Merged, Declined, Deleted. Active: enable Here is setting screenshots.\nAt first, I also enable Modified event, but I found when there is new merged commits into our develop branch(this is our PR target branch), the holding Pull Request will be triggered and merge develop branch back to source branch then re-build.\nThen I notice the Modified summary: A pull request\u0026rsquo;s description, title, or target branch is changed.\nThis is a nice feature to make sure the source code integrate with target branch and build passed, but this is too frequent for our product builds, because our product pull request build on some Unix platform need almost 3 hours, if has 5 Pull Requests waiting to review, when new commits into develop branch, these 5 PR need to rebuild again, this takes up all the build machines, resulting in those that need to be built not getting the resources.\nAfter enable above Pull Request event and have these functions.\nwhen open a new Pull Request on Bitbucket, auto create Pull Request branch and build in Jenkins. when merge the Pull Request Bitbucket, auto delete Pull Request branch in Jenkins. when decline the Pull Request Bitbucket, auto delete Pull Request branch in Jenkins. when delete the Pull Request Bitbucket, auto delete Pull Request branch in Jenkins. For other specific branches build my Jenkins job support manually build and timing trigger, so this event settings currently good to me.\nIf there is new settings need to add, I will keep update this article in the future.\n","date":"2020-04-24","externalUrl":null,"permalink":"/en/posts/bitbucket-webhooks/","section":"Posts","summary":"How to configure Bitbucket webhooks to trigger Jenkins builds for multi-branch pipelines, ensuring that Jenkins can automatically respond to events like pull requests and branch updates.","title":"Bitbucket Webhooks Configuration","type":"posts"},{"content":"","date":"2020-04-24","externalUrl":null,"permalink":"/en/tags/webhook/","section":"Tags","summary":"","title":"Webhook","type":"tags"},{"content":"","date":"2020-04-20","externalUrl":null,"permalink":"/en/tags/pipeline/","section":"Tags","summary":"","title":"Pipeline","type":"tags"},{"content":"This is the second time I\u0026rsquo;ve encountered this problem while using Jenkins declarative pipelines. The first time I encountered this problem was in a Pipeline with about 600 lines of code, and I encountered the following error:\norg.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed: General error during class generation: Method code too large! java.lang.RuntimeException: Method code too large! at groovyjarjarasm.asm.MethodWriter.a(Unknown Source) [...] At that time, I also used Jenkins Shared Libraries, but the code organization was not very good, and many steps had not yet been separated as individual methods. To solve this problem, after a refactoring, I changed the original 600+ lines of Pipeline to the current 300+ lines. Unfortunately, as I continued to add features, I encountered this problem again recently.\nThe reason for this problem is that Jenkins puts the entire declarative pipeline into a single method, and at a certain size, the JVM fails due to java.lang.RuntimeException: Method code too large!. It seems that I have a method exceeding 64k.\nThere is already a ticket for this issue on Jenkins JIRA, but it has not been resolved yet. There are currently three solutions to this problem, but they each have their own advantages and disadvantages.\nMethod 1: Move Steps into Methods Outside the Pipeline # Since mid-2017, you can declare a method at the end of the pipeline and then call it in the declarative pipeline. This allows us to achieve the same effect as shared libraries, but avoids the maintenance overhead.\npipeline { agent any stages { stage(\u0026#39;Test\u0026#39;) { steps { whateverFunction() } } } } void whateverFunction() { sh \u0026#39;ls /\u0026#39; } Advantages Disadvantages No extra maintenance cost It\u0026rsquo;s unknown if this solution will always be effective All functionality is reflected in the Jenkinsfile When a method is used in multiple Jenkinsfiles, this method will still write a lot of repetitive code Method 2: Migrate from Declarative to Scripted Pipeline # Finally, we can migrate to a scripted pipeline. With it, we have complete freedom. But this will also lose the reason why we initially decided to use declarative pipelines. With a dedicated DSL, it\u0026rsquo;s easy to understand how the pipeline works.\nAdvantages Disadvantages No limitations Requires significant refactoring More prone to errors May require more code to achieve the same functionality Method 3: Use Shared Libraries # I currently use Jenkins Shared Libraries, with a shared library to execute some complex steps. Shared libraries seem to be widely used, especially in maintaining large and complex projects.\nMy final solution was to further reduce the code in the Pipeline. I also used the solution in Method 1, moving some steps outside the Pipeline {} block, especially those steps that are called repeatedly.\nAdvantages Disadvantages Reduces a lot of duplicate code Any modification will affect all references; it must be thoroughly tested before merging changes into referenced branches Can be used in chunks Difficult to understand what a step does if unfamiliar The generated Jenkinsfile will be easier to read Conclusion # Method 1: For single-repository integration, it can be implemented quickly, and most people can get started quickly. Method 2: Scripted pipelines offer few limitations and are suitable for advanced users familiar with Java and Groovy, and those with more complex needs. Method 3: For enterprise-level projects with many repositories that require extensive integration and a desire to learn about shared libraries, this method is recommended.\n","date":"2020-04-20","externalUrl":null,"permalink":"/en/posts/jenkins-troubleshooting/","section":"Posts","summary":"This article introduces three methods to solve the \u0026ldquo;Method code too large\u0026rdquo; exception in Jenkins declarative pipelines, including moving steps outside the pipeline, migrating from declarative to scripted pipelines, and using Shared Libraries.","title":"Three Ways to Solve the Jenkins Declarative Pipeline \"Method code too large\" Exception","type":"posts"},{"content":"\u0026ldquo;Quality at Speed\u0026rdquo; 是软件开发中的新规范。\n企业正在朝着 DevOps 方法论和敏捷文化迈进，以加快交付速度并确保产品质量。在 DevOps 中，连续和自动化的交付周期使快速可靠的交付成为可能的基础。\n这导致我们需要适当的持续集成和持续交付（CI/CD）工具。 一个好的 CI/CD 工具可以利用团队当前的工作流程，以最佳利用自动化功能并创建可靠的 CI/CD 管道为团队发展提供所需的动力。\n随着市场上大量 CI/CD 工具的出现，团队可能难以做出艰难的决定来挑选合适的工具。该列表包含市场上最好的 14 种 CI/CD 工具及其主要特性，使你和团队在选择过程中更加轻松。\n以下罗列出了目前市场上最流行的 14 种最佳 CI/CD 工具，希望该列表为你在选择 CI/CD 前提供了足够的信息，更多详细信息你也可以查看官网做更深入的了解。最终结合你的需求以及现有基础架构以及未来潜力和改进的空间是将影响你最终选择的因素，帮助你选择到最适合你的规格的 CI/CD 软件。\nJenkins # Jenkins 是一个开源自动化服务器，在其中进行集中构建和持续集成。它是一个独立的基于 Java 的程序，带有 Windows，macOS，Unix 的操作系统的软件包。Jenkins 支持软件开发项目的构建，部署和自动化，以及成百上千的插件来满足你的需求。它是市场上最具影响力的 CI/CD 工具之一。\nJenkins 主要特性：\n易于在各种操作系统上安装和升级 简单易用的界面 可通过社区提供的巨大插件资源进行扩展 在用户界面中轻松配置环境 支持主从架构的分布式构建 根据表达式构建时间表 在预构建步骤中支持 Shell 和 Windows 命令执行 支持有关构建状态的通知 许可：免费。Jenkins 是一个拥有活跃社区的开源工具。\n主页：https://jenkins.io/\nCircleCI # CircleCI 是一种 CI/CD 工具，支持快速的软件开发和发布。CircleCI 允许从代码构建，测试到部署的整个用户管道自动化。\n你可以将 CircleCI 与 GitHub，GitHub Enterprise 和 Bitbucket 集成，以在提交新代码行时创建内部版本。CircleCI 还可以通过云托管选项托管持续集成，或在私有基础架构的防火墙后面运行。\nCircleCI 主要特性:\n与 Bitbucket，GitHub 和 GitHub Enterprise 集成 使用容器或虚拟机运行构建 简易调试 自动并行化 快速测试 个性化的电子邮件和IM通知 连续和特定于分支机构的部署 高度可定制 自动合并和自定义命令以上传软件包 快速设置和无限构建 许可：Linux 计划从选择不运行任何并行操作开始。开源项目获得了另外三个免费容器。在注册期间，将看到价格以决定所需的计划。\n主页： https://circleci.com/\nTeamCity # TeamCity 是 JetBrains 的构建管理和持续集成服务器。\nTeamCity 是一个持续集成工具，可帮助构建和部署不同类型的项目。 TeamCity 在 Java 环境中运行，并与 Visual Studio 和 IDE 集成。该工具可以安装在 Windows 和 Linux 服务器上，支持 .NET 和开放堆栈项目。\nTeamCity 2019.1 提供了新的UI和本机 GitLab 集成。它还支持 GitLab 和 Bitbucket 服务器拉取请求。该版本包括基于令牌的身份验证，检测，Go测试报告以及 AWS Spot Fleet 请求。\nTeamCity主要特性:\n提供多种方式将父项目的设置和配置重用到子项目 在不同环境下同时运行并行构建 启用运行历史记录构建，查看测试历史记录报告，固定，标记以及将构建添加到收藏夹 易于定制，交互和扩展服务器 保持CI服务器正常运行 灵活的用户管理，用户角色分配，将用户分组，不同的用户身份验证方式以及带有所有用户操作的日志，以透明化服务器上所有活动 许可：TeamCity 是具有免费和专有许可证的商业工具。\n主页： https://www.jetbrains.com/teamcity/\nBamboo # Bamboo 是一个持续集成服务器，可自动执行软件应用程序版本的管理，从而创建了持续交付管道。Bamboo 涵盖了构建和功能测试，分配版本，标记发行版，在生产中部署和激活新版本。\nBamboo主要特性:\n支持多达 100 个远程构建代理 并行运行批次测试并快速获得反馈 创建图像并推送到注册表 每个环境的权限，使开发人员和测试人员可以在生产保持锁定状态的情况下按需部署到他们的环境中 在 Git，Mercurial，SVN Repos 中检测新分支，并将主线的CI方案自动应用于它们 触发器基于在存储库中检测到的更改构建。 推送来自 Bitbucket 的通知，已设置的时间表，另一个构建的完成或其任何组合。 许可：Bamboo 定价层基于代理（Slave）而不是用户，代理越多，花费越多。\n主页：https://www.atlassian.com/software/bamboo\nGitLab # GitLab 是一套用于管理软件开发生命周期各个方面的工具。 核心产品是基于 Web 的 Git 存储库管理器，具有问题跟踪，分析和 Wiki 等功能。\nGitLab 允许你在每次提交或推送时触发构建，运行测试和部署代码。你可以在虚拟机，Docker 容器或另一台服务器上构建作业。\nGitLab主要特性:\n通过分支工具查看，创建和管理代码以及项目数据 通过单个分布式版本控制系统设计，开发和管理代码和项目数据，从而实现业务价值的快速迭代和交付 提供真实性和可伸缩性的单一来源，以便在项目和代码上进行协作 通过自动化源代码的构建，集成和验证，帮助交付团队完全接受CI。 提供容器扫描，静态应用程序安全测试（SAST），动态应用程序安全测试（DAST）和依赖项扫描，以提供安全的应用程序以及许可证合规性 帮助自动化并缩短发布和交付应用程序的时间 许可：GitLab 是一个商业工具和免费软件包。它提供了在 GitLab 或你的本地实例和/或公共云上托管 SaaS 的功能。\n主页：https://about.gitlab.com/\nBuddy # Buddy 是一个 CI/CD 软件，它使用 GitHub，Bitbucket 和 GitLab 的代码构建，测试，部署网站和应用程序。它使用具有预安装语言和框架的 Docker 容器以及 DevOps 来监视和通知操作，并以此为基础进行构建。\nBuddy主要特性:\n易于将基于 Docker 的映像自定义为测试环境 智能变更检测，最新的缓存，并行性和全面的优化 创建，定制和重用构建和测试环境 普通和加密，固定和可设置范围：工作空间，项目，管道，操作 与 Elastic，MariaDB，Memcached，Mongo，PostgreSQL，RabbitMQ，Redis，Selenium Chrome 和 Firefox 关联的服务 实时监控进度和日志，无限历史记录 使用模板进行工作流管理，以克隆，导出和导入管道 一流的Git支持和集成 许可：Buddy 是免费的商业工具。\n主页：https://buddy.works/\nTravis CI # Travis CI 是用于构建和测试项目的CI服务。Travis CI 自动检测新提交并推送到 GitHub 存储库的提交。每次提交新代码后，Travis CI 都会构建项目并相应地运行测试。\n该工具支持许多构建配置和语言，例如 Node，PHP，Python，Java，Perl 等。\nTravis 主要特性:\n快速设置 GitHub项目监控的实时构建视图 拉取请求支持 部署到多个云服务 预装的数据库服务 通过构建时自动部署 为每个版本清理虚拟机 支持 macOS，Linux 和 iOS 支持多种语言，例如 Android，C，C＃，C ++，Java，JavaScript（带有Node.js），Perl，PHP，Python，R，Ruby 等。 许可：Travis CI 是一项托管的 CI/CD 服务。私人项目可以在 travis-ci.com 上进行收费测试。可以在 travis-ci.org 上免费应用开源项目。\n主页：https://travis-ci.com\nCodeship # Codeship 是一个托管平台，可多次支持早期和自动发布软件。通过优化测试和发布流程，它可以帮助软件公司更快地开发更好的产品。\nCodeship 主要特性:\n与所选的任何工具，服务和云环境集成 易于使用。提供快速而全面的开发人员支持。 借助CodeShip的交钥匙环境和简单的UI，使构建和部署工作更快 选择AWS实例大小，CPU和内存的选项 通过通知中心为组织和团队成员设置团队和权限 无缝的第三方集成，智能通知管理和项目仪表板，可提供有关项目及其运行状况的高级概述。 许可：每月最多免费使用100个版本，无限版本从$49/月开始。你可以为更大的实例大小购买更多的并发构建或更多的并行管道。\n主页： https://codeship.com/\nGoCD # GoCD 来自 ThoughtWorks，是一个开放源代码工具，用于构建和发布支持 CI/CD 上的现代基础结构的软件。\n轻松配置相关性以实现快速反馈和按需部署 促进可信构件：每个管道实例都锚定到特定的变更集 提供对端到端工作流程的控制，一目了然地跟踪从提交到部署的更改 容易看到上游和下游 随时部署任何版本 允许将任何已知的良好版本的应用程序部署到你喜欢的任何位置 通过比较内部版本功能获得用于任何部署的简单物料清单 通过 GoCD 模板系统重用管道配置，使配置保持整洁 已经有许多插件 许可：免费和开源\n主页：https://www.gocd.org/\nWercker # 对于正在使用或正在考虑基于 Docker 启动新项目的开发人员，Wercker 可能是一个合适的选择。Wercker 支持组织及其开发团队使用 CI/CD，微服务和 Docker。\n2017 年 4 月 17 日，甲骨文宣布已签署最终协议收购 Wercker。\nWercker 主要特性:\nGit 集成，包括 GitHub，Bitbucket，GitLab 和版本控制 使用 Wercker CLI 在本地复制 SaaS 环境，这有助于在部署之前调试和测试管道 支持 Wercker 的 Docker 集成以构建最少的容器并使尺寸可管理 Walterbot – Wercker 中的聊天机器人 – 允许你与通知交互以更新构建状态 环境变量有助于使敏感信息远离存储库 Wercker 利用关键安全功能（包括源代码保护）来关闭测试日志，受保护的环境变量以及用户和项目的可自定义权限 许可：甲骨文在收购后未提供 Wercker 的价格信息。\n主页：https://app.wercker.com\nSemaphore # Semaphore 是一项托管的 CI/CD 服务，用于测试和部署软件项目。 Semaphore 通过基于拉取请求的开发过程来建立 CI/CD 标准。\nSemaphore 主要特性:\n与 GitHub 集成 自动执行任何连续交付流程 在最快的 CI/CD 平台上运行 自动缩放你的项目，以便你仅需支付使用费用 本机 Docker 支持。测试和部署基于 Docker 的应用程序 提供 Booster –一种功能，用于通过自动并行化Ruby项目的构建来减少测试套件的运行时间 许可：灵活。使用传统的CI服务，你会受到计划容量的限制。同时，Semaphore 2.0 将根据你团队的实际需求进行扩展，因此你无需使用该工具就不必付费\n主页：https://semaphoreci.com/\nNevercode # Nevercode 支持移动应用程序的 CI/CD。它有助于更​​快地构建，测试和发布本机和跨平台应用程序。\nNevercode 主要特性:\n自动配置和设置 测试自动化：单元和UI测试，代码分析，真实设备测试，测试并行化 自动发布：iTunes Connect，Google Play，Crashlytics，T​​estFairy，HockeyApp 你的构建和测试状态的详细概述 许可：灵活。针对不同需求进行持续集成的不同计划。你可以从标准计划中选择，也可以请求根据自己的需求量身定制的计划。\n主页：https://nevercode.io/\nSpinnaker # Spinnaker 是一个多云连续交付平台，支持在不同的云提供商之间发布和部署软件更改，包括 AWS EC2，Kubernetes，Google Compute Engine，Google Kubernetes Engine，Google App Engine 等。\nSpinnaker主要特性:\n创建部署管道，以运行集成和系统测试，旋转服务器组和降低服务器组以及监视部署。通过 Git 事件，Jenkins，Travis CI，Docker，cron 或其他 Spinnaker 管道触发管道 创建和部署不可变映像，以实现更快的部署，更轻松的回滚以及消除难以调试的配置漂移问题 使用它们的指标进行金丝雀分析，将你的发行版与诸如 Datadog，Prometheus，Stackdriver 或 SignalFx 的监视服务相关联 使用Halyard – Spinnaker的CLI管理工具安装，配置和更新你的 Spinnaker 实例 设置电子邮件，Slack，HipChat 或 SMS 的事件通知（通过 Twilio） 许可：开源\n主页：https://www.spinnaker.io/\nBuildbot # Buildbot 是一个基于 Python 的 CI 框架，可自动执行编译和测试周期以验证代码更改，然后在每次更改后自动重建并测试树。因此，可以快速查明构建问题。\nBuildbot 主要特性:\n自动化构建系统，应用程序部署以及复杂软件发布过程的管理 支持跨多个平台的分布式并行执行，与版本控制系统的灵活集成，广泛的状态报告 在各种从属平台上运行构建 任意构建过程并使用 C 和 Python 处理项目 最低主机要求：Python 和 Twisted 注意：Buildbot 将停止支持 Python 2.7，并需要迁移到 Python 3。 许可：开源\n主页：https://buildbot.net/\n英文原文\n","date":"2020-03-29","externalUrl":null,"permalink":"/posts/ci-cd-tools/","section":"Posts","summary":"本文列出了市场上最流行的 14 种 CI/CD 工具，包括 Jenkins、CircleCI、TeamCity 等，并介绍了它们的主要特性和使用场景。","title":"2021 年务必知道的最好用的 14 款 CI/CD 工具","type":"posts"},{"content":"","date":"2020-03-29","externalUrl":null,"permalink":"/tags/cicd/","section":"标签","summary":"","title":"CICD","type":"tags"},{"content":" DevOps术语和定义 # 什么是DevOps\n用最简单的术语来说，DevOps是产品开发过程中开发（Dev）和运营（Ops）团队之间的灰色区域。 DevOps是一种在产品开发周期中强调沟通，集成和协作的文化。因此，它消除了软件开发团队和运营团队之间的孤岛，使他们能够快速，连续地集成和部署产品。\n什么是持续集成\n持续集成（Continuous integration，缩写为 CI）是一种软件开发实践，团队开发成员经常集成他们的工作。利用自动测试来验证并断言其代码不会与现有代码库产生冲突。理想情况下，代码更改应该每天在CI工具的帮助下，在每次提交时进行自动化构建（包括编译，发布，自动化测试），从而尽早地发现集成错误，以确保合并的代码没有破坏主分支。\n什么是持续交付\n持续交付（Continuous delivery，缩写为 CD）以及持续集成为交付代码包提供了完整的流程。在此阶段，将使用自动构建工具来编译工件，并使其准备好交付给最终用户。它的目标在于让软件的构建、测试与发布变得更快以及更频繁。这种方式可以减少软件开发的成本与时间，减少风险。\n什么是持续部署\n持续部署（Continuous deployment）通过集成新的代码更改并将其自动交付到发布分支，从而将持续交付提升到一个新的水平。 更具体地说，一旦更新通过了生产流程的所有阶段，便将它们直接部署到最终用户，而无需人工干预。因此，要成功利用连续部署，软件工件必须先经过严格建立的自动化测试和工具，然后才能部署到生产环境中。\n什么是持续测试及其好处\n连续测试是一种在软件交付管道中尽早、逐步和适当地应用自动化测试的实践。在典型的CI/CD工作流程中，将小批量发布构建。因此，为每个交付手动执行测试用例是不切实际的。自动化的连续测试消除了手动步骤，并将其转变为自动化例程，从而减少了人工。因此，对于DevOps文化而言，自动连续测试至关重要。\n持续测试的好处\n确保构建的质量和速度。 支持更快的软件交付和持续的反馈机制。 一旦系统中出现错误，请立即检测。 降低业务风险。 在潜在问题变成实际问题之前进行评估。 什么是版本控制及其用途？\n版本控制（或源代码控制）是一个存储库，源代码中的所有更改都始终存储在这个代码仓库中。版本控件提供了代码开发的操作历史记录，追踪文件的变更内容、时间、人等信息忠实地了记录下来。版本控制是持续集成和持续构建的源头。\n什么是Git？\nGit是一个分布式版本控制系统，可跟踪代码存储库中的更改。利用GitHub流，Git围绕着一个基于分支的工作流，该工作流随着团队项目的不断发展而简化了团队协作。\n实施DevOps的原因 # DevOps为什么重要？DevOps如何使团队在软件交付方面受益？\n在当今的数字化世界中，组织必须重塑其产品部署系统，使其更强大，更灵活，以跟上竞争的步伐。\n这就是DevOps概念出现的地方。DevOps在为整个软件开发管道（从构思到部署，再到最终用户）产生移动性和敏捷性方面发挥着至关重要的作用。DevOps是将不断更新和改进产品的更简化，更高效的流程整合在一起的解决方案。\n解释DevOps对开发人员有何帮助\n在没有DevOps的世界中，开发人员的工作流程将首先建立新代码，交付并集成它们，然后，操作团队有责任打包和部署代码。之后，他们将不得不等待反馈。而且如果出现问题，由于错误，他们将不得不重新执行一次。沿线是项目中涉及的不同团队之间的无数手动沟通。\n由于CI/CD实践已经合并并自动化了其余任务，因此应用DevOps可以将开发人员的任务简化为仅构建代码。随着流程变得更加透明并且所有团队成员都可以访问，将工程团队和运营团队相结合有助于建立更好的沟通和协作。\n为什么DevOps最近在软件交付方面变得越来越流行？\nDevOps在过去几年中受到关注，主要是因为它能够简化组织运营的开发，测试和部署流程，并将其转化为业务价值。\n技术发展迅速。因此，组织必须采用一种新的工作流程-DevOps和Agile方法-来简化和刺激其运营，而不能落后于其他公司。DevOps的功能通过Facebook和Netflix的持续部署方法所取得的成功得到了清晰体现，该方法成功地促进了其增长，而没有中断正在进行的运营。\nCI/CD有什么好处？\nCI和CD的结合将所有代码更改统一到一个单一的存储库中，并通过自动化测试运行它们，从而在所有阶段全面开发产品，并随时准备部署。\nCI/CD使组织能够按照客户期望的那样快速，高效和自动地推出产品更新。\n简而言之，精心规划和执行良好的CI/CD管道可加快发布速度和可靠性，同时减轻产品的代码更改和缺陷。这最终将导致更高的客户满意度。\n持续交付有什么好处？\n通过手动发布代码更改，团队可以完全控制产品。 在某些情况下，该产品的新版本将更有希望：具有明确业务目的的促销策略。\n通过自动执行重复性和平凡的任务，IT专业人员可以拥有更多的思考能力来专注于改进产品，而不必担心集成进度。\n持续部署有哪些好处？\n通过持续部署，开发人员可以完全专注于产品，因为他们在管道中的最后任务是审查拉取请求并将其合并到分支。通过在自动测试后立即发布新功能和修复，此方法可实现快速部署并缩短部署持续时间。\n客户将是评估每个版本质量的人。新版本的错误修复更易于处理，因为现在每个版本都以小批量交付。\n如何有效实施DevOps # 定义典型的DevOps工作流程\n典型的DevOps工作流程可以简化为4个阶段：\n版本控制：这是存储和管理源代码的阶段。 版本控件包含代码的不同版本。 持续集成：在这一步中，开发人员开始构建组件，并对其进行编译，验证，然后通过代码审查，单元测试和集成测试进行测试。 持续交付：这是持续集成的下一个层次，其中发布和测试过程是完全自动化的。 CD确保将新版本快速，可持续地交付给最终用户。 持续部署：应用程序成功通过所有测试要求后，将自动部署到生产服务器上以进行发布，而无需任何人工干预。 DevOps的核心操作是什么？\nDevOps在开发和基础架构方面的核心运营是：\nSoftware development:\nCode building Code coverage Unit testing Packaging Deployment Infrastructure:\nProvisioning Configuration Orchestration Deployment 在实施DevOps之前，团队需要考虑哪些预防措施？\n当组织尝试应用这种新方法时，对DevOps做法存在一些误解，有可能导致悲惨的失败：\nDevOps不仅仅是简单地应用新工具和/或组建新的“部门”并期望它能正常工作。实际上，DevOps被认为是一种文化，开发团队和运营团队遵循共同的框架。 企业没有为其DevOps实践定义清晰的愿景。对开发团队和运营团队而言，应用DevOps计划是一项显着的变化。因此，拥有明确的路线图，将DevOps集成到您的组织中的目标和期望将消除任何混乱，并从早期就提供清晰的指导方针。 在整个组织中应用DevOps做法之后，管理团队需要建立持续的学习和改进文化。系统中的故障和问题应被视为团队从错误中学习并防止这些错误再次发生的宝贵媒介。 SCM团队在DevOps中扮演什么角色？\n软件配置管理（SCM）是跟踪和保留开发环境记录的实践，包括在操作系统中进行的所有更改和调整。\n在DevOps中，将SCM作为代码构建在基础架构即代码实践的保护下。\nSCM为开发人员简化了任务，因为他们不再需要手动管理配置过程。 现在，此过程以机器可读的形式构建，并且会自动复制和标准化。\n质量保证（QA）团队在DevOps中扮演什么角色？\n随着DevOps实践在创新组织中变得越来越受欢迎，QA团队的职责和相关性在当今的自动化世界中已显示出下降的迹象。\n但是，这可以被认为是神话。 DevOps的增加并不等于QA角色的结束。 这仅意味着他们的工作环境和所需的专业知识正在发生变化。 因此，他们的主要重点是专业发展以跟上这种不断变化的趋势。\n在DevOps中，质量保证团队在确保连续交付实践的稳定性以及执行自动重复性测试无法完成的探索性测试任务方面发挥战略作用。 他们在评估测试和检测最有价值的测试方面的见识仍然在缓解发布的最后步骤中的错误方面起着至关重要的作用。\nDevOps使用哪些工具？ 描述您使用任何这些工具的经验\n在典型的DevOps生命周期中，有不同的工具来支持产品开发的不同阶段。 因此，用于DevOps的最常用工具可以分为6个关键阶段：\n持续开发：Git, SVN, Mercurial, CVS, Jira 持续整合：Jenkins, Bamboo, CircleCI 持续交付：Nexus, Archiva, Tomcat 持续部署：Puppet, Chef, Docker 持续监控：Splunk, ELK Stack, Nagios 连续测试：Selenium，Katalon Studio\n如何在DevOps实践中进行变更管理\n典型的变更管理方法需要与DevOps的现代实践适当集成。 第一步是将变更集中到一个平台中，以简化变更，问题和事件管理流程。\n接下来，企业应建立高透明度标准，以确保每个人都在同一页面上，并确保内部信息和沟通的准确性。\n对即将到来的变更进行分层并建立可靠的策略，将有助于最大程度地降低风险并缩短变更周期。 最后，组织应将自动化应用到其流程中，并与DevOps软件集成。\n如何有效实施CI/CD # CI/CD的一些核心组件是什么？\n稳定的CI/CD管道需要用作版本控制系统的存储库管理工具。 这样开发人员就可以跟踪软件版本中的更改。\n在版本控制系统中，开发人员还可以在项目上进行协作，在版本之间进行比较并消除他们犯的任何错误，从而减轻对所有团队成员的干扰。\n连续测试和自动化测试是成功建立无缝CI / CD管道的两个最关键的关键。 自动化测试必须集成到所有产品开发阶段（包括单元测试，集成测试和系统测试），以涵盖所有功能，例如性能，可用性，性能，负载，压力和安全性。\nCI/CD的一些常见做法是什么？\n以下是建立有效的CI / CD管道的一些最佳实践：\n发展DevOps文化 实施和利用持续集成 以相同的方式部署到每个环境 失败并重新启动管道 应用版本控制 将数据库包含在管道中 监控您的持续交付流程 使您的CD流水线流畅 什么时候是实施CI/CD的最佳时间？\n向DevOps的过渡需要彻底重塑其软件开发文化，包括工作流，组织结构和基础架构。 因此，组织必须为实施DevOps的重大变化做好准备。\n有哪些常见的CI/CD服务器\nVisual Studio Visual Studio支持具有敏捷计划，源代码控制，包管理，测试和发布自动化以及持续监视的完整开发的DevOps系统。\nTeamCity TeamCity是一款智能CI服务器，可提供框架支持和代码覆盖，而无需安装任何额外的插件，也无需模块来构建脚本。\nJenkins 它是一个独立的CI服务器，通过共享管道和错误跟踪功能支持开发和运营团队之间的协作。 它也可以与数百个仪表板插件结合使用。\nGitLab GitLab的用户可以自定义平台，以进行有效的持续集成和部署。 GitLab帮助CI / CD团队加快代码交付，错误识别和恢复程序的速度。\nBamboo Bamboo是用于产品发布管理自动化的连续集成服务器。 Bamboo跟踪所有工具上的所有部署，并实时传达错误。\n描述持续集成的有效工作流程\n实施持续集成的成功工作流程包括以下实践：\n实施和维护项目源代码的存储库 自动化构建和集成 使构建自检 每天将更改提交到基准 构建所有添加到基准的提交 保持快速构建 在生产环境的克隆中运行测试 轻松获取最新交付物 使构建结果易于所有人监视 自动化部署 每种术语之间的差异 # 敏捷和DevOps之间有哪些主要区别？\n基本上，DevOps和敏捷是相互补充的。敏捷更加关注开发新软件和以更有效的方式管理复杂过程的价值和原则。同时，DevOps旨在增强由开发人员和运营团队组成的不同团队之间的沟通，集成和协作。\n它需要采用敏捷方法和DevOps方法来形成无缝工作的产品开发生命周期：敏捷原理有助于塑造和引导正确的开发方向，而DevOps利用这些工具来确保将产品完全交付给客户。\n持续集成，持续交付和持续部署之间有什么区别？\n持续集成（CI）是一种将代码版本连续集成到共享存储库中的实践。这种做法可确保自动测试新代码，并能快速检测和修复错误。\n持续交付使CI进一步迈出了一步，确保集成后，随时可以在一个按钮内就可以释放代码库。因此，CI可以视为持续交付的先决条件，这是CI / CD管道的另一个重要组成部分。\n对于连续部署，不需要任何手动步骤。这些代码通过测试后，便会自动推送到生产环境。\n所有这三个组件：持续集成，持续交付和持续部署是实施DevOps的重要阶段。\n一方面，连续交付更适合于活跃用户已经存在的应用程序，这样事情就可以变慢一些并进行更好的调整。另一方面，如果您打算发布一个全新的软件并且将整个过程指定为完全自动化的，则连续部署是您产品的更合适选择。\n连续交付和连续部署之间有哪些根本区别？\n在连续交付的情况下，主分支中的代码始终可以手动部署。 通过这种做法，开发团队可以决定何时发布新的更改或功能，以最大程度地使组织受益。\n同时，连续部署将在测试阶段之后立即将代码中的所有更新和修补程序自动部署到生产环境中，而无需任何人工干预。\n持续集成和持续交付之间的区别是什么？\n持续集成有助于确保软件组件紧密协作。 整合应该经常进行； 最好每小时或每天一次。 持续集成有助于提高代码提交的频率，并降低连接多个开发人员的代码的复杂性。 最终，此过程减少了不兼容代码和冗余工作的机会。\n持续交付是CI / CD流程中的下一步。 由于代码不断集成到共享存储库中，因此可以持续测试该代码。 在等待代码完成之前，没有间隙可以进行测试。 这样可确保找到尽可能多的错误，然后将其连续交付给生产。\nDevOps和持续交付之间有什么区别？\nDevOps更像是一种组织和文化方法，可促进工程团队和运营团队之间的协作和沟通。\n同时，持续交付是成功将DevOps实施到产品开发工作流程中的重要因素。 持续交付实践有助于使新发行的版本更加乏味和可靠，并建立更加无缝和短的流程。\nDevOps的主要目的是有效地结合Dev和Ops角色，消除所有孤岛，并实现独立于持续交付实践的业务目标。\n另一方面，如果已经有DevOps流程，则连续交付效果最佳。 因此，它扩大了协作并简化了组织的统一产品开发周期。\n敏捷，精益IT和DevOps之间有什么区别？\n敏捷是仅专注于软件开发的方法。 敏捷旨在迭代开发，建立持续交付，缩短反馈循环以及在整个软件开发生命周期（SDLC）中改善团队协作。\n精益IT是一种旨在简化产品开发周期价值流的方法。 精益专注于消除不必要的过程，这些过程不会增加价值，并创建流程来优化价值流。\nDevOps专注于开发和部署-产品开发过程的Dev和Ops。 其目标是有效整合自动化工具和IT专业人员之间的角色，以实现更简化和自动化的流程。\n准备好在下一次DevOps面试中取得成功吗？ # 目前有无数的DevOps面试问题，我们目前还不能完全解决。但是，我们希望这些问题和建议的答案能使您掌握DevOps和CI/CD的大量知识，并成功地帮助您完成面试。\n将来，我们将在此列表中添加更多内容。 因此，如果您对此主题有任何建议，请随时与我们联系。最后，我们祝您在测试事业中一切顺利！\n","date":"2020-03-29","externalUrl":null,"permalink":"/posts/top-30-devops-interview-questions/","section":"Posts","summary":"本文列出了 DevOps 领域的 30 多个常见面试问题，涵盖 DevOps 基础知识、CI/CD、DevOps 工具和实践等方面，帮助求职者准备 DevOps 面试。","title":"DevOps Top 30+ 面试问题","type":"posts"},{"content":"","date":"2020-03-29","externalUrl":null,"permalink":"/tags/interview/","section":"标签","summary":"","title":"Interview","type":"tags"},{"content":"Maintaining a Git repository often involves reducing its size. If you\u0026rsquo;ve imported a repository from another version control system, you may need to clean up unnecessary files after the import. This article mainly discusses how to remove unwanted files from a Git repository.\nPlease exercise extreme caution\u0026hellip;\nThe steps and tools in this article employ advanced techniques involving destructive operations. Ensure you carefully read and back up your repository before you begin. The easiest way to create a backup is to clone your repository using the --mirror flag, and then package and compress the entire cloned directory. With this backup, you can restore from the backup repository if key elements of your repository are accidentally corrupted during maintenance.\nRemember, repository maintenance can be disruptive to repository users. Communicating with your team or repository followers is essential. Ensure everyone has checked out their code and agrees to halt development during repository maintenance.\nUnderstanding File Removal from Git History # Recall that cloning a repository clones the entire history—including all versions of every source code file. If a user commits a large file, such as a JAR, then every subsequent clone includes that file. Even if the user eventually deletes the file in a later commit, the file remains in the repository\u0026rsquo;s history. To completely remove the file from your repository, you must:\nDelete the file from your project\u0026rsquo;s current file tree; Remove the file from the repository\u0026rsquo;s history—rewriting Git history to remove the file from all commits containing it; Delete all reflog history pointing to the old commit history; Repack the repository using git gc to garbage collect now-unused data. Git\u0026rsquo;s gc (garbage collection) will remove all actual unused data or data not referenced in any of your branches or tags in the repository. For it to work, we need to rewrite all Git repository history containing the unwanted files, the repository will no longer reference it, and git gc will discard all unused data.\nRewriting repository history is a tricky business because each commit depends on its parent commits, so even a small change alters every subsequent commit\u0026rsquo;s hash. There are two automated tools that can help you do this:\nBFG Repo Cleaner—Fast, simple, and easy to use; requires Java 6 or later runtime environment. git filter-branch—Powerful, more cumbersome to configure, slower for larger repositories, and is part of the core Git suite. Remember, after rewriting history, whether you use BFG or filter-branch, you must delete reflog entries pointing to the old history and finally run the garbage collector to remove the old data.\nRewriting History with BFG # BFG is specifically designed to remove unwanted data like large files or passwords from a Git repository, so it has a simple flag to remove large historical files (not in the current commit): --strip-blobs-bigger-than\nBFG Download\njava -jar bfg.jar --strip-blobs-bigger-than 100M Any files larger than 100MB (excluding files in your recent commits—as BFG protects your latest commits by default) will be removed from your Git repository\u0026rsquo;s history. You can also specify files by name:\njava -jar bfg.jar --delete-files *.mp4 BFG is 10–1000 times faster than git filter-branch and generally easier to use—see the full usage instructions and examples for more details.\nAlternatively, Rewriting History with git filter-branch # The filter-branch command can rewrite a Git repository\u0026rsquo;s history, like BFG, but the process is slower and more manual. If you don\u0026rsquo;t know where these large files are, your first step will be to find them:\nManually Finding Large Files in Your Git Repository # Antony Stubbs wrote a BASH script that does this well. The script inspects the contents of your pack files and lists large files. Before you start deleting files, do the following to obtain and install this script:\nDownload the script to your local system.\nPlace it in an easily accessible location relative to your Git repository.\nMake the script executable:\nchmod 777 git_find_big.sh Clone the repository to your local system.\nChange the current directory to your repository\u0026rsquo;s root.\nManually run the Git garbage collector:\ngit gc --auto Find the size of the .git folder:\n# Note the file size for later reference du -hs .git/objects 45M .git/objects Run git_find_big.sh to list large files in your repository:\ngit_find_big.sh All sizes are in kB\u0026#39;s. The pack column is the size of the object, compressed, inside the pack file. size pack SHA location 592 580 e3117f48bc305dd1f5ae0df3419a0ce2d9617336 media/img/emojis.jar 550 169 b594a7f59ba7ba9daebb20447a87ea4357874f43 media/js/aui/aui-dependencies.jar 518 514 22f7f9a84905aaec019dae9ea1279a9450277130 media/images/screenshots/issue-tracker-wiki.jar 337 92 1fd8ac97c9fecf74ba6246eacef8288e89b4bff5 media/js/lib/bundle.js 240 239 e0c26d9959bd583e5ef32b6206fc8abe5fea8624 media/img/featuretour/heroshot.png The large files are all JAR files. The pack size column is the most relevant. aui-dependencies.jar compresses to 169kb, but emojis.jar only compresses to 500kb. emojis.jar is a candidate for deletion.\nRunning filter-branch # You pass this command a filter that modifies the Git index. For example, a filter can delete each retrieved commit. This usage is:\ngit filter-branch --index-filter \u0026#39;git rm --cached --ignore-unmatch\u0026amp;nbsp; _pathname_ \u0026#39; commitHASH The --index-filter option modifies the repository\u0026rsquo;s index, and the --cached option deletes files from the index, not the disk. This is faster because you don\u0026rsquo;t need to check each revision before running the filter. The ignore-unmatch option in git rm prevents the command from failing when trying to remove a non-existent file pathname. By specifying a commit HASH, you delete pathname from every commit starting at that HASH. To delete from the beginning, omit this parameter or specify HEAD.\nIf your large files are in different branches, you will need to delete each file by name. If the large files are all on a single branch, you can delete the branch itself.\nOption 1: Deleting Files by Filename # Use the following steps to delete large files:\nUse the following command to delete the first large file you found:\ngit filter-branch --index-filter \u0026#39;git rm --cached --ignore-unmatch filename\u0026#39; HEAD Repeat step 1 for each remaining large file.\nUpdate references in your repository. filter-branch creates backups of your original references under refs/original/. Once you are sure you have deleted the correct files, run the following to remove the backups and allow the garbage collector to reclaim large objects:\ngit for-each-ref --format=\u0026#34;%(refname)\u0026#34; refs/original/ | xargs -n 1 git update-ref -d Option 2: Deleting a Branch Directly # If all your large files are on a single branch, you can delete the branch directly. Deleting the branch automatically removes all references.\nDelete the branch:\ngit branch -D PROJ567bugfix Remove all reflog references to the branch:\ngit reflog expire --expire=now PROJ567bugfix Garbage Collecting Unused Data # Delete all reflog references from now to the past (unless you explicitly operated only on one branch):\ngit reflog expire --expire=now --all Repack the repository by running the garbage collector and removing old objects:\ngit gc --prune=now Push all your changes back to the repository:\ngit push --all --force Ensure that all your tags are also up-to-date:\ngit push --tags --force Original Article\n","date":"2020-03-21","externalUrl":null,"permalink":"/en/posts/maintaining-git-repository/","section":"Posts","summary":"How to remove unnecessary files and history from a Git repository to reduce its size, providing two methods using BFG Repo Cleaner or \u003ccode\u003egit filter-branch\u003c/code\u003e.","title":"How to Slim Down Your Git Repository","type":"posts"},{"content":"Due to historical reasons, our current product\u0026rsquo;s code repository has many warnings that cannot be resolved immediately. Only by continuously enriching automated test cases to ensure the final quality control can we proceed with the orderly repair of warnings. Before that, how to effectively prevent the introduction of more warnings is what we should be doing now.\nTherefore, I want to integrate C/C++ static code scanning into the Pull Request phase. However, many tools related to C/C++ are often commercial, such as SonarQube, which is our first choice. The Community version does not support C/C++ code scanning; only the Developer and Enterprise paid versions support it. Before static code scanning brings any return on investment, blindly paying will only increase the cost for the product. Therefore, I decided to look for other open-source tools as a replacement.\nFinally, I chose CPPCheck, mainly for the following reasons:\nIt\u0026rsquo;s one of the few open-source static code scanning tools for C/C++. It can be integrated with Jenkins, allowing you to view the report in Jenkins. It supports Jenkins Pipeline. This article documents my investigation and usage experience. If you have similar needs, this might provide some reference.\nInstalling Cppcheck # Installation on Linux\nsudo yum install cppcheck.x86_64 For installation on other platforms, please refer to the cppcheck official website.\nIf you can\u0026rsquo;t install it with a single command on Linux, you can also build cppcheck from source code. The following steps show how to manually build a cppcheck executable file from the source code:\ncd opt \u0026amp;\u0026amp; mkdir cppcheck \u0026amp;\u0026amp; cd cppcheck # Download the code wget https://github.com/danmar/cppcheck/archive/1.90.tar.gz # Extract tar -xvf 1.90.tar.gz # Make build cd cppcheck-1.90 mkdir build cd build cmake .. cmake --build . # link sudo ln -s /opt/cppcheck/cppcheck-1.90/build/bin/cppcheck /usr/bin/cppcheck # Check if the installation was successful which cppcheck /usr/bin/cppcheck cppcheck --version Cppcheck 1.90 Using cppcheck for Static Code Scanning # Before integrating with Jenkins, let\u0026rsquo;s see how to use this tool. By consulting the Cppcheck official documentation, the general usage is as follows:\n# For example, scan the code under the src/public and src/themes directories and output the results to cppcheck.xml cppcheck src/public src/themes --xml 2\u0026gt; cppcheck.xml Integrating Cppcheck with Jenkins # First, download the Cppcheck Jenkins plugin. The following code publishCppcheck pattern: 'cppcheck.xml' was generated through Pipeline Syntax.\nHowever, when reading the XML file to display the report, I encountered two problems:\nProblem 1: The analysis of cppcheck.xml succeeded on some Linux machines but failed on others. I suspect this is due to different JDK versions. I also found this issue on Jenkins JIRA JENKINS-60077, but it hasn\u0026rsquo;t been resolved yet.\nThe main reason I didn\u0026rsquo;t continue trying to solve Problem 1 is that it has a more fatal flaw, as described below.\nProblem 2: I cannot directly view the code through the Cppcheck Results report. Even if problems are scanned, I still need to check the specific problems in Git or the local IDE, which greatly reduces efficiency.\n# An error occurs when viewing the code file Can\u0026#39;t read file: Can\u0026#39;t access the file: file:/disk1/agent/workspace/cppcheck-ud113/src/public/dummy/err_printf.c The official ticket also records this issue JENKINS-42613 and JENKINS-54209. JENKINS-42613 is waiting to be merged, and as of this writing, it is still unresolved.\nFinally, I found that Warnings Next Generation plugin will replace the entire Jenkins static analysis suite, including plugins like Android Lint, CheckStyle, Dry, FindBugs, PMD, Warnings, Static Analysis Utilities, Static Analysis Collector. Finally, the Warnings Next Generation plugin solved the problem of report display.\nHere, you can generate the code for reading the report through Pipeline Syntax: recordIssues(tools: [cppCheck(pattern: 'cppcheck.xml')])\nFor more information on using the Warnings Next Generation plugin, please refer to the documentation.\nFinal Pipeline Example # pipeline{ agent { node { label \u0026#39;cppcheck\u0026#39; customWorkspace \u0026#34;/agent/workspace/cppcheck\u0026#34; } } parameters { string(name: \u0026#39;Branch\u0026#39;, defaultValue: \u0026#39;develop\u0026#39;, description: \u0026#39;Which branch do you want to do cppcheck?\u0026#39;) } options { timestamps () buildDiscarder(logRotator(numToKeepStr:\u0026#39;50\u0026#39;)) } stage(\u0026#34;Checkout\u0026#34;){ steps{ checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/${Branch}\u0026#39;]], browser: [$class: \u0026#39;BitbucketWeb\u0026#39;, repoUrl: \u0026#39;https://git.yourcompany.com/projects/repos/cppcheck-example/browse\u0026#39;], doGenerateSubmoduleConfigurations: false, extensions: [ [$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#39;**\u0026#39;], [$class: \u0026#39;CheckoutOption\u0026#39;, timeout: 30], [$class: \u0026#39;CloneOption\u0026#39;, depth: 1, noTags: false, reference: \u0026#39;\u0026#39;, shallow: true, timeout: 30]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;CREDENTIALS_ID\u0026#39;, url: \u0026#39;https://git.yourcompany.com/scm/cppcheck-example.git\u0026#39;]]]) } } stage(\u0026#34;Cppcheck\u0026#34;){ steps{ script { sh \u0026#39;cppcheck src/public src/themes --xml 2\u0026gt; cppcheck.xml\u0026#39; } } } stage(\u0026#39;Publish results\u0026#39;){ steps { recordIssues(tools: [cppCheck(pattern: \u0026#39;cppcheck.xml\u0026#39;)]) } } } Report Display # I applied CPPCheck to each Pull Request. When a developer submits new code, CPPCheck scans the code and compares it with the previous history. When CPPCheck executes successfully and generates a report, a button appears, which you can click to access it.\nAfter opening it, you will see the scan results for the current branch code.\nCPPCheck uses three dimensions to display static code scan results:\nSeverity Distribution: This is divided into three levels: High, Normal, and Low.\nReference Comparison: This compares the data with previous data. New issues are displayed as New, existing issues as Outstanding, and resolved issues as Fixed.\nHistory: This displays a historical trend as the code is added and modified.\nNote: The cppcheck-related XML files are stored on the Jenkins master. Only when the current Jenkins Job is manually deleted will the cppcheck XML files be deleted.\n-sh-4.2$ ls -l cppcheck* -rw-r--r-- 1 jenkins jenkins 418591 Feb 27 05:54 cppcheck-blames.xml -rw-r--r-- 1 jenkins jenkins 219 Feb 27 05:54 cppcheck-fixed-issues.xml -rw-r--r-- 1 jenkins jenkins 142298 Feb 27 05:54 cppcheck-forensics.xml -rw-r--r-- 1 jenkins jenkins 219 Feb 27 05:54 cppcheck-new-issues.xml -rw-r--r-- 1 jenkins jenkins 488636 Feb 27 05:54 cppcheck-outstanding-issues.xml Clicking the corresponding link will directly jump to the specific code warning location.\nIsn\u0026rsquo;t it pretty good?\n","date":"2020-02-16","externalUrl":null,"permalink":"/en/posts/cppcheck/","section":"Posts","summary":"This article introduces the installation, usage, and integration of Cppcheck with Jenkins to improve C/C++ code quality and static analysis capabilities.","title":"A Free C/C++ Static Code Analysis Tool—Cppcheck—Integrated with Jenkins","type":"posts"},{"content":"","date":"2020-02-16","externalUrl":null,"permalink":"/en/tags/cppcheck/","section":"Tags","summary":"","title":"Cppcheck","type":"tags"},{"content":"","date":"2020-02-05","externalUrl":null,"permalink":"/tags/hp-ux/","section":"标签","summary":"","title":"HP-UX","type":"tags"},{"content":" 安装 Java8 # 安装包下载链接是 https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=JDKJRE8018\n需要先注册，然后登陆后才能下载，我下载的是 Itanium_JDK_8.0.18_June_2019_Z7550-96733_java8_18018_ia.depot\n在线安装文档 https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-c04481894\nswinstall -s /tmp/Itanium_JDK_8.0.18_June_2019_Z7550-96733_java8_18018_ia.depot # if swinstall not found /usr/sbin/swinstall -s /tmp/Itanium_JDK_8.0.18_June_2019_Z7550-96733_java8_18018_ia.depot 安装成功后，会在根目录 opt 下多了一个 java8 目录，检查下 java 版本：\nbash-5.0$ pwd /opt/java8 bash-5.0$ cd bin bash-5.0$ java -version java version \u0026#34;1.8.0.18-hp-ux\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0.18-hp-ux-b1) Java HotSpot(TM) Server VM (build 25.18-b1, mixed mode) 创建软连接\nsudo ln -s /opt/java8/bin/java /bin/java 安装 gzip 和 gunzip # # 安装 gzip /usr/bin/sudo /usr/local/bin/depothelper gzip 如果你机器上已经有 zip 和 gunzip 了，只需要软连接一下即可，防止出现命令找不到的问题\n/usr/bin/sudo ln -s /usr/contrib/bin/gzip /usr/bin/gzip /usr/bin/sudo ln -s /usr/contrib/bin/gunzip /usr/bin/gunzip Can not use bash in HP-UX # For example, when you run bash command, you have the following error:\n$ bash /usr/lib/hpux64/dld.so: Unable to find library \u0026#39;libtermcap.so\u0026#39;. Here is the solution：https://community.hpe.com/t5/HP-UX-General/Unable-to-use-bash-for-ia-machine-11-23/m-p/3980789#M128592\nIt bcasue the LIBTERMCAP is not installed, you can go here to see bash\u0026rsquo;s dependencies include gettext libiconv termcap, etc.\nHere are two very useful commands of install and uninstall.\nDownload bash to command depothelper\ndepothelper bash If you wang to remove the package on your HP-UX system, you can run the command\nsudo /usr/sbin/swremove [package-name]\n","date":"2020-02-05","externalUrl":null,"permalink":"/posts/hpxu-tips/","section":"Posts","summary":"本文介绍了在 HP-UX 系统上安装 Java 8、gzip 和 gunzip 的方法，以及如何解决 HP-UX 上使用 bash 时遇到的库依赖问题。","title":"HP-UX 安装工具以及一些使用总结","type":"posts"},{"content":" ls 命令 # 列出当前目录的文件和文件夹。参数:\n-l 列出时显示详细信息\n-a 显示所有文件，包括隐藏的和不隐藏的\n可以组合使用，像这样\nls -la cp 命令 # 将源文件复制到目标。参数：\n-i 交互模式意味着等待确认，如果目标上有文件将被覆盖。\n-r 递归复制，意味着包含子目录（如果有的话）。\ncp –ir source_dir target_dir /tmp 空间不够怎么办 # 在 /etc/fstab 文件里增加一行\nsudo vi /etc/fstab # 添加如下一行 tmpfs /tmp tmpfs defaults,size=4G 0 0 重启之后，df -h 查看，/tmp 目录已经就变成 4G 了。\nMore, Refer to these links\nhttps://likegeeks.com/main-linux-commands-easy-guide/ https://dzone.com/articles/most-useful-linux-command-line-tricks ","date":"2020-02-05","externalUrl":null,"permalink":"/posts/linux-tips/","section":"Posts","summary":"本文介绍了一些最有用的 Linux 命令行技巧，以提高开发和运维的效率。","title":"最有用的 Linux 命令行技巧","type":"posts"},{"content":"上一篇（GitStats - Git 历史统计信息工具），我已经给老板提供了他想看的所有仓库的 Git 提交历史分析报告了，并且把报告都部署到了一台虚拟机的 tomcat 上了，老板可以通过网址访问直接查看每个仓库的分析报告了，看看谁的贡献大，谁活跃，谁偷懒了，谁周末写代码了（这里不鼓励 996）。\n最近老板提需求了。\n老板：你弄个这个网址的数据咋不更新呢？报告上咋没见你们提交代码呢？ 小开：老板儿，您看到这些一个个仓库的数据都是小开我人肉手动生成的，要不您给我点时间，我来做个自动化任务吧。\n我这么积极主动，不是我奉承老板，我心里也知道老板如果觉得 Git Stats 这个工具好用，肯定希望看到的分析报告是最新的。既然老板先提了，那我就别磨蹭了，赶紧干吧。\n不过用啥实现呢？肯定是 Jenkins 了。一来我已经在 Jenkins 上做了很多的自动化任务了，轻车熟路；二来使用同一套系统不但可以减少繁多的系统入口，降低学习成本，也提高 Jenkins 服务器的利用率。\n设身处地的考虑了下老板的使用需求，他肯定不希望自己去 Jenkins 服务器上去运行 Job 来生成这个Git 仓库的多维度代码分析报告，那么，如果我是老板，我希望：\n这个 Jenkins 任务要定期执行，不需要太频繁，一周更新一次就行； 另外还要支持对单独仓库的单独执行，一旦老板要立即马上查看某个仓库的的分析报告呢。 最后实现的效果如下：\n手动执行 # 老板可以勾选他最关心的代码仓库进行更新\n每周末定时执行 # 老板在周一上班就能看到最新的分析数据了，可以看到这个任务 Started by timer\n最终的 Jenkinsfile 是这样的 # pipeline{ agent{ node { label \u0026#39;main-slave\u0026#39; customWorkspace \u0026#34;/workspace/gitstats\u0026#34; } } environment { USER_CRE = credentials(\u0026#34;d1cbab74-823d-41aa-abb7\u0026#34;) webapproot = \u0026#34;/workspace/apache-tomcat-7.0.99/webapps/gitstats\u0026#34; } parameters { booleanParam(defaultValue: true, name: \u0026#39;repo1\u0026#39;, summary: \u0026#39;uncheck to disable [repo1]\u0026#39;) booleanParam(defaultValue: true, name: \u0026#39;repo2\u0026#39;, summary: \u0026#39;uncheck to disable [repo2]\u0026#39;) } triggers { cron \u0026#39;0 3 * * 7\u0026#39; # 每周日早上进行定时运行，因此此时机器是空闲的。 } options { buildDiscarder(logRotator(numToKeepStr:\u0026#39;10\u0026#39;)) } stages{ stage(\u0026#34;Checkout gitstats\u0026#34;){ steps{ # 准备存放 html 报告目录 sh \u0026#34;mkdir -p html\u0026#34; # 下载 gitstats 代码 sh \u0026#34;rm -rf gitstats \u0026amp;\u0026amp; git clone https://github.com/hoxu/gitstats.git\u0026#34; } } stage(\u0026#34;Under statistics\u0026#34;) { parallel { stage(\u0026#34;reop1\u0026#34;) { when { expression { return params.repo1 } # 判断是否勾选了 } steps { # 下载要进行分析的仓库 repo1 sh \u0026#39;git clone -b master https://$USER_CRE_USR:\u0026#34;$USER_CRE_PSW\u0026#34;@git.software.com/scm/repo1.git\u0026#39; # 进行仓库 repo1 的历史记录分析 sh \u0026#34;cd gitstats \u0026amp;\u0026amp; ./gitstats ../repo1 ../html/repo1\u0026#34; } post { success { # 如果分析成功，则将分析结果放到 apache-tomcat-7.0.99/webapps/gitstats 目录下 sh \u0026#39;rm -rf ${webapproot}/repo1 \u0026amp;\u0026amp; mv html/repo1 ${webapproot}\u0026#39; # 然后删掉 repo1 的代码和 html 报告，以免不及时清理造成磁盘空间的过度占用 sh \u0026#34;rm -rf repo1\u0026#34; sh \u0026#34;rm -rf html/repo1\u0026#34; } } } } stage(\u0026#34;repo2\u0026#34;) { when { expression { return params.repo2 } } steps { sh \u0026#39;git clone -b master https://$USER_CRE_USR:\u0026#34;$USER_CRE_PSW\u0026#34;@git.software.com/scm/repo2.git\u0026#39; sh \u0026#34;cd gitstats \u0026amp;\u0026amp; ./gitstats ../repo2 ../html/repo2\u0026#34; } post { success { sh \u0026#39;rm -rf ${webapproot}/repo2 \u0026amp;\u0026amp; mv html/repo2 ${webapproot}\u0026#39; sh \u0026#34;rm -rf repo2\u0026#34; sh \u0026#34;rm -rf html/repo2\u0026#34; } } } } } } post{ always{ # 总是给执行者分送邮件通知，不论是否成功都会对工作空间进行清理 script { def email = load \u0026#34;vars/email.groovy\u0026#34; email.build(currentBuild.result, \u0026#39;\u0026#39;) } cleanWs() } } } 最后 # 如果你是测试、DevOps或是从事研发效能方面的工作，那么利用好开源工具，比如 Jenkins 和 Git Stats 就可以快速帮助老板或是你自己提供一个 Git 仓库的多维度代码分析报告，有助于更加了解产品的代码情况。\n","date":"2020-01-21","externalUrl":null,"permalink":"/posts/git-stats-jenkins/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 定期自动生成 Git 仓库的多维度代码分析报告，包括手动执行和定时执行的实现方式，帮助开发者和团队更好地了解代码质量和贡献情况。","title":"通过 Jenkins 定期自动给老板提供 Git 仓库的多维度代码分析报告","type":"posts"},{"content":"记录在使用 AIX 时所遇到的问题和解决办法，以备以后遇到同样问题不要再因为这些再浪费时间，希望也能帮助到你。\n在 AIX 上无法解压超一个大约 600 MB 文件 # bash-4.3$ ls data_cdrom_debug_AIX_05949fb.tar.Z bash-4.3$ gzip -d data_cdrom_debug_AIX_05949fb.tar.Z # 错误信息 gzip: data_cdrom_debug_AIX_05949fb.tar: File too large # 解决办法 bash-4.3$ sudo vi /etc/security/limits default: fsize = -1 # 修改为 -1 core = 2097151 cpu = -1 data = -1 rss = 65536 stack = 65536 nofiles = 2000 # 需要重启 bash-4.3$ sudo reboot Rebooting . . . 修改之后，重启，再解压就没有问题了。\n安装 Java Standard Edition on AIX # 下载地址 https://developer.ibm.com/javasdk/support/aix-download-service/\ndownload Java8_64.sdk.8.0.0.600.tar.gz Java8_64.jre.8.0.0.600.tar.gz gzip -d Java8_64.sdk.8.0.0.600.tar.gz and Java8_64.jre.8.0.0.600.tar.gz tar -xvf Java8_64.sdk.8.0.0.600.tar and Java8_64.jre.8.0.0.600.tar installp -agXYd . Java8_64.jre Java8_64.sdk 2\u0026gt;\u0026amp;1 | tee installp.log # install output Installation Summary -------------------- Name Level Part Event Result ------------------------------------------------------------------------------- Java8_64.sdk 8.0.0.600 USR APPLY SUCCESS Java8_64.jre 8.0.0.600 USR APPLY SUCCESS Java8_64.jre 8.0.0.600 ROOT APPLY SUCCESS smitty install_all Input: Type \u0026ldquo;./\u0026rdquo; in the field Acceptance Install the Agreement, then start install. Troubleshooting\nbash-4.4# ./java -version Error: Port Library failed to initialize: -70 Error: Could not create the Java Virtual Machine. Error: A fatal exception has occurred. Program will exit. ","date":"2020-01-09","externalUrl":null,"permalink":"/posts/aix-tips/","section":"Posts","summary":"记录在使用 AIX 时所遇到的问题和解决办法，以备以后遇到同样问题不要再因为这些再浪费时间，希望也能帮助到你。","title":"AIX 上安装工具以及一些使用总结","type":"posts"},{"content":"","date":"2020-01-07","externalUrl":null,"permalink":"/tags/solaris/","section":"标签","summary":"","title":"Solaris","type":"tags"},{"content":"记录在使用 Solaris 时所遇到的问题和解决办法，以备以后遇到同样问题不要再因为这些再浪费时间，希望也能帮助到你。\ninstall packages on solaris # https://www.opencsw.org/get-it/packages/\nInstall Git # https://www.opencsw.org/packages/git/\n","date":"2020-01-07","externalUrl":null,"permalink":"/posts/solaris-tips/","section":"Posts","summary":"本文记录了在使用 Solaris 时遇到的问题和解决办法，包括安装工具、配置网络、安装软件包等，帮助用户更高效地使用 Solaris 系统。","title":"Solaris 安装工具以及一些使用总结","type":"posts"},{"content":"时间过得飞快，转眼已经是 2020 年的第三天了，回顾 2019 年，我给自己的年终关键词是：尽力。\n这是我作为开发工程师的第二年，虽然 Title 是 SE (Software Engineer)，但主要的工作内容是产品的构建和发布以及 CI/CD/DevOps 的落地（自称打杂）。流水的记录一下 2019 年发生在工作上的“成绩”。\n2019 年在工作中除了完成日常产品构建、发布、Git 管理、VM 管理等，尝试在构建和发布自动化上做大的调整，将手工构建和部分自动构建从 Bamboo 迁移到 Jenkins，通过 Jenkins 的 multi-branch pipeline、Shared Libraries 与 Artifactory 做持续集成。\n2019 年在公司内部提交了一个创新项目，有幸拿到了第一名和首席产品奖。\n因此有机会再去美国，参加了公司的开发者大会。很开心这个项目最终进入到了产品的 Roadmap 里。\n2019 年底 12 月去北京参加了一次两天的《JFrog Jenkins，Artifactory \u0026amp; Kubernetes》训练营，跟 DevOps 行业工具里最有影响力（之一）的公司的工程师学习最佳实践。\n这一年关于持续集成和持续交付收获不少新的知识，但只更新了 11 篇公众号相关原创文章，33 篇 Blog。这差别是因为 Blog 更像笔记，随时记录修改不怕错；公众号更像报纸，发出去的内容无法修改和补充，每次更新都需要一字一句反复修改和阅览确认，最终输出一篇完整的原创内容需要花费比写 Blog 多几倍的时间。\n希望 2020 年能完成更多有价值的内容输出，有机会的话用 8 到 10 个月的业余时间完成一件以前一直不敢想的事（如果完成就写下来，没成就烂在肚子里）。\n分享在 2019 年最后一天收到的一封邮件：\n前段时间大家都做了年末总结。试问，有多少人对自己一年的表现满意？你是否在工作中体现出你的价值？你提交了多少次代码？提交的代码质量怎么样？你解决了多少个客户问题？你发现了多少 bug？ 你是否在技术和业务知识有所提高？和你的同事比较，你的进步速度你自己是否满意？又有多少人把时间都花在了微信和聊天上？有没有人是待着混日子，或者应付公事的思路在这里工作。 我们年龄都差不多，多半是三四十岁。这个年龄被认为是最好的工作年龄。希望你不要在这里浪费你最好的时光。 有人可能说，我已经实现或者接近时间财务自由了，我对工作没有大的要求。首先我恭喜你实现财务自由，但同时我想说的是工作绝不只是一份收入来源这么简单。你是否能在工作中得到同事和领导的认可，是否能体现自己的价值。 我喜欢和有求知欲，有责任感，有上进心的人一起工作。我同时不喜欢工作时间，整天拿着手机不离手的， 工作上没有上进心的人。如果确实有事情，可以离开作为去打个电话，迅速解决一下。没有事情的闲看，是不赞成的。 还有人，到点就准时下班，很像多干几分钟自己就吃亏了一样，这种心态千万不可取。试问，如果你的能力水平不及你的同事，你每天还不比别人多努力，你怎么才能接近或者超过别人？难道永远都想落后？每天多走一里路，每天多做一些吧。\n我读了好几遍，感谢上面的话，从个人角度非常认同以上观点，尤其是说工作绝不是一份收入来源这么简单，它是一个人的价值体现。如果喜欢这份工作，尽力去做到最好吧；如果不喜欢，还是趁早找到自己的乐趣所在。\n","date":"2019-12-28","externalUrl":null,"permalink":"/misc/2019-summary/","section":"Miscs","summary":"\u003cp\u003e时间过得飞快，转眼已经是 2020 年的第三天了，回顾 2019 年，我给自己的年终关键词是：尽力。\u003c/p\u003e","title":"2019 年终总结","type":"misc"},{"content":" Jenkins Warnings Next Generation 插件 # Jenkins Warnings Next Generation 插件可收集编译器警告或静态分析工具报告的问题并可视化结果，它内置了对众多静态分析工具（包括多个编译器）的支持，更多支持的报告格式。\n支持的项目类型 # Warnings Next Generation 插件支持以下 Jenkins 项目类型：\n自由式项目 Maven 项目 矩阵项目 脚本化管道（顺序和并行步骤） 声明式管道（顺序步骤和并行步骤） 多分支管道 功能概述 # 当作为后续构建任务操作（或步骤）添加时，Warnings Next Generation 插件提供以下功能：\n该插件会扫描 Jenkins 版本的控制台日志或你工作区中的文件中是否存在任何问题。支持一百多种报告格式，它可以检测到的问题包括： 来自编译器的错误（C，C＃，Java等） 来自静态分析工具（CheckStyle，StyleCop，SpotBugs 等）的警告 来自复制粘贴检测器（CPD, Simian 等）的重复 漏洞 在源文件的注释中打开任务 该插件会发布有关在构建中发现的问题的报告，因此可以从以下位置导航到摘要报告，主构建页面。你还可以从那里深入了解细节： 发行新的，固定的和未解决的问题 按严重性，类别，类型，模块或程序包分发问题 所有问题的列表，包括来自报告工具的有用评论 受影响文件的带注释的源代码 问题趋势图 该插件不会运行静态分析，它只是可视化此类工具报告的结果。你仍然需要在构建文件或 Jenkinsfile 中启用和配置静态分析工具。\n配置 # 你可以在 Jenkins 作业配置用户界面中配置插件的每个选项（在自由式，maven 或矩阵作业中）。在这里你需要在工作中添加并启用生成后操作“记录编译器警告和静态分析结果”。\n在管道中，将通过添加 recordIssues 激活插件。也可以使用相同的用户界面来配置此步骤（通过使用 Snippet 编辑器）。请注意，对于脚本化管道，一些其他功能可用于汇总和分组问题，有关详细信息，请参阅“高级管道配置”部分。\n在以下各节中，将同时显示图形配置和管道配置。\n工具选择 # 下图显示了插件的基本配置：\n首先，你需要指定用于创建问题的工具，根据所选工具，你可能还会配置一些其他参数。\n对于所有读取报告文件的解析器，你需要指定应分析和扫描问题的报告文件的模式。如果未指定模式，则将扫描构建的控制台日志。对于几种流行的工具，提供了默认模式，在这种情况下，如果模式为空，则将使用默认模式。\n为了让扫描程序正确解析你的报告，需要设置文件的编码，否则将使用平台编码，这可能不正确。\n每个工具都由一个 ID 标识，该 ID 用作分析结果的 URL。对于每个工具，都提供了一个默认 URL（和名称），可以根据需要进行更改。例如，如果你打算多次使用解析器，则需要为每个调用指定不同的 ID。\n你可以指定将用于同一配置的多个工具（和模式），由于 Jenkins 的技术（或市场）限制，无法通过使用多个后期构建操作来选择不同的配置。\n通过使用“汇总结果”复选框，可以使用一项新功能。如果选中此选项，则将创建一个结果，其中包含所选工具的所有问题的汇总。这是之前静态分析收集器插件提供的。激活此选项后，你将获得所有问题的唯一入口点。以下屏幕截图显示了此新行为：\n如果未启用此选项，则将为每个工具创建单独的结果。此结果具有唯一的 URL 和图标，因此你可以快速查看创建的报告之间的区别：\n在基本配置部分中，你还可以选择是否针对失败的构建也运行该步骤。默认情况下禁用此选项，因为如果构建失败，分析结果可能会不准确。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues( enabledForFailure: true, aggregatingResults: true, tools: [java(), checkStyle(pattern: \u0026#39;checkstyle-result.xml\u0026#39;, reportEncoding: \u0026#39;UTF-8\u0026#39;)] ) 如果使用单个工具，则可以使用属性工具代替工具：\nrecordIssues enabledForFailure: true, aggregatingResults: true, tool: checkStyle(pattern: \u0026#39;checkstyle-result.xml\u0026#39;) 创建对自定义工具的支持 # 如果你的项目中没有内置工具，则可以通过多种方式添加其他工具。\n将问题导出为受支持的格式 # 将工具的分析结果获取到 Warnings 插件中的一种简单方法是将信息导出为一种已经支持的格式。例如，几种工具将其问题导出为 CheckStyle 或 PMD 格式。如果要使用警告插件的所有功能，则最好将信息导出为本机 XML 或 JSON 格式（此解析器使用 ID 问题）。 这些格式已经在用户界面中注册，你可以直接使用它们。你甚至可以在包含单行 JSON 问题的简单日志文件中提供问题，请参见示例。\n这是一个示例步骤，可用于解析本机 JSON（或 XML）格式：\nrecordIssues(tool: issues()) 使用自定义插件部署新工具 # 最灵活的方法是通过编写将在你自己的小型 Jenkins 插件中部署的 Java 类来定义新工具，有关详细信息，请参见文档“为自定义静态分析工具提供支持”。\n使用Groovy解析器创建新工具 # 如果日志消息的格式非常简单，则可以通过在 Jenkins 的用户界面中创建简单的工具配置来定义对工具的支持。 出于安全原因（Groovy 脚本可能会危害你的主服务器），此配置仅在系统配置中可用。 新解析器的配置采用正则表达式，该正则表达式将用于匹配报告格式。 如果表达式匹配，则将调用 Groovy 脚本，该脚本将匹配的文本转换为问题实例。 这是基于 Groovy 的解析器的示例：\n以编程方式创建 Groovy 解析器 # 还可以使用 Groovy 脚本从管道，Jenkins 启动脚本或脚本控制台中创建基于 Groovy 的解析器，请参见以下示例：\ndef config = io.jenkins.plugins.analysis.warnings.groovy.ParserConfiguration.getInstance() if(!config.contains(\u0026#39;pep8-groovy\u0026#39;)){ def newParser = new io.jenkins.plugins.analysis.warnings.groovy.GroovyParser( \u0026#39;pep8-groovy\u0026#39;, \u0026#39;Pep8 Groovy Parser\u0026#39;, \u0026#39;(.*):(\\\\d+):(\\\\d+): (\\\\D\\\\d*) (.*)\u0026#39;, \u0026#39;return builder.setFileName(matcher.group(1)).setCategory(matcher.group(4)).setMessage(matcher.group(5)).buildOptional()\u0026#39;, \u0026#34;optparse.py:69:11: E401 multiple imports on one line\u0026#34; ) config.setParsers(config.getParsers().plus(newParser)) } 使用配置作为代码导入解析器（JCasC） # 还可以使用 JCasC yaml 文件中的部分来指定基于 Groovy 的解析器。这是一个小示例，展示了如何添加这样的解析器：\nunclassified: warningsParsers: parsers: - name: \u0026#34;Example parser\u0026#34; id: example-id regexp: \u0026#34;^\\\\s*(.*):(\\\\d+):(.*):\\\\s*(.*)$\u0026#34; script: | import edu.hm.hafner.analysis.Severity builder.setFileName(matcher.group(1)) .setLineStart(Integer.parseInt(matcher.group(2))) .setSeverity(Severity.WARNING_NORMAL) .setCategory(matcher.group(3)) .setMessage(matcher.group(4)) return builder.buildOptional(); example: \u0026#34;somefile.txt:2:SeriousWarnings:SomethingWentWrong\u0026#34; 使用定义的工具 # 一旦注册了 Groovy 解析器，就可以在作业的工具配置部分中使用它：\n首先，你需要选择工具 “Groovy Parser” 以获取 Groovy 解析器的配置屏幕。 然后，你可以从可用解析器列表中选择解析器。 该列表是根据 Jenkins 的“系统配置”部分中定义的解析器动态创建的。可以使用与其他工具相同的方式来设置自定义 ID 和名称属性。\n为了在管道中使用 Groovy 解析器，你需要使用以下形式的脚本语句：\nrecordIssues sourceCodeEncoding: \u0026#39;UTF-8\u0026#39;, tool: groovyScript(parserId: \u0026#39;groovy-id-in-system-config\u0026#39;, pattern:\u0026#39;**/*report.log\u0026#39;, reportEncoding:\u0026#39;UTF-8\u0026#39;) 处理受影响的源代码文件的属性 # 为了让插件解析并显示你的源代码文件，需要为这些文件设置正确的编码。 此外，如果你的源代码不在工作区中（例如，它已签出到共享代理文件夹中），则该插件将不会自动找到你的源文件。 为了让插件显示这些文件，你可以添加一个附加的源目录：\n以下代码段显示了带有这些选项的示例管道，请注意，如果需要，可以不同地设置报告文件的编码：\nrecordIssues sourceCodeEncoding: \u0026#39;ISO-8859-1\u0026#39;, sourceDirectory: \u0026#39;/path/to/sources\u0026#39;, tool: java(reportEncoding: \u0026#39;UTF-8\u0026#39;) 请注意，工作区外部的文件内容可能很敏感。 为了防止意外显示此类文件，你需要在 Jenkins 系统配置屏幕中提供允许的源代码目录的白名单：\n另外，此配置设置可以由 JCasC yaml 文件中的以下子节提供\nunclassified: warningsPlugin: sourceDirectories: - path: \u0026#34;C:\\\\Temp\u0026#34; - path: \u0026#34;/mnt/sources\u0026#34; 控制参考构建的选择（基准） # 警告下一代插件的一项重要功能是将问题分类为新问题，未解决问题和已解决问题：\n新增：所有问题，属于当前报告的一部分，但未在参考报告中显示 已修复：所有问题，属于参考报告的一部分，但不再存在于当前报告中 未解决：所有问题，是当前报告和参考报告的一部分 为了计算此分类，插件需要参考构建（基准）。 然后，通过比较当前版本和基准中的问题来计算新的，已修复的和未解决的问题。 有三个选项可控制参考构建的选择。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(), ignoreQualityGate: false, ignoreFailedBuilds: true, referenceJobName: \u0026#39;my-project/master\u0026#39; 筛选问题 # 创建的问题报告可以随后进行过滤。 你可以指定任意数量的包含或排除过滤器。 当前，支持按模块名称，程序包或名称空间名称，文件名，类别或类型过滤问题。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(pattern: \u0026#39;*.log\u0026#39;), filters: [includeFile(\u0026#39;MyFile.*.java\u0026#39;), excludeCategory(\u0026#39;WHITESPACE\u0026#39;)] Quality gate 配置 # 你可以定义几个 Quality gate (质量门)，在报告问题后将对其进行检查。这些质量门使你可以修改詹金斯的生产状态，以便立即查看是否满足所需的产品质量。对于这些质量门中的每一个，都可以将构建设置为不稳定或失败。所有质量门都使用一个简单的度量标准：给定质量门将失败的问题数量。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(pattern: \u0026#39;*.log\u0026#39;), qualityGates: [[threshold: 1, type: \u0026#39;TOTAL\u0026#39;, unstable: true]] 类型确定将用来评估质量门的属性。请参阅枚举 QualityGateType 以查看支持哪些不同类型。\n健康报告配置 # 该插件可以参与你项目的运行状况报告。你可以更改将运行状况更改为 0％ 和 100％ 的问题数。此外，可以选择在创建运行状况报告时应考虑的严重性。\n健康报告配置!\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(pattern: \u0026#39;*.log\u0026#39;), healthy: 10, unhealthy: 100, minimumSeverity: \u0026#39;HIGH\u0026#39; 该作业根据严重性为 HIGH 和错误的所有警告来调整构建运行状况。如果内部版本包含 10 条或更少的警告，则运行状况为 100％。如果内部版本有 100 个以上的警告，则运行状况为 0％。\n管道配置 # 在 Jenkins Pipeline 中使用 Warnings 插件的要求可能很复杂，有时会引起争议。为了尽可能灵活，我决定将主要步骤分为两个独立的部分，然后可以彼此独立使用。\n简单的管道配置 # 步骤 recordIssues 提供了简单的管道配置，它提供了与构建后操作相同的属性（请参见上文）。此步骤扫描给定文件集（或控制台日志）中的问题，并在构建中报告这些问题。你可以使用代码片段生成器来创建一个有效的代码片段，以调用此步骤。以下示例显示了此步骤的典型示例：\nrecordIssues( enabledForFailure: true, tool: java(pattern: \u0026#39;*.log\u0026#39;), filters: [includeFile(\u0026#39;MyFile.*.java\u0026#39;), excludeCategory(\u0026#39;WHITESPACE\u0026#39;)] ) 在此示例中，将扫描文件 * .log 中的 Java 问题。仅包括文件名与模式 MyFile.*.java 匹配的问题。类别 WHITESPACE 的问题将被排除，即使构建失败，也会执行该步骤。\n为了查看所有配置选项，你可以研究步骤实现。\n声明式管道配置 # 声明性管道作业中的插件配置与脚本管道中的配置相同，请参见以下示例，该示例在 Jenkins 上构建分析模型库：\npipeline { agent \u0026#39;any\u0026#39; tools { maven \u0026#39;mvn-default\u0026#39; jdk \u0026#39;jdk-default\u0026#39; } stages { stage (\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;${M2_HOME}/bin/mvn --batch-mode -V -U -e clean verify -Dsurefire.useFile=false -Dmaven.test.failure.ignore\u0026#39; } } stage (\u0026#39;Analysis\u0026#39;) { steps { sh \u0026#39;${M2_HOME}/bin/mvn --batch-mode -V -U -e checkstyle:checkstyle pmd:pmd pmd:cpd findbugs:findbugs spotbugs:spotbugs\u0026#39; } } } post { always { junit testResults: \u0026#39;**/target/surefire-reports/TEST-*.xml\u0026#39; recordIssues enabledForFailure: true, tools: [mavenConsole(), java(), javaDoc()] recordIssues enabledForFailure: true, tool: checkStyle() recordIssues enabledForFailure: true, tool: spotBugs() recordIssues enabledForFailure: true, tool: cpd(pattern: \u0026#39;**/target/cpd.xml\u0026#39;) recordIssues enabledForFailure: true, tool: pmdParser(pattern: \u0026#39;**/target/pmd.xml\u0026#39;) } } } 高级管道配置 # 有时仅使用一个步骤发布和报告问题是不够的。例如，如果你使用多个并行步骤来构建产品，并且想要将所有这些步骤中的问题合并为一个结果。然后，你需要拆分扫描和聚合。该插件提供以下两个步骤：\nscanForIssues 此步骤使用特定的解析器扫描报告文件或控制台日志，并创建一个包含报告的中 间 AnnotatedReport 对象。有关详细信息，请参见步骤实现。 publishIssues：此步骤在你的构建中发布一个新报告，其中包含几个 scanForIssues 步骤的汇总结果。有关详细信息，请参见步骤实现。 node { stage (\u0026#39;Checkout\u0026#39;) { git branch:\u0026#39;5.0\u0026#39;, url: \u0026#39;git@github.com:jenkinsci/warnings-plugin.git\u0026#39; } stage (\u0026#39;Build\u0026#39;) { def mvnHome = tool \u0026#39;mvn-default\u0026#39; sh \u0026#34;${mvnHome}/bin/mvn --batch-mode -V -U -e clean verify -Dsurefire.useFile=false\u0026#34; junit testResults: \u0026#39;**/target/*-reports/TEST-*.xml\u0026#39; def java = scanForIssues tool: java() def javadoc = scanForIssues tool: javaDoc() publishIssues issues: [java, javadoc], filters: [includePackage(\u0026#39;io.jenkins.plugins.analysis.*\u0026#39;)] } stage (\u0026#39;Analysis\u0026#39;) { def mvnHome = tool \u0026#39;mvn-default\u0026#39; sh \u0026#34;${mvnHome}/bin/mvn --batch-mode -V -U -e checkstyle:checkstyle pmd:pmd pmd:cpd findbugs:findbugs\u0026#34; def checkstyle = scanForIssues tool: checkStyle(pattern: \u0026#39;**/target/checkstyle-result.xml\u0026#39;) publishIssues issues: [checkstyle] def pmd = scanForIssues tool: pmdParser(pattern: \u0026#39;**/target/pmd.xml\u0026#39;) publishIssues issues: [pmd] def cpd = scanForIssues tool: cpd(pattern: \u0026#39;**/target/cpd.xml\u0026#39;) publishIssues issues: [cpd] def spotbugs = scanForIssues tool: spotBugs(pattern: \u0026#39;**/target/findbugsXml.xml\u0026#39;) publishIssues issues: [spotbugs] def maven = scanForIssues tool: mavenConsole() publishIssues issues: [maven] publishIssues id: \u0026#39;analysis\u0026#39;, name: \u0026#39;All Issues\u0026#39;, issues: [checkstyle, pmd, spotbugs], filters: [includePackage(\u0026#39;io.jenkins.plugins.analysis.*\u0026#39;)] } } 新功能 # 以下各节介绍了最重要的新功能。\n发行记录：New, Fixed, Outstanding 问题 # 该插件的一大亮点是能够将后续版本的问题分类为 New, Fixed, Outstanding。\n使用此功能可以更轻松地控制项目的质量：你只能专注于最近引入的警告。\n注意：新警告的检测基于复杂的算法，该算法试图在源代码的两个不同版本中跟踪同一警告。根据源代码的修改程度，它可能会产生一些误报，即，即使应该没有警告也可能会收到一些新的固定警告。该算法的准确性仍在研究中，并将在接下来的几个月中进行完善。\nSeverities 严重程度 # 该插件在图表中显示问题严重性的分布，它定义了以下默认严重级别，但是扩展警告插件的插件可能会添加其他默认级别。\nError：表示通常会导致构建失败的错误 Warning (High, Normal, Low)：指示给定优先级的警告。映射到优先级取决于各个解析器。 请注意，并非每个解析器都能产生不同严重性的警告。某些解析器仅对所有问题使用相同的严重性。\nBuild trend 构建趋势 # 为了查看分析结果的趋势，几个图表显示了每个构建的问题数量。这些图表用于详细信息页面和作业概述中。当前提供以下不同的趋势图类型：\n问题的严重程度分布 # 默认趋势图显示问题总数，按严重性堆叠。使用此图表，你可以查看哪种严重程度对问题总数贡献最大。\n每种静态分析类型的问题 # 如果你要汇总几个静态分析结果，则类型图将使用单独的一行显示每个工具的问题数量。你可以通过单击相应的图例符号暂时隐藏工具。\n新问题与已修复问题 # 如果你对积压的问题感兴趣，可以打开新的与固定的图表。它映射了引入的问题与通过一系列构建解决的问题。这可以帮助你了解整个待办事项列表是在增加还是在减少。\n项目健康 # 仅当启用了运行状况报告后，运行状况图表才可用。在这种情况下，趋势图将显示健康和不健康区域中的警告数量。你的项目目标应该是使警告数量不逃避图表的绿色部分。\n缩放 # 细节视图中的所有趋势图都支持使用图表底部的范围滑块缩放构建轴。\n构建与日期轴 # 详细信息视图中的所有趋势图都可以显示每个构建或每天的警告数量。你可以通过选择右上角的相应图标在X轴变体之间切换，每天显示平均警告数。\n问题概述 # 你可以在几个聚合视图中快速，高效地查看报告的问题集。根据问题的数量或类型，你将看到问题的分布\nStatic Analysis Tool（静态分析工具） Module（模组） Package or Namespace（包或命名空间） Severity（严重程度） Category（类别） Type（类型） 这些详细信息视图中的每一个都是交互式的，即，你可以导航到已分类问题的子集。\n问题详情 # 一组已报告的问题显示在一个现代化的响应表中。该表使用 Ajax 调用按需加载，它提供以下功能：\nPagination（分页）：问题的数量分为几个页面，可以使用提供的页面链接进行选择。请注意，目前分页是在客户端进行的，即从服务器获取整个问题表可能要花费一些时间。 Sorting（排序）：可以通过单击表列中的仅一个来对表内容进行排序。 Filtering, Searching（过滤，搜索）：你可以通过在搜索框中输入一些文本来过滤显示的问题。 Content Aware（内容感知）：仅当有必要显示的内容时才显示列。也就是说，如果工具未报告问题类别，则该类别将被自动隐藏。 Responsive（响应式）：布局应适应实际的屏幕阈值。 Details（详细信息）：问题的详细信息消息（如果由相应的静态分析工具提供）在表中显示为子行。 源代码 Blame（归咎于） # 这个功能需要安装其他插件：Git Forensics 插件\n如果未在作业配置中禁用，则插件将执行 git blame 以确定谁是问题的负责 author。在相应的 SCM Blames 视图中，所有问题将与 auther name, email, 和 commit ID 一起列出。\n为了禁用 git blame 功能，请将属性 blameDisabled 设置为 true，请参见以下示例：\nrecordIssues blameDisabled: true, tool: java(pattern: \u0026#39;*.log\u0026#39;) Git 仓库取证 # 此功能需要安装其他插件：Git Forensics 插件\n如果未在作业配置中禁用，则该插件将以“犯罪现场代码”的样式（Adam Tornhill，2013年11月）挖掘源代码存储库，以确定受影响文件的统计信息。在相应的 “SCM 取证” 视图中，将列出所有问题以及受影响文件的以下属性：\n提交总数 不同作者总数 创作时间 最后修改时间 源代码控制概述 为了禁用 Git 取证功能，请将属性 forensicsDisabled设置为 true，请参见以下示例：\nrecordIssues forensicsDisabled: true, tool: java(pattern: \u0026#39;*.log\u0026#39;) 源代码视图 # 现在，源代码视图使用 JS 库 Prism 在受影响的文件中显示警告。该库为最流行的语言提供语法高亮显示，并在客户端呈现所有内容。\n发行总数栏 # 你可以在 Jenkins 作业表的单独列中显示作业的总数。 默认情况下，Jenkins 主列表视图将显示一个新列，该列计算所有工具的发行总数。 你可以添加可以配置的其他列\n列名 应考虑的实际工具 要显示的总计类型（总体警告，新警告，特定严重性等），请参阅 “token宏支持” 部分。 仪表板视图支持 # 还提供对 Jenkins 仪表板视图的支持。当前，以下 portlet 可用：\n每个工具和作业表的问题 # 问题表显示了作业的问题总数（由每个工具分开）。\n问题趋势 # 可以将趋势图添加为 portlet，该趋势图显示所有作业的发行总数。\n远程API # 该插件提供以下 REST API 端点。\n所有分析结果的汇总摘要 # 可以使用 URL [build-url]/warnings-ng/api/json（或 [build-url]/warnings-ng/api/xml）查询构建中已配置的所有静态分析工具。此汇总显示每个工具的 ID，名称，URL 和问题总数。\n{ \u0026#34;_class\u0026#34;: \u0026#34;io.jenkins.plugins.analysis.core.restapi.AggregationApi\u0026#34;, \u0026#34;tools\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;maven\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/maven\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Maven Warnings\u0026#34;, \u0026#34;size\u0026#34;: 9 }, { \u0026#34;id\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/java\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Java Warnings\u0026#34;, \u0026#34;size\u0026#34;: 1 }, { \u0026#34;id\u0026#34;: \u0026#34;javadoc\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/javadoc\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;JavaDoc Warnings\u0026#34;, \u0026#34;size\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;checkstyle\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/checkstyle\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;CheckStyle Warnings\u0026#34;, \u0026#34;size\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;pmd\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/pmd\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;PMD Warnings\u0026#34;, \u0026#34;size\u0026#34;: 671 }, { \u0026#34;id\u0026#34;: \u0026#34;spotbugs\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/spotbugs\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SpotBugs Warnings\u0026#34;, \u0026#34;size\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;cpd\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/cpd\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;CPD Warnings\u0026#34;, \u0026#34;size\u0026#34;: 123 }, { \u0026#34;id\u0026#34;: \u0026#34;open-tasks\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/open-tasks\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Open Tasks Scanner Warnings\u0026#34;, \u0026#34;size\u0026#34;: 11 } ] } 分析结果汇总 # 你可以使用 URL [build-url]/[tool-id]/api/xml（或 [build-url]/[tool-id]/api/json）获得特定分析报告的摘要。摘要包含问题数量，质量门状态以及所有信息和错误消息。\n这是一个示例 XML 报告：\n\u0026lt;analysisResultApi _class=\u0026#39;io.jenkins.plugins.analysis.core.restapi.AnalysisResultApi\u0026#39;\u0026gt; \u0026lt;totalSize\u0026gt;3\u0026lt;/totalSize\u0026gt; \u0026lt;fixedSize\u0026gt;0\u0026lt;/fixedSize\u0026gt; \u0026lt;newSize\u0026gt;0\u0026lt;/newSize\u0026gt; \u0026lt;noIssuesSinceBuild\u0026gt;-1\u0026lt;/noIssuesSinceBuild\u0026gt; \u0026lt;successfulSinceBuild\u0026gt;-1\u0026lt;/successfulSinceBuild\u0026gt; \u0026lt;qualityGateStatus\u0026gt;WARNING\u0026lt;/qualityGateStatus\u0026gt; \u0026lt;owner _class=\u0026#39;org.jenkinsci.plugins.workflow.job.WorkflowRun\u0026#39;\u0026gt; \u0026lt;number\u0026gt;46\u0026lt;/number\u0026gt; \u0026lt;url\u0026gt;http://localhost:8080/view/White%20Mountains/job/Full%20Analysis%20-%20Model/46/\u0026lt;/url\u0026gt; \u0026lt;/owner\u0026gt; \u0026lt;infoMessage\u0026gt;Searching for all files in \u0026#39;/tmp/node1/workspace/Full Analysis - Model\u0026#39; that match the pattern \u0026#39;**/target/spotbugsXml.xml\u0026#39; \u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; found 1 file\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Successfully parsed file /tmp/node1/workspace/Full Analysis - Model/target/spotbugsXml.xml\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; found 3 issues (skipped 0 duplicates)\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Post processing issues on \u0026#39;node1\u0026#39; with encoding \u0026#39;UTF-8\u0026#39;\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Resolving absolute file names for all issues\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; affected files for all issues already have absolute paths\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Copying affected files to Jenkins\u0026#39; build folder /Users/hafner/Development/jenkins/jobs/Full Analysis - Model/builds/46 \u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; 2 copied, 0 not in workspace, 0 not-found, 0 with I/O error\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Resolving module names from module definitions (build.xml, pom.xml, or Manifest.mf files)\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; all issues already have a valid module name\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Resolving package names (or namespaces) by parsing the affected files\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; all affected files already have a valid package name\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Creating fingerprints for all affected code blocks to track issues over different builds\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;No filter has been set, publishing all 3 issues\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;No valid reference build found - all reported issues will be considered outstanding\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Evaluating quality qualityGates\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; WARNING - Total number of issues: 3 - Quality Gate: 1\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; Some quality qualityGates have been missed: overall result is WARNING\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Health report is disabled - skipping\u0026lt;/infoMessage\u0026gt; \u0026lt;/analysisResultApi\u0026gt; Token 宏支持 # Warnings 插件提供了 token ANALYSIS_ISSUES_COUNT，可用于其他后期构建处理步骤，例如在邮件中。为了使用此 token，你需要安装 token 宏插件。token 具有以下可选参数：\ntool：选择特定的分析结果，如果未定义，则将所有结果相加 type：选择要使用的计数器的类型，请选择以下之一 Total（任何严重性） Total（仅错误） Total（仅严重度高） Total（仅严重级别正常） Total（仅限严重性低） New （任何严重程度） New （仅限错误） New （仅限严重性高） New （仅严重性为正常） New （仅限严重性低） Delta（任何严重程度） Delta（仅错误） Delta（仅严重度高） Delta（仅严重等级正常） Delta（仅严重度低） Fixed（任何严重性） 例子：\n${ANALYSIS_ISSUES_COUNT}：扩展到所有分析工具的合计数量\n${ANALYSIS_ISSUES_COUNT, tool=\u0026quot;checkstyle\u0026quot;}：扩展到CheckStyle问题的总数\n${ANALYSIS_ISSUES_COUNT, tool=\u0026quot;checkstyle\u0026quot;, type: \u0026quot;NEW\u0026quot;}：扩展到新的 CheckStyle 问题数\n从静态分析套件过渡 # 以前，静态分析套件的插件提供了相同的功能集（CheckStyle，PMD，FindBugs，静态分析实用工具，Analysis Collector，任务扫描器，Warnings 等）。为了简化用户体验和开发过程，这些插件和核心功能已合并到Warnings Next Generation 插件中。这些旧的静态分析插件不再需要，现在已经停产。如果当前使用这些旧插件之一，则应尽快迁移到新的记录器和步骤。我仍然会保留旧代码一段时间，但是主要的开发工作将花在新的代码库中。\n迁移 Pipelines 调用旧的静态分析步骤（例如，findbug，checkstyle 等）的管道需要立即调用新的 recordIssues 步骤。所有静态分析工具都使用相同的步骤，使用 step 属性工具选择实际的解析器。有关可用参数集的更多详细信息，请参见“配置”部分。\n迁移其他所有工作 使用旧版 API 的 Freestyle，Matrix 或 Maven Jobs 使用了由每个插件提供的所谓的 Post Build Action。例如，FindBugs 插件确实提供了构建后操作“发布 FindBugs 分析结果”。这些旧的插件特定操作不再受支持，它们现在在用户界面中标记为 [Deprecated]。现在，你需要添加一个新的后期构建步骤-对于所有静态分析工具，此步骤现在称为“记录编译器警告和静态分析结果”。工具的选择是此后期构建步骤配置的一部分。注意：新的后期制作操作无法读取使用旧 API 的后期制作步骤所产生的警告。也就是说，你看不到新旧结果的合并历史记录-你仅看到两个不相关的结果。也不会自动转换以旧格式存储的结果。\n插件的迁移取决于分析核心 以下插件已集成到此警告插件的新版本中：\nAndroid-Lint 插件 Analysis Collector 插件 CheckStyle 插件 CCM 插件 Dry 插件 PMD 插件 FindBugs 插件 Tasks Scanner 插件 Warnings 插件 所有其他插件仍需要集成或需要重构以使用新的 API\n","date":"2019-12-28","externalUrl":null,"permalink":"/posts/jenkins-warnings-next-generation-plugin/","section":"Posts","summary":"本文介绍了 Jenkins Warnings Next Generation 插件的功能和配置方法，包括如何收集编译器警告和静态分析工具报告的问题，并可视化结果。","title":"Jenkins Warnings Next Generation 插件","type":"posts"},{"content":"If you are a member of a research and development efficiency team or are engaged in CI/CD or DevOps, in addition to providing infrastructure, metrics and data are also a very important aspect. For example, you need to analyze the code submission status of a certain Git repository:\nWho submitted the most code to this repository? What is the activity level of this repository? Submission analysis data for different periods Contribution ranking for each version Weekly/monthly/yearly contribution ranking, etc. A few days ago, I discovered a Git history statistics generation tool called GitStats (https://github.com/shenxianpeng/gitstats)\nThis is a tool written in Python. It has a small codebase but very powerful analytical capabilities. It\u0026rsquo;s one of the few open-source projects I\u0026rsquo;ve found that can generate beautiful reports and is easy to use.\nThe gitstats report is also powerful (https://shenxianpeng.github.io/gitstats/previews/main/index.html). Those interested can try it out.\nHow to Use # Dependencies required: Git, Python3, Gnuplot.\nIf you have Linux, it\u0026rsquo;s recommended to download and install on Linux. I tried setting up the environment on Windows, but I had to configure Cygwin and manually configure Gnuplot (Gnuplot is a portable command-line driven graphics tool), which was quite troublesome. The following are the installation and usage steps on Linux.\n# Install Gnuplot sudo yum -y install gnuplot # Install gitstats pip install gitstats # Clone the code repository you want to analyze git clone https://github.com/alibaba/fastjson.git # Execute the command to generate the report gitstats ../fastjson ../html/fastjson # After 15 seconds of execution, the report is generated Generating report... [0.00393] \u0026gt;\u0026gt; git --git-dir=/workspace/gitstats/.git --work-tree=/workspace/gitstats rev-parse --short HEAD [0.00236] \u0026gt;\u0026gt; git --version [0.00716] \u0026gt;\u0026gt; gnuplot --version Generating graphs... [0.01676] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/day_of_week.plot\u0026#34; [0.01571] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/files_by_date.plot\u0026#34; [0.01281] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/month_of_year.plot\u0026#34; [0.09293] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/lines_of_code_by_author.plot\u0026#34; [0.01340] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/commits_by_year.plot\u0026#34; [0.01799] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/hour_of_day.plot\u0026#34; [0.01627] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/domains.plot\u0026#34; [0.01268] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/commits_by_year_month.plot\u0026#34; [0.09435] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/commits_by_author.plot\u0026#34; [0.01522] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/lines_of_code.plot\u0026#34; Execution time 15.16208 secs, 4.58247 secs (30.22 %) in external commands) You may now run: sensible-browser \u0026#39;/workspace/html/fastjson/index.html\u0026#39; Report Analysis # General Statistics # You can see that this project is called fastjson. The first code submission was on July 31, 2011, and it has been 3060 days since then, with 893 active days. There are a total of 2960 files, with a total of 250,000 lines of code.\nActivity # The number of commits per hour of the day, per day of the week, per hour of the week, per month of the year, and per year.\nIn the past 32 weeks, the period from week 12 to week 17 was very active, with many submissions. Also, you can see that there are very few submissions between 12 pm and 8 pm. Most programmers are dealing with company work during this time and cannot contribute to open-source projects.\nSubmissions start to increase after 8 pm, indicating a very dedicated developer. You can also see that there are submissions from Monday to Sunday, with relatively fewer submissions on Saturday, likely due to rest and relaxation. Sunday submissions are significantly more than Saturday, showing that considerable personal weekend time was spent.\nYou can also see that the main completion time of this project was from 2016 to 2017, and the completion time zone was UTC+8. This dimension allows for analysis of the contribution number of developers in different regions.\nContributors # Lists all authors, number of submissions, first submission, and last submission.\nThis chart also shows who the creator of the project is, and who contributed the most each year over the years. You can also see the email accounts used by the contributors.\nFiles and Lines of Code # The total number of files is 2960, and the number of lines of code is 250,000.\nYou can also see the yearly trend chart of file increases and the ranking of the number of these file types. You can see that Java files account for 96.08%, followed by JSON.\nTags # Tags are an important analytical indicator for the team (provided that the repository to be analyzed has created a tag after the release). You can see the contribution ranking of each version.\nConclusion # If you\u0026rsquo;re interested, you can analyze your own projects or find an interesting and influential project on GitHub to analyze, such as 996.ICU and vue, and have fun.\n","date":"2019-12-17","externalUrl":null,"permalink":"/en/posts/git-stats/","section":"Posts","summary":"GitStats, a Git history statistics generation tool written in Python, can generate detailed code submission statistical reports to help developers analyze project activity and contributor information.","title":"Git History Statistics Generator","type":"posts"},{"content":"","date":"2019-12-17","externalUrl":null,"permalink":"/en/tags/stats/","section":"Tags","summary":"","title":"Stats","type":"tags"},{"content":"最近做了 Black Duck 与 Jenkins 的集成，目标是给测试和开发提供定制化、定时的对各个开发代码仓库的进行源码扫描。\n为什么要做源码扫描 # 在产品开发中经常需要引入一些开源组件，但这些开源的代码会给产品风险。因此我们在发布自己产品的时候需要对这些开源组件的漏洞和许可信息进行评估。 Black Duck（黑鸭）是一款对源代码进行扫描、审计和代码管理的软件工具（同类型的工具还有 JFrog Xray）。能够搜索安全的开源代码，检测产品的开源代码使用情况，以检查外来代码的开源代码使用情况和风险情况。\n如果不能及时的进行代码扫描，在产品发布快要发布才进行扫描，如果发现问题这时候再去解决就会变得非常被动，因此团队需要尽早发现并解决问题，将 CI 工具进行集成，进行每日、每周、每月扫描就变得十分重要。\nBlack Duck 手动执行一般步骤 # 手动下载指定 Git 仓库及分支代码 去掉不相关的代码（也可以通过 Black Duck 参数去指定要扫描的特定文件或文件夹） 手动执行 Black Duck 扫描命令​ 扫描成功后，结果传到内部 Black Duck 网站供相关人员进行审查 Black Duck 与 Jenkins 的集成目标 # 一个流水线支持定制化仓库的代码下载 给开发和测试提供简单的、可随时可以执行源码扫描的界面 支持定期自动扫描，以及与其他 Jenkins 任务联动执行​ Black Duck 参数介绍 # --blackduck.url # 你的 Black Duck 网址 --blackduck.username # 你的登录用户 --blackduck.api.token # 你的登录用户 Token --detect.project.name # Black Duck 下面的项目 --detect.project.version.name # 项目版本号 --detect.source.path # 要扫描的代码目录 --logging.level.com.synopsys.integration # 扫描日志级别 --blackduck.trust.cert=TRUE # 是否信任 socket (SSL) --detect.blackduck.signature.scanner.snippet.matching # 扫描片段模式 更多其他参数可以参照官方的 CI 集成文档 Synopsys Detect for Jenkins\nBlack Duck 配置 # 首先，安装 Black Duck 插件 Synopsys Detect 到 Jenkins\n然后，配置 Synopsys Detect 插件\nJenkins -\u0026gt; Confiruration（系统配置） Black Duck URL： 公司内部的 Black Duck 网址，例如 https://yourcompany.blackducksoftware.com Black Duck credentials： 注意要选择 credentials 类型为 Secret text, Secret 填写你用户的 Token 配置完成后点击 Test connections to Black Duck，显示 Connection successful 表示配置成功。 Black Duck 流水线任务效果 # Black Duck 流水线代码 # pipeline{ agent { node { label \u0026#39;black-duck\u0026#39; customWorkspace \u0026#34;/agent/workspace/blackduck\u0026#34; } } parameters { choice( name: \u0026#39;VERSION\u0026#39;, choices: [\u0026#39;MVSURE_v1.1\u0026#39;, \u0026#39;MVSURE_v1.2\u0026#39;, \u0026#39;MVSURE_v2.2\u0026#39;], summary: \u0026#39;Which version do you want scan on black duck? MVSURE_v1.1, MVSURE_v1.2 or others?\u0026#39;) choice( name: \u0026#39;REPO\u0026#39;, choices: [\u0026#39;blog-server\u0026#39;, \u0026#39;blog-client\u0026#39;, \u0026#39;blog-docker\u0026#39;], summary: \u0026#39;Which repository code does above VERSION belong to?\u0026#39;) string( name: \u0026#39;BRANCH\u0026#39;, defaultValue: \u0026#39;develop\u0026#39;, summary: \u0026#39;Which branch does above VERSION belong to?\u0026#39;) choice( name: \u0026#39;SNIPPET-MODES\u0026#39;, choices: [\u0026#39;SNIPPET_MATCHING\u0026#39;, \u0026#39;SNIPPET_MATCHING_ONLY\u0026#39;, \u0026#39;FULL_SNIPPET_MATCHING\u0026#39;, \u0026#39;FULL_SNIPPET_MATCHING_ONLY\u0026#39;, \u0026#39;NONE\u0026#39;], summary: \u0026#39;What snippet scan mode do you want to choose?\u0026#39;) } environment { ROBOT = credentials(\u0026#34;d1cbab74-823d-41aa-abb7-858485121212\u0026#34;) hub_detect = \u0026#39;https://blackducksoftware.github.io/hub-detect/hub-detect.sh\u0026#39; blackduck_url = \u0026#39;https://yourcompany.blackducksoftware.com\u0026#39; blackduck_user = \u0026#39;robot@yourcompany.com\u0026#39; detect_project = \u0026#39;GITHUB\u0026#39; detect_project_version = \u0026#39;${VERSION}\u0026#39; detect_source_path = \u0026#39;${WORKSPACE}/${REPO}/src\u0026#39; } # 只保留最近十次 Jenkins 执行结果 options {buildDiscarder(logRotator(numToKeepStr:\u0026#39;10\u0026#39;))} # 定时触发可以在这里添加 stages { stage(\u0026#34;git clone\u0026#34;){ # 参数化 git clone 代码过程 steps{ sh \u0026#39;\u0026#39;\u0026#39; if [ -d ${REPO} ]; then rm -rf ${REPO} fi git clone -b ${BRANCH} --depth 1 https://$ROBOT_USR:\u0026#34;$ROBOT_PSW\u0026#34;@git.yourcompany.com/scm/github/${REPO}.git \u0026#39;\u0026#39;\u0026#39; } } stage(\u0026#34;black duck scan\u0026#34;){ # 参数化 Black Duck 所用到的参数值 steps { withCredentials([string(credentialsId: \u0026#39;robot-black-duck-scan\u0026#39;, variable: \u0026#39;TOKEN\u0026#39;)]) { # 用 withCredentials 来获得 Token synopsys_detect \u0026#39;bash \u0026lt;(curl -s ${hub_detect}) --blackduck.url=${blackduck_url} --blackduck.username=${blackduck_user} --blackduck.api.token=${TOKEN} --detect.project.name=${detect_project} --detect.project.version.name=${detect_project_version} --detect.source.path=${detect_source_path} --logging.level.com.synopsys.integration=debug --blackduck.trust.cert=TRUE --detect.blackduck.signature.scanner.snippet.matching=${SNIPPET-MODES}\u0026#39; } } } } post { # 不论结果任何都给执行者发送邮件通知 always { script { def email = load \u0026#34;vars/email.groovy\u0026#34; wrap([$class: \u0026#39;BuildUser\u0026#39;]) { def user = env.BUILD_USER_ID email.build(currentBuild.result, \u0026#34;${user}\u0026#34;) } } } success { echo \u0026#34;success, cleanup blackduck workspace\u0026#34; cleanWs() } } } ","date":"2019-12-08","externalUrl":null,"permalink":"/posts/blackduck-interate-with-jenkins/","section":"Posts","summary":"本文介绍如何将 Black Duck 与 Jenkins 集成，实现对代码仓库的自动化安全扫描和漏洞检测。","title":"Black Duck 与 Jenkins 集成","type":"posts"},{"content":" Docker 常用命令小纸条\nDocker start|stop|restart # # 查看 Docker 版本 docker -v # or docker --version # 重启 docker sudo systemctl restart docker.service # 停止 docker sudo systemctl stop docker.service # 启动 docker sudo systemctl start docker.service Docker run # 我们通过 docker 的两个参数 -i -t，让 docker 运行的容器实现\u0026quot;对话\u0026quot;的能力：\ndocker run -i -t ubuntu:15.10 /bin/bash Login Artifactory # 注意：Open Source 版本 Artifactory 不支持 Docker，需要下载 JFrog Container Registry 或是 Artifactory 企业版。\ndocker login -u \u0026lt;USER_NAME\u0026gt; -p \u0026lt;USER_PASSWORD\u0026gt; devasvm.dev.org.com:\u0026lt;REPOSITORY_PORT\u0026gt; -sh-4.2$ sudo docker login devasvm.dev.org.com:8040 Username: admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 把 Docker image 推送到远程仓库\n// docker tag SOURCE_IMAGE[:TAG] devasvm.dev.org.com:8040/docker-local/IMAGE[:TAG] -sh-4.2$ sudo docker tag ubuntu:15.10 devasvm.dev.org.com:8040/docker-local/ubuntu:15.10 // docker push devasvm.dev.org.com:8040/docker-local/IMAGE[:TAG] -sh-4.2$ sudo docker push devasvm.dev.org.com:8040/docker-local/ubuntu:15.10 The push refers to repository [devasvm.dev.org.com:8040/docker-local/ubuntu] 98d59071f692: Pushed af288f00b8a7: Pushed 4b955941a4d0: Pushed f121afdbbd5d: Pushed 15.10: digest: sha256:a3f5e428c0cfbfd55cffb32d30b1d78fedb8a9faaf08efdd9c5208c94dc66614 size: 1150 TODO # 更多 Docker 常用命令记录到这里。\n","date":"2019-12-02","externalUrl":null,"permalink":"/posts/docker-commands/","section":"Posts","summary":"一个 Docker 常用命令小纸条，记录一些常用的 Docker 命令和操作，方便日常使用和参考。","title":"Docker 常用命令","type":"posts"},{"content":" Docker 可分为三个版本 # Docker Engine - Community\nDocker Engine - Enterprise\nDocker Enterprise\nDocker Engine - Community 是希望开始使用 Docker 并尝试基于容器的应用程序的个人开发人员和小型团队的理想选择。\nDocker Engine - Enterprise 专为企业开发容器运行时而设计，同时考虑了安全性和企业级SLA。\nDocker Enterprise 专为企业开发和IT团队而设计，他们可以大规模构建，交付和运行关键业务应用程序。\n能力 Docker Engine - Community Docker Engine - Enterprise Docker Enterprise 容器引擎和内建的编配，网络，安全 √ √ √ 认证的基础设施，插件和ISV容器 √ √ 镜像管理 √ 容器应用程序管理 √ 镜像安全扫描 √ 安装 Docker 社区版本 # 以 CentOS 安装为例： https://docs.docker.com/install/linux/docker-ce/centos/ 其他 Docker 版本安装 # 参考 Docker 官网：https://docs.docker.com/install/overview/ ","date":"2019-12-01","externalUrl":null,"permalink":"/posts/overview-of-docker-editions/","section":"Posts","summary":"概述 Docker 的不同版本，包括社区版、企业版和企业级解决方案，适用于不同规模和需求的用户。","title":"Docker 版本概述","type":"posts"},{"content":"本周二下班我没有像往常一样加会班（我一般都会赶在晚6点后下班来躲过晚高峰期），而是直接挤地铁奔向机场，准备坐八点半去往北京的一班飞机，因为第二天要参加 JFrog 中国在北京望京举办的 Jenkins, Artifactory \u0026amp; Kubernetes 实战训练营。\n一是由于公司每人每年都有两天带薪培训假期，如果有特别适合我的，我会自费前往。二是培训内容本身也十分贴近我目前的工作内容，想了解下行业最佳实践与相关同行交流。\n在出行前，跟领导请了假说明意向，顺便问了下去参加类似培训是否会有预算，领导让我报一下，我大概算了车票和酒店的钱，领导就跟公司申请，并且还附带帮我申请了饭钱。公司同意，前提只有一个就是回来要做好分享。就这样我就带着任务去参加这次 JFrog DevOps 训练营了，这里我特别感谢我的领导以及公司。\n再次见北京 # 想想离开北京已经5年多了，这是唯一一次在北京停留时间最长的 48 小时，之前有过出差路过北京。以前虽然在北京工作，但那时候我还没有太多关注过生活，这次有意的注意了下，脑子里一直思考一个问题，如果再让我回到北京我还会像当初的执念仅仅想着北京工作几年而已吗？\n下了飞机已经十点半了，从机场到望京这段的机场高速车流量还是很大的，北京的夜生活跟以前的感觉一样，比起二三线城市足足晚了两个多小时。五年前在北我京通常都是九点后从公司走，赶上上线的时候后半夜才到家，第二天中午到公司，每天的时间比二三线城市现在我足足延后了五六个小时。\n我现在的生活是早九晚六，不推崇加班，上班期间也是很忙，但绝大多数人也不会加班，加不加班是自愿。期间去过美国出差，美国的同事很多是早上七八点上班，他们一般中午吃的很简单也很快，吃完饭如果工作忙，他们会立马投入工作，下午三四点钟就离开办公室了，待到五六点钟的极少数，但工作效率其实感觉不差。这让我想起了今年轰轰烈烈的讨论过的996，可能归根到底是我们的社会发展阶段所造成的，年轻人压力很大需要努力赚钱买房，有娃的人有自己的父母来帮忙照顾孩子，年老的人也不考虑自己的退休生活，更多的是希望能帮忙自己的孩子解决一些后顾之忧，就这样年轻人就可以安心的的996了。但是美国人他们不行，很现实的他们如果不三四点钟下班，他们的孩子就没人接，他们需要自己的家庭生活。因此，我们本应该由企业甚至是社会来承担事情，被全社会的来承担了，996就自然而然不可避免的发生了。\n早高峰时段，望京有的路口有年长的大爷大妈在指挥交通，每当绿灯时，大量的电动车和自行车与行人一同穿行，我总担心会被刮到。晚上下班吃完饭回去的时候，路口没有指挥了，行人、自行车以及电动车在红灯时过马路的情况还是挺普遍的，造成了绿灯时机动车通行效率很低。另外，就是走在路上，后面不时的有自行车、电动车骑过，我总是挺担心被撞到，这种走路时候担心的感觉其实是不太舒服的。\n中午跟朋友约了吃饭，听他聊起过去五年多的工作情况，期间他换了好几个公司，有勾心斗角的，有 P2P 黄的（工资还欠着的，还在仲裁中），谈起最近一年北京大厂裁员以及网易最近的裁员风波，都让我感受在哪混着都挺不容易的。随着他要当爸爸了，从没有考虑过要离开北京的他也有了离开的念头，如果不能在将来在北京购买个小房子，他可能就回到家乡，住他自己已经购买房子里生活。\n随着企业成本控制，不少企业已经去二三线城市发展，那里的租金甚至比北京便宜四五倍，如果能招到人的话，他们就可以落地二三线了。当初我在北京的时候，我的室友就在中国移动研究院，后来中国移动研究院搬到了苏州，他和几个小伙伴也都去了苏州，现在已经早早在那里买房了，那边环境很好，有自己的房子，工资也不低，其实生活挺舒服的。尤其是软件行业，有的公司允许远程办公，那只要能满足岗位要求，其他也挺好的。身边就有朋友他们的公司在北京撤除了办公室，他们现在就在家办公，隔一段时间可以去出差去二三线城市的办公室与同事工作交流。\n此时，脑海中的问题一直缠绕，再给你一次机会你还想在留在北京生活吗？我的决定和当初一样回到现在的城市。\n我喜欢这里的工作生活的平衡，加班也都是主动加班学习，没有被迫的加班给领导看；我喜欢这里的海，中午吃完饭就可以跟同事一起散步走到海边看海；我喜欢这里离父母很近，开车半个小时就能到，可以经常与他们相聚照顾他们；我喜欢住在自己的房子，不用担心搬家了，可以不断的改善生活环境；我喜欢这里人不太多、该有的专卖店和商城都有、有地铁，去哪里都不算太远。\n周四下午 5 点培训完，吃了个饭就直奔机场连夜回到家，第二天回来继续上班了。这短短的两天，往返两座城市之间工作和培训，飞机上只需要40分钟，就像没有离开过一样，让人感叹交通的便利。\n祝愿每个人都收获自己享受的生活。\n2019 年 12 月 1 日 23 : 55 : 00\n","date":"2019-12-01","externalUrl":null,"permalink":"/misc/48h-in-beijing/","section":"Miscs","summary":"记录我在北京参加 JFrog DevOps 训练营的经历，分享培训内容和个人感受，以及对未来工作的思考。","title":"北京48小时：记一次参加 DevOps 训练营","type":"misc"},{"content":"对于如何备份 Jenkins 除了用 Jenkins 插件来定期备份之外，如果把 Jenkins 安装到 Docker 里，定期备份一个 Docker Image 最后传到 Artifactory 中，也是一个不错的方案。\n安装 Docker 版 Jenkins # 在 CentOS 上安装 Docker 版 Jenkins，这里推荐用 Long-term Support (LTS) 版本，可以从 Jenkins 官网下载。\n# 下载指定 lts 版本 2.130 sudo docker pull jenkins/jenkins:2.130 # 运行指定 docker Jenkins sudo docker run -p 8080:8080 -p 50000:50000 jenkins/jenkins:2.130 # 如果想下载最新的 lts 版 sudo docker pull jenkins/jenkins:lts # 运行最新的 lts 版 docker Jenkins sudo docker run -p 8080:8080 -p 50000:50000 jenkins/jenkins:lts 启动成功后即可打开 http://hostname:8080/ 网址\n修改登录密码 # 显示所有的 image 以及正在运行的 container\n# 列出来所有 image sudo docker image list # 列出当前运行的 container sudo docker ps # 进入容器，使用 -it 参数 sudo docker exec -it 39bc7a8307d9 /bin/bash # 查看默认 admin 密码 jenkins@a6195912b579:/$ cat /var/jenkins_home/secrets/initialAdminPassword 5193d06c813d46d3b18babeda836363a 建议登录之后，修改 admin 密码，方便下次登录\nsudo docker commit 39bc7a8307d9 myjenkins:v0.1 将宿主机目录映射到 Jenkins Docker 中 # 如果想让 Docker 里的 Jenkins 可以访问宿主机的目录，在运行 docker 时使用 -v 参数进行 mount volume\nsudo docker run -p 8080:8080 -p 50000:50000 --name mydata -v /data/backup:/home/backup jenkins/jenkins:2.130 # 映射成功，可以看到宿主机上的备份文件了 jenkins@c85db3f88115:/home/backup$ ls FULL-2019-09-14_02-00 FULL-2019-09-28_02-00 FULL-2019-10-19_02-00 FULL-2019-11-02_02-00 FULL-2019-11-23_02-00 FULL-2019-09-21_02-00 FULL-2019-10-05_02-00 FULL-2019-10-26_02-00 FULL-2019-11-09_02-00 FULL-2019-11-30_02-00 将 Jenkins Docker Image 保存在 Artifactory # 下载并安装 Artifactory 企业版或是 JFrog Container Registry，注意 Artifactory Open Source 版本不支持 Docker Registry。\n例如我的 JFrog Container Registry 是：dln.dev.mycompany.com:8040，并创建了一个 docker repository 叫 docker-local。\n上传 Docker Image 一共分为三步：\ndocker login\n# 在登录前需要添加如下配置到 /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34; : [\u0026#34;dln.dev.mycompany.com:8040\u0026#34;] } # docker login \u0026lt;DOCKER_SERVER\u0026gt;, example below: sudo docker login dln.dev.mycompany.com:8040 docker tag\n# docker tag \u0026lt;IMAGE_ID\u0026gt; artprod.mycompany/\u0026lt;DOCKER_REPOSITORY\u0026gt;:\u0026lt;DOCKER_TAG\u0026gt;, example below: sudo docker tag myjenkins:v0.1 dln.dev.mycompany.com:8040/docker-local/myjenkins:v0.1 docker push\n# docker push artprod.mycompany/\u0026lt;DOCKER_REPOSITORY\u0026gt;:\u0026lt;DOCKER_TAG\u0026gt;, example below: $ sudo docker push dln.dev.mycompany.com:8040/docker-local/myjenkins::v0.1 The push refers to repository [dln.dev.mycompany.com:8040/docker-local/myjenkins] 98d59071f692: Pushed af288f00b8a7: Pushed 4b955941a4d0: Pushed f121afdbbd5d: Pushed 15.10: digest: sha256:a3f5e428c0cfbfd55cffb32d30b1d78fedb8a9faaf08efdd9c5208c94dc66614 size: 1150 登录 JFrog Container Registry 刷新就可以到已经上次的 Image 了。说明：截图是我上传的另外一个镜像 ubuntu:15.10\n","date":"2019-12-01","externalUrl":null,"permalink":"/posts/install-docker-jenkins/","section":"Posts","summary":"如何定制一个 Docker 版 Jenkins 镜像，并将其备份到 Artifactory，便于在需要时快速恢复 Jenkins 环境。","title":"定制一个 Docker 版 Jenkins 镜像","type":"posts"},{"content":"上一篇 初识 JFrog Artifactory，介绍了什么是 Artifactory，以及如何安装、启动和升级。\n本篇介绍 Artifactory 与 Jenkins 的集成，因为没有与 CI 工具集成的 Artifactory 是没有灵魂的。\n通过集成，可以让 Jenkins 在完成构建之后，可以直接将制品（比如 build）推送到 Artifactory，供测试下载、部署或是后续的 Jenkins 任务去继续进行持续集成。\nJenkins 里配置 Artifactory # 打开 Manage Jenkins-\u0026gt;Configure System，找到 Artifactory，点击 Add Artifactory Server， 输入 Server ID 和 URL\nServer ID 是给你的 Artifactory 起个别名，这样使用 Jenkins pipeline 的时候会用到 URL 是你的 Artifactory 服务器的地址，例如 http://art.company.com:8040/artifactory 配置完成后，点击Test Connection，返回 Found Artifactory 6.14.0 表示配置成功。 如图所示: 使用 Pipeline 调用 Artifactory # 这里演示了两种方式，我在项目中用的是 Jenkins Shared Library；当然你也可以仅仅使用 Jenkinsfile，把如下两个 groovy 文件组合成一个 Jenkinsfile。\n方式1：Jenkins Shared Library # build.groovy\ndef call() { pipeline { # 省略其他代码 post { # 这里只有在 Jenkins Job 成功的时候才将 build post 到 artifactory success { script { if (env.BRANCH_NAME == \u0026#39;develop\u0026#39;) { # 如果当前是 develop 分支，则将 release 和 debug build 都 post 到 artifactory artifactory(\u0026#34;${PATTERN_RELEASE_PATH}\u0026#34;, \u0026#34;${TARGET_PATH}\u0026#34;, \u0026#34;${BUILD_NAME}\u0026#34;, \u0026#34;${BUILD_NUMBER}\u0026#34;) artifactory(\u0026#34;${PATTERN_DEBUG_PATH}\u0026#34;, \u0026#34;${TARGET_PATH}\u0026#34;, \u0026#34;${BUILD_NAME}\u0026#34;, \u0026#34;${BUILD_NUMBER}\u0026#34;) } else if (env.BRANCH_NAME.startsWith(\u0026#39;PR\u0026#39;)) { # 如果当前是 pull request 分支，则只将 release build 都 post 到 artifactory artifactory(\u0026#34;${PATTERN_RELEASE_PATH}\u0026#34;, \u0026#34;${TARGET_PATH}\u0026#34;, \u0026#34;${BUILD_NAME}\u0026#34;, \u0026#34;${BUILD_NUMBER}\u0026#34;) } } } } } } artifactory.groovy\nimport groovy.transform.Field @Field artifactoryServerId = \u0026#34;art-1\u0026#34; @Field artifactoryURL = \u0026#34;http://art.company.com:8040/artifactory\u0026#34; @Field artifactoryCredential = \u0026#34;d1cbab74-823d-41aa-abb7\u0026#34; def call(String patternPath, String targetPath, String buildName, String buildNumber) { rtServer ( id: \u0026#34;${artifactoryServerId}\u0026#34;, url: \u0026#34;${artifactoryURL}\u0026#34;, credentialsId: \u0026#34;${artifactoryCredential}\u0026#34; ) rtPublishBuildInfo ( serverId: \u0026#34;${artifactoryServerId}\u0026#34; ) rtUpload ( serverId: \u0026#34;${artifactoryServerId}\u0026#34;, spec: \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;files\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;${patternPath}\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;${targetPath}\u0026#34; } ] }\u0026#34;\u0026#34;\u0026#34;, buildNumber: \u0026#34;${buildNumber}\u0026#34;, buildName: \u0026#34;${buildName}\u0026#34;, ) } 方式2：Jenkinsfile # pipeline { # 省略其他代码 stage(\u0026#39;config art\u0026#39;){ rtServer ( id: \u0026#34;art-1\u0026#34;, url: \u0026#34;http://art.company.com:8040/artifactory\u0026#34;, credentialsId: \u0026#34;d1cbab74-823d-41aa-abb7\u0026#34;) } post { # 这里只有在 Jenkins Job 成功的时候才将 build post 到 artifactory success { script { if (env.BRANCH_NAME == \u0026#39;develop\u0026#39;) { rtUpload ( serverId: \u0026#34;art-1\u0026#34;, spec: \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;files\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;/release/build/*.zip\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;demo/develop/\u0026#34; } ] }\u0026#34;\u0026#34;\u0026#34;, buildNumber: \u0026#34;${buildNumber}\u0026#34;, buildName: \u0026#34;${buildName}\u0026#34;, ) } else if (env.BRANCH_NAME.startsWith(\u0026#39;PR\u0026#39;)) { rtUpload ( serverId: \u0026#34;art-1\u0026#34;, spec: \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;files\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;/release/build/*.zip\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;demo/pull-request/\u0026#34; } ] }\u0026#34;\u0026#34;\u0026#34;, buildNumber: \u0026#34;${buildNumber}\u0026#34;, buildName: \u0026#34;${buildName}\u0026#34;, ) } } } } } Jenkins 与 Artifactory 集成成功 # 蓝色表示构建成功，绿色圆圈表示 Build 已经 Post 到 Artifactory 上了。\n点击绿色圆圈可以跳转到 Artifactory 看到制品。\nJenkins 与 Artifactory 打通了。完！\n","date":"2019-11-17","externalUrl":null,"permalink":"/posts/artifactory-integrate-with-jenkins/","section":"Posts","summary":"本文介绍如何将 JFrog Artifactory 与 Jenkins 集成，实现持续集成和制品管理。","title":"Artifactory 与 Jenkins 集成","type":"posts"},{"content":" What is Artifactory # Artifactory is a product from JFrog that serves as a binary repository manager. A binary repository can unify the management of all these binaries, making team management more efficient and simpler.\nJust like you use Git to manage code, Artifactory is used to manage binary files, typically referring to jar, war, pypi, DLL, EXE, and other build files.\nI believe the biggest advantage of using Artifactory is that it creates a better continuous integration environment, helping other continuous integration tasks to call from Artifactory and then deploy to different test or development environments. This is crucial for implementing DevOps.\nTo learn more about Artifactory, please refer to the Chinese website and the English Website.\nInstalling Artifactory # Download the Open Source Artifactory from the official website. This demonstration shows installation on Linux, so click Download RPM to download. Upload the downloaded jfrog-artifactory-oss-6.14.0.rpm to Linux. # Create a file in the root directory. You can also create a folder in any directory. sudo mkdir /artifactory cd /artifactory # Upload the downloaded jfrog-artifactory-oss-6.15.0.rpm to your Linux. $ ls jfrog-artifactory-oss-6.14.0.rpm # Install artifactory sudo rpm -ivh jfrog-artifactory-oss-6.14.0.rpm Starting and Stopping the Artifactory Service # # Start the service sudo systemctl start artifactory.service # If you encounter the following error when starting the service using the above command: # Job for artifactory.service failed because a configured resource limit was exceeded. See \u0026#34;systemctl status artifactory.service\u0026#34; and \u0026#34;journalctl -xe\u0026#34; for details. # Details: https://www.jfrog.com/jira/browse/RTFACT-19988 # You can try starting with the following command cd /opt/jfrog/artifactory/app/bin \u0026amp;\u0026amp; ./artifactory.sh start \u0026amp; # Stop the service sudo systemctl stop artifactory.service # Check service status sudo systemctl status artifactory.service Accessing Artifactory # Artifactory\u0026rsquo;s default port is 8040. After successful installation, access http://hostname:8040 to log in (default username: admin, password: password). Upgrading Artifactory # Download the latest Artifactory from the official website.\nUpload the downloaded jfrog-artifactory-oss-6.15.0.rpm (currently the latest) to your Linux.\ncd /artifactory ls jfrog-artifactory-oss-6.14.0.rpm jfrog-artifactory-oss-6.15.0.rpm # Stop the service sudo systemctl stop artifactory.service # Perform the upgrade sudo rpm -U jfrog-artifactory-oss-6.15.0.rpm # Output log, showing successful upgrade warning: jfrog-artifactory-oss-6.15.0.rpm: Header V4 DSA/SHA1 Signature, key ID d7639232: NOKEY Checking if ARTIFACTORY_HOME exists Removing tomcat work directory Removing Artifactory\u0026#39;s exploded WAR directory Initializing artifactory service with systemctl... ************ SUCCESS **************** The upgrade of Artifactory has completed successfully. Start Artifactory with: \u0026gt; systemctl start artifactory.service Check Artifactory status with: \u0026gt; systemctl status artifactory.service NOTE: Updating the ownership of files and directories. This may take several minutes. Do not stop the installation/upgrade process. Uninstalling Artifactory # Stop the Artifactory service systemctl stop artifactory.service Use the root user to execute the RPM uninstall command # remove OSS version yum erase jfrog-artifactory-oss # remove PRO version, etc. yum erase jfrog-artifactory-pro For more information on uninstalling JFrog products, see: https://www.jfrog.com/confluence/display/JFROG/Uninstalling+JFrog+Products\nInstalling JFrog CLI # # ON MAC brew install jfrog-cli-go # WITH CURL curl -fL https://getcli.jfrog.io | sh # WITH NPM npm install -g jfrog-cli-go # WITH DOCKER docker run docker.bintray.io/jfrog/jfrog-cli-go:latest jfrog -v CLI for JFrog Artifactory\nHow to use the JFrog CLI on Artifactory\n","date":"2019-11-10","externalUrl":null,"permalink":"/en/posts/artifactory-install-and-upgrade/","section":"Posts","summary":"JFrog Artifactory is a powerful binary repository manager. This article introduces its installation, upgrade, and usage.","title":"Getting Started with JFrog Artifactory","type":"posts"},{"content":"For example, I have a shared repository for code that\u0026rsquo;s very large (over 20G). Each product build requires this repository\u0026rsquo;s code (copying third-party libraries from it). If everyone needs to git clone this third-party repository, the network overhead is very large, git clone takes a long time, and it occupies a lot of physical space.\nThis can be solved by using NFS sharing.\nAdditionally, I want this code repository to update automatically. This introduces Jenkins. It checks for code commits to this large repository and automatically executes git pull to update the latest code to the shared server.\nWhat is NFS? NFS (Network File System) is a type of file system supported by FreeBSD that allows computers on a network to share resources. In NFS applications, local NFS clients can transparently read and write files located on a remote NFS server, just like accessing local files. On Windows, this is commonly known as a share.\nSetting Up NFS # # For example, on Linux, the shared server\u0026#39;s IP is 192.168.1.1 sudo vi /etc/exports # The following is the configuration of my exports file # Assume the internal network IP range is 192.168.1.1 ~ 192.168.1.250 # ro means read-only # all_squash means regardless of the user using NFS, their identity will be limited to a specified ordinary user identity (nfsnobody) /agent/workspace/opensrc 192.168.1.*(ro,all_squash) /agent/workspace/opensrc dev-team-a*.com(ro,all_squash) /agent/workspace/opensrc dev-team-b*.com(ro,all_squash) /agent/workspace/opensrc dev-ci*(ro,all_squash) NFS Operations # Starting the NFS Service # To start the NFS service, you need to start both the portmap and nfs services. portmap must be started before nfs.\nservice portmap start service nfs start # Check portmap status service portmap status Checking Service Status # service nfs status Stopping the Service # service nfs stop Exporting Configuration # After changing the /etc/exports configuration file, you don\u0026rsquo;t need to restart the NFS service; use exportfs directly.\nsudo exportfs -rv Mounting on Different Platforms # Windows # # Install the NFS Client (Services for NFS) # Step 1: Open Programs and Features # Step 2: Click Turn Windows features on or off # Step 3: Find and check option Services for NFS # Step 4: Once installed, click Close and exit back to the desktop. refer to https://graspingtech.com/mount-nfs-share-windows-10/ $ mount -o anon 192.168.1.1:/agent/workspace/opensrc Z: Linux/Unix # # Linux sudo mount -t nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc # AIX sudo nfso -o nfs_use_reserved_ports=1 # should only first time mount need to run this command sudo mount -F nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc # HP-UX sudo mount -F nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc # Solaris-SPARC # If you can\u0026#39;t execute mount directly from the command line sudo /usr/sbin/mount -F nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc ","date":"2019-09-10","externalUrl":null,"permalink":"/en/posts/nfs/","section":"Posts","summary":"This article introduces the steps and commands for setting up NFS sharing and mounting it on different platforms (Windows/Linux/Unix).","title":"How to Set Up NFS Sharing and Mount on Different Platforms—Windows/Linux/Unix","type":"posts"},{"content":"","date":"2019-09-10","externalUrl":null,"permalink":"/en/tags/nfs/","section":"Tags","summary":"","title":"NFS","type":"tags"},{"content":"Recently, while running a Jenkins Job, I encountered this error during a git clone operation:\n$ git clone ssh://git@git.companyname.com:7999/repo/opensrc.git Cloning into \u0026#39;opensrc\u0026#39;... fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. I only encountered this error when I first started using Git, back when I didn\u0026rsquo;t know how to clone code using SSH. Why did this error appear now? I haven\u0026rsquo;t changed anything, which is very confusing.\nCommon Solutions # My Google searches didn\u0026rsquo;t yield a solution for my specific problem. Most results suggested that the issue was due to not having generated an SSH key, and that generating one and adding the public key to GitHub or another web Git management platform would solve the problem. Using GitHub as an example:\nFirst, generate an SSH key:\n# Remember to replace with your own email address ssh-keygen -t rsa -C xianpeng.shen@gmail.com Second, copy the SSH public key to your Git web platform, such as GitHub.\ncd %userprofile%/.ssh # Open id_rsa.pub and copy its contents notepad id_rsa.pub Finally, open https://github.com/settings/ssh/new and paste the copied content there to save it.\nThis solution was ineffective for my problem, because the same account worked without issue on other virtual machines. Since it was also an HP-UX virtual machine, I generated an SSH key using a different account and the git clone operation worked perfectly. This led me to suspect a difference between the two accounts.\nTroubleshooting via SSH Connection Test # First, I examined the .gitconfig files for both accounts and found differences. Copying the contents of the .gitconfig file from the working account to the problematic one didn\u0026rsquo;t resolve the issue.\nSecond, I noticed that a core file was generated in the current directory during the git clone execution, indicating a coredump. However, opening the core file mostly resulted in gibberish, making it difficult to pinpoint the error.\nFinally, I tested the SSH connection using commands. For GitHub, the command is:\nssh -T git@github.com My problematic clone was using Bitbucket, and its SSH connection test command is:\nssh -vvv git\\@bitbucket.org First, I tested with the account that successfully performed git clone. I\u0026rsquo;m omitting some of the output here for brevity.\n$ ssh -vvv git\\@bitbucket.org OpenSSH_6.2p1+sftpfilecontrol-v1.3-hpn13v12, OpenSSL 0.9.8y 5 Feb 2013 # Different OpenSSH version HP-UX Secure Shell-A.06.20.006, HP-UX Secure Shell version # Different call path debug1: Reading configuration data /opt/ssh/etc/ssh_config debug3: RNG is ready, skipping seeding debug2: ssh_connect: needpriv 0 debug1: Connecting to bitbucket.org [18.205.93.1] port 22. debug1: Connection established. ... ... debug2: we did not send a packet, disable method debug1: No more authentication methods to try. Permission denied (publickey). Then, I tested with the account that failed the git clone operation:\n$ ssh -vvv git\\@bitbucket.org OpenSSH_8.0p1, OpenSSL 1.0.2s 28 May 2019 # Different OpenSSH version debug1: Reading configuration data /usr/local/etc/ssh_config # Different call path debug2: resolving \u0026#34;bitbucket.org\u0026#34; port 22 debug2: ssh_connect_direct debug1: Connecting to bitbucket.org [180.205.93.10] port 22. debug1: Connection established. Memory fault(coredump) $ Clearly, different versions of OpenSSH were used, indicating differences in their environment variables. I had previously checked the environment variables, but the large number of variables made it difficult to immediately identify the culprit.\nFinal Solution # I revisited the .profile file for the account where the git clone failed. There was an extra /usr/bin entry in the environment variables, causing this account to use a different version of OpenSSH. HP-UX has very strict requirements on package dependencies and versions. After removing this environment variable, saving the changes, logging back into the virtual machine, and executing git clone, the operation returned to normal.\n","date":"2019-09-01","externalUrl":null,"permalink":"/en/posts/could-not-read-from-remote-repository/","section":"Posts","summary":"Resolving the \u0026ldquo;Could not read from remote repository\u0026rdquo; error encountered when cloning code using Git, analyzing the causes, and providing solutions.","title":"Resolving the \"Could not read from remote repository\" Issue","type":"posts"},{"content":" If your commits on local not pushed to remote # combine local commits, you could follow this flow # Here is short video (only 3 minutes) and good explanation of git rebase -i usage.\nlist your local repository log\nIf you want to combine these 3 commits (add6152, 3650100, 396a652) to 1 commit, execute this command\ngit rebase -i HEAD~3 # last three commits Select which commit you want to squash (type s or squash are OK)\nthen press ESC, enter :wq! to save and exit.\nComment out some commits message you don\u0026rsquo;t need, press ESC, enter :wq! to save and exit.\nCheck log, you will see your local repository logs has combine to one commit\nIf your commits had pushed to remote # combine remote commits, you could follow this flow # list your repository logs\n# so you can create another branch from bugfix/UNV-1234 named bugfix/UNV-1234-for-squash xshen@dln-l-xs01 MINGW64 /c/U2GitCode/git-test (bugfix/UNV-1234) $ git checkout -b bugfix/UNV-1234-for-squash Switched to a new branch \u0026#39;bugfix/UNV-1234-for-squash\u0026#39; # combine last 2 commits $ git rebase -i HEAD~2 change one commit from pick to squash, see the screenshot below. press ESC, enter :wq! to save and exit.\nchange commit message, for example \u0026ldquo;UNV-1234 combine all commit to one commit\u0026rdquo;, then press ESC, enter :wq! to save and exit.\n# push your new create branch to remote. git push -u origin bugfix/UNV-1234-for-squash ","date":"2019-08-20","externalUrl":null,"permalink":"/en/posts/git-commit-squash/","section":"Posts","summary":"How to squash multiple Git commits into a single commit, both locally and remotely, using interactive rebase and merge strategies in Bitbucket.","title":"Git Commit Squash","type":"posts"},{"content":"","date":"2019-08-20","externalUrl":null,"permalink":"/en/tags/squash/","section":"Tags","summary":"","title":"Squash","type":"tags"},{"content":" 业务场景 # 日常工作中需要切换到不同平台（包括 Linux, AIX, Windows, Solris, HP-UX）不同的版本进行开发和验证问题，但是由于虚拟机有限，并不能保证每个开发和测试都有所以平台的虚拟机并且安装了不同的版本，因此准备各种各样的开发和测试环境会花费很长时间。\n需求分析 # 对于这样的需求，一般都会首先想到 Docker；其次是从 Artifactory 取 Build 然后通过 CI 工具进行安装；最后从 Source Code 进行构建然后安装。\n先说 Docker，由于我们所支持的平台繁多，包括 Linux, AIX, Windows, Solris, HP-UX, Docker 只适用于 Linux 和 Windows，因此不能满足这样的需求。\n由于其他原因我们的 Artifactory 暂时还不能使用，最后只能选择用 Source Code 进行构建然后进行安装。这两种方式都需要解决锁定资源以及释放资源的问题。如果当前环境有人正在使用，那么这台虚拟机的资源应该被锁住，不允许 Jenkins 再去调用这台正在使用的 node，以保证环境在使用过程中不被破坏。\n本文主要介绍如何通过 Jenkins Lockable Resources Plugin 来实现资源的上锁和解锁。\n演示 Demo # 设置 Lockable Resources\nJenkins -\u0026gt; configuration -\u0026gt; Lockable Resources Manager -\u0026gt; Add Lockable Resource 这里的 Labels 是你的 node 的 Label，在 Jenkins -\u0026gt; Nodes 设置 查看 Lockable Resources 资源池\n测试锁资源\n这里我配置的是参数化类型的 Job，可以选择不同平台，不同仓库进行构建 build-with-parameters 运行第一个 Job 查看当前可用资源数量 Free resources = 1，看到已经被 #47 这个 Job 所使用 继续运行第二个 Job 查看当前可用资源数量 Free resources = 0，看到已经被 #48 这个 Job 所使用 最关键是这一步，如果继续运行第三个 Job，是否能够被继续行呢 可以看到这个任务没有开始执行，看下 log 是否真的没有被执行。通过日志发现，当前正在等待可用的资源 测试释放锁\n现在释放一个资源，看下第三个 Job 是否能拿到资源，并且执行 从下图可以看到 第三个 Job 已经运行成功了 Jenkins pipeline 代码 # 整个 pipeline 最关键的部分就是如何上锁和释放，这里是通过 lock 和 input message 来实现。\n当前 Job 只要用户不点击 Yes，就会一直处于没有完成的状态，那么的它的锁会一直生效中。直到点击 Yes， Job 结束，锁也就释放了。\n具体可以参考下面的 Jenkinsfile。\npipeline { agent { node { label \u0026#39;PreDevENV\u0026#39; } } options { lock(label: \u0026#39;PreDevENV\u0026#39;, quantity: 1) } parameters { choice( name: \u0026#39;platform\u0026#39;, choices: [\u0026#39;Linux\u0026#39;, \u0026#39;AIX\u0026#39;, \u0026#39;Windows\u0026#39;, \u0026#39;Solris\u0026#39;, \u0026#39;HP-UX\u0026#39;], summary: \u0026#39;Required: which platform do you want to build\u0026#39;) choice( name: \u0026#39;repository\u0026#39;, choices: [\u0026#39;repo-0.1\u0026#39;, \u0026#39;repo-1.1\u0026#39;, \u0026#39;repo-2.1\u0026#39;, \u0026#39;repo-3.1\u0026#39;, \u0026#39;repo-4.1\u0026#39;], summary: \u0026#39;Required: which repository do you want to build\u0026#39;) string( name: \u0026#39;branch\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Required: which branch do you want to build\u0026#39;) } stages { stage(\u0026#39;git clone\u0026#39;){ steps { echo \u0026#34;git clone source\u0026#34; } } stage(\u0026#39;start build\u0026#39;){ steps { echo \u0026#34;start build\u0026#34; } } stage(\u0026#39;install build\u0026#39;){ steps{ echo \u0026#34;installing\u0026#34; } } stage(\u0026#39;unlock your resource\u0026#39;){ steps { input message: \u0026#34;do you have finished?\u0026#34; echo \u0026#34;Yes, I have finished\u0026#34; } } } } ","date":"2019-08-10","externalUrl":null,"permalink":"/posts/jenkins-lock-resource/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 的 Lockable Resources 插件来管理和锁定资源，确保在多任务环境中资源的独占性和安全性。","title":"如何通过 Jenkins 进行资源的锁定和释放","type":"posts"},{"content":"在持续集成中，你可能需要通过 Jenkins 来修改代码，并且将修改后的代码提交到Git仓库里。怎么做呢？最方便的做法还是 Jenkins 提供对应的插件，但是很遗憾我没找到合适的。另外我也觉得通过脚本的方式来实现会更加稳定，不用担心 Jenkins 以及插件升级带来潜在不好用的可能。\n以下 pipeline 片段供参考使用：\n// This pipeline is used for bumping build number pipeline { environment { MYGIT = credentials(\u0026#34;d1cbab74-823d-41aa-abb7\u0026#34;) } stages { stage(\u0026#39;Git clone repo\u0026#39;) { steps { sh \u0026#39;git clone -b develop --depth 1 https://$MYGIT_USR:\u0026#34;$MYGIT_PSW\u0026#34;@github.com/shenxianpeng/blog.git\u0026#39; } } stage(\u0026#39;Change code stage\u0026#39;){ steps { sh \u0026#39;\u0026#39; } } stage(\u0026#39;Git push to remote repo\u0026#39;) { steps { sh label: \u0026#39;\u0026#39;, script: \u0026#39;\u0026#39;\u0026#39; cd blog git add . git commit -m \u0026#34;Bld # 1001\u0026#34; git push https://$MYGIT_USR:\u0026#34;$MYGIT_PSW\u0026#34;@github.com/shenxianpeng/blog.git --all\u0026#39;\u0026#39;\u0026#39; } } } } 这里面我所遇到最大的坑，我之前脚本是这样写的：\nstage(\u0026#39;Git push to remote\u0026#39;) { // not works script steps { sh \u0026#39;cd blog\u0026#39; sh \u0026#39;git add .\u0026#39; sh \u0026#39;git commit -m \u0026#34;${JIRA_NO} Bld # ${BUILD_NO}\u0026#34;\u0026#39; sh \u0026#39;git push https://$MYGIT_USR:\u0026#34;$MYGIT_PSW\u0026#34;@github.com/shenxianpeng/blog.git --all\u0026#39; } } 在最后一个阶段提交代码时，shell 脚本不能使用单引号 \u0026lsquo;\u0026rsquo;，要使用三引号才行\u0026rsquo;\u0026rsquo;\u0026rsquo; \u0026lsquo;\u0026rsquo;\u0026rsquo;。我在这里花了很多时间，一直找不到问题所在，因为我在上面的shell脚本使用的时候用单引号 \u0026rsquo;\u0026rsquo; 可以正常 git clone 代码，但在提交代码时不行，最后我 Jenkins 的 Pipeline Syntax 生成的脚本，提交代码成功。\n","date":"2019-07-22","externalUrl":null,"permalink":"/posts/git-push-by-jenkins/","section":"Posts","summary":"如何通过 Jenkins Pipeline 脚本来提交修改的代码到 Git 仓库，包括克隆仓库、修改代码和推送更改等步骤。","title":"通过 Jenkins 来提交修改的代码 git push by Jenkins","type":"posts"},{"content":"","date":"2019-07-16","externalUrl":null,"permalink":"/tags/os/","section":"标签","summary":"","title":"OS","type":"tags"},{"content":"在使用 Jenkins pipeline 的时候，在 Linux 需要用 root 来执行，我想通过 Jenkins pipeline 的语法来解决，但是只找到这种方式：SSH Pipeline Steps\ndef remote = [:] remote.name = \u0026#39;test\u0026#39; remote.host = \u0026#39;test.domain.com\u0026#39; remote.user = \u0026#39;root\u0026#39; remote.password = \u0026#39;password\u0026#39; remote.allowAnyHosts = true stage(\u0026#39;Remote SSH\u0026#39;) { sshCommand remote: remote, command: \u0026#34;ls -lrt\u0026#34; sshCommand remote: remote, command: \u0026#34;for i in {1..5}; do echo -n \\\u0026#34;Loop \\$i \\\u0026#34;; date ; sleep 1; done\u0026#34; } * command * Type: String * dryRun (optional) * Type: boolean * failOnError (optional) * Type: boolean * remote (optional) * Nested Choice of Objects * sudo (optional) * Type: boolean 从 example 来看需要提供的参数比较多，很多参数我已经在 Pipeline 的 environment 已经设置过了，这里再设置就显得不够优美，且限于没有足够的 example，你知道的 Jenkinsfile 调试非常痛苦和麻烦，我就没通过这种方式来尝试解决。\n通过 Linux 设置来解决\n// open a shell console and type $ sudo visudo // type your user name jenkins ALL=(ALL) NOPASSWD: ALL 但即使这样设置，通过 Jenkins 执行 shell 脚本的时候还是出现如下问题\nsudo: no tty present and no askpass program specified 最后通过如下脚本解决了我的问题\n// Jenkinsfile environment { JENKINS = credentials(\u0026#34;d1cbab74-823d-41aa-abb7-85848595\u0026#34;) } sh \u0026#39;sudo -S \u0026lt;\u0026lt;\u0026lt; \u0026#34;$JENKINS_PSW\u0026#34; sh test.sh\u0026#39; 如果你有更好的方式，欢迎留言评论，谢谢。\n","date":"2019-07-16","externalUrl":null,"permalink":"/posts/execute-sudo-without-password/","section":"Posts","summary":"本文介绍了如何在 Jenkins Pipeline 中执行 sudo 命令而无需输入密码，提供了具体的实现方法和示例代码。","title":"在 Jenkins pipeline 中执行 sudo 的时候不需要输入密码","type":"posts"},{"content":"","date":"2019-07-07","externalUrl":null,"permalink":"/tags/disqus/","section":"标签","summary":"","title":"Disqus","type":"tags"},{"content":" 查找是否有遗漏提交 # 从一个分支找到所有的 commit 和 ticket 号，然后去另外一个分支去查找这些提交是否也在这个分支里。\n找一个分支的所有 commit 和 ticket 号\n# 从 develop 分支上获取所有的 commit 和 ticket 号，然后根据 ticket 号进行排序 git log origin/develop --pretty=oneline --abbrev-commit | cut -d\u0026#39; \u0026#39; -f2,1 | sort -t \u0026#39; \u0026#39; -k 2 \u0026gt;\u0026gt; develop_involve_tickets.txt --pretty=oneline # 显示为一行 --abbrev-commit # 显示短的提交号 cut --help # 切出来所需要的字段 -d # 字段分隔符, \u0026#39; \u0026#39;分隔空格 -f # 只选择某些字段 sort --help # 利用 sort 将剪出来的字段进行排序 -t # 字段分隔， \u0026#39; \u0026#39;分隔空格 -k # 通过键进行键定义排序;KEYDEF 给出位置和类型 然后去另外一个分支去找是否有次提交\n由于在 SVN 时代时，每次修改都会在描述里添加 ticket 号，所以切换到 master 分支后，直接搜索所有 ticket 号是否存在就好了.\n#!/bin/bash filename=\u0026#39;C:\\develop_involve_tickets.txt\u0026#39; while read line do echo $line var=`grep -ir $line src` if [[ -z $var ]];then echo \u0026#34;not found\u0026#34; echo $line \u0026gt;\u0026gt; ../not_found_in_master.txt else echo \u0026#34;found\u0026#34; echo $line \u0026gt;\u0026gt; ../found_in_master.txt fi done \u0026lt; \u0026#34;$filename\u0026#34; ","date":"2019-07-07","externalUrl":null,"permalink":"/posts/git-management/","section":"Posts","summary":"本文介绍了 Git 的常见管理操作，包括分支管理、提交规范、代码审查等，帮助开发者更好地使用 Git 进行版本控制。","title":"Git 管理","type":"posts"},{"content":"","date":"2019-07-07","externalUrl":null,"permalink":"/categories/hexo/","section":"Categories","summary":"","title":"Hexo","type":"categories"},{"content":" 在你的 Hexo 网站添加 Disqus # 去 Disqus 创建一个账号，在这个过程中有需要选择一个 shortname，完成后，你可以在设置页码找到你的 shortname\nhttps://YOURSHORTNAMEHERE.disqus.com/admin/settings/general 在你 Hexo 博客里打开 _config.yml, 然后输入 disqus_shortnameand: YOURSHORTNAMEHERE，像这样：\ndisqus_shortname: myshortnamegoeshere comments: true 也需要更改 _config.yml 文件如下，例如我的：\n# 修改默认 url: http://yoursite.com 为： url: https://shenxianpeng.github.io 复制这段代码到 blog\\themes\\landscape\\layout\\_partial\\footer.ejs\n\u0026lt;% if (config.disqus_shortname){ %\u0026gt; \u0026lt;script\u0026gt; var disqus_shortname = \u0026#39;\u0026lt;%= config.disqus_shortname %\u0026gt;\u0026#39;; \u0026lt;% if (page.permalink){ %\u0026gt; var disqus_url = \u0026#39;\u0026lt;%= page.permalink %\u0026gt;\u0026#39;; \u0026lt;% } %\u0026gt; (function(){ var dsq = document.createElement(\u0026#39;script\u0026#39;); dsq.type = \u0026#39;text/javascript\u0026#39;; dsq.async = true; dsq.src = \u0026#39;//go.disqus.com/\u0026lt;% if (page.comments){ %\u0026gt;embed.js\u0026lt;% } else { %\u0026gt;count.js\u0026lt;% } %\u0026gt;\u0026#39;; (document.getElementsByTagName(\u0026#39;head\u0026#39;)[0] || document.getElementsByTagName(\u0026#39;body\u0026#39;)[0]).appendChild(dsq); })(); \u0026lt;/script\u0026gt; \u0026lt;% } %\u0026gt; 也需要复制这些文件到 footer.ejs 到最底部：\n\u0026lt;div id=\u0026#34;disqus_thread\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 最后 footer.ejs 文件是这样的：\n\u0026lt;% if (theme.sidebar === \u0026#39;bottom\u0026#39;){ %\u0026gt; \u0026lt;%- partial(\u0026#39;_partial/sidebar\u0026#39;) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;% if (config.disqus_shortname){ %\u0026gt; \u0026lt;script\u0026gt; var disqus_shortname = \u0026#39;\u0026lt;%= config.disqus_shortname %\u0026gt;\u0026#39;; \u0026lt;% if (page.permalink){ %\u0026gt; var disqus_url = \u0026#39;\u0026lt;%= page.permalink %\u0026gt;\u0026#39;; \u0026lt;% } %\u0026gt; (function(){ var dsq = document.createElement(\u0026#39;script\u0026#39;); dsq.type = \u0026#39;text/javascript\u0026#39;; dsq.async = true; dsq.src = \u0026#39;//go.disqus.com/\u0026lt;% if (page.comments){ %\u0026gt;embed.js\u0026lt;% } else { %\u0026gt;count.js\u0026lt;% } %\u0026gt;\u0026#39;; (document.getElementsByTagName(\u0026#39;head\u0026#39;)[0] || document.getElementsByTagName(\u0026#39;body\u0026#39;)[0]).appendChild(dsq); })(); \u0026lt;/script\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;footer id=\u0026#34;footer\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;outer\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;footer-info\u0026#34; class=\u0026#34;inner\u0026#34;\u0026gt; \u0026amp;copy; \u0026lt;%= date(new Date(), \u0026#39;YYYY\u0026#39;) %\u0026gt; \u0026lt;%= config.author || config.title %\u0026gt;\u0026lt;br\u0026gt; \u0026lt;%= __(\u0026#39;powered_by\u0026#39;) %\u0026gt; \u0026lt;a href=\u0026#34;http://hexo.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Hexo\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;div id=\u0026#34;disqus_thread\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 最后清理和构建\nhexo clean hexo generate \u0026amp;\u0026amp; hexo server 现在你可以看到我的博客已经可以添加评论了 : )\n","date":"2019-07-07","externalUrl":null,"permalink":"/posts/hexo-add-disqus/","section":"Posts","summary":"在 Hexo 博客中集成 Disqus 评论系统，允许读者留言和互动。","title":"Hexo 添加 Disqus 留言功能","type":"posts"},{"content":"这个pipeline里包含了如下几个技术：\n如何使用其他机器，agent 如何使用环境变量，environment 如何在build前通过参数化输入，parameters 如何使用交互，input 如何同时clone多个repos 如何进行条件判断，anyOf pipeline { agent { node { label \u0026#39;windows-agent\u0026#39; } } environment { MY_CRE = credentials(\u0026#34;2aee7e0c-a728-4d9c-b25b-ad5451a12d\u0026#34;) } parameters { // Jenkins parameter choice( name: \u0026#39;REPO\u0026#39;, choices: [\u0026#39;repo1\u0026#39;, \u0026#39;repo2\u0026#39;, \u0026#39;repo3\u0026#39;, \u0026#39;repo4\u0026#39;], summary: \u0026#39;Required: pick a repo you want to build\u0026#39;) string( name: \u0026#39;BRANCH\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Required: chose a branch you want to checkout\u0026#39;) string( name: \u0026#39;BUILD_NO\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Required: input build number\u0026#39;) string( name: \u0026#39;JIRA_NO\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Optional: input jira ticket number for commit message\u0026#39;) } stages { stage(\u0026#34;Are you sure?\u0026#34;){ steps{ // make sure you want to start this build input message: \u0026#34;${REPO}/${BRANCH}:${BUILD_NO}, are you sure?\u0026#34; echo \u0026#34;I\u0026#39;m sure!\u0026#34; } } stage(\u0026#39;Git clone repos\u0026#39;) { steps { // git clone one repo source code checkout([ $class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;refs/heads/${BRANCH}\u0026#39;]], browser: [$class: \u0026#39;GitHub\u0026#39;, repoUrl: \u0026#39;https://github.com/${REPO}\u0026#39;], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;CleanBeforeCheckout\u0026#39;], [$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#39;**\u0026#39;], [$class: \u0026#39;RelativeTargetDirectory\u0026#39;, relativeTargetDir: \u0026#39;../${REPO}\u0026#39;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;2aee7e0c-a728-4d9c-b25b\u0026#39;, url: \u0026#39;https://github.com/${REPO}.git\u0026#39;]]]) // git clone another repo source code checkout([ $class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;refs/heads/${BRANCH}\u0026#39;]], browser: [$class: \u0026#39;GitHub\u0026#39;, repoUrl: \u0026#39;https://github.com/${REPO}\u0026#39;], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;CleanBeforeCheckout\u0026#39;], [$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#39;**\u0026#39;], [$class: \u0026#39;RelativeTargetDirectory\u0026#39;, relativeTargetDir: \u0026#39;../${REPO}\u0026#39;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;2aee7e0c-a728-4d9c-b25b\u0026#39;, url: \u0026#39;https://github.com/${REPO}.git\u0026#39;]]]) } } stage(\u0026#39;Build repo1 and repo2\u0026#39;) { when { // if REPO=repo1 or REPO=repo2, execute build_repo12.sh anyOf { environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo1\u0026#39; environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo2\u0026#39; } } steps { sh label: \u0026#39;\u0026#39;, script: \u0026#39;${REPO}/build_repo12.sh ${REPO} ${BUILD_NO} ${JIRA_NO}\u0026#39; } } stage(\u0026#39;Build repo3 and repo4\u0026#39;) { when { // if REPO=repo3 or REPO=repo4, execute build_repo34.sh anyOf { environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo3\u0026#39; environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo4\u0026#39; } } steps { sh label: \u0026#39;\u0026#39;, script: \u0026#39;${REPO}/build_repo34.sh ${REPO} ${BUILD_NO} ${JIRA_NO}\u0026#39; } } stage(\u0026#39;Git push to remote repo\u0026#39;) { steps { // commit code to remote repo sshagent([\u0026#39;2aee7e0c-a728-4d9c-b25b\u0026#39;]) { sh \u0026#34;git push https://%MY_CRE_USR%:%MY_CRE_PSW%@github.com/${REPO}.git\u0026#34; } } } } } ","date":"2019-07-07","externalUrl":null,"permalink":"/posts/jenkinsfile-example/","section":"Posts","summary":"这个 Jenkinsfile 示例展示了如何在 Jenkins Pipeline 中实现交互式输入、克隆多个 Git 仓库，并在构建完成后将代码推送到远程仓库。","title":"Jenkinsfile example - 实现交互、clone 多个仓库以及 git push","type":"posts"},{"content":" Problems # Like database product, it runs on multi-platform, but for software enginner they may only works on one platform, how they could identify their code works on all platform? manually build the various platforms? NO!\nSolution # Most people would know we can use Jenkins pipeline, they may create multi Jenkins job for different stuation.\nHow to do it in an elegant way, I would want to share how to use multibranch pipeline to achieve.\nWhen create a pull request, auto parallel start simple build. Reviewers can decide whether to merge base on build results. After code merged, auto start full build. Benefits # What are the benefits:\nOne Jenkins job and one pipeline can manage multi branches. Do not need to compile platforms to verify, save huge time and machines. Stop looking for other people\u0026rsquo;s mistakes, no one can break the build. Builds can be generated quickly for QA testing Jenkinsfile example # In case of reduce simple build time and let PR creater and reviewer know the builds status as soon as possbile, you may need to do something different here, like below, used when condition and branch variable to check it is a develop branch or pull request branch.\nwhen { branch \u0026#39;PR-*\u0026#39; } when { branch \u0026#39;develop\u0026#39; } The entire code pipeline looks like this:\n// This Jenkinsfile is an explame for multibranch pipeline pipeline { agent none stages { stage(\u0026#34;All platform builds\u0026#34;) { parallel { stage(\u0026#34;Windows build\u0026#34;) { agent { node { label \u0026#39;windows-vm01\u0026#39; customWorkspace \u0026#39;C:\\\\agent\\\\workspace\\\\blog\u0026#39; } } stages { stage(\u0026#34;PR build\u0026#34;) { when { branch \u0026#39;PR-*\u0026#39; } steps { checkout scm dir(\u0026#39;src\\\\build\u0026#39;) { bat label: \u0026#39;\u0026#39;, script: \u0026#39;build.bat PR\u0026#39; } } } stage(\u0026#34;Release build\u0026#34;) { when { branch \u0026#39;develop\u0026#39; } steps { cleanWs() checkout scm dir(\u0026#39;src\\\\build\u0026#39;) { bat label: \u0026#39;\u0026#39;, script: \u0026#39;build.bat release\u0026#39; } } } stage(\u0026#34;Deploy\u0026#34;) { echo \u0026#34;====if you have more stage, can add stage like this===\u0026#34; } } } stage(\u0026#34;Linux build\u0026#34;) { agent { node { label \u0026#39;linux-vm01\u0026#39; customWorkspace \u0026#39;/agent/workspace/blog\u0026#39; } } stages { stage(\u0026#34;PR build\u0026#34;) { when { branch \u0026#39;PR-*\u0026#39; } steps { checkout scm dir(\u0026#39;src/build\u0026#39;) { bat label: \u0026#39;\u0026#39;, script: \u0026#39;build.sh PR\u0026#39; } } } stage(\u0026#34;Release build\u0026#34;) { when { branch \u0026#39;develop\u0026#39; } steps { cleanWs() checkout scm dir(\u0026#39;src/build\u0026#39;) { bat label: \u0026#39;\u0026#39;, script: \u0026#39;build.sh release\u0026#39; } } } } } stage(\u0026#34;AIX build\u0026#34;){ steps{ echo \u0026#34;====same as windows/Linux example, can write the code here you need ====\u0026#34; } } } } } } ","date":"2019-06-25","externalUrl":null,"permalink":"/en/posts/jenkins-multi-branch-pipeline/","section":"Posts","summary":"Discusses the use of Jenkins Multibranch Pipeline to manage multiple branches in a project, enabling parallel builds for pull requests and efficient code review processes.","title":"Jenkins Multibranch Pipeline","type":"posts"},{"content":" Preparation # You need to ask for a free trial license and install You will receive a mail with username/password to login for downloading I test it on the Windows platform, so I download the Windows installer, then install Squish Coco and Add-in Installed Visual Studio 2010 or higher, I used VS2017 Professional Add-in # go to ..squishcoco\\Setup, see quishCocoVSIX2017.vsix, double click, reopen VS2017, squishcoco will be there Create a project # Start Visual Studio and create a new C++ application\nClick on \u0026ldquo;File→New→Project\u0026hellip;\u0026rdquo; to pop up the new project wizard. Choose a project type of \u0026ldquo;Visual C++2\u0026rdquo; and the \u0026ldquo;Win32 Console Application\u0026rdquo; template. Enter a project name of squishcoco_sample, then click the \u0026ldquo;OK\u0026rdquo; button. When the wizard’s second page appears, click the \u0026ldquo;Finish\u0026rdquo; button. At this stage the application is not yet instrumented, so now we will create a copy of the build. Open the configuration manager by clicking \u0026ldquo;Build→Configuration Manager\u0026hellip;\u0026rdquo;. In the \u0026ldquo;Configuration\u0026rdquo; column, select \u0026ldquo;New\u0026hellip;\u0026rdquo; in the combobox. In the \u0026ldquo;New Project Configuration\u0026rdquo; dialog: Enter Code Coverage in the \u0026ldquo;Name\u0026rdquo; field, Select Release or Debug in the \u0026ldquo;Copy settings from\u0026rdquo; selection dialog. Click the \u0026ldquo;OK\u0026rdquo; button. Add test code\nsquishcoco_sample.cpp\n// squishcoco_sample.cpp : Defines the entry point for the console application. // #include \u0026#34;stdafx.h\u0026#34; extern int myprint(); int _tmain(int argc, _TCHAR* argv[]) { int age; printf(\u0026#34;Enter your age: \u0026#34;); scanf(\u0026#34;%d\u0026#34;,\u0026amp;age); if (age \u0026gt; 0 \u0026amp;\u0026amp; age \u0026lt;=40) printf(\u0026#34;You\u0026#39;re young guys\\n\u0026#34;); else if (age \u0026gt;40 \u0026amp;\u0026amp; age \u0026lt;=70) printf(\u0026#34;You\u0026#39;re midle guys\\n\u0026#34;); else if (age \u0026gt; 70 \u0026amp;\u0026amp; age \u0026lt;=100) printf(\u0026#34;You\u0026#39;re old guys\\n\u0026#34;); else printf(\u0026#34;You\u0026#39;re awesome\\n\u0026#34;); myprint(); return 0; } myprint.cpp\n#include\u0026#34;stdafx.h\u0026#34; int myprint () { printf (\u0026#34;you have call printf function\\n\u0026#34;); return 0; } Activate instrumentation # use the Microsoft® Visual Studio® Add-In: Click \u0026ldquo;Tools→Code Coverage Build Mode\u0026hellip;\u0026rdquo; to pop up the Squish Coco wizard. In the \u0026ldquo;Project:\u0026rdquo; selection dialog, select squishcoco_sample. In the selection dialog \u0026ldquo;Configuration:\u0026rdquo;, select Code Coverage. In the Configuration section at the bottom, select the radio button \u0026ldquo;Modify\u0026rdquo;, and then click on the button, \u0026ldquo;Enable code coverage for C++ projects\u0026rdquo;. The Code Coverage configuration has now been modified to generate code coverage information. The \u0026ldquo;SquishCoco\u0026rdquo; output window summarizes all the modifications that have been made:\n... Modifying configuration \u0026#39;Code Coverage\u0026#39; for the project \u0026#39;squishcoco_sample\u0026#39; for the platform \u0026#39;Code Coverage|Win32\u0026#39; Compiler Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended Linker Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended Librarian Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended File Specific Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended for the file \u0026#39;squishcoco_sample.cpp\u0026#39; Modifying configuration \u0026#39;Code Coverage\u0026#39; for the project \u0026#39;squishcoco_sample\u0026#39; for the platform \u0026#39;Code Coverage|x64\u0026#39; Compiler Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended Linker Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended Librarian Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended File Specific Configuration ... Build project # Build project will cause the executable squishcoco_sample.exe to be built and the code coverage instrumentation file squishcoco_sample.exe.csmes to be generated\nDouble click on squishcoco_sample.exe.csmes to inspect this file in CoverageBrowser\nRight now there is no code coverage statistics visible in CoverageBrowser, this is because the application has not yet been executed. Click on squishcoco_sample.cpp in the source list to display the main function. All the instrumented lines are shown grayed out, to indicate that nothing has been executed.\nNow execute squishcoco_sample.exe by double clicking it. This will result in a file called squishcoco_sample.exe.csexe being generated. The file contains a code coverage snapshot which can be imported into Coverage Browser\nClick \u0026ldquo;File-\u0026gt;Load Execution Report\u0026hellip;\u0026rdquo;. Select the \u0026ldquo;File\u0026rdquo; item and enter the path of the squishcoco_sample.exe.csexe file. Click on the \u0026ldquo;Import\u0026rdquo; button. This will cause the code coverage statistics to be updated. Now, in the source code window, the main function’s return statement will be colored green to indicate that this line has been executed. Final result # ","date":"2019-05-21","externalUrl":null,"permalink":"/en/posts/squishcoco/","section":"Posts","summary":"introduction to Squish Coco, a code coverage tool, with examples of how to set it up and use it in Visual Studio for C++ projects.","title":"A Code Coverage Tool - Squish Coco use examples","type":"posts"},{"content":"","date":"2019-05-21","externalUrl":null,"permalink":"/en/tags/c/","section":"Tags","summary":"","title":"C","type":"tags"},{"content":"Code Coverage is a measurement of how many lines, statements, or blocks of your code are tested using your suite of automated tests. It’s an essential metric to understand the quality of your QA efforts.\nCode coverage shows you how much of your application is not covered by automated tests and is therefore vulnerable to defects. it is typically measured in percentage values – the closer to 100%, the better.\nWhen you’re trying to demonstrate test coverage to your higher-ups, code coverage tools (and other tools of the trade, of course) come in quite useful.\nList of Code Coverage Tools\nTools Support Language Cost Partners Squish Coco C, C++, C#, SystemC, Tcl and QML Not disclosed Botom of this page Selected Clients BullseyeCoverage C, C++ $800 for 1-year license and up Testwell C, C++, C#, Java Not disclosed Parasoft C/C++test C, C++ Not disclosed partners VECTOR Code Coverage C, C++ Not disclosed (free trial available) partners JaCoCo Java Open Source Most famous code coverage tool in Java area ","date":"2019-05-21","externalUrl":null,"permalink":"/en/posts/code-coverage-tools/","section":"Posts","summary":"Code Coverage is a measurement of how many lines, statements, or blocks of your code are tested using your suite of automated tests. It’s an essential metric to understand the quality of your QA efforts.","title":"Code Coverage tools of C/C++","type":"posts"},{"content":"","date":"2019-05-21","externalUrl":null,"permalink":"/en/tags/squishcoco/","section":"Tags","summary":"","title":"SquishCoco","type":"tags"},{"content":"最近遇到一个 regression bug，是产品完成构建之后，build commit number 不对，显示的 HEAD 而不是常见的 97b34931ac HASH number,这是什么原因呢？ 我检查了 build 脚本没有发现问题，branch 的输出是正确的，那我怀疑是引入 Jenkins 的原因，果然登录到远程的 agent 上去查看分支名称如下：\nC:\\workspace\\blog\u0026gt;git branch * (HEAD detached at 97b3493) 果然问题出在了 Jenkins 上。这个问题有简单办法解决，就是直接使用git命令来clone代码，而不使用Git插件\ngit clone --depth 1 -b u2opensrc https://username:\u0026#34;passwowrd\u0026#34;@git.github.com/scm/blog.git blog 这种方式固然简单，不会出错，但它是明码显示，我岂能容忍这种不堪的处理方式吗？肯定还是要在 Git 插件上找到解决办法的。 随后google一下，果然有遇到和我一样问题的人，问题链接 这里。\n他说他做了很多调查，还跟专业的 Jenkins 人士联系，试了很多次，最后找到这个办法\ncheckout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/feature/*\u0026#39;]], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#34;**\u0026#34;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;99f978af-XXXX-XXXX-8147-2cf8f69ef864\u0026#39;, url: \u0026#39;http://TFS_SERVER:8080/tfs/DefaultCollection/Product/_git/Project\u0026#39;]]]) 主要是在 extensions:[] 中加入这句 [$class: \u0026lsquo;LocalBranch\u0026rsquo;, localBranch: \u0026ldquo;**\u0026rdquo;]\n这是 Jenkins 的 Bug 吗？带着这个疑问随后通过 Pipeline Syntax，找到 checkout: Check out from version control，在 Additional Behaviours 里有 Check out to specific local branch 这个配置项\nIf given, checkout the revision to build as HEAD on this branch. If selected, and its value is an empty string or \u0026ldquo;**\u0026rdquo;, then the branch name is computed from the remote branch without the origin. In that case, a remote branch origin/master will be checked out to a local branch named master, and a remote branch origin/develop/new-feature will be checked out to a local branch named develop/newfeature.\n看介绍原来 Jenkins 自带这个设置，只是它不是默认选项，所以才遇到刚才那个问题。随后选择这个设置，然后填入\u0026quot;**\u0026quot;，然后生成 Pipeline 脚本，就跟上面的脚本一样了。\n","date":"2019-05-14","externalUrl":null,"permalink":"/posts/gitscm-clone-code-don-t-display-branch/","section":"Posts","summary":"如何在 Jenkins 中使用 GitSCM插件克隆代码时，确保正确显示分支信息，避免出现 HEAD detached 状态的问题。","title":"GitSCM clone code don't display branch","type":"posts"},{"content":"","date":"2019-05-13","externalUrl":null,"permalink":"/tags/automation/","section":"标签","summary":"","title":"Automation","type":"tags"},{"content":"","date":"2019-05-13","externalUrl":null,"permalink":"/tags/ftp/","section":"标签","summary":"","title":"FTP","type":"tags"},{"content":"实现 CI/CD 过程中，常常需要将构建好的 build 上传到一个公共的服务器，供测试、开发来获取最新的 build。如何上传 build 成果物到 FTP server，又不想把 FTP server登录的用户名和密码存在脚本里，想做这样的参数化如何实现呢？\nupload_to_ftp.bat [hostname] [username] [password] [local_path] [remote_pat] windows batch 由于它的局限性，在实现上是比较麻烦的，但还是有办法。如何用 windows batch 来实现呢？借助一个临时文件，把需要的参数写入到临时文件里，然后通过 ftp -s 参数读取文件，最后把临时文件删除的方式来实现。\n@echo off set ftp_hostname=%1 set ftp_username=%2 set ftp_password=%3 set local_path=%4 set remote_path=%5 if %ftp_hostname%! == ! ( echo \u0026#34;ftp_hostname not set correctly\u0026#34; \u0026amp; goto USAGE ) if %ftp_username%! == ! ( echo \u0026#34;ftp_username not set correctly\u0026#34; \u0026amp; goto USAGE ) if %ftp_password%! == ! ( echo \u0026#34;ftp_password not set correctly\u0026#34; \u0026amp; goto USAGE ) if %local_path%! == ! ( echo \u0026#34;local_path not set correctly\u0026#34; \u0026amp; goto USAGE ) if %remote_path%! == ! ( echo \u0026#34;remote_path not set correctly\u0026#34; \u0026amp; goto USAGE ) echo open %ftp_hostname% \u0026gt; ftp.txt echo user %ftp_username% %ftp_password% \u0026gt;\u0026gt; ftp.txt echo cd %remote_path% \u0026gt;\u0026gt; ftp.txt echo lcd %local_path% \u0026gt;\u0026gt;ftp.txt echo prompt off \u0026gt;\u0026gt;ftp.txt echo bin \u0026gt;\u0026gt; ftp.txt echo mput * \u0026gt;\u0026gt; ftp.txt echo bye \u0026gt;\u0026gt; ftp.txt ftp -n -s:ftp.txt del ftp.txt goto END :USAGE echo. echo. - ------------------------------------------------------------------------------- echo. - upload_to_ftp.bat [hostname] [username] [password] [local_path] [remote_pat] - echo. - Example: - echo. - upload_to_ftp.bat 192.168.1.1 guest guest D:\\Media\\* C:\\Builds\\ - echo. - ------------------------------------------------------------------------------- echo. :END ","date":"2019-05-13","externalUrl":null,"permalink":"/posts/upload-to-ftp-parameterization-by-bat/","section":"Posts","summary":"本文介绍了如何使用 Windows Batch 脚本通过参数化的方式上传文件到 FTP 服务器，避免在脚本中硬编码 FTP 凭据。","title":"通过参数化上传文件到 FTP 服务器","type":"posts"},{"content":" Prepare Java runtime # Check if had installed java # $ java -version openjdk version \u0026#34;1.8.0_65\u0026#34; OpenJDK Runtime Environment (build 1.8.0_65-b17) OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode) if not Here is an article telling you how to install it # Create Node # 1. Jenkins home page-\u0026gt;Manage Node-\u0026gt;New Node, such as window-build-machine # 2. List Linux agent settings # Items Settings Name Linux-build-machine Description used for Linux build of executors 1 Remote root directory /home/agent Labels Linux, build Usage Use this node as much as possible Launch method Launch agent agents via SSH Host 192.168.1.112 Credentials username/password Host Key Verification Strategy Manually trusted key Verification Strategy Availability Keep this agent online as much as paossible 3. How to set credentials # credentials configuration Domain Global credentials (unrestricted) Kind Username with password Scope Global(Jenkins, nodes, items, all child items, etc) Username root Password mypassword Description Linux agent username \u0026amp; password 4. Save then Connect # Remoting version: 3.29 This is a Unix agent Evacuated stdout Agent successfully connected and online SSHLauncher{host=\u0026#39;192.168.1.112\u0026#39;, port=22, credentialsId=\u0026#39;d1cbab74-823d-41aa-abb7-8584859503d0\u0026#39;, jvmOptions=\u0026#39;\u0026#39;, javaPath=\u0026#39;/usr/bin/java\u0026#39;, prefixStartSlaveCmd=\u0026#39;\u0026#39;, suffixStartSlaveCmd=\u0026#39;\u0026#39;, launchTimeoutSeconds=210, maxNumRetries=10, retryWaitTime=15, sshHostKeyVerificationStrategy=hudson.plugins.sshslaves.verifiers.ManuallyTrustedKeyVerificationStrategy, tcpNoDelay=true, trackCredentials=true} [05/11/19 01:33:37] [SSH] Opening SSH connection to 192.168.1.112:22. [05/11/19 01:33:37] [SSH] SSH host key matches key seen previously for this host. Connection will be allowed. [05/11/19 01:33:37] [SSH] Authentication successful. [05/11/19 01:33:37] [SSH] The remote user\u0026#39;s environment is: Troubleshooting # Problem how to fix [04/22/19 23:15:07] [SSH] WARNING: No entry currently exists in the Known Hosts file for this host. Connections will be denied until this new host and its associated key is added to the Known Hosts file. ssh-keyscan HOSTNAME \u0026raquo; known_hosts /var/lib/jenkins/.ssh/known_hosts [SSH] No Known Hosts file was found at /var/lib/jenkins/.ssh/known_hosts. changing the Host key verification strategy in LAUNCH METHOD from \u0026ldquo;Known Hosts file verification strategy\u0026rdquo; to \u0026ldquo;Manually trusted key verification strategy\u0026rdquo; ","date":"2019-05-12","externalUrl":null,"permalink":"/en/posts/jenkins-linux-agent/","section":"Posts","summary":"Provides a step-by-step guide on how to configure a Jenkins Linux agent, including setting up the Java runtime, creating the node, and troubleshooting common issues.","title":"Jenkins Linux agent configuration","type":"posts"},{"content":" Prepare Java runtime # 1. Download Java # 2. Configure Java Windows path # JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_201 CLASSPATH=.;%JAVA_HOME%\\lib;%JAVA_HOME%\\jre\\lib Create Node # 1. Jenkins home page-\u0026gt;Manage Node-\u0026gt;New Node, such as window-build-machine # 2. List windows agent settings # Items Settings Name window-build-machine Description used for windows build of executors 1 Remote root directory C:\\agent Labels windows, build Usage Use this node as much as possible Launch method Let Jenkins control this Windows slave as a Windows service Administrator user name .\\Administrator Password mypassword Host 192.168.1.111 Run service as Use Administrator account given above Availability Keep this agent online as much as paossible 3. Save then Connect # [2019-05-11 01:32:50] [windows-slaves] Connecting to 192.168.1.111 Checking if Java exists java -version returned 1.8.0. [2019-05-11 01:32:50] [windows-slaves] Copying jenkins-slave.xml [2019-05-11 01:32:50] [windows-slaves] Copying slave.jar [2019-05-11 01:32:50] [windows-slaves] Starting the service [2019-05-11 01:32:50] [windows-slaves] Waiting for the service to become ready [2019-05-11 01:32:55] [windows-slaves] Connecting to port 52,347 \u0026lt;===[JENKINS REMOTING CAPACITY]===\u0026gt;Remoting version: 3.29 This is a Windows agent Agent successfully connected and online Troubleshooting # The following issues I met and how I fixed them.\n1. ERROR: Message not found for errorCode: 0xC00000AC # You need need to install JDK, and config JAVA environment variable.\n2. How to fix add windows node as Windows service error # Ref to JENKINS-16418.\n3. org.jinterop.dcom.common.JIException: Message not found for errorCode: 0x00000005 # Fixed permission for the following registry keys\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Classes\\Wow6432Node\\CLSID{72C24DD5-D70A-438B-8A42-98424B88AFB8} HKEY_CLASSES_ROOT\\CLSID{76A64158-CB41-11D1-8B02-00600806D9B6} Steps to fix it\nOpen \u0026lsquo;regedit\u0026rsquo; (as Administrator), Find (Ctrl+F) the registry key: \u0026ldquo;{72C24DD5-D70A-438B-8A42-98424B88AFB8}\u0026rdquo; in HKEY_LOCAL_MACHINE\\SOFTWARE\\Classes\\Wow6432Node\\CLSID Right click and select \u0026lsquo;Permissions\u0026rsquo;, Change owner to administrators group (Advanced\u0026hellip;). Change permissions for administrators group. Grant Full Control。 Change owner back to TrustedInstaller (user is \u0026ldquo;NT Service\\TrustedInstaller\u0026rdquo; on local machine) Repeat the above steps to fix permission for HKEY_CLASSES_ROOT\\CLSID{76A64158-CB41-11D1-8B02-00600806D9B6}\nFinally, Restart Remote Registry Service (Administrative Tools / Services).\n4. ERROR: Unexpected error in launching an agent # This is probably a bug in Jenkins.\nLogin remote machine and open Services find jenkinsslave-C__agent Set startup type: Automatic Log On: select This account, type correct account and password start jenkinsslave-C__agent 5. Caused by: org.jinterop.dcom.common.JIRuntimeException: Message not found for errorCode: 0x800703FA # Slave under domain account, If your slave is running under a domain account and you get an error code 0x800703FA, change a group policy:\nopen the group policy editor (gpedit.msc) go to Computer Configuration-\u0026gt;Administrative Templates-\u0026gt;System-\u0026gt; UserProfiles, \u0026ldquo;Do not forcefully unload the user registry at user logoff\u0026rdquo; Change the setting from \u0026ldquo;Not Configured\u0026rdquo; to \u0026ldquo;Enabled\u0026rdquo;, which disables the new User Profile Service feature (\u0026lsquo;DisableForceUnload\u0026rsquo; is the value added to the registry) 6. ERROR: Message not found for errorCode: 0xC0000001 Caused by: jcifs.smb.SmbException: Failed to connect: 0.0.0.0\u0026lt;00\u0026gt;/10.xxx.xxx.xxx # Need to enable SMB1\nSearch in the start menu for ‘Turn Windows features on or off’ and open it. Find \u0026lsquo;SMB1.0/CIFS File Sharing Support\u0026rsquo; in the list of optional features that appears, and select the checkbox next to it. Click OK and Windows will add the selected feature. You’ll be asked to restart your computer as part of this process.\n7. .NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service # Need to upgrade your .NET Framework. Here is a link for update .NET Framework.\n6. more connect jenkins agent problem on windows # Please refer to this link https://github.com/jenkinsci/windows-slaves-plugin/blob/master/docs/troubleshooting.adoc\n","date":"2019-05-12","externalUrl":null,"permalink":"/en/posts/jenkins-windows-agent/","section":"Posts","summary":"Provides a step-by-step guide on how to configure a Jenkins Windows agent, including setting up the Java runtime, creating the node, and troubleshooting common issues.","title":"Jenkins Windows agent configuration","type":"posts"},{"content":"每当工作闲暇，我都会时常想起好久没有更新微信公众号的文章了，总想等工作不忙的时候赶紧跟大家分享我从测试转开发这段时间的经历和感受，但工作总是有忙不完的忙，一刻都停不下来。\n终于等到这一周有两天工作不是那么忙碌了，才决定将前几天写到一半的文章更新完。这是我这几个月下来感受最轻松的两天，暂时没有bug需要去调查和测试，不用去看十几年的C代码，终于有大块时间去写我负责的Python Client端代码了。这种写着代码，听着歌曲去重构，Debug，修改Unit Test Suite感觉真是幸福。\n幸福的时光总是短暂的，今天就又来了两个Bug需要去调查 ε=(´ο｀*)))唉\u0026hellip;\n又把我打回原形，调查大半天之后发现原来是QA测的不对，可以松口气晚上可以不用工作更新下微信公众号了。\n这五个月来，几乎每天都是白天八小时，晚上继续背着电脑回家准备继续工作，周日偶尔去公司，经常在家学习。因为角色的转变，新的项目，需要学习的地方很多。从业务到技术，再加上产品发布在即，作为一名开发新人也肩负起Bug Fix的任务，十年前的代码，全英文的文档，复杂的系统，如果不全力一搏，真担心自己转型失败，那就太打脸了。\n一天的工作忙碌和压力，使得我晚上总是吃的停不下来，吃饭是我一天当中最轻松的时刻。去年我跟别人打赌减肥赢奖金，我毫无怨念的拿到了第一的奖金，可是今年再和别人打赌减肥，至今我都还没开始，马上年底了，输掉奖金是毫无悬念的。总结下来，大概是因为今年工作太忙，工作压力大的缘故，使得我无法在八小时之余安心去继续练习吉他，做keep，年假还没来得及休，真是计划不如变化快。\n虽然我还是个小开发，当角色变了，角度也会有变化。\n自动化测试是本分，DevOps是阶梯 # 这几年下来相信你也会真切感受到，如果一名测试人员不懂自动化测试，不会写自动化测试脚本，不但难有升职或是跳槽的机会，很有可能会被企业所淘汰。\n个人觉得DevOps是未来一段时间很多企业要走的路，一般的二线城市能把DevOps讲明白并且实施的人太少了，所以尽早掌握实施DevOps的人，就有机会成为DevOps教练或是测试架构师这样的角色。\n没有做好抗压的准备，不要去做开发 # 这几个月来遇到压力非常多，从刚开始的学习C语言，到C语言考核；从学习全英文的业务文档，到业务文档的分享（也是一种考核）；从调研C代码的代码覆盖率、Git分享，到调查并解决Bug；从每天的站立会汇报到每周与国外同事的例会。终于等到九月份，Title从Quality Assurance Engineer变成了Software Engineer，这其中的压力、痛苦和短暂的喜悦只有走过的人才知道。\n与年龄想匹配的能力 # 这点非常重要，如果现在问你，你与刚毕业两三年的同行年轻人有哪些优势？如果你不能肯定和清楚的说出自己优势的话，那就要好好反思一下了。\n如果从开发角度来说，我现在就是与年龄不相匹配的能力，因此测试相关的技能以及DevOps相关知识依旧是我要好好掌握的功课。\n学好英语 # 对于国内公司来说，工作上不会用到英语，但我想说如果想在测试和开发领域有更长远发展，英文非常重要。一般最流行开源的自动化测试框架、技术、DevOps相关的工具以及搜索最有效的解决问题的方案一般都是英文。如果你的英语不好，坚持一年半载去硬啃一手英文资料，形成习惯，受益终生。\n","date":"2018-12-26","externalUrl":null,"permalink":"/misc/from-qa-to-dev/","section":"Miscs","summary":"从测试转开发的五个月，工作、学习、生活的感悟和总结。","title":"从测试转开发","type":"misc"},{"content":"","date":"2018-08-07","externalUrl":null,"permalink":"/tags/functiontest/","section":"标签","summary":"","title":"FunctionTest","type":"tags"},{"content":"当你第一次开始接触测试这个行业的时候，首先听说的应该都是功能测试。\n功能测试是通过一些测试手段来验证开发做出的代码是否符合产品需求。这些年功能测试好像不太受欢迎了，不少同学开始尝试自动化测试，测试开发等等，结果是功能测试、自动化测试、测试开发一样都没做好。\n我们通常认为的功能测试是根据需求，采取以下测试流程：需求分析，用例编写，用例评审，提测验证，Bug回归验证，上线与线上回归等测试。如此日复一日，年复一年，可是等准备换工作的时候却得不到认可，你也遇到这种情况吗？\n那么如何做好功能测试？功能测试用到哪些知识？有哪些相关的建议呢？\n需求分析 # 业务方在提出需求的时候，产品是要分析这个需求的价值，影响范围和实现代价的。在需求评审的时候，作为一个测试人员必须了解这次需求的内容，影响到哪些现有的功能，涉及到的操作系统或是类别等，然后准确的评估出工作量，防止因评估不足造成后期测试不充分。\n再者，关注开发和产品的讨论，关注需求最后如何实现？其中做出的变动和难点就是测试的时候必须重点关注的部分，不能因为这些暂时和你没有关系就不去关注，防止欠债越来越多，不能做好充分的的测试。\n第三，需求评审结束后，要求产品更新此次评审过程中的所有改动部分，同时确保之后的任何需求变化都及时更新。\n第四，根据产品需求，同时与在会人员进行探讨，设计测试方案及时间安排，此时可以粗粒度考虑，时间上要合理。\n用例设计与评审 # 测试用例是每个测试人员工作过程中必须要完成的工作，它对测试工作起到指导作用，也是相关业务的一个文档沉淀。在以往面试的经验中，有许多人的测试用例写的没有章法，他们是凭着感觉去写测试用例，也没有从用户的角度来思考如何编写测试用例，对于测试用例设计较为常见的方法论也不清楚。\n假如面试的时候给你一个场景：一个全新的App要发布，如果让你来测试，你能想到哪些测试方案？如果你只能想到如何去测试app的功能的话，作为功能测试人员就考虑不够全面。此时除了App的功能以外，还应关注App的兼容性，易用性，接口的功能测试和性能测试，数据的存储以及容灾情况等等都应考虑在内。\n测试用例可设计为两类： 一类是开发自测和验收提测试标准的冒烟测试用例；一类是针对需求的全面测试用例。\n编写完测试用例后主动联系相关人员进行用例评审，在评审过程中及时修改不合适的用例。\n测试流程，注重项目控制 # 项目的流程控制在需求开始的时候就应该重视起来，只是很多时候我们没有意识到这是测试的工作，有的是产品来控制，有的是专门的项目经理来控制。\n测试人员需要有关注整体项目的意识。如果你不关注项目进度，什么时候提测什么时候开始测试，那么在测试过程中会遇到测试的内容和最初的需求不一致时候就会额外需要时间来解决，导致项目延期。另外主动关注项目，长此以往，你的这份主动性也会是你有效的竞争力。\n需求一旦明确了由你来负责的时候，就要时刻来关注项目的情况。中间变更需求的时候，要评估是否影响项目进度，如果影响了重新进行排期。如果开发提测试晚了，是否影响上线时间，如果影响需要及时跟相关的人员沟通，发风险邮件，通知大家详细的情况。\n同时在测试过程中，发现了bug需要详细描述问题，以方便开发去进行重现和修改。同时给bug准确分级，实时跟踪进度，保证项目高质量的按期完成。\n上线回归与项目总结 # 一个需求上线完成后，要及时进行线上回归，同时必须回归我们在需求评审的时候考虑到的可能影响到的原有的功能，以确保新功能完全上线成功。\n在一个项目完成后，最好有一份个人总结报告，总结整个项目过程中遇到的问题及最后的解决办法，有哪些需要注意的问题？有什么可以借鉴的方案或是改进策略？项目中有没有通用性的问题等等。\n能力的总结和沉淀 # 在找工作的时候，很多做功能测试多年的同学都遭遇过面试失败，究其原因，我觉得最核心的原因是：不具备相应工作年限应该具备的能力。\n我们应该时常问自己一句话：离开现有的平台，我还有什么？如果仅仅是对现在公司业务和工具的熟悉，那是没有任何优势可言的。\n对同类业务流程的掌握，项目的整体把控，快速了解业务并能根据需求选择测试方案，引入提高测试效率测试方案和工具，测试过程中遇到问题的预判和解决办法等才是功能测试人员必须具备的能力。\n这些方面你做到了吗？不要抱怨功能测试如何如何，认清行业现状和自己的优缺点，做好自己的职业规划。\n如果你不善于编码，那么做务专家也是功能测试人员一个很好的选择。\n","date":"2018-08-07","externalUrl":null,"permalink":"/posts/how-to-do-functional-testing/","section":"Posts","summary":"介绍功能测试的基本流程、用例设计、项目控制、上线回归等方面的建议，帮助测试人员提升功能测试的质量和效率。","title":"如何做好功能测试","type":"posts"},{"content":"I haven\u0026rsquo;t updated my public account articles in the past few months because, starting in May, I had the opportunity to switch to development due to project needs. I cherish this opportunity and have been working hard to learn development-related skills.\nWhy did I switch to development after 9 years of testing?\nThree years ago, I planted the seed of becoming a developer. Years of writing automated test cases made me realize I enjoyed coding. I wanted to delve deeper into technology, and development was the most direct path. So I consistently worked on reading and writing more code, preparing for the day I could become a development test engineer or a developer.\nI was also inspired by a recent article by Ru Bingcheng, \u0026ldquo;Why I switched from development to testing and stuck with it for 16 years.\u0026rdquo; Our career paths are the opposite, but we both strive for better professional development. He mentioned many future possibilities for testers in his video (check it out if you\u0026rsquo;re unfamiliar; it broadens your perspective). I believe strong coding skills are fundamental for any engineering role. Testing isn\u0026rsquo;t just about meticulous clicking; it\u0026rsquo;s not a job better suited for women; it\u0026rsquo;s not easier than development; and it\u0026rsquo;s not a path to complacency. Successful testing requires more effort than it appears.\nSince I\u0026rsquo;m learning C, a language I haven\u0026rsquo;t touched since graduation, these past few months have highlighted the vast unknown in development. Learning a development language requires in-depth study, unlike learning automated testing. With automated testing, you can learn scripting languages on the fly. C is different; it demands systematic learning, mastering arrays, pointers, structures, linked lists, binary trees, data structures, and algorithms, operating systems, and compiler principles.\nAlthough I\u0026rsquo;ve switched to development, I will continue to focus on testing.\nI hope this role change allows me to view product quality, testing-related thinking, and technology from a more comprehensive perspective, and share more valuable content.\n","date":"2018-07-21","externalUrl":null,"permalink":"/en/posts/why-i-move-to-development/","section":"Posts","summary":"This article documents my experience and insights from transitioning from testing to development. I share my learning and work arrangements during paternity leave, including reading books, participating in open-source projects, and physical exercise, emphasizing how to maintain learning and growth while caring for a family.","title":"Why I Switched from Testing to Development After 9 Years","type":"posts"},{"content":"","date":"2018-07-21","externalUrl":null,"permalink":"/en/tags/work/","section":"Tags","summary":"","title":"Work","type":"tags"},{"content":"如果你想在一台电脑上配置 github 和 bitbucket，如何配置多个 SSH git key？ 输入以下命令生成 SSH Key，注意在生成过程中最好输入新的名字，比如 id_rsa_github 和 id_rsa_bitbucket\nssh-keygen -t rsa -C \u0026#34;your_email@youremail.com\u0026#34; 然后将生成的 SSH key 文件内容复制到对应网址的个人用户设置中即可。但是明明按照官方教程做的但是在 git clone 的时候还是遇到以下问题： Error: Permission denied (publickey) 困恼了几天的错误终于解决了。\n参看这个文档\n由于我用的是macOS Sierra 10.13.3，文档这里写着如果是macOS Sierra 10.12.2 及以后的版本需要在 ~/.ssh 目录下创建一个 config 文件 congfig 文件的具体配置如下：\nHost * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa_github Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa_bitbucket 配置了这个文件之后，再次尝试\ngit clone git@github.com:shenxianpeng/blog.git 可以 download 代码了，两个 SSH git 都好用了 : )\n","date":"2018-05-06","externalUrl":null,"permalink":"/posts/permission-denied-publickey/","section":"Posts","summary":"本文介绍了如何在配置多个 SSH Git Key 时解决 \u0026ldquo;Permission denied (publickey)\u0026rdquo; 错误，确保 GitHub 和 Bitbucket 的 SSH 连接正常工作。","title":"Error: Permission denied (publickey)","type":"posts"},{"content":"What kind of technical skills and experience define a senior engineer? Let me share my perspective on senior engineers.\nExtensive Industry Testing Experience # Ideally, experience in both traditional and internet companies is preferred. If not, at least engage with senior test engineers from these companies to understand their testing approaches and broaden your perspective.\nStrong Testing Fundamentals # Master necessary testing theories, be familiar with testing processes, requirements analysis, and test case design methods. Develop test plans based on project needs.\nStrong Business Acumen # Proficiency in functional testing requires strong business understanding. This allows you to design test cases from a product perspective, identifying issues beyond basic functionality and providing constructive feedback and suggestions to the product team.\nFamiliarity with Relevant Testing Tools # Many tools are used in software testing. Understanding and using these tools allows you to effectively choose the appropriate tools based on company requirements and project needs, improving work efficiency.\nManagement Tools: e.g., JIRA, Testlink, Wiki, Confluence Continuous Integration: Jenkins, Bamboo, Travis CI, etc. Understand their differences and implementation. Automated Testing: Understand how to perform automated testing for web and mobile platforms, and the tools involved. Familiarity with Selenium, WebDriver, Appium, Robotium testing frameworks, and programming languages used for developing automated test cases (Python? Java? JavaScript?). Know how to choose and implement these tools. Performance Testing: Understand Jmeter and LoadRunner, two major performance testing tools, and how to conduct performance testing. Strong Coding Skills # Strong coding skills enable rapid mastery of automated testing and even the development of testing platforms. Furthermore, this allows for quick familiarization with automated test cases written in Java, Python, Javascript, etc., when transitioning to a new company.\nLanguage Proficiency # This includes communication and foreign language skills. Communication is an essential skill for testers. Good communication ensures developers understand the issues and accept your suggestions, and allows you to better understand requirements from product personnel. While English is mainly required in foreign companies, learning English is beneficial because many technical resources and materials from the open-source community are in English.\nTherefore, becoming an excellent senior test engineer requires many skills. Let\u0026rsquo;s work hard together! 💪\n","date":"2018-05-06","externalUrl":null,"permalink":"/en/posts/senior-test-engineer/","section":"Posts","summary":"This article outlines the skills and experience required for a senior test engineer, including testing theories, business acumen, tool proficiency, and coding abilities, helping readers understand how to become an excellent senior test engineer.","title":"My Perspective on Senior Test Engineers","type":"posts"},{"content":"我想大多数的团队都面临这样的问题：\n发布周期长\n开发和测试时间短\n开发和测试是两个独立的团队\n不稳定的交付质量\n低收益难维护的UI自动化测试脚本\n不合理的测试权重分配\n解决方法：\n引入 DevOps 和分层自动化\n组件化产品 产品开发引入模块化，数据驱动会使得产品更加容易实施 Unit，Server，UI 自动化测试 优化工程师 开发和测试在未来将没有界限，他们都是开发者，都会产品的质量和客户负责 分层自动化 更合理的测试权重分配，更底层的测试收益越高 引入工具 实施DevOps引入必要的工具，Bitbucket, Jenkins, Sonar, Pipelines, Docker, test framework … ","date":"2018-04-14","externalUrl":null,"permalink":"/posts/devops-practice/","section":"Posts","summary":"本文介绍了 DevOps 实践的核心概念、目标和实施方法，强调了持续集成、持续交付和自动化的重要性。","title":"DevOps 实践","type":"posts"},{"content":"最近在做有关 DevOps Build 的时候，学习了 Jenkins 的 Pipeline 的功能，不得不提到的就是 Jenkinsfile 这个文件。\n以下面是我配置的 Jenkinsfile 文件及简单说明，更多有关 Pipeline 请看官方文档。\npipeline { agent any stages { // Build 阶段 stage(\u0026#39;Build\u0026#39;) { steps { echo \u0026#39;Building...\u0026#39; bat \u0026#39;npm run build webcomponent-sample\u0026#39; } } // 单元测试阶段 stage(\u0026#39;Unit Test\u0026#39;) { steps { echo \u0026#39;Unit Testing...\u0026#39; bat \u0026#39;npm test webcomponent-sample\u0026#39; } post { success { // 执行成功后生产报告 publishHTML target: [ allowMissing: false, alwaysLinkToLastBuild: false, keepAll: true, reportDir: \u0026#39;components/webcomponent-sample/coverage/chrome\u0026#39;, reportFiles: \u0026#39;index.html\u0026#39;, reportName: \u0026#39;RCov Report\u0026#39; ] } } } // E2E 测试阶段 stage(\u0026#39;E2E Test\u0026#39;) { steps { bat \u0026#39;node nightwatch e2e/demo/test.js\u0026#39; } } stage(\u0026#39;Release\u0026#39;) { steps { echo \u0026#39;Release...\u0026#39; } } } post { // 执行成功是触发 success { mail bcc: \u0026#39;email@qq.com\u0026#39;, body: \u0026#34;\u0026lt;b\u0026gt;Project: ${env.JOB_NAME} \u0026lt;br\u0026gt;Build Number: ${env.BUILD_NUMBER} \u0026lt;br\u0026gt;Build URL: ${env.BUILD_URL} \u0026#34;, cc: \u0026#39;\u0026#39;, charset: \u0026#39;UTF-8\u0026#39;, from: \u0026#39;jenkins@qq.com\u0026#39;, mimeType: \u0026#39;text/html\u0026#39;, replyTo: \u0026#39;\u0026#39;, subject: \u0026#34;SUCCESS: Project Name -\u0026gt; ${env.JOB_NAME}\u0026#34;, to: \u0026#34;\u0026#34;; } // 执行失败时触发 failure { mail bcc: \u0026#39;email@qq.com\u0026#39;, body: \u0026#34;\u0026lt;b\u0026gt;Project: ${env.JOB_NAME} \u0026lt;br\u0026gt;Build Number: ${env.BUILD_NUMBER} \u0026lt;br\u0026gt;Build URL: ${env.BUILD_URL} \u0026#34;, cc: \u0026#39;\u0026#39;, charset: \u0026#39;UTF-8\u0026#39;, from: \u0026#39;jenkins@qq.com\u0026#39;, mimeType: \u0026#39;text/html\u0026#39;, replyTo: \u0026#39;\u0026#39;, subject: \u0026#34;FAILURE: Project Name -\u0026gt; ${env.JOB_NAME}\u0026#34;, to: \u0026#34;\u0026#34;; } } } ","date":"2018-04-14","externalUrl":null,"permalink":"/posts/jenkinsfile-configure/","section":"Posts","summary":"本文介绍了如何使用 Jenkinsfile 配置 Jenkins Pipeline，包括构建、测试和发布阶段的示例，以及如何处理邮件通知。","title":"Jenkinsfile 配置","type":"posts"},{"content":" 对于一个不善于言表的我工作中遇到过 # 过度理解在与同事之间的Email和Chat中的意思； 同事之间的沟通中出现的分歧事后还会继续琢磨； 十分关注自己的工作失与得在上级领导中的看法。 以下方式对我来说还比较有效的 # 让自己的精力更多的聚焦在工作上； 工作中对事不对人，做对的事情； 眼光放长远，不忘初心，专注应该做的事情上； 领导说的一句“向前看”我印象深刻，过去的就过去了，别再纠结，向前看。 ","date":"2017-11-23","externalUrl":null,"permalink":"/misc/weather-setbacks-at-work/","section":"Miscs","summary":"工作中遇到挫折时，如何调整心态，专注于工作本身，而不是过度纠结于人际关系。","title":"度过工作中挫折心结","type":"misc"},{"content":"","date":"2017-11-20","externalUrl":null,"permalink":"/en/tags/hexo/","section":"Tags","summary":"","title":"Hexo","type":"tags"},{"content":"Hexo\u0026rsquo;s default theme uses black code highlighting. Want a different style? Here\u0026rsquo;s how to do it:\n# Modify the highlight.styl file, located at themes/landscape/source/css/_partial/highlight.styl Modify the default code theme to Tomorrow Night Eighties:\nhighlight-background = #2d2d2d highlight-current-line = #393939 highlight-selection = #515151 highlight-foreground = #cccccc highlight-comment = #999999 highlight-red = #f2777a highlight-orange = #f99157 highlight-yellow = #ffcc66 highlight-green = #99cc99 highlight-aqua = #66cccc highlight-blue = #6699cc highlight-purple = #cc99cc For the Tomorrow theme:\nhighlight-background = #ffffff highlight-current-line = #efefef highlight-selection = #d6d6d6 highlight-foreground = #4d4d4c highlight-comment = #8e908c highlight-red = #c82829 highlight-orange = #f5871f highlight-yellow = #eab700 highlight-green = #718c00 highlight-aqua = #3e999f highlight-blue = #4271ae highlight-purple = #8959a8 For more details, please refer to the tomorrow-theme modifications.\n","date":"2017-11-20","externalUrl":null,"permalink":"/en/posts/change-hexo-code-highlight/","section":"Posts","summary":"Hexo\u0026rsquo;s default theme uses black code highlighting. Want a different style? This article explains how to modify Hexo theme code highlighting styles.","title":"Modifying Hexo Theme Code Highlighting","type":"posts"},{"content":" Hexo 配置 rss 订阅功能 # 安装 hexo-generator-feed 插件\nnpm install hexo-generator-feed --save 如果国内 npm 安装不成功，可以先安装 cnpm\nnpm install -g cnpm --registry=https://registry.npm.taobao.org 然后再\ncnpm install hexo-generator-feed --save 在 _config.yml 中配置这个插件\nfeed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: \u0026#39; \u0026#39; Hexo 博客文章中插入图片 # 如果想在 Hexo 文章中插入图片怎么做？\n网络上很容易搜到 Markdown 的语法是 ![Alt text](/path/to/img.jpg) 前面 Alt text 是指在图片下面命名，后面是图片的地址。那么如何配置？\n经过几番尝试得知：在你的 hexo 项目根目录下面 source 创建一个 images 文件夹， 把你以后用的到图片都放在这个目录下面就 OK 了。\n![示例图1](../images/color.png) ","date":"2017-10-25","externalUrl":null,"permalink":"/posts/hexo-practice/","section":"Posts","summary":"本文介绍如何在 Hexo 博客中配置 RSS 订阅功能，包括安装插件和使用。","title":"Hexo 的配置和使用","type":"posts"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. \u0026#x1f680;\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","externalUrl":null,"permalink":"/en/tags/advanced/","section":"Tags","summary":"\u003cp\u003eThis is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. \u0026#x1f680;\u003c/p\u003e","title":"Advanced","type":"tags"},{"content":"","externalUrl":null,"permalink":"/en/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/en/portfolio/","section":"Portfolios","summary":"","title":"Portfolios","type":"portfolio"},{"content":"","externalUrl":null,"permalink":"/en/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"DevOps Engineer, Also known as @shenxianpeng, is a software engineer and open source enthusiast.\nHe has contributed to open source since 2021, focusing on DevOps, CI/CD, and Python. He is the creator of several projects including cpp-linter, commit-check, conventional-branch and devops-maturity.\nBlog: https://shenxianpeng.github.io/\n","externalUrl":null,"permalink":"/en/authors/shenxianpeng/","section":"Authors","summary":"\u003cp\u003eDevOps Engineer, Also known as \u003cspan class=\"relative inline-block align-text-bottom icon\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 496 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/\u003e\u003c/svg\u003e\u003c/span\u003e\n\u003ca\n  href=\"https://github.com/shenxianpeng\"\n    target=\"_blank\"\n  \u003e@shenxianpeng\u003c/a\u003e, is a software engineer and open source enthusiast.\u003c/p\u003e","title":"Xianpeng Shen","type":"authors"},{"content":" About Me Tech Lead in DevOps/Build/Release, built the DevOps practice from scratch and drove best practices adoption across teams and departments. Specialized in DevOps since 2018, experienced with Windows, Linux, AIX, Solaris, HP-UX, and the full software development lifecycle. Strong in the Scan → Try → Scale approach to adopt and scale industry best practices. Proficient in Python / Shell / Groovy for developing DevOps tools and automation solutions. Open Source Creator: Founded and maintain cpp-linter, commit-check, conventional-branch, and devops-maturity. cpp-linter-action is used by Microsoft, Apache, NASA, and other notable projects. Tech Writer: Published hundreds of original articles on my blog and WeChat public account “DevOps攻城狮”, reaching and inspiring a broad developer audience. Work Experience Senior DevOps Engineer | Rocket Software, Lithuania | Jul 2024 – Present\nDriving advanced DevOps initiatives and scaling delivery best practices across teams. DevOps Engineer | Rocket Software, Dalian | 2015 – Jun 2024\nLed CI/CD transformation from manual/Bamboo builds to Jenkins with shared libraries. Built IaC with Ansible for provisioning Jenkins, development environments. Dockerized MVAS products using buildx, health checks, and Kubernetes deployments. Proposed and scaled DevOps maturity badge and conventional commits. Automated VM management with Jira + Python, adopted company-wide. Enabled code coverage reporting for multiple product lines. Won Rocket Build Awards and added to product roadmap. Built Rocket Discover automation framework from scratch. Software Engineer in Test | JD.COM, Beijing | 2012 – 2014\nDeveloped automated test scripts and maintained CI pipelines. QA Engineer | SIMCOM (Shanghai) \u0026amp; Neusoft (Beijing) | 2009 – 2011\nDesigned and executed test cases; led small QA teams and shared best practices. Key Projects Internal pipeline-library Jenkins shared library for CI/CD as code, improving SDLC consistency. docker-images Dockerized MVAS with buildx, pytest, health checks, and Kubernetes. ansible-playbooks Managed build/dev infrastructure as code for fast recovery. U2Box CLI Go-based tool to set up MV dev/testing environments quickly. MV Intelligent Terminal CLI with auto-completion via UOPY API. Won Rocket Build First Prize \u0026amp; CPO Award (2019). VM Management with JIRA Automated VM lifecycle with Jira dashboards \u0026amp; Python scripts. Open Source cpp-linter – C/C++ linting solutions using clang-format and clang-tidy, used by Microsoft, Apache, NASA. commit-check – Ensures consistent commit messages, branch names, and more. conventional-branch – Git branch naming conventions for clear workflows devops-maturity – Specs and tools for assessing DevOps maturity explain-error-plugin – Explains Jenkins job failures with AI gitstats – Visualize Git repo insights and contribution analytics. Skills DevOps, CI/CD ★★★★★ Jenkins, Docker, Ansible ★★★★☆ Python, Shell ★★★★☆ Go, Groovy ★★★☆☆ Languages Chinese — Native English — Professional Working Proficiency Lithuanian — Beginner (A1) Education College Degree, Software Technology — Liaoning Provincial College of Communications (2006 – 2009) ","externalUrl":null,"permalink":"/en/resume/","section":"","summary":"\u003ch2 class=\"relative group\"\u003eAbout Me \n    \u003cdiv id=\"about-me\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTech Lead\u003c/strong\u003e in DevOps/Build/Release, built the DevOps practice from scratch and drove best practices adoption across teams and departments.\u003c/li\u003e\n\u003cli\u003eSpecialized in DevOps since 2018, experienced with Windows, Linux, AIX, Solaris, HP-UX, and the full software development lifecycle.\u003c/li\u003e\n\u003cli\u003eStrong in the \u003cstrong\u003eScan → Try → Scale\u003c/strong\u003e approach to adopt and scale industry best practices.\u003c/li\u003e\n\u003cli\u003eProficient in \u003cstrong\u003ePython / Shell / Groovy\u003c/strong\u003e for developing DevOps tools and automation solutions.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOpen Source Creator\u003c/strong\u003e: Founded and maintain \u003ca\n  href=\"https://github.com/cpp-linter\"\n    target=\"_blank\"\n  \u003ecpp-linter\u003c/a\u003e, \u003ca\n  href=\"https://github.com/commit-check\"\n    target=\"_blank\"\n  \u003ecommit-check\u003c/a\u003e, \u003ca\n  href=\"https://github.com/conventional-branch\"\n    target=\"_blank\"\n  \u003econventional-branch\u003c/a\u003e, and \u003ca\n  href=\"https://github.com/devops-maturity\"\n    target=\"_blank\"\n  \u003edevops-maturity\u003c/a\u003e. \u003ca\n  href=\"https://github.com/cpp-linter/cpp-linter-action\"\n    target=\"_blank\"\n  \u003ecpp-linter-action\u003c/a\u003e is used by Microsoft, Apache, NASA, and other notable projects.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTech Writer\u003c/strong\u003e: Published hundreds of original articles on my \u003ca\n  href=\"https://shenxianpeng.github.io\"\n    target=\"_blank\"\n  \u003eblog\u003c/a\u003e and WeChat public account “\u003cstrong\u003eDevOps攻城狮\u003c/strong\u003e”, reaching and inspiring a broad developer audience.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\n\u003ch2 class=\"relative group\"\u003eWork Experience \n    \u003cdiv id=\"work-experience\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eSenior DevOps Engineer\u003c/strong\u003e | Rocket Software, Lithuania | \u003cem\u003eJul 2024 – Present\u003c/em\u003e\u003c/p\u003e","title":"Xianpeng Shen's Resume","type":"page"}]
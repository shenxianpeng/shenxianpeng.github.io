[{"content":"","date":"2026-01-30","externalUrl":null,"permalink":"/tags/weekly/","section":"标签","summary":"","title":"Weekly","type":"tags"},{"content":"这里记录每周值得分享的 DevOps 与 AI 技术内容,周五发布。\n本杂志开源，欢迎投稿。合作请邮件联系（xianpeng.shen@gmail.com）。\n本周封面 # GitHub Copilot CLI 的 ASCII 动画 banner，展示了 AI 如何深入到开发者的终端工作流程中。（via）\nAI Agent 时代来临 # 本周最引人注目的趋势是 AI Agent 的全面爆发。从 AWS DevOps Agent 到 GitHub Copilot SDK，再到各种 Model Context Protocol (MCP) 服务器的发布，我们看到 AI 正在从简单的代码补全工具进化为能够自主执行复杂任务的智能助手。\nAWS 发布的 DevOps Agent 能够自主进行根因分析和故障排除，将原本需要数小时的问题定位工作缩短到分钟级别。GitHub 则通过 Copilot SDK 让开发者能够将 AI Agent 集成到任何应用中。这些进展标志着软件工程正在进入一个新时代——工程师将从纯粹的代码生产者转变为 AI 的协调者和验证者。\n对于 DevOps 工程师而言，这意味着我们需要重新思考工作流程的设计，学会如何有效地与 AI Agent 协作，同时保持对关键决策的控制权。\n行业动态 # 1、AWS DevOps Agent 正式发布\nAWS 在 re:Invent 2025 上宣布的 DevOps Agent 终于正式发布生产环境最佳实践指南。这个 AI Agent 能够自主进行根因分析和事故响应，通过关联多个服务的遥测数据、审查部署历史和理解复杂的应用依赖关系来快速定位问题。AWS 团队分享了从原型到产品的开发经验，特别强调了如何确保 Agent 生成有用的发现和观察结果。\n这标志着 DevOps 领域进入了自动化运维的新阶段。传统的人工分析方式效率低下且容易出错，而 AI Agent 能够在压力之下快速恢复服务，对于提升系统可靠性具有重要意义。\n2、Kubernetes 宣布 Ingress NGINX 将于 2026 年 3 月退役\nKubernetes Steering 和 Security Response 委员会发布声明，宣布作为云原生环境中约一半基础设施核心组件的 Ingress NGINX 将于 2026 年 3 月退役。这一决定是在多年公开警告该项目缺乏维护者之后做出的。官方推荐用户迁移到 Gateway API，这是 Kubernetes 下一代流量管理方案。\n这对依赖 Ingress NGINX 的团队来说是一个重要的提醒：开源项目的可持续性不能被视为理所当然。Gateway API 提供了更灵活和强大的功能，迁移虽然需要投入精力，但长期来看是值得的。\n3、Amazon 据报道洽谈向 OpenAI 投资 500 亿美元\n据多家媒体报道，Amazon 正在与 OpenAI 就一笔高达 500 亿美元的投资进行谈判。这将是 AI 领域有史以来最大规模的投资之一，也反映出科技巨头在 AI 竞赛中的激进态度。此前，微软已经向 OpenAI 投资超过 130 亿美元。\nAmazon 的这一举动可能会改变 AI 市场的竞争格局。对于开发者来说，这意味着我们将看到更多基于 OpenAI 技术的 AWS 服务集成，以及更强大的 AI 能力。\n4、Microsoft 持续采购 Nvidia 和 AMD AI 芯片\n尽管 Microsoft 推出了自己的 AI 芯片，CEO Satya Nadella 表示公司不会停止从 Nvidia 和 AMD 采购 AI 芯片。这反映出 AI 基础设施建设对算力的巨大需求，单一供应商难以满足快速增长的计算需求。Microsoft 采取多供应商策略，既保证了供应安全，也推动了芯片市场的竞争。\n这对 AI 工程师来说是个好消息，意味着我们将有更多硬件选择，也能推动 AI 训练和推理成本的下降。\n5、SonicWall 防火墙遭黑客攻击导致金融科技公司数据泄露\n金融科技公司 Marquis 将其数据泄露归咎于防火墙供应商 SonicWall 遭受的黑客攻击。这起事件再次提醒我们，供应链安全是现代企业面临的重要挑战。即使自身安全措施得当，第三方服务商的漏洞也可能成为攻击者的入口。\n对于 DevOps 团队，这意味着需要建立更完善的供应商安全评估机制，实施零信任架构，并制定完善的事故响应预案。\n深度阅读 # 1、从像素到字符：GitHub Copilot CLI 动画 ASCII Banner 背后的工程实践（英文）\nGitHub 工程团队详细介绍了如何为 Copilot CLI 构建一个可访问、多终端兼容的 ASCII 动画。文章深入讲解了自定义工具链的开发、ANSI 颜色角色的使用，以及高级终端工程技术。这不仅是一个技术实现案例，更展示了对用户体验细节的极致追求。\n文章特别值得关注的是如何在不同终端环境下保证动画的一致性和可访问性，这对于开发跨平台 CLI 工具的工程师很有参考价值。\n2、从 AI Agent 原型到产品：构建 AWS DevOps Agent 的经验教训（英文）\nAWS DevOps Agent 团队成员 Efe Karakus 分享了从原型到产品的完整开发历程。文章重点介绍了如何确保 AI Agent 在事故响应场景下生成有用的发现和观察结果，包括如何设计 Agent 的能力边界、如何处理不确定性，以及如何与现有的运维工具集成。\n这篇文章对于正在构建 AI Agent 的团队非常有价值，特别是在如何从概念验证走向生产环境方面提供了实用的指导。\n3、Anders Hejlsberg 的 7 条经验：C# 和 TypeScript 架构师的智慧（英文）\n作为 C# 和 TypeScript 的架构师，Anders Hejlsberg 分享了他在语言设计和软件工程方面的核心经验，包括快速反馈循环的重要性、如何扩展软件、开源可见性的价值，以及如何构建持久的工具。\n文章中最有价值的洞见是关于语言设计的权衡：如何在添加新特性和保持语言简洁之间取得平衡，以及如何通过开源社区的反馈来改进语言。\n4、使用 Datadog MCP 服务器和 AWS DevOps Agent 加速自主事故解决（英文）\nAWS 与 Datadog 合作推出的 MCP 服务器集成，展示了如何将 AI Agent 与可观测性平台结合。值班工程师以往需要花费数小时在多个工具之间手动调查故障，现在 AI Agent 可以自动关联日志、指标和追踪数据，大幅缩短故障恢复时间。\n这篇文章展示了 Model Context Protocol 的实际应用价值，为 AI Agent 与企业工具集成提供了标准化的方案。\n5、AI 驱动的开发生命周期（AI-DLC）开源自适应工作流（英文）\nAWS 开源了 AI-DLC 自适应工作流，这是一套将 AI 整合到软件开发全生命周期的方法论。通过强调 AI 主导的工作流和以人为中心的决策制定，AI-DLC 承诺同时提升开发速度和质量。文章分享了与不同行业工程团队合作的经验，说明了如何有效地将 AI 集成到工程工作流中。\n这个开源项目为希望采用 AI 辅助开发的团队提供了具体的实施框架和最佳实践。\n效率工具 # 1、GitHub Copilot CLI\nGitHub Copilot CLI 让你可以直接在终端中与 Copilot 交互，支持 agentic 工作流。通过斜杠命令，你可以运行测试、修复代码、获取支持，而无需离开终端。这大幅提升了命令行工作的效率，特别适合习惯在终端环境下工作的开发者。\nCLI 支持自然语言查询，可以帮助你快速找到和执行复杂的命令，还能根据上下文提供智能建议。对于需要频繁在终端操作的 DevOps 工程师来说，这是一个必备工具。\n2、GitHub Copilot SDK\n现已进入技术预览阶段的 GitHub Copilot SDK，可以让你将 AI Agent 集成到任何应用中。SDK 提供了规划、调用工具、编辑文件和运行命令的可编程层。这意味着你可以在自己的应用中嵌入类似 Copilot 的智能能力，而无需从头构建。\nSDK 的发布降低了 AI Agent 的开发门槛，让更多应用能够具备智能辅助能力。对于产品开发者来说，这是一个将 AI 能力快速集成到产品中的绝佳机会。\n3、Mermaid ASCII\nMermaid ASCII 让你可以在终端中渲染 Mermaid 图表。这个工具对于需要在命令行环境下查看架构图、流程图的场景非常有用。支持多种图表类型，包括流程图、序列图、类图等。\n对于喜欢在终端工作的工程师，或者需要在 SSH 连接的远程服务器上查看文档的场景，这个工具能够大幅提升效率。\n4、AWS Infrastructure as Code MCP Server\nAWS 发布的 IaC MCP Server 为 AI 助手提供了与 AWS 基础设施开发工作流集成的能力。基于 Model Context Protocol 构建，这个服务器支持 AI 助手进行文档搜索、验证和故障排除。\n这个工具特别适合正在使用 CDK 或 CloudFormation 的团队，可以大幅减少查找文档和调试基础设施代码的时间。\n5、EmulatorJS\nEmulatorJS 是一个基于 JavaScript 的多系统模拟器，支持在浏览器中运行多种经典游戏平台。虽然不是典型的 DevOps 工具，但展示了 WebAssembly 技术的强大能力，对于理解浏览器性能优化和 WebAssembly 应用很有参考价值。\nAI 相关 # 1、Project Genie：实时生成交互式世界\nGoogle 发布的 Project Genie 能够实时生成可交互的 3D 世界。用户可以在生成的环境中自由探索和互动，AI 会根据用户的行为动态生成新的场景内容。这项技术展示了生成式 AI 在游戏和虚拟环境领域的应用潜力。\n虽然目前主要面向游戏和创意应用，但这种实时生成和交互技术未来可能应用于模拟训练、虚拟协作等企业场景。\n⭐ 相关链接：https://blog.google/technology/google-labs/project-genie/\n2、Prism：OpenAI 的新项目\nOpenAI 推出的 Prism 项目聚焦于提升 AI 系统的可解释性和透明度。该项目旨在帮助用户更好地理解 AI 的决策过程，增强对 AI 系统的信任。\n对于在生产环境中部署 AI 的企业来说，可解释性一直是一个关键挑战。Prism 的发布为解决这个问题提供了新的思路。\n⭐ 相关链接：https://openai.com/index/introducing-prism/\n3、Unrolling the Codex Agent Loop\nOpenAI 工程团队详细解析了 Codex Agent 的工作循环机制。文章深入探讨了 Agent 如何理解代码上下文、规划执行步骤、生成代码并验证结果。这对于理解 AI 编程助手的内部工作原理非常有帮助。\n文章特别有价值的部分是关于如何处理长上下文和多步骤推理，这些技术对于构建复杂的 AI Agent 系统很有启发。\n⭐ 相关链接：https://openai.com/index/unrolling-the-codex-agent-loop/\n学习资源 # 1、Cluster API v1.12 发布：引入原地更新和链式升级\nCluster API v1.12 带来了两个重要新特性：原地更新（In-place Updates）和链式升级（Chained Upgrades）。这些功能让 Kubernetes 集群的生命周期管理更加灵活和高效。原地更新允许在不重建节点的情况下更新配置，而链式升级支持多个版本的连续升级。\n这是 Kubernetes 平台工程师必读的更新，新特性能够显著减少集群维护的停机时间和复杂度。\n2、使用 kind 实验 Gateway API\n这篇教程详细介绍了如何使用 kind（Kubernetes in Docker）搭建本地环境来学习和测试 Gateway API。考虑到 Ingress NGINX 即将退役，学习 Gateway API 变得更加紧迫。文章提供了完整的设置步骤和示例，帮助开发者快速上手。\n对于准备从 Ingress 迁移到 Gateway API 的团队，这是一个很好的起点。\n行业观点 # 1、\u0026ldquo;AI 的影响可能与预期不同。虽然很多人担心 AI 会取代工程师，但实际上 AI 更可能改变工程师的工作方式——从纯粹的代码编写者转变为 AI 的协调者和验证者。\u0026rdquo; —— AI\u0026rsquo;s Impact on Engineering Jobs May Be Different Than Expected\n现有数据显示，AI 在简化重复性任务方面表现出色，但在需要创造性思维和复杂决策的场景下仍需要人类主导。工程师的价值将更多体现在系统设计、问题定义和质量把控上。\n2、\u0026ldquo;开源项目的可持续性不能被视为理所当然。Ingress NGINX 的退役提醒我们，即使是关键基础设施也可能因为缺乏维护者而面临生命周期终结。\u0026rdquo; —— Ingress NGINX Statement\n这个案例凸显了企业在选择开源项目时需要评估项目的健康度和社区活跃度。投入资源参与开源项目维护，而不仅仅是使用，对于保障技术栈的长期稳定性至关重要。\n3、\u0026ldquo;Model Context Protocol 正在成为 AI Agent 与企业工具集成的事实标准。它为 AI 系统访问外部数据和工具提供了一个统一的接口。\u0026rdquo; —— AWS DevOps Blog\nMCP 的标准化降低了 AI Agent 集成的复杂度，让不同的 AI 系统能够更容易地与各种企业工具协作。这对于构建 AI 驱动的工作流至关重要。\n（完）\n","date":"2026-01-30","externalUrl":null,"permalink":"/posts/2026/weekly-3/","section":"Posts","summary":"这里记录每周值得分享的 DevOps 与 AI 技术内容，周五发布。本杂志开源，欢迎投稿。","title":"攻城狮周刊（第 3 期）：AI Agent 时代来临","type":"posts"},{"content":"","date":"2026-01-28","externalUrl":null,"permalink":"/en/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"2026-01-28","externalUrl":null,"permalink":"/en/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2026-01-28","externalUrl":null,"permalink":"/en/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"Recently, I received some user feedback, and I immediately enhanced the Explain Error Plugin, adding two very practical features:\nSupport for specifying the language of explanation content output Support for obtaining AI return values in Pipeline This update makes the plugin more flexible and user-friendly in multilingual support and automated usage scenarios.\nI. Support for Specifying the Language of Explanation Content Output # Background and Feature # In previous versions, the error explanation generated by the plugin was in English by default, which was not very user-friendly for non-English speakers.\nThrough PR #76, I added an optional language parameter to explainError(), allowing users to specify the language in which they want to receive the error explanation.\nThis means you can have AI explain Jenkins build failure logs in Chinese, Japanese, or other languages, better serving team members from different linguistic backgrounds.\nHow to Use # You can use it in a Jenkins Pipeline like this:\npipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;make build\u0026#39; // Simulate build process } } } post { failure { // Specify language as Chinese explainError(language: \u0026#39;中文\u0026#39;) } } } With the above configuration, when a build fails, the plugin will pass the language parameter to the AI, which will then generate the error explanation content in the specified language.\nII. Support for Return Values for Programmatic Use in Pipeline # Background and Feature # In some usage scenarios, users not only want to see the error explanation content in the Jenkins UI but also want to be able to obtain this content within the Pipeline script for further automated processing, such as sending notifications, emails, or pushing to chat tools.\nThrough PR #77, the plugin added return value support: now the explainError() step can return a string result, making it convenient to save and use within a Pipeline.\nHow to Use Return Values # Example Pipeline:\npipeline { agent any stages { stage(\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;make build\u0026#39; } } } post { failure { script { // Get the explanation content def explanation = explainError(language: \u0026#39;中文\u0026#39;) echo \u0026#34;AI Explanation Result: ${explanation}\u0026#34; // You can do more automated processing here // e.g., send to Slack or email } } } } This way, you can not only see the AI-generated explanation content on the Jenkins page but also seamlessly integrate it into your CI/CD workflow.\nSummary # Feature Description 🌐 Multilingual Support Allows AI explanation content to be returned in a specified language (Chinese, Japanese, etc.) via the language: parameter. 🔁 Return Value Support Explanation content can be returned as a Pipeline step value for automated processing. These two new features make Explain Error Plugin even more practical in terms of internationalization and automation, and closer to real-world CI/CD usage scenarios.\nWelcome to update to the latest version to experience these new features. If you have any suggestions or feedback, you are also very welcome to raise them on GitHub. Happy CI/CD! 🚀\n","date":"2026-01-28","externalUrl":null,"permalink":"/en/posts/2026/explain-error-plugin/","section":"Posts","summary":"Recently, I received some user feedback, and I immediately enhanced the Explain Error Plugin, adding two very practical features: support for specifying the language of the explanation content output and support for obtaining AI return values in Pipeline.","title":"Explain Error Plugin Updated Again—Two Practical New Features from User Feedback","type":"posts"},{"content":"","date":"2026-01-28","externalUrl":null,"permalink":"/en/tags/jenkins/","section":"Tags","summary":"","title":"Jenkins","type":"tags"},{"content":"","date":"2026-01-28","externalUrl":null,"permalink":"/en/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"Blowfish has full support for Hugo taxonomies and will adapt to any taxonomy set up. Taxonomy listings like this one also support custom content to be displayed above the list of terms.\nThis area could be used to add some extra descriptive text to each taxonomy. Check out the advanced tag below to see how to take this concept even further.\n","date":"2026-01-28","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"Blowfish has full support for Hugo taxonomies and will adapt to any taxonomy set up. Taxonomy listings like this one also support custom content to be displayed above the list of terms.\n","title":"Tags","type":"tags"},{"content":" Hi 👋, I\u0026rsquo;m Xianpeng # DevOps \u0026amp; Build Engineer | Python Enthusiast | Open Source Maintainer\n","date":"2026-01-28","externalUrl":null,"permalink":"/en/authors/shenxianpeng/","section":"Authors","summary":"Hi 👋, I’m Xianpeng # DevOps \u0026 Build Engineer | Python Enthusiast | Open Source Maintainer\n","title":"Xianpeng Shen","type":"authors"},{"content":"","date":"2026-01-27","externalUrl":null,"permalink":"/en/tags/aiops/","section":"Tags","summary":"","title":"AIOps","type":"tags"},{"content":"","date":"2026-01-27","externalUrl":null,"permalink":"/tags/clawdbot/","section":"标签","summary":"","title":"ClawdBot","type":"tags"},{"content":"最近在 X 上看到一个非常有意思的现象：一大批开发者和技术爱好者，竟然在疯狂抢购 Mac Mini。\n这事儿听起来挺反直觉的。毕竟在这个万物皆云的时代，大家都在追求轻量化、去本地化，怎么突然间开始往家里搬小盒子了？这就是今天要讲的一个开源项目叫 Clawdbot。\n有人为了跑它，专门配了一台“永不关机”的 Mac Mini 有人把它称为“真正的数字管家”； 甚至还有人因为它，把 Cloudflare 的股价拉高了一截。 这种狂热，让我想起了当年刚看到大模型时的那种兴奋。\nClawdbot 本质上是一个以“即时通讯”为核心的开源 AI 助手网关。它把我们日常用的 WhatsApp、Telegram、iMessage 等通讯工具，与 Anthropic 的 Claude 或 OpenAI 的 GPT 直接挂钩。\n它打破了“对话即目的地”的旧模式。以前我们用 AI，得专门打开一个网页，输入一段提示词，然后等待结果。但 Clawdbot 让助手走进了你通信 App，你像发微信一样给它发指令，它在后台处理好后，直接在原对话框回你。\n拆解 Clawdbot 的逻辑 # 为了让你看清这个“小机器人”是怎么运作的，我把原文中那个流传甚广的架构图翻译成了中文。它的核心逻辑其实非常清晰：\n即时通讯应用（WhatsApp/Telegram/iMessage 等） ⇄ Clawdbot “网关” ⇄ AI 模型（Claude/GPT） + 外部工具。\n在传统的 AI 对话里，逻辑是线性的。但在 Clawdbot 的网关模式下，它变成了一个调度中心：\n消息入口： 它可以接入你所有的收件箱，把零散的对话统一起来。\n逻辑大脑： 这里的网关不仅是转发消息，它还有“持久化记忆”。它记得你上周说了什么，知道你的工作习惯。\n执行末梢： 这也是它最迷人的地方。通过连接插件，它能操作你的浏览器、读取你的邮件、更新你的日历，甚至执行一段代码。\n它不再是一个只能“说”的聊天机器人，而是一个能“做”的数字员工。 这种从“对话”到“代理（Agent）”的转变，正是大家突然对它上瘾的原因。\n为什么大家都在抢 Mac Mini？\n回到那个疯狂买硬件的话题。很多人不解，跑个开源项目，租个 5 刀一个月的 VPS 虚拟机不香吗？\n其实，这里面藏着一种对“永远在线”和“本地控制”的执念。\nClawdbot 的核心卖点之一是主动性（Proactivity）。 传统的助手是“你不问，它不答”。但 Clawdbot 因为是跑在本地或独立服务器上的“背景服务”，它可以根据预设的逻辑，在早上 8 点主动发消息告诉你今天的日程摘要，或者在检测到某个网页更新时第一时间私信你。\n这种“主动敲门”的功能，需要一个 24 小时待命的环境。而 Mac Mini 这种功耗极低、性能强劲的小机器，成了承载这个“数字灵魂”的最佳容器。\n更深层的原因在于数据主权的回归。 当你把 AI 助手跑在自己的机器上，所有的路由逻辑、集成权限和对话历史都握在自己手里。虽然模型调用还是通过 API，但那个“助手的大脑”是在你的书房里跳动。\n从 Clawdbot 到 Moltbot：进化的阵痛\n就在这两天，这个项目还上演了一出“更名大戏”。\n因为名字里带了“Clawd”（和 Claude 谐音），Anthropic 公司出于商标保护发了一封措辞委婉的邮件。于是，这个项目原地起跳，直接更名为 Moltbot。\n这个名字取得很有深意。“Molt”的意思是脱壳。开发者们把这个过程比作龙虾的成长——为了变得更强大，必须褪去旧的外壳。\n这其实也反映了当前 AI 生态的一种现状：大厂在后面追赶商标，开源社区在前面疯狂迭代。 这种冲突非但没有让热度消退，反而让社区产生了一种“龙虾宇宙”的文化认同感。大家不仅在讨论技术，还在讨论一种新的、不受大厂完全掌控的数字生存方式。\n狂热背后的冷静思考\n虽然我非常看好这种“网关式”助手的未来，但作为一名老程序员，我也得泼盆冷水。\n安全，是这类工具最大的软肋。\n安全研究员 Jamieson O’Reilly 最近就发出了警告。他发现网上有很多公开暴露的 Clawdbot 控制服务器，因为配置不当，有的甚至直接把 API 秘钥、私密对话甚至是服务器的执行权限拱手让人。\n当你给一个 AI 赋予了操作你电脑、读取你邮件的权力，你就相当于把家里的钥匙交给了它。 如果这个“管家”自己忘了锁门，后果是不堪设想的。\n这让我想起一句话：便利性往往是安全性的天敌。 我们在追求 AI 24 小时随叫随到的爽感时，绝不能忽视底层架构的防御。对于普通用户来说，如果你没有基本的服务器维护和安全配置能力，直接跟风上马这种工具，其实是把自己暴露在荒野之中。\n写在最后\nClawdbot（或者说 Moltbot）的火爆，其实揭示了我们对 AI 的终极期待。\n我们不想要一个只会写诗、写代码的“文弱书生”，我们需要的是一个能融入日常工作流、有记忆、能执行、且触手可及的“行动派”。\nAI 正在从一个“目的地”，变成一种类似水电煤的“基础设施”。 这种演进不再仅仅取决于模型的参数量，更取决于它连接现实世界的触角有多深。\n也许过不了多久，买个 Mac Mini 放在家里当“数字大脑”，会像当年买路由器一样普遍。但在此之前，请先学会如何给你的“管家”锁好门。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2026-01-27","externalUrl":null,"permalink":"/posts/2026/get-start-clawdbot/","section":"Posts","summary":"最近 Clawdbot 改名为 Moltbot，越来越多的人开始在家里部署这个 AI 助手。本文分享了 Moltbot 的特点、使用体验以及一些注意事项，帮助你更好地理解和使用这款工具。","title":"Clawdbot 变身 Moltbot：把 AI “养”在你的电脑里","type":"posts"},{"content":"Today, with the widespread adoption of microservices, hybrid clouds, and containerized deployments, IT systems have become exceptionally complex. When thousands of alert messages flood in, traditional operations models struggle to cope.\nAIOps (Artificial Intelligence for IT Operations), this AI-driven transformation, is becoming the \u0026ldquo;lifeline\u0026rdquo; for IT operations management. This article combines the core insights from IBM, ServiceNow, GitHub, and Red Hat to give you a complete picture of AIOps.\nI. What is AIOps? More Than Just \u0026ldquo;AI + Ops\u0026rdquo; # According to definitions from IBM and Red Hat, AIOps (Artificial Intelligence for IT Operations) is not a single product but rather a combination of capabilities.\nIt integrates big data, machine learning (ML), and natural language processing (NLP) to consolidate disparate operations tools into an intelligent platform. Its essence is to leverage AI to automate, simplify, and optimize IT Service Management (ITSM) and operations workflows.\nServiceNow points out that the core formula for AIOps is: Ingest → Analyze → Act. It transforms operations from \u0026ldquo;reactive firefighting\u0026rdquo; to \u0026ldquo;proactive prediction.\u0026rdquo;\nII. Core Value: Addressing \u0026ldquo;Data Overload\u0026rdquo; and \u0026ldquo;Cognitive Load\u0026rdquo; # Why must businesses embrace AIOps today?\nFinding Signals in the \u0026ldquo;Noise\u0026rdquo; (IBM \u0026amp; ServiceNow): Most alerts generated by modern enterprise tech stacks are repetitive or irrelevant \u0026ldquo;noise.\u0026rdquo; AIOps can intelligently filter out critical signals, identify abnormal patterns that truly impact business performance, and prevent operations staff from being drowned in a sea of alerts. Reducing \u0026ldquo;Cognitive Load\u0026rdquo; (GitHub): GitHub emphasizes that a major contribution of AIOps is reducing the mental burden on engineers. When a system fails, AI can automatically correlate related events, freeing developers from manually sifting through thousands of lines of logs, allowing them to focus more on writing high-quality code. Shortening MTTR (Mean Time To Resolution) (IBM): Through Root Cause Analysis (RCA), AIOps can pinpoint the source of a fault and suggest remedies within seconds, even achieving \u0026ldquo;self-healing\u0026rdquo; before users discover the problem. III. The Four Core Components of AIOps # Combining various perspectives, a complete AIOps architecture includes:\nData Aggregation: Ingests historical data, real-time metrics, system logs, network packets, and service tickets. Machine Learning Algorithms: Utilizes supervised and unsupervised learning for anomaly detection, event correlation, and trend prediction. Automation and Orchestration: Automatically triggers scaling, backup, or remediation scripts based on analysis results. Visual Interaction: Provides teams with a global view across environments (hybrid/multi-cloud) through intuitive dashboards. IV. AIOps vs. DevOps: Competition or Collaboration? # Many people worry that AIOps will replace DevOps. However, GitHub and IBM believe they are complementary.\nDevOps focuses on the speed and collaboration (CI/CD pipelines) of the software development lifecycle. AIOps focuses on the stability and efficiency of the production environment. When combined, AIOps provides DevOps teams with the necessary visibility, allowing them to continually change infrastructure without worrying about systems going out of control.\nV. Implementation Strategy: Domain-Agnostic vs. Domain-Centric (IBM Perspective) # When implementing AIOps, enterprises face two choices:\nDomain-agnostic: Collects data from across all domains (network, storage, security), providing a global view, suitable for solving complex, cross-departmental problems. Domain-centric: Focuses on specific scenarios (e.g., specifically for network protocols). Its models are highly targeted and can accurately distinguish between \u0026ldquo;a DDoS attack or a configuration error.\u0026rdquo; Red Hat suggests that enterprises should gradually introduce these capabilities based on the complexity of their hybrid cloud, rather than expecting an overnight solution.\nVI. The Future is Now: From Proactive Response to Predictive Operations # ServiceNow mentions in its insights that the ultimate goal of AIOps is predictive operations.\nThrough continuous learning, AI can discover: \u0026ldquo;Every time the CPU exhibits a certain fluctuation pattern, the database will inevitably crash in 10 minutes.\u0026rdquo; Based on this prediction, the system can intervene proactively.\nRed Hat reminds us that in the era of AIOps, data quality is paramount. Only by training transparent, fair AI models and maintaining human-in-the-loop oversight can a truly trustworthy intelligent operations system be established.\nConclusion # As GitHub states, the goal of AIOps is not to replace humans, but to enhance their ability to handle complexity. In this era of rapidly evolving IT architectures, AIOps will become the \u0026ldquo;digital hub\u0026rdquo; for enterprise digital transformation.\nHas your team started exploring AIOps? Feel free to share your thoughts in the comments section.\nReferences: IBM Think Topics, ServiceNow, GitHub Articles, Red Hat Topics.\n","date":"2026-01-27","externalUrl":null,"permalink":"/en/posts/2026/aiops/","section":"Posts","summary":"Today, with the widespread adoption of microservices, hybrid clouds, and containerized deployments, IT systems have become exceptionally complex. When thousands of alert messages flood in, traditional operations models struggle. AIOps (Artificial Intelligence for IT Operations), an AI-driven transformation, is emerging as the “lifeline” for IT operations management. This article combines key insights from IBM, ServiceNow, GitHub, and Red Hat to provide a comprehensive overview of AIOps.","title":"What is AIOps—A Systematic Introduction to Intelligent Operations","type":"posts"},{"content":" 回家，好像按下了快进键 # 12 月 18 日落地大连，身体回来了，精神还在路上。最初几天很简单：吃饭、睡觉、倒时差，去商场、营业厅、超市，顺便回爸妈那吃饭。生活一下子就落回了熟悉的轨道。\n接下来的日子，每天都在陪家人、带孩子、见朋友的循环里度过。日子很碎，但很满。\n那些琐碎却满满的日子 # 带娃去姐姐家，老婆去听演唱会； 带妈去医院，在万达偶遇同一天出国回来的同事； 和同事游泳、吃饭、去滨海路、喝咖啡、聊工作、聊技术、聊 AI； 洗大澡、逛优衣库、去爸妈、丈母娘家吃饭； 聚餐、踢球、看电影、逛宜家、去迪卡侬…… 这些事情单拎出来都普通，但密集连在一起，就形成了“回家才有的生活节奏”。 不用计划，不追效率，顺着来就好。想去哪就去哪，谁有空就见谁。\n中间几天，孩子感冒发烧，需要去医院、拿药、观察，一连几天，生活的重心只剩一件事：希望孩子快快好起来。好在慢慢恢复了。\n难忘的片段 # 带娃和爸妈出去溜达，回来他们提前下车要走回家，想起上一次出国告别的一模一样的情景，心里一阵酸楚 一天独自带娃开车去爸妈那，路上听着歌，孩子在后座睡着，那一刻特别安静，也特别满足 一家人周日待在爸妈家，从中午到晚上，什么也没做，但时间飞快地过去了 生活就是这样，重要的事和琐碎的事，总是混在一起发生。\n最难的是分别 # 1 月 23 日下午，去爸妈那吃饭。心里知道——这是最后一顿了。\n一个月来几乎每隔一天就见面，突然要结束，真的很不适应。关上门和爸妈说再见时，我还是忍不住哭了。抬头看他们在楼上窗户看着我们上车，眼眶也湿润了。\n开车回家的路上，情绪复杂。觉得时间太快，也觉得自己好像还是没陪够。\n觉得时间过得太快，也觉得自己好像还是没陪够。\n1 月 24 日，孩子的大姨、二姨一家来了，热热闹闹坐了一下午。晚上被送去机场，老婆和她姐姐抱在一起哭。\n我知道： 不只是我们在离开，他们也在经历失去。\n写在最后 # 这一场回家，没有旅行打卡，也没有宏大的计划。\n只有吃饭、聊天、逛街、看病、踢球、游泳、带娃，还有一遍遍的告别。\n正是这些看似“没什么”的日子，组成了最真实、也最珍贵的时间。\n假期结束了，我们一家人又要回到原来的生活轨道里。 但这一个多月，会在心里留下很久。\n剩下的只有一句话，反复提醒自己：\n相聚的时候，好好珍惜。\n还有，期待下一次的相聚。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2026-01-24","externalUrl":null,"permalink":"/misc/2026-vacation-notes/","section":"Miscs","summary":"记录一下 2026 年春节回国探亲的点滴感受。","title":"2026 年假期随笔：那些琐碎却满满的日子","type":"misc"},{"content":"","date":"2026-01-24","externalUrl":null,"permalink":"/tags/vacation/","section":"标签","summary":"","title":"Vacation","type":"tags"},{"content":"前几周和几位老同事聚会，聊起了一个我们以前讨论过的话题：要不要出国。\n我们几个人的背景都差不多，在外企待过，做的是技术类的工作。这几年，身边有的人已经在国内扎根，有的人有过短期的海外经历，再回头聊起这个话题时，发现彼此的想法其实都变了不少。\n如果放在几年前，我们的想法基本是一致的：有机会的话，还是想出去看看。那时候更多是一种向往，也带着一点对外面世界的期待。\n但几年过去，再聊起这件事时，反而会觉得，不出去，其实也挺好的。在国内生活，本身就已经很舒服了。\n真的想看看外面的世界，其实也不一定非得长期在外生活。攒点钱，找个时间，每年出国玩一趟，完全可以。对我们这些从外企出来的程序员来说，如果能在国内找到一份海外远程工作，但生活仍然在国内，整体体验其实会非常不错（上网除外）。当然，如果国内的企业本身节奏合适、环境也能接受，那同样是一个很好的选择。\n我现在越来越清楚的一点是，如果没有孩子，在国内会更从容一些。当然，没有孩子的话，在哪里都会相对轻松，出去转转也很好，自由度高，选择也多。\n但一旦有了孩子，很多事情的考虑逻辑就会发生变化。如果还有机会、而且孩子还比较小的话，我个人会更倾向于出去看看，尤其是在幼儿园、小学这个阶段。更多还是希望孩子能轻松一点，家庭整体的生活节奏也能慢一些。\n不过这里有一个我认为必须摆在最前面的前提：你得在国外有一份工作。除非你本身经济条件非常充裕，可以不工作，只是单纯在国外生活，那是另一种情况，对大多数普通家庭来说，并不具备太多参考意义。\n所以对大多数人来说，我并不太建议单纯为了孩子而出国，而是应该以自己的工作为中心。在这个前提下，带着孩子一起选择一个适合你、也适合整个家庭的地方。如果是在这样的条件下，有机会出国，可以出去看看的；但如果只是为了出国而出国，或者单纯为了孩子的教育而出国，我觉得还是需要多想一步。\n如果你现在是单身，或者已经结婚但还没有孩子，在国内有一份还不错的工作，其实真的挺舒服的，也没必要给自己加码。想出去玩，就攒点钱，找个机会，每年出去一趟就好。\n对我个人来说，这一年多、不到两年的出国经历，更像是一个对国外生活的“去魅”过程。我并不觉得国外有多好，也不觉得有多差，只是对这件事的理解，比以前更具体、更真实了一些。\n这就是我站在一个工程师、一个父亲、也是一个普通打工人的角度，对“要不要出国”这件事的阶段性看法。未来随着孩子慢慢长大，我的想法大概率还会继续变化，到那时候，再记录一次，也挺好的。\n偶尔记录一些和技术不直接相关、但和工程师生活有关的思考。\n（完）\n","date":"2026-01-20","externalUrl":null,"permalink":"/misc/go-abroad-or-not/","section":"Miscs","summary":"前几周和几位老同事聚会，聊起了一个我们以前讨论过的话题：要不要出国。\n","title":"几年后再聊“要不要出国”：一些阶段性的思考","type":"misc"},{"content":"这里记录每周值得分享的 DevOps 与 AI 技术内容，周五发布。\n本杂志开源，欢迎投稿。合作请邮件联系（xianpeng.shen@gmail.com）。\n本周封面 # 英伟达总裁黄仁勋在 2026 国际消费电子展上提出了\u0026quot;AI 移民\u0026quot;概念，认为机器人可以解决劳动力短缺问题。\nAI 与平台工程的深度融合 # 进入 2026 年，AI 与平台工程的融合成为技术领域最值得关注的趋势。过去，AI 在 DevOps 中主要用于智能告警和辅助仪表盘，现在它已经深度嵌入平台本身，从根本上改变基础设施的设计、操作和治理方式。\n平台工程的出现本是为了解决 DevOps 成功带来的复杂性问题，通过内部开发者平台（IDP）提供标准化的\u0026quot;铺就之路\u0026quot;。如今，AI 正在加速这一过程，让平台能够自主决策、建议行动并影响开发者行为。\n未来的领导者不是追逐每一个 AI 工具，而是理解这一转变，设计出让团队更\u0026quot;冷静\u0026quot;而非仅仅更\u0026quot;快速\u0026quot;的平台。工程师也需要从传统的自动化思维转向\u0026quot;意图理解\u0026quot;和\u0026quot;系统解释\u0026quot;，让平台从执行指令的工具进化为理解复杂系统行为的智能实体。\n行业动态 # 1、CES 2026 聚焦 AI 与机器人，消费电子步入智能新纪元\n2026 年国际消费电子展（CES）1 月 6 日在拉斯维加斯开幕，AI 和机器人成为焦点。从 AI 家用机器人到智能家居，从显示技术到边缘计算，AI 正在成为消费电子的核心。LG 展示了\u0026quot;情感智能\u0026quot;机器人，三星、索尼等也推出了 AI 新品。NVIDIA 发布了 G-Sync Pulsar 显示技术和 DLSS 4.5，Intel 推出 Panther Lake AI 芯片，摩托罗拉发布搭载 AI 的可折叠手机 Razr Fold。\n2、平台工程市场正处于高速增长期，预计 2035 年达到约 473 亿美元规模\n根据最新报告，全球平台工程服务市场将从 2025 年的 57.6 亿美元增长到 2035 年的 473.2 亿美元，复合年增长率达 23.4%。平台工程已从新兴概念发展为软件开发的核心。目前 55% 的组织已采纳平台工程实践，通过内部开发者平台（IDP）提升生产力、管理系统复杂性。\n3、OpenAI 发布 AI 在医疗健康领域的报告，强调 AI 作为医疗盟友的潜力\nOpenAI 在 1 月发布了 AI 医疗健康报告，数据显示全球超过 5% 的 ChatGPT 消息与医疗相关，每周有四分之一用户咨询医疗问题。报告强调 AI 在加速科学发现、强化医疗基础设施、支持医护人员等方面的潜力。OpenAI 计划发布完整的医疗保健政策蓝图，通过安全连接全球医疗数据，加速药物研发和治疗创新。\n深度阅读 # 1、当AI编写几乎所有代码时，软件工程会发生什么？\nPragmatic Engineer 创始人 Gergely Orosz 的开年长文，探讨 AI 编码工具的颠覆性影响。文章记录了多位大佬的\u0026quot;顿悟时刻\u0026quot;：Andrej Karpathy 从批评 AI 工具为\u0026quot;垃圾\u0026quot;到承认\u0026quot;从未如此落后\u0026quot;；Boris Cherny 一个月内提交的代码全由 AI 生成；Malte Ubl 断言\u0026quot;软件生产成本趋向于零\u0026quot;。\n对软件工程师来说，2026 年最重要的技能不再是\u0026quot;写代码\u0026quot;，而是\u0026quot;指导 AI 写代码\u0026quot;。\n2、2026年开发者工作流的25款AI编码工具\nDevin Rosario 在 Medium 上详细介绍了 25 款 AI 编码工具。文章指出，软件开发已从\u0026quot;AI 辅助\u0026quot;进入\u0026quot;AI 编排\u0026quot;时代，高级工程师现在管理自主代理来处理复杂重构和安全补丁。\n重点提到了 Cursor 这个 AI 原生代码编辑器，支持跨文件编辑和重构，以及 Cody AI 结合代码搜索和 AI 理解的能力。\n3、塑造2026年的6大软件开发和DevOps趋势\nBoris Zaikin 在 DZone 概述了 2026 年软件开发和 DevOps 的六大趋势：AI 代理、语义层、平台工程、供应链安全、可观测性和 FinOps。重点是平台工程 2.0 向 AI 就绪平台的演进，以及供应链安全成为 DevSecOps 新基线。\n4、OpenTelemetry能否拯救2026年的可观测性？\nThe New Stack 聚焦 2025 年爆发的可观测性危机：84% 的企业被遥测成本和复杂性困扰。当前可观测性面临成本失控、复杂度爆炸、供应商锁定三大问题。\nAI 在其中扮演双重角色：一方面 AI 工作负载带来更多监控复杂性，另一方面 AI 技术能帮助降低数据量和成本。\n5、容器vs虚拟机:2026年家庭实验室的转变\nVirtualization Howto 观察到一个有趣趋势：家庭实验室（Home Lab）爱好者正从虚拟机转向容器优先架构。驱动因素包括资源限制（AI 模型和大型应用消耗大量 RAM，虚拟机开销难以承受）、快速部署（容器秒级启动 vs 虚拟机分钟级）、以及 Kubernetes 技能成为职场必备。\n关键观察：许多人开始用单个虚拟机运行 Kubernetes，然后在 K8s 上部署所有应用作为容器，而不是为每个服务创建单独虚拟机。这种方式既保持隔离性，又最大化资源利用率。\n效率工具 # 1、6 个轻量级 Docker 容器，每周节省数小时\nXDA Developers 推荐的 6 个超轻量级 Docker 容器，通过自动化日常任务提升生产力。\n这些容器体积小（几十 MB）、资源占用低、启动快，适合在家庭服务器或 VPS 上长期运行。\n2、4 个 Docker 容器通过 Chrome 扩展增强功能\nXDA Developers 介绍了 4 个自托管 Docker 容器，通过浏览器扩展实现更好的工作流集成。这些工具的优势是数据完全自主可控，避免隐私风险和订阅费用，同时保持与云服务相当的体验。\n3、Valkey 发布官方 Kubernetes Helm Chart\nValkey（Redis 的开源分支）发布官方 Kubernetes Helm Chart。这是继 Bitnami 宣布政策变更后，社区维护的官方版本。支持 Standalone、Replicated、Sentinel 等部署模式，提供 ACL 和 TLS 加密、Prometheus metrics 集成、持久化存储配置。\n4、n8n\nn8n 是开源工作流自动化工具，类似 Zapier 和 Integromat，但更灵活且可自托管。支持超过 200 种应用集成，通过可视化界面创建自动化工作流。\nAI 相关 # 1、Meta 的\u0026quot;Conversation Focus\u0026quot;\nMeta 在 2025 年 12 月为 Ray-Ban Meta 和 Oakley Meta HSTN 智能眼镜发布的 v21 更新功能。利用 AI 音频处理技术，在嘈杂环境中放大你正在看的人的声音，同时抑制背景噪音，解决\u0026quot;鸡尾酒会问题\u0026quot;（在嘈杂环境中选择性聆听特定人说话的能力）。\n学习资源 # 1、如何构建个人 Python 学习路线图\nReal Python 的系统性指南，教你制定个人化 Python 学习计划。基于 Dominican 大学 Gail Matthews 博士的目标设定研究，提供三个步骤。核心观点：\u0026ldquo;为什么\u0026quot;比\u0026quot;做什么\u0026quot;更重要，没有强烈目的感很难在遇到困难时坚持。\n提供免费 PDF 学习路线图模板，可打印填写。\n2、2026 年 1 月最佳免费 AI 培训课程\nTech.co 整理的 2026 年 1 月最佳免费 AI 培训课程清单，涵盖从生成式 AI 基础到提示工程、机器学习和大型语言模型（LLMs）实际应用。包括杜克大学的\u0026quot;GenAI 基础 – LLMs 如何工作\u0026quot;和 Udemy 的\u0026quot;AI 基础：从基础到生成式 AI\u0026quot;等课程。\n3、2026 年工程师应学习的顶级 AI 技能 Morson Jobs 列出了 2026 年工程师最应学习的 AI 技能，包括 Python 熟练度、LLM 微调、MLOps、机器学习、深度学习和数据分析。强调提示工程、数据和自然语言处理（NLP）工程、云 AI 平台、计算机视觉以及 AI 伦理等关键领域。AI 正在重新定义\u0026quot;优秀\u0026quot;的工程实践，工程师需要学会智能地应用 AI 提高决策质量和减少重复工作。\n精彩摘要 # 1、\u0026ldquo;我从未感到作为程序员如此落后。这个职业正在被大幅重构，程序员贡献的代码越来越稀疏。我有种感觉，如果我能恰当地串联起过去一年所出现的工具，我可以强大 10 倍，而未能获得这种提升明显是一个技能问题。\u0026rdquo; —— Andrej Karpathy，OpenAI 联合创始人\n2、\u0026ldquo;上个月是我作为工程师的第一个月，完全没有打开 IDE。Opus 4.5 写了大约 200 个 PR，每一行代码都是它写的。软件工程正在发生根本性变化，即使对于像我们这样的早期采用者和从业者来说，最困难的部分是持续重新调整我们的期望。而这仍然只是开始。\u0026rdquo; —— Boris Cherny，Claude Code 创建者\n3、\u0026ldquo;软件生产的成本正在趋向于零。\u0026rdquo; —— Malte Ubl，Vercel CTO\n行业观点 # 1、你不能因为充斥着“垃圾内容”和令人尴尬的产出，就否认 AI 所带来的奇迹。无论这些噪音多么刺耳，这仍然是自我们把计算机连接到互联网以来，最令人兴奋的一次技术飞跃。如果你在 2025 年对 AI 感到悲观或怀疑，也许可以在 2026 年，用一点乐观与好奇重新审视它。 —— DHH，Ruby on Rails 创建者\n从怀疑到拥抱，DHH 的态度转变代表了许多资深开发者的心路历程。\n2、\u0026ldquo;AI 不会终结工程职业，它正在对它们进行分类。\u0026rdquo; —— Morson Jobs\n这个观点准确概括了当前趋势：AI 不是取代工程师，而是重新定义\u0026quot;优秀工程师\u0026quot;的标准。那些能够有效利用 AI 工具、具备产品思维、掌握架构能力的工程师将更有价值，而仅仅是\u0026quot;代码编写者\u0026quot;的角色将逐渐消失。\n（完）\n","date":"2026-01-09","externalUrl":null,"permalink":"/posts/2026/weekly-2/","section":"Posts","summary":"这里记录每周值得分享的 DevOps 与 AI 技术内容，周五发布。本杂志开源，欢迎投稿。","title":"攻城狮周刊（第 2 期）：AI 与平台工程的深度融合，重塑开发者未来","type":"posts"},{"content":"这里记录每周值得分享的 DevOps 与 AI 技术内容，周五发布。\n本杂志开源，欢迎投稿。合作请邮件联系（xianpeng.shen@gmail.com）。\n本周封面 # 看龙家昇与他的精灵们，如何把怪异化作可爱，更亲揭 Labubu 九颗尖牙的设计巧思！\nAI 工程化与 DevOps 韧性 # 过去一周，技术领域最引人注目的趋势无疑是人工智能在工程实践中的深度融合，以及 DevOps 实践中对系统韧性的前所未有的关注。2025 年见证了 AI 模型从实验性工具转变为生产关键基础设施，这迫使 DevOps 团队开始像管理数据库一样思考 AI 模型的延迟、成本和服务等级协议（SLA）。谷歌发布的 Gemini 3 Flash 模型，以及 Z.ai 的 GLM-4.7 和 MiniMax 的 M2.1 等模型的更新，进一步推动了 AI 在编码、任务自动化和多语言编程中的应用，预示着 AI 优先的云平台将成为 2026 年的主导趋势。\n与此同时，随着 AI 驱动的交付加速了发布周期和产出量，DevOps 团队也面临着新的挑战：如何在追求速度的同时，确保软件部署的安全性与可靠性。2025 年的经验表明，基础设施的脆弱性通过连锁中断暴露无遗，促使企业重新审视其技术堆栈的韧性。平台工程和内部开发者平台（IDP）的兴起，旨在通过提供标准化、自服务的“铺设之路”，减少手动工作，并为安全与合规提供统一的控制平面，从而提升整体韧性。2026 年，DevOps 的成功将不再仅仅取决于软件交付的速度，更将取决于系统应对持续变化和需求的能力，韧性将成为新的衡量标准。\n行业动态 # 1、AI 模型进入生产核心：成本与性能成为关键考量\n2025 年末，AI 模型已不再是实验性工具，而是生产环境中的关键基础设施。谷歌的 Gemini 3.0 和 OpenAI 的 GPT 5.2 等大型语言模型（LLM）的发布，使得 DevOps 团队必须关注其延迟、请求成本、SLA 和供应商锁定等问题。例如，Gemini 3.0 的平均延迟为 2.1 秒，每月 100 万次请求的成本为 4.8 万美元，而 GPT 5.2 的平均延迟为 4.8 秒，成本则高达 33 万美元。这标志着 AI 已从研究问题转变为基础设施经济学问题。\n工程师视角的点评： 这对 DevOps/AI 工程师意味着，LLM 的成本优化将像云成本优化一样成为常态。团队可能需要根据任务需求运行多个模型（例如，Gemini 用于高吞吐量，GPT 用于推理密集型任务），并且 LLM 可观测性和成本跟踪工具将迅速成熟。\n2、Kubernetes 成为 AI 工作负载的事实标准\n到 2026 年，最繁重的 AI 工作负载，特别是机器学习操作（MLOps）平台，将以 Kubernetes 为骨干。Kubernetes 提供了统一的控制平面来调度、扩展和管理 AI 组件，无论是突发性的资源密集型任务（数据处理和训练）还是高吞吐量的持续运行服务（实时推理）。这一趋势反映了 GPU 密集型工作负载的快速增长以及最大化昂贵加速器硬件利用率的需求。\n3、Google Cloud 与 Palo Alto Networks 达成百亿美元安全合作\n2025 年 12 月 22 日，Google Cloud 和 Palo Alto Networks 宣布了一项近 100 亿美元的合作，这是历史上最大的云安全交易。Palo Alto 将其工作负载迁移到 GCP，并利用 Gemini AI 为其安全协处理器提供支持。\n4、其他\n（1）RondoDox Botnet 利用 React2Shell 漏洞\n网络安全研究人员披露了一个持续九个月的 RondoDox 僵尸网络活动细节，该僵尸网络利用最近披露的 React2Shell（CVE-2025-55182，CVSS 评分：10.0）漏洞劫持 IoT 设备和 Web 服务器。\n（2）Stack Overflow 2025 开发者调查：开发者对 AI 仍持谨慎态度\nStack Overflow 于 2025 年 12 月 29 日发布了开发者调查结果，显示开发者仍然愿意使用 AI，但AI工具的积极评价在2025年呈现下降趋势。\n深度阅读 # 1、2025 年 DevOps 年度回顾：五大基础设施转变及其对 2026 年的意义（英文）\n这篇文章回顾了 2025 年 DevOps 领域的五大基础设施转变，包括 AI 模型进入生产、云安全重塑、平台工程成为必需、eBPF 的普及以及厂商锁定的回归。文章深入分析了这些转变对 2026 年的影响，强调了 LLM 成本优化、云安全作为差异化因素以及平台工程的重要性。\n2、现代 DevOps CI/CD 管道内部：自动化、工具和最佳实践（英文）\n这篇发布于 2025 年 12 月 31 日的文章探讨了现代 DevOps CI/CD 管道的工作原理、涉及的工具以及构建可扩展和可靠管道的最佳实践。它强调了持续反馈循环、模块化管道设计、左移安全以及微服务架构下的独立部署等关键要素。\n3、MLOps：2025 年部署和监控机器学习模型（英文）\n该文章聚焦于 2025 年 MLOps 在企业环境中部署和监控机器学习模型，强调了可扩展性和可靠性。它介绍了 ModelBit 3.2 和 Control Plane 等 MLOps 工具，以及实施最佳实践和现实世界应用。\n效率工具 # 1、LangChain\n一个开源框架，用于连接大型语言模型与外部数据源，简化和加速 AI 应用和代理的开发。LangChain 1.0 于 2025 年 10 月 22 日发布了第一个主要稳定版本。\n2、GoReleaser\n自动化 Go 项目的打包、发布和分发，帮助开发者更快、更轻松地交付软件。\n3、Uptime Kuma\n一个免费的开源服务器监控工具，支持容器化部署。\nAI 相关 # 1、A2UI：Google 的代理驱动界面开放项目\nGoogle 于 2025 年 12 月 15 日公开了 A2UI 项目，这是一个用于代理驱动界面的开放项目。A2UI 旨在解决可互操作、跨平台、生成式或基于模板的代理 UI 响应的特定挑战，允许代理生成最适合当前对话的界面，并发送到前端应用程序。\n2、Moondream：小巧而强大的视觉语言模型\nMoondream 是一个开源的视觉语言模型，体积小巧（1GB），性能卓越，无需 GPU 即可在笔记本电脑到边缘设备上运行。它允许开发者使用自然语言提示来标注图像、检测物体、跟踪视线、阅读文档等。该项目在 GitHub 上拥有数千颗星。\n学习资源 # 1、2025 年 DevOps 学习资源路线图\nKodeKloud 提供的 2025 年 DevOps 学习路线图，包含大量免费实验室、课程和指南。它涵盖了从 Linux 基础到 Git、脚本、容器、Kubernetes、IaC、CI/CD、云平台以及监控、安全和可观测性等全面的 DevOps 技能。\n精彩摘录 # 1、2025 年 DevOps 年度回顾\n“如果科技界的一年感觉像十年，那么 2025 年就是整整十年。DevOps 团队目睹了 AI 模型从实验性辅助工具变为生产关键基础设施，经历了历史上最大的云安全合作，并不得不弄清楚哪些‘下一个大事件’真正重要。当我们结束 2025 年时，以下是诚实的总结：真正改变我们构建、部署和操作系统的 5 大基础设施转变——以及它们对 2026 年的意义。”\n行业观点 # 1、\n“AI 现在是基础设施：它不再是实验性的。DevOps 团队拥有 LLM 的成本、延迟和 SLA。云安全再次变得重要：GCP 的 100 亿美元赌注证明了这一点。平台工程不是可选的：如果开发者还在与 Kubernetes YAML 搏斗，你就落后了。eBPF 赢了：网络、可观测性和安全都在 eBPF 上运行得更好。在 2026 年学习它。锁定又回来了：‘多云一切’的梦想已经破灭。”\n\u0026ndash; Rick Whiting 在 CRN 上的评论，总结 2025 年的开源工具趋势。\n2、\n“2026 年，DevOps 的未来不会根据软件交付的速度来评判，而是根据其运行的可靠性来评判。韧性是新的速度——而运行时是其获得之处。”\n\u0026ndash; Marcus Holm 在 Tech Monitor 上的评论，关于 2026 年 DevOps 的转变。\n（完）\n","date":"2026-01-01","externalUrl":null,"permalink":"/posts/2026/weekly-1/","section":"Posts","summary":"这里记录每周值得分享的 DevOps 与 AI 技术内容，周五发布。本杂志开源，欢迎投稿。","title":"攻城狮周刊（第 1 期）：AI 工程化与 DevOps 韧性：2026 年技术发展双主线","type":"posts"},{"content":"","date":"2025-12-26","externalUrl":null,"permalink":"/en/tags/github/","section":"Tags","summary":"","title":"GitHub","type":"tags"},{"content":"First, a disclaimer: I am not an AI expert myself. The following content is my understanding, summarized from official documentation and practical experience. I hope it helps everyone avoid detours and clearly understand GitHub AI\u0026rsquo;s overall layout and individual functions.\nIn the past year, AI terms on GitHub have been proliferating:\nModels、Agents、Spaces、Spark、Instructions、Agent Skills、MCP…… The names are similar, but many people get confused at first glance.\nMany people ask:\nWhat is Copilot? What\u0026rsquo;s the difference between Agents and Models? Is Spark a low-code tool? What are Skills? What exactly does MCP do?\nToday, I\u0026rsquo;ll help you clarify these concepts using a familiar approach of fact-based explanations + analogies.\nI. Clarify Core Concepts First # Let\u0026rsquo;s start with GitHub\u0026rsquo;s official documentation.\nWhat are GitHub Models # The official documentation states:\nGitHub Models provides a model catalog, prompt management, and evaluation tools, allowing you to compare models, fine-tune prompts, evaluate performance, and integrate models directly within GitHub. It aims to lower the barrier for enterprise-grade AI adoption and integrate seamlessly with GitHub workflows.(GitHub Docs)\nIn other words:\nModels are a \u0026ldquo;model resource library + debugging and evaluation toolset,\u0026rdquo; not a specific AI product. It helps you within GitHub to:\nSearch for different large models Debug and compare prompt outputs Evaluate which model is more suitable for your task Therefore, GitHub Models is not Copilot, nor is it an Agent; it is an \u0026ldquo;experimental/integration platform for AI models.\u0026rdquo;\nII. Let\u0026rsquo;s use an analogy: AI is not a single tool, but like an AI-powered software company # To better understand the following terms, let\u0026rsquo;s use an analogy:\nLet\u0026rsquo;s liken these AI concepts to the internal organizational structure and tools of a software company undergoing full AI transformation.\nIII. Explaining Them One by One # 1️⃣ Copilot: The \u0026ldquo;AI Assistant\u0026rdquo; sitting next to your IDE # This needs little introduction:\nIt provides you with code completion Understands context Can suggest as you write in the IDE The official offering has also evolved over the years:\nAdded multi-model support (Anthropic Claude, Google Gemini, various OpenAI models, etc.) (The Verge) Introduced a stronger Agent mode (capable of executing commands) (Wikipedia) So Copilot fundamentally remains:\nA real-time code assistant serving human developers (AI next to your IDE)\n2️⃣ Agents: \u0026ldquo;AI Employees\u0026rdquo; that can be assigned tasks # The Agent here cannot be simply understood as \u0026ldquo;a mode of Copilot.\u0026rdquo; It carries a stronger semantic meaning:\nGitHub news reports (Agent HQ) introduce:\nGitHub will allow developers in the future to create and manage multiple AI agents from different models and services, unifying scheduling, executing tasks, and even running them in parallel, then choosing the most suitable results. (The Verge)\nThus, Agent ≠ Copilot\u0026rsquo;s basic completion; it is closer to:\nAn AI assistant that can be assigned tasks and perform specific work (complete tasks).\nFor example:\nAutomatically create PRs Batch process files Run tests Automatically respond to Issues You tell it \u0026ldquo;go do the work,\u0026rdquo; it executes, and can even form a task queue.\nThis concept also appears in community tools, where many people use Agents for automatic PRs, monitoring pipelines, etc. This mode is more \u0026ldquo;automated\u0026rdquo; than Copilot\u0026rsquo;s traditional completion.\n3️⃣ GitHub Spaces: The \u0026ldquo;Dedicated Workspace\u0026rdquo; for AI and Context # This is one that many people find most confusing:\nOfficial documentation clarifies:\nSpaces provide a \u0026ldquo;space\u0026rdquo; that you can access, where you put together the context you want to give to the AI. When you use Copilot in your IDE, this context is loaded in the background. (GitHub Docs)\nMany people mistakenly think Spaces are a type of Repo, but they are not.\nIt\u0026rsquo;s more like:\nA \u0026ldquo;collaboration space\u0026rdquo; specifically prepared with context, data, and documentation, allowing the AI to better understand what you want to do.\nFor example:\nIf you\u0026rsquo;re working on a new feature, putting requirements documents, design mockups, and relevant Markdown into a Space, the AI has this complete context and can naturally respond more accurately.\nSo Spaces = Context + Knowledge Hub, not a model or Agent itself.\n4️⃣ GitHub Spark: The \u0026ldquo;Project Sandbox\u0026rdquo; for Rapid Prototyping # GitHub Spark is a newly launched GitHub product (currently in preview):\nAccording to community messages:\nSpark allows Copilot Pro+ users to generate full-stack applications using natural language, going directly from idea to deployment, automatically handling code, configuration, permissions, etc., and supporting in-code and subsequent extensions. (Reddit)\nIn other words:\nSpark is a platform that automatically transforms \u0026ldquo;ideas → working products\u0026rdquo;. It packages various models, workflows, and deployment mechanisms, allowing you to see results just by writing a single requirement.\nThis goes a step further than traditional Copilot completion or Agent execution — it is an all-in-one rapid prototyping workshop.\n5️⃣ GitHub Instructions / Repository Custom Instructions (Official Feature) # This is a feature appearing in GitHub Docs:\nYou can write Custom Instructions in your Repo to guide Copilot and Agents:\nTell it:\nYour code standards Output style Business constraints Specific task requirements In other words:\nInstructions = \u0026ldquo;Behavioral Guidelines/Manual\u0026rdquo; written for AI To make its responses more aligned with project requirements.\nThis differs from Copilot\u0026rsquo;s \u0026ldquo;prompt\u0026rdquo;; it\u0026rsquo;s more like a set of long-term effective project habit guidelines.\n6️⃣ Agent Skills / Claude Skills: Callable Capability Modules # There are two concepts here:\nGitHub Agent Skills (Official) Claude Skills (Anthropic community ecosystem) Official explanation:\nAgent Skills are a set of instructions, scripts, and resources placed in .github/skills that are loaded by Copilot when needed to improve task performance. (GitHub Docs)\nAnd Claude Skills in the community are also:\nA \u0026ldquo;skill set\u0026rdquo; that can extend the capabilities of the Claude model, allowing it to perform specific tasks (e.g., data analysis, table processing, automated scripting). (GitHub)\nThis means:\nSkills are essentially \u0026ldquo;detailed instructions + examples telling AI how to perform a certain type of task\u0026rdquo; They have a structured definition (SKILL.md + metadata) Unlike random prompts, they are a set of reusable modules To use an analogy:\nSkills = Modular \u0026ldquo;professional skill packs\u0026rdquo; that AI can call upon when executing tasks.\n7️⃣ MCP (Model Context Protocol): The \u0026ldquo;Standard Interface Protocol\u0026rdquo; of the AI World # This last concept is one that many mention but find least clear.\nOfficial documentation and community explanations both state:\nMCP is a protocol standard that allows AI assistants to interact uniformly with external services, secure connections, and access data. (GitHub Docs)\nMany in the community have also built MCP servers based on it, allowing various models and tools to interact with GitHub via JSON configuration, such as cloning repositories and submitting PRs. (playbooks)\nSo essentially:\nMCP is not a model, nor is it an Agent; It is \u0026ldquo;a general protocol that allows models/Agents to acquire external context and execute external tasks.\u0026rdquo;\nIt can be seen as:\nJust as HTTP is the standard protocol for browsers to access web pages MCP is the standard protocol for AI Agents to access tools and data There are related explanations in the official documentation, but it is not a specific product component; rather, it is a standard-layer protocol.\nIV. Remember Their Relationships (Analogy Version) # To help everyone remember at a glance, here is my pictorial analogy:\nConcept Analogy Core Function Copilot Your AI Coding Assistant Real-time code suggestions GitHub Models Model resource library + debugging platform Manage, evaluate various models Agents AI that \u0026ldquo;gets work done\u0026rdquo; Automatically execute specific tasks Spaces AI\u0026rsquo;s collaborative workstation Provide context environment Spark Rapid prototyping room Quickly turn ideas into runnable results Instructions AI\u0026rsquo;s behavioral guidelines Long-term regulation of project performance Skills Reusable skill pack Modularly enhance AI task execution capabilities MCP AI\u0026rsquo;s standard protocol interface Enable AI to access external resources and data V. Summary and Actionable Advice # These concepts may seem numerous, but essentially:\nModels + MCP are infrastructure Agents + Skills + Instructions are productivity tools Spaces + Spark are workspace/product layers Understanding these layers, you can:\nStart by using Copilot for code completion/generation Experiment with Agents to automate tasks Create rapid prototypes with Spark Use Instructions and Skills to regulate and enhance AI behavior GitHub AI is evolving rapidly, and concepts are developing. By understanding these layers, you won\u0026rsquo;t easily get confused and can more reasonably integrate AI into your development workflow.\n","date":"2025-12-26","externalUrl":null,"permalink":"/en/posts/2025/github-ai/","section":"Posts","summary":"This article provides a detailed explanation of GitHub’s AI-related concepts and their hierarchical relationships through fact-based explanations and analogies, helping readers clarify the meaning and function of terms like Models, Agents, Spaces, and Spark.","title":"GitHub AI Terminology Explained—Copilot, Agents, Models to MCP, Who is Who?","type":"posts"},{"content":"","date":"2025-12-14","externalUrl":null,"permalink":"/en/tags/backstage/","section":"Tags","summary":"","title":"Backstage","type":"tags"},{"content":"Recently, I\u0026rsquo;ve been pondering a question: Can we use Kubernetes to provide cloud-based development environments similar to Codespaces, Gitpod, or Code Server for development teams?\nTechnically, this isn\u0026rsquo;t new:\nKubernetes handles resource isolation and lifecycle. Containers solve environment consistency. Cloud-based development can theoretically be out-of-the-box, ephemeral, and instantly available. However, a more practical problem quickly emerged:\nWhat should \u0026ldquo;manage\u0026rdquo; these development environments?\nWho can create them? Which project does the created environment belong to? How can it be associated with code repositories, CI, permissions, and documentation? Where should developers access it from, instead of having to remember yet another URL?\nIn a discussion with a friend, he mentioned Backstage. The name wasn\u0026rsquo;t unfamiliar to me, but I had never really taken the time to delve into it.\nSo, I decided to systematically research Backstage. I soon discovered:\nWhat it solves is not a Kubernetes problem, but rather the problem of how developers \u0026ldquo;find their way in\u0026rdquo; complex systems.\nIt was from that moment on that I gradually understood why Backstage has become so important in the domain of Platform Engineering.\nThe following article compiles some of my insights and judgments based on these thoughts.\nWhat is Backstage? # A one-sentence version:\nBackstage is not CI, not Kubernetes, nor a new DevOps tool; instead, it\u0026rsquo;s a \u0026ldquo;unified portal that connects your existing tools.\u0026rdquo;\nIt originated from Spotify. The pain points Spotify engineers encountered back then are almost identical to ours today:\nMany services Many teams Many tools People come and go, knowledge relies entirely on \u0026ldquo;tribal memory\u0026rdquo; Backstage\u0026rsquo;s original intention was actually very simple:\nTo provide a \u0026ldquo;household registration book\u0026rdquo; for all software assets.\nWho is responsible, where it runs, what it depends on, where the documentation is—all visible at a glance.\nLater, Spotify open-sourced it and donated it to CNCF. Today, Backstage has become one of the most representative open-source implementations in the IDP (Internal Developer Platform) domain, and is often considered a de facto reference standard.\nWhy Platform Engineering? # Many teams initially say:\nWe\u0026rsquo;re already doing DevOps, why do we need Platform Engineering?\nBut the reality is:\nDevOps emphasizes \u0026ldquo;you build it, you run it\u0026rdquo; Platform Engineering focuses on \u0026ldquo;I\u0026rsquo;ll help you simplify the act of building and running it\u0026rdquo; When system complexity increases, expecting every developer to be proficient in Kubernetes, CI, security, and monitoring is actually an efficiency disaster.\nThe goal of Platform Engineering is not to control developers, but to:\nReduce cognitive load Provide a \u0026ldquo;default correct\u0026rdquo; path (Golden Path) And Backstage happens to be an important vehicle for this philosophy at the engineering level.\nBackstage\u0026rsquo;s 3 Most Core Capabilities # If you only remember three points, remember these three.\nSoftware Catalog # This is Backstage\u0026rsquo;s soul.\nYou can understand it as:\nAn internal \u0026ldquo;software asset search engine\u0026rdquo; for the enterprise.\nEach service places a catalog-info.yaml file in its repository to describe:\nWhose service this is Which system it belongs to Which APIs it exposes Which databases or cloud resources it depends on Backstage organizes this information into a visualized software relationship network.\nThe effect is very intuitive:\nNewcomers no longer rely on asking people to find services When problems arise, the scope of impact can be quickly assessed \u0026ldquo;Who is responsible for this thing\u0026rdquo; is no longer a mystery Of course, it needs to be clarified: Backstage cannot solve the problem of \u0026ldquo;no one wants to take responsibility\u0026rdquo;; it merely exposes whether responsibility is clear or not.\nSoftware Templates (Scaffolder / Golden Path) # This is the capability I personally most recommend prioritizing for implementation.\nIn reality, creating a new service often goes like this:\nRequesting a repository Configuring CI Integrating monitoring Integrating security scanning Adding a bunch of \u0026ldquo;compliance requirements\u0026rdquo; Backstage\u0026rsquo;s templates do something very critical:\nThey turn the \u0026ldquo;correct posture\u0026rdquo; directly into a one-click operation.\nDevelopers only need to fill in a few fields:\nProject name Tech stack Whether a database is needed The remaining tasks, such as:\nRepository creation CI configuration Catalog registration Are all automatically completed.\nThis is not just an efficiency issue; more importantly:\nThe platform team can finally turn \u0026ldquo;specifications\u0026rdquo; into code, instead of leaving them in documentation.\nTechDocs (Docs as Code) # This point is a huge plus for someone like me who has long been skeptical of Wikis.\nTechDocs advocates a very important principle:\nDocumentation and code are kept together and managed with Markdown.\nThere\u0026rsquo;s only one benefit, but it\u0026rsquo;s critical:\nDocumentation is less likely to become outdated long-term In Backstage, clicking into a service shows:\nCode Owner Documentation Forming a complete closed loop on a single page.\nWhat Problems Can\u0026rsquo;t Backstage Solve? # If you\u0026rsquo;ve only heard success stories, here\u0026rsquo;s a dose of cold water.\nIt\u0026rsquo;s Not \u0026ldquo;Install and Use\u0026rdquo; # Backstage is more like a platform framework than an out-of-the-box product.\nThe reality usually is:\nYou need to write React / TypeScript You need to develop or customize plugins You need to continuously maintain Catalog metadata Many companies underestimate this, and the result is often:\nBackstage went live, but developers didn\u0026rsquo;t buy in.\nOnce Metadata is Inaccurate, Trust Collapses Instantly # As soon as it happens even once:\nThe found owner has already left The documentation is clearly outdated or incorrect Developers will quickly revert to the old ways:\nSlack + private messages + word-of-mouth\nAnd the problem with the Catalog is often not that it \u0026ldquo;breaks in a day,\u0026rdquo; but rather that it slowly loses accuracy when no one is responsible for it.\nWho is Backstage Suitable For? # My personal judgment is:\nSmall teams 👉 Self-building is not recommended; the cost is quite high.\nMedium to large engineering organizations 👉 If you have a platform team, Backstage is definitely worth a serious evaluation.\nThose hoping for quicker results 👉 You can directly look into hosted solutions like Port, Cortex, Roadie.\nThe tool itself isn\u0026rsquo;t the focus; the IDP philosophy is.\nSummary # Finally, a somewhat \u0026ldquo;anti-tool\u0026rdquo; statement.\nThe core of Backstage\u0026rsquo;s success has never been React, nor its plugin ecosystem, but rather three things:\nWhether ownership is clear Whether standardization is prioritized Whether the platform is operated as a product If you\u0026rsquo;re just looking to \u0026ldquo;add another tool,\u0026rdquo; Backstage is likely to fail. But if what you truly want to do is:\nLet developers spend their time coding, rather than searching for information\nThen whether you use Backstage or not, you will eventually walk this path.\n","date":"2025-12-14","externalUrl":null,"permalink":"/en/posts/2025/backstage/","section":"Posts","summary":"What is Backstage? Why has it become so important in the platform engineering domain? Sharing some insights and judgments on Backstage.","title":"Backstage (Developer Portal)—What It Is, What It Solves, and What It Doesn't Solve","type":"posts"},{"content":"In recent years, Python versions have been updated quite frequently — new versions are constantly released, while older versions are gradually End-Of-Life (EOL).\nMany times, we specify Python versions in our projects without realizing they might have already stopped being maintained, potentially introducing security vulnerabilities. To address this \u0026ldquo;small but easily overlooked\u0026rdquo; issue, I developed a lightweight tool — py-eol.\nIts goal is simple: To help you more easily detect if your Python version is EOL in your daily development.\nWhat is py-eol? # py-eol can check whether a Python version is already (or soon to be) EOL. It supports:\nPython module calls Command-line calls pre-commit hook integration It\u0026rsquo;s suitable for local checks, as well as CI/CD and team collaboration.\nQuick Installation and Usage # pip install py-eol As a Module # from py_eol import is_eol, get_eol_date, supported_versions, eol_versions, latest_supported_version # For example: print(is_eol(\u0026#34;3.7\u0026#34;)) # True print(get_eol_date(\u0026#34;3.8\u0026#34;)) # 2024-10-07 print(supported_versions()) # [\u0026#39;3.14\u0026#39;, \u0026#39;3.13\u0026#39;, \u0026#39;3.12\u0026#39;, \u0026#39;3.11\u0026#39;, \u0026#39;3.10\u0026#39;, \u0026#39;3.9\u0026#39;] print(eol_versions()) # [\u0026#39;3.8\u0026#39;, \u0026#39;3.7\u0026#39;, \u0026#39;3.6\u0026#39;, \u0026#39;3.5\u0026#39;, \u0026#39;3.4\u0026#39;, \u0026#39;3.3\u0026#39;, \u0026#39;3.2\u0026#39;, \u0026#39;2.7\u0026#39;, \u0026#39;3.1\u0026#39;, \u0026#39;3.0\u0026#39;, \u0026#39;2.6\u0026#39;] print(latest_supported_version()) # 3.14 As a Command-line Tool # # Check if a specified Python version is EOL py-eol versions 3.9 # Check if the current Python version is EOL py-eol check-self # Check if Python versions in pyproject.toml or setup.py are EOL py-eol files pyproject.toml setup.py # List currently supported Python versions py-eol list As a pre-commit hook # repos: - repo: https://github.com/shenxianpeng/py-eol rev: v0.4.0 hooks: - id: py-eol When an EOL version is detected, it will pinpoint the specific file and line number for easy identification.\nWhy You Should Care? # 1. Avoid Hidden Risks After EOL, there are no more security patches. Continuing to use it may introduce potential vulnerabilities.\n2. Smoother Team Collaboration With hooks or CI, you get prompts at the commit stage, avoiding issues like \u0026ldquo;forgetting to update the version in a certain file.\u0026rdquo;\n3. Lightweight, Easy to Use, No Learning Curve Install and use immediately, no external service dependencies, can be checked anytime.\nThe Motivation Behind This Tool # py-eol addresses a small pain point I encountered in my daily development: Using a tool to maintain awareness of Python EOL versions, preventing them from being frequently overlooked.\nSince this is a scenario everyone encounters, I wanted to create a simple tool to expose this EOL issue proactively.\npy-eol is fully open source (MIT). Feel free to use, improve, or discuss it.\nIf you have colleagues or friends who write Python,\nforwarding this article or sharing py-eol might help them. For me, it\u0026rsquo;s also a motivation to continue improving it.\npy-eol project address: https://github.com/shenxianpeng/py-eol\n","date":"2025-12-04","externalUrl":null,"permalink":"/en/posts/2025/py-eol/","section":"Posts","summary":"py-eol is a lightweight tool that helps you quickly determine if a Python version is expired or approaching EOL. It’s suitable for local use and can also be integrated into CI/CD for more reliable version management in projects.","title":"py-eol — You Might Be Using 'Expired Python' Without Realizing It","type":"posts"},{"content":"","date":"2025-12-04","externalUrl":null,"permalink":"/en/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"2025-12-01","externalUrl":null,"permalink":"/en/tags/gitstats/","section":"Tags","summary":"","title":"Gitstats","type":"tags"},{"content":"","date":"2025-12-01","externalUrl":null,"permalink":"/en/tags/gnuplot/","section":"Tags","summary":"","title":"Gnuplot","type":"tags"},{"content":"After coding for a long time, one inevitably develops a certain \u0026ldquo;paranoia.\u0026rdquo;\nOne is a paranoia for order—unified naming, neat indentation, clean and crisp formatting.\nThe other is a paranoia for simplicity—the more automated, the better; the more out-of-the-box, the better; the more seamless, the better.\nEspecially when you\u0026rsquo;re excitedly trying a new open-source project and hit enter after typing the installation command, what you expect to see is:\nLines of green progress bars An easily understandable \u0026ldquo;Successfully installed\u0026rdquo; Not:\nA screen full of red errors A cold, blunt message \u0026ldquo;Please install XXX system dependency first\u0026rdquo; That feeling is like opening a takeout box when you\u0026rsquo;re starving, only to find—\nThe restaurant forgot to give you chopsticks. The food is right there, the aroma is wafting, but you just can\u0026rsquo;t get it into your mouth.\nThis sense of frustration is a common pain for all developers.\nRecently, while maintaining my own open-source project, I was deeply bothered by these \u0026ldquo;chopsticks.\u0026rdquo; To thoroughly solve it, I simply\u0026hellip; built a new \u0026ldquo;wheel.\u0026rdquo;\nToday, let\u0026rsquo;s talk about the story behind it.\nThe Genesis: gitstats and its Only \u0026ldquo;Less Smooth\u0026rdquo; Aspect # I\u0026rsquo;ve been maintaining a tool called gitstats, primarily used to generate statistical reports for Git repositories:\nCommit activity Contributor ranking Project growth trends Graphical visualization It\u0026rsquo;s open-source, simple, and easy to use—except for one point: It depends on gnuplot.\ngnuplot is a very mature plotting tool, commonly found in scientific research and data analysis.\nThe problem is: it must be installed manually at the system level.\nLinux requires apt install; macOS requires brew install; Windows\u0026hellip; you know, it\u0026rsquo;s even more troublesome.\nThis is like the only grain of sand in the process, making the whole experience less smooth.\nI started thinking: Can gnuplot be made into a Python package, installable with a single pip install command?\nNo administrator privileges required Does not pollute the system environment Automatically adapts whether you\u0026rsquo;re on Linux / Windows / macOS Runs as long as Python is available This way, developers can include gnuplot just like installing any ordinary Python package.\nThus, gnuplot-wheel was Born # After some research and effort, I finally packaged gnuplot\u0026rsquo;s binary files into Python wheel files.\nFrom now on, you just need to type:\npip install gnuplot-wheel gnuplot\u0026rsquo;s binary files will be automatically installed into the virtual environment No system-level dependencies required Will not conflict with already installed gnuplot No administrator privileges required After installation, you can directly execute the gnuplot command— without installing anything extra.\nWhen I first saw it running smoothly in a clean environment, I had only one thought:\nAh, this is the romance of technology—keeping the complexity for oneself, and simplicity for the user.\nWho Will Use This Little Wheel? # Actually, this wheel isn\u0026rsquo;t complex, but it\u0026rsquo;s very practical.\nIf You Work in Scientific Research or Data Visualization # You can directly call gnuplot within your Python programs, without bothering with system dependencies.\nIf You Work in DevOps or Automation # You can have your scripts automatically render curve plots and trend graphs, without having to manually install gnuplot on every machine.\nIf You Are Developing Tools That Depend on gnuplot (Like Me) # You can directly add gnuplot-wheel to your dependencies, making it zero-cost for users to get started.\nCurrently, gnuplot-wheel supports mainstream platforms and is published on PyPI.\nThis \u0026ldquo;Little Wheel\u0026rdquo; Also Benefited gitstats # I have already integrated it into gitstats.\nNow, the latest version of gitstats no longer requires users to manually install gnuplot.\nAs long as:\npip install gitstats It will automatically prepare all dependencies, and users don\u0026rsquo;t need to care about what\u0026rsquo;s happening behind the scenes.\nThis is the most charming aspect of the open-source community:\nI built a \u0026lsquo;back scratcher\u0026rsquo; to solve my own little pain point, and it turned out it could help others relieve their itching too.\nWant to Give It a Try? # If your project, workflow, or script needs to use gnuplot, why not give it a try? It won\u0026rsquo;t change the world, but it can make your development process—just a little bit smoother.\nProject links:\nGitHub: https://github.com/shenxianpeng/gnuplot-wheel PyPI: https://pypi.org/project/gnuplot-wheel/ Feel free to check it out, use it, open an Issue, or even contribute together.\nPlease credit the author and source when reprinting articles from this site. Do not use for any commercial purposes. Welcome to follow the official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-12-01","externalUrl":null,"permalink":"/en/posts/2025/gnuplot-wheel/","section":"Posts","summary":"As an open-source project maintainer, I deeply understand the pain of “installing dependencies”. To save users a couple of command lines, I spent time packaging gnuplot into an out-of-the-box Python package—gnuplot-wheel. This article shares the birth story and practical value of this little wheel.","title":"To Save Everyone Two Lines of Commands—I Packaged Gnuplot into an Out-of-the-Box Python Package","type":"posts"},{"content":"","date":"2025-11-30","externalUrl":null,"permalink":"/en/tags/software/","section":"Tags","summary":"","title":"Software","type":"tags"},{"content":"Today I saw an image that made me smile—every engineering team has probably encountered this scenario: The PM asks, \u0026ldquo;If we add two more people, can the project go three times faster?\u0026rdquo;\nSome people still believe that engineering output grows linearly: add a few more people, and progress naturally accelerates. The logic is simple and seems reasonable—on the surface, adding more people appears to speed things up.\nHowever, software development is never an assembly line where tasks can be infinitely parallelized. Cramming more developers into a project won\u0026rsquo;t double or triple the delivery speed. It\u0026rsquo;s like how you can\u0026rsquo;t make two pregnant women deliver babies twice as fast as one.\nInstead, more people often bring increased communication overhead, coordination efforts, reviews, and ramp-up time for newcomers, adding extra complexity. External expectations are almost always much faster than the team\u0026rsquo;s actual pace.\nTruly fast-moving teams are never the largest in terms of headcount; rather, they are teams with clear goals, sharp focus, predictable processes, defined responsibilities, and excellent engineering practices.\nSometimes, the right choice isn\u0026rsquo;t to blindly hire, but to hire the right people. The wrong people are worse than no hire at all; they will only make the team seem larger while actually causing disunity.\nGoal: To deeply discuss Brooks\u0026rsquo;s Law, use the analogies of roasted chicken and pregnant women to enhance resonance, provide technical-level insights, and encourage readers to share and comment.\n","date":"2025-11-30","externalUrl":null,"permalink":"/en/posts/2025/the-mythical-man-month/","section":"Posts","summary":"In software development, Brooks’s Law states that “adding manpower to a late software project makes it later.” This post uses the analogies of roasted chicken and pregnant women to deeply explore the reasons behind this phenomenon and offers practical solutions.","title":"The Truth About Software Development—Adding Two People Won't Make a Project Three Times Faster","type":"posts"},{"content":"","date":"2025-11-27","externalUrl":null,"permalink":"/en/tags/devops/","section":"Tags","summary":"","title":"DevOps","type":"tags"},{"content":"Hello everyone!\nToday, I\u0026rsquo;d like to share an open-source tool I recently developed: jenkinsfilelint.\nWhy I wrote this tool? # I believe anyone who has written Jenkins Pipelines has experienced this painful process:\nYou painstakingly write your Jenkinsfile. Commit the code and push it to the repository. Watch Blue Ocean or the Console, waiting for the build to start. Boom! A few seconds later, the build fails because of a missing brace } or a misspelled keyword. Modify code -\u0026gt; Commit again -\u0026gt; Wait -\u0026gt; Pray\u0026hellip; To solve this problem, I searched for Jenkinsfile linting tools. While there are existing solutions, I found that they were either overly complicated to configure or couldn\u0026rsquo;t be well integrated into a pre-commit workflow.\nSince there wasn\u0026rsquo;t a convenient tool available, I decided to build one myself! And that\u0026rsquo;s how jenkinsfilelint came to be.\nWhat is it? # jenkinsfilelint is a command-line tool developed in Python, designed to help you validate the syntax of your Jenkinsfile before committing code.\nIt performs validation by calling Jenkins\u0026rsquo; native API, thus ensuring that the inspection results are fully consistent with the parsing logic during actual runtime.\nProject Address:\nhttps://github.com/shenxianpeng/jenkinsfilelint\nCore Features # ✅ Jenkins API-based: Utilizes Jenkins\u0026rsquo; official /pipeline-model-converter/validate endpoint for validation. The results are not only accurate but also provide specific error line numbers. 🚀 Perfect pre-commit support: This was my original motivation for developing it. You can easily integrate it into your project\u0026rsquo;s git hooks to ensure automatic checks before every commit. 🛠 Flexible Configuration: Supports configuring Jenkins URL and credentials via environment variables or command-line parameters. 🔍 Intelligent Filtering: When using Jenkins Shared Libraries, your repository might contain pure Groovy helper class files (not Pipeline scripts). The tool supports skipping these files using the --skip parameter to avoid false positives. How to Use? # 1. Installation # You can install it directly using pip:\npip install jenkinsfilelint 2. Integrate into pre-commit (highly recommended) # Create or modify the .pre-commit-config.yaml file in your project\u0026rsquo;s root directory and add the following configuration:\nrepos: - repo: https://github.com/shenxianpeng/jenkinsfilelint rev: v1.2.0 # It is recommended to use the latest version tag hooks: - id: jenkinsfilelint Then install the hooks:\npre-commit install 3. Configure Credentials # Since validation requires calling Jenkins\u0026rsquo; API, you need to provide the Jenkins URL and access credentials.\n⚠️ Security Tip: It is strongly recommended not to hardcode credentials in configuration files! It is recommended to use environment variables:\n# Linux/macOS export JENKINS_URL=https://your-jenkins-instance.com export JENKINS_USER=your-username export JENKINS_TOKEN=your-api-token Once configured, when you try to execute git commit, jenkinsfilelint will automatically check the Jenkinsfile(s) in your changes!\nCommand-line Manual Check # Of course, you can also run it manually locally to check specific files:\n# Check a single file jenkinsfilelint Jenkinsfile # Check multiple files jenkinsfilelint Jenkinsfile Jenkinsfile.prod # Skip files matching a specific pattern (e.g., Groovy files in the src directory) jenkinsfilelint --skip \u0026#39;*/src/*.groovy\u0026#39; Jenkinsfile src/Utils.groovy In Conclusion # This project is currently published on PyPI, and everyone is welcome to download and try it out.\nIf you\u0026rsquo;re also tired of \u0026lsquo;post-commit errors\u0026rsquo; with your Jenkinsfiles, why not give jenkinsfilelint a try? If you find it useful, please give it a ⭐️ Star on GitHub to show your support!\nIf you have any questions or suggestions, feel free to open an Issue or PR.\nGitHub Portal: https://github.com/shenxianpeng/jenkinsfilelint\n","date":"2025-11-27","externalUrl":null,"permalink":"/en/posts/2025/jenkinsfilelint/","section":"Posts","summary":"A magical local Jenkinsfile checker, uses API validation, perfectly supports pre-commit.","title":"Tired of Jenkinsfile Errors Only After Committing—Try This Pre-check Tool!","type":"posts"},{"content":"","date":"2025-11-20","externalUrl":null,"permalink":"/tags/installshield/","section":"标签","summary":"","title":"InstallShield","type":"tags"},{"content":"正好需要激活 InstallShield StandaloneBuild？这篇文章可能帮你省不少麻烦，记得收藏备用，也可以分享给需要的朋友！ 如果暂时用不到，也可以直接跳过~\nInstallShield 许可证激活指南 # InstallShield 是一款流行的软件安装制作工具，广泛应用于各种软件的打包和分发。为了确保软件的合法使用，用户需要激活 InstallShield 许可证。以下是激活 InstallShield 许可证的步骤：\n以 InstallShield 2023 StandaloneBuild 为例，其他版本的步骤类似。\n步骤一：获取许可证密钥 # 在购买 InstallShield 许可证后，您将收到一个许可证密钥。请妥善保管该密钥，因为它是激活软件的唯一凭证。\n许可证示例如下：\n# License for 005056bcfa9h sagdevagent01 INCREMENT IS2.win.SAB mvsn 31.0 18-apr-2027 uncounted \\ VENDOR_STRING=Instance=5 HOSTID=005056bcfa9h ISSUER=\u0026#34;Flexera \\ Software, Inc.\u0026#34; ISSUED=19-nov-2025 SN=8B11BQA-D09-526E69FDEN \\ TS_OK SIGN=\u0026#34;1DC7 9726 1B05 F7E8 9589 AC18 C866 5083 BEFD 7490 \\ 3F56 A325 C33A D3E5 9002 056D 87D7 1040 13FD AFAB C6FB 4824 \\ 839C 4C42 10FB 0132 D462 44DF 23E1 E2F2\u0026#34; 其中 005056bcfa9h 是您的机器的 Host ID，sagdevagent01 是机器名称。\n将其保存为 license.lic 文件。\n步骤二：安装 InstallShield # 如果尚未安装 InstallShield，请访问官方渠道下载并安装最新版本软件。\n在安装过程中，选择 license.lic 文件并完成安装。\n如果已经安装了 InstallShield，你可以把 license.lic 文件复制到安装目录下的 License 文件夹中，通常路径如下：\nC:\\Program Files (x86)\\InstallShield\\2023 SAB\\System\\license.lic 步骤三：验证许可证是否激活成功 # 可以通过命令行工具验证许可证状态。打开命令提示符，运行以下命令：\n\u0026#34;C:\\Program Files (x86)\\InstallShield\\2023 SAB\\System\\IsCmdBld.exe\u0026#34; -P \u0026#34;myproduct.ism\u0026#34; InstallShield (R) Release Builder Copyright (c) 2023 Flexera. All Rights Reserved. Build started at Nov 18 2025 08:38 AM ISDEV : fatal error -7159: The product license has expired or has not yet been initialized. default - 1 error(s), 0 warning(s) Build finished at Nov 18 2025 08:38 AM 上述输出表示许可证未激活成功。如果许可证激活成功，你将看到类似如下的输出：\n\u0026#34;C:\\Program Files (x86)\\InstallShield\\2023 SAB\\System\\IsCmdBld.exe\u0026#34; -P \u0026#34;myproduct.ism\u0026#34; InstallShield (R) Release Builder Copyright (c) 2023 Flexera. All Rights Reserved. Build started at Nov 20 2025 02:27 AM Created release folders Checking string table references... Generating RC file: _ISUser_0x0409.rc Building dialog 12006 Building dialog 12031 ... 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-11-20","externalUrl":null,"permalink":"/posts/2025/active-installshileld/","section":"Posts","summary":"本文介绍了如何激活 InstallShield 许可证的步骤和注意事项，帮助用户顺利完成软件安装和使用。","title":"如何激活 InstallShield 许可证","type":"posts"},{"content":"如果你问我：2025 年最“好用、省心、便宜”的 AI 编程助手是谁？\n我肯定会毫不犹豫地回答：GitHub Copilot。\n等等……便宜？ 是的，你没听错。\n💰 为什么说它便宜？ # 先来看一组价格：\nGitHub Copilot：10 美元/月，或者 100 美元/年 Cursor：20 美元/月 Codeium：15 美元/月（150 美元/年） OpenAI Codex：需要 ChatGPT Plus，每月 20 美元） 横向一比：\nCopilot = 半价 Cursor / 半价 OpenAI Codex\n在这个 AI 工具动不动就是 20 美元起步的时代，\n一个 10 美元/月 的工具，的确能算“便宜”。\n为什么它这么有吸引力？ # 尤其是对于我这种每天要：\n要写代码 要写文档 要维护开源项目 的开发者来说，Copilot 的存在几乎等同于：一个永不下班、永不摆烂、永不疲惫的编程搭档。\n它能做到的远比你想象得多：\n丢一句注释，它能补出完整函数 扔一段旧代码，它能给出重构版本 提一个需求，它能给模板、测试、CI 配置、文档 甚至能帮你分析 Issue / PR，做代码 Review 我公司虽然配了企业版 Copilot，但我在个人项目依然坚持 公私分明，自己花钱订了一整年：100 美元。\n换汇后大致一天不到 2 元，对开发者来说是不是“性价比天花板”。\n最重要的是：订了 Copilot 后，我彻底不用再担心额度、延迟、换工具这些破事。\n🧠 Copilot 是什么体验？ # 一句话总结：想用就用，随叫随到，完全不挑你状态。\n更像一个熟悉你代码风格、能读懂你上下文的工程师朋友。\n至于“值不值”？\n说实话，取决于你写代码是为了谁。\n如果你像我一样做开源、为工程师写代码——不用指望赚到钱（工程师都死扣儿 😂）。 如果是面向客户交付、面向业务产出，省下的一点时间，往往就能抵掉它一年费用。 两个关键提醒（非常重要） # 1. 记得关闭自动续订 # 订完 Copilot 后，务必去设置关闭 auto-renew，避免到期自动扣费。\n2. Copilot Chat 使用要 “少次但精准” # 不论是不是 Pro 用户，Copilot Chat 都有请求额度。\n秘诀只有一个：一次把需求说完整。\n不要问：\n“帮我写一个 xxx？”\n要问：\n“请根据下面代码结构，按我提供的约束生成一个可复用的 xxx ，语言为 xxx，且包括单元测试。”\n这样能让回答更准，还能省额度。\nCopilot 也不是完美的 # 客观的说，AI 也有它的局限性：\n对复杂架构类设计帮助有限 生成的代码一定要人工 review 有时会给出过度自信但不完全正确的答案 某些语言/框架支持程度不同步（取决于模型权重更新） 但不管怎么说，它对日常编码效率的提升绝对不是“锦上添花”，而是把效率从线性增长推进到接近指数级的加速。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-10-21","externalUrl":null,"permalink":"/posts/2025/copilot/","section":"Posts","summary":"经过一段时间的对比和尝试，我还是回到了 GitHub Copilot。本文分享我为何认为它是 2025 年最值得订阅的 AI 编程助手，以及实际使用体验。","title":"兜兜转转，我发现 GitHub Copilot 才是最香的","type":"posts"},{"content":"今天在逛 Pydantic 的 GitHub 仓库时，我看到一个熟悉的评论和一个链接：\nAny updates on this?\nhttps://justinmayer.com/posts/any-updates/\n顺着这个链接，我第一次读到了 Justin Mayer 写的这篇文章：《“Any updates?”》\n下面我就来分享一下这篇文章的主要内容和我的看法。\n在开源世界里，这样的场景你一定熟悉：\n有人在项目仓库里发现了一个问题（issue），看到最后一条评论已经是几周前的，于是便在下面留言：\n“有什么最新进展吗？”\n“Any updates?”\n看似无害的一句话，其实往往会带来不小的麻烦。\n为什么不该这么问 # 如果一个问题有了更新，它一定会出现在对应的 issue 中，所有人都能第一时间看到。\n开源项目不像公司内部那样会有“私下会议”“内部冲刺”或“隐藏计划”，所有的开发进展都公开、透明。\n所以，当有人留言“有什么更新吗？”，并不会带来任何新信息， 却会让所有订阅这个 issue 的人都收到一条无意义的通知。\n要知道，其中很多人是无偿贡献者或维护者。他们投入自己的业余时间来改进项目，而这样的“提醒”通知，只会让他们感到烦躁甚至挫败。\n那该怎么办？ # 其实，想知道项目的进展是可以理解的。但与其发一条“有更新吗？”，不如做点更有价值的事。\n资助项目\n维护者的时间是有限的。如果项目能获得资金支持，他们更有动力、也有条件去持续改进。 哪怕是一笔小额捐助，都比一句“有更新吗？”更有帮助。\n贡献时间\n如果你暂时不打算捐款，也可以想想自己能否贡献一点时间。 比如可以这样留言：\n“非常感谢所有为这个项目做出贡献的人！ 我能帮忙做点什么，让这个问题更快推进吗？”\n这类留言既友善，又能传递出积极的合作意愿。\n什么也不做（真的）\n如果你暂时没办法捐助，也无法参与贡献， 那么最友善的选择，其实是——什么也不做。\n静静等待项目的更新，让维护者按照自己的节奏前进。\n附：别用留言订阅通知 # 有些人会在评论里发“有更新吗？”只是为了订阅后续消息。\n千万别这样。\n只要点击 GitHub 页面右侧的 “Subscribe（订阅）” 按钮，就能收到所有更新通知，不必额外留言。\n保持友善 # “有什么更新吗？”这句话背后的含义其实是：\n“我希望这件事能尽快完成，但目前看来还没有人去做。”\n这会无形中给维护者带来压力。 而开源项目最需要的，不是催促，而是理解与支持。\n所以，请做一个友善的开源公民：\n有能力，就出一份力； 没时间，就多一点耐心。\n💫 世界因为你的善意与尊重，会变得更美好。\n我的看法 # 确实，这种“Any updates?”的留言，看着就让人很烦躁，属于是白嫖式的“催更”。\n下次如果还有人发“Any updates?” 或 “有什么更新吗？”，你就直接把这篇文章的链接发给他，让他自己去看吧！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-10-21","externalUrl":null,"permalink":"/posts/2025/any-updates/","section":"Posts","summary":"在开源项目的 issue 里，你是否也曾留言“Any updates?”（有什么更新吗？）来询问进展？看似无害的一句话，实际上却可能让维护者感到烦躁和挫败。本文分享了一篇关于这个话题的文章，并提出了更友善的替代做法。","title":"开源维护者最讨厌的一句话\"Any updates?\"，你说过吗？","type":"posts"},{"content":"During every Code Review, do your colleagues\u0026rsquo; commit messages always vary widely?\n\u0026ldquo;fix\u0026rdquo;, \u0026ldquo;temp change\u0026rdquo;, \u0026ldquo;update\u0026rdquo;\u0026hellip; these make people shake their heads.\nDevelopers know how important a clean and clear commit history is.\nIt not only records code changes but also the team\u0026rsquo;s \u0026ldquo;thought process\u0026rdquo;.\nThis is precisely the original intention of Commit Check—to help you and your team maintain consistency and standardization in commit messages and branch naming.\nIts goal is simple: ensure every code commit follows a consistent standard.\nSince 2022, I have been maintaining this open-source project—Commit Check.\nOver three years, it has grown from a small tool into an influential DevOps tool in the community, with many teams integrating it into their CI/CD pipelines.\nAs my understanding of such tools deepened, I realized Commit Check could be even better.\nSo, after a month of intermittent development and testing, I finally completed this major update.\nThis is also the biggest version upgrade Commit Check has seen since its inception. 🎉\nWhat\u0026rsquo;s New in Commit Check v2.0.0? # This update primarily includes three major highlights:\nTOML Configuration File Support Simplified CLI \u0026amp; Hooks Rebuilt Validation Engine In a nutshell: simpler, faster, and easier to use.\nWhy Switch to TOML? # Previously, Commit Check used .commit-check.yml as its configuration file.\nWhile highly customizable for users, it brought maintenance complexity, a less intuitive configuration experience, wasn\u0026rsquo;t modern enough, and was prone to errors due to indentation and formatting.\nThus, the decision was made—to fully switch to TOML.\nTOML\u0026rsquo;s syntax is more intuitive and better suited for declarative configuration.\nLet\u0026rsquo;s take a look at the before-and-after comparison of the configuration files:\nOld YAML Configuration File # # .commit-check.yml checks: - check: message regex: \u0026#39;^(build|chore|ci|docs|feat|fix|perf|refactor|revert|style|test){1}(\\([\\w\\-\\.]+\\))?(!)?: ([\\w ])+([\\s\\S]*)|(Merge).*|(fixup!.*)\u0026#39; error: \u0026#34;The commit message should be structured as follows:\\n\\n \u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt;\\n [optional body]\\n [optional footer(s)]\\n\\n More details please refer to https://www.conventionalcommits.org\u0026#34; suggest: please check your commit message whether matches above regex - check: branch regex: ^(bugfix|feature|release|hotfix|task|chore)\\/.+|(master)|(main)|(HEAD)|(PR-.+) error: \u0026#34;Branches must begin with these types: bugfix/ feature/ release/ hotfix/ task/ chore/\u0026#34; suggest: run command `git checkout -b type/branch_name` New TOML Configuration File # # commit-check.toml or cchk.toml [commit] conventional_commits = true allow_commit_types = [\u0026#34;feat\u0026#34;, \u0026#34;fix\u0026#34;, \u0026#34;docs\u0026#34;, \u0026#34;style\u0026#34;, \u0026#34;refactor\u0026#34;, \u0026#34;test\u0026#34;, \u0026#34;chore\u0026#34;, \u0026#34;ci\u0026#34;] [branch] conventional_branch = true allow_branch_types = [\u0026#34;feature\u0026#34;, \u0026#34;bugfix\u0026#34;, \u0026#34;hotfix\u0026#34;, \u0026#34;release\u0026#34;, \u0026#34;chore\u0026#34;, \u0026#34;feat\u0026#34;, \u0026#34;fix\u0026#34;] Isn\u0026rsquo;t that instantly much cleaner?\nNo nesting, no indentation pitfalls, easy to understand at a glance.\nTOML\u0026rsquo;s structure is naturally suitable for this \u0026ldquo;rule-declarative\u0026rdquo; configuration method, allowing direct readability of configuration content.\nOther Updates and Improvements # Other updates primarily revolve around the migration of configuration files, simplification of CLI and Hooks, and the rebuilding of the validation engine.\nBesides code updates, I also rewrote the entire Commit Check documentation system and the official website.\nNow you can find complete example configurations and FAQs directly on the official website:\nWhat\u0026rsquo;s New in v2.0.0\nConclusion # If you haven\u0026rsquo;t used Commit Check yet, I highly recommend you try it now, and feel free to share it with your developer friends.\nIt helps you and your team easily adopt Conventional Commits and Conventional Branch, and through automated checks, makes code commits more standardized, clean, and traceable.\n📍 Project Address: github.com/commit-check/commit-check\n📄 More Details: https://commit-check.github.io/commit-check/\nWhen reprinting articles from this site, please credit the author and source, and do not use them for any commercial purposes. Feel free to follow the official account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2025-10-13","externalUrl":null,"permalink":"/en/posts/2025/commit-check-v2/","section":"Posts","summary":"After a month of intermittent development and testing, I have finally completed this major update. This is the biggest update Commit Check has received since its inception.","title":"Commit Check v2.0.0 Released—TOML Config Support, Simplified CLI \u0026 Hooks, Rebuilt Validation Engine!","type":"posts"},{"content":"","date":"2025-10-13","externalUrl":null,"permalink":"/en/tags/commit-check/","section":"Tags","summary":"","title":"Commit-Check","type":"tags"},{"content":"I previously released the Jenkins Explain Error Plugin, which allows Jenkins users to analyze and resolve build errors faster with the help of AI.\nRecently, some friends asked: Can it support local models, such as Ollama?\nThe answer is: It\u0026rsquo;s happening!\nToday, I\u0026rsquo;m excited to announce: The Explain Error Plugin now supports Ollama models! 🎉\nThe Power of the Community # This update, in fact, wouldn\u0026rsquo;t have been possible without the community\u0026rsquo;s drive. While I was still validating user needs, a Jenkins community member directly contributed a PR—introducing langchain4j, enabling the plugin to extend support for more models, including Ollama.\nEven more surprisingly, this contributor turned out to be the co-founder and CEO of a Dutch company. This once again made me feel the charm of the open-source community: Needs + Contributions = Better Tools.\nConfiguration in future versions will also be simpler; users will no longer need to fill in complex API URLs, they can just select the model and use it.\nNew Version Update Highlights\n✨ Added support for Ollama models ✨ Introduced langchain4j, making it easier to extend more AI models in the future ✨ Simpler configuration, no need to manually enter API addresses How to Enable Ollama # Upgrade to the latest version of Explain Error Plugin in Jenkins Plugin Manager. Open: Manage Jenkins → Configure System → Explain Error Plugin Configuration Select Ollama from the dropdown, fill in the corresponding API address (required for local models) and key. Click Test Configuration to confirm the connection is normal, then you can start using it! Why Is It Worth Trying? # Local model support: No worries about cloud data leakage One-click build error analysis: Quickly pinpoint issues with AI, saving troubleshooting time Community-driven: Features stem from real needs and contributions Whether you are an individual developer or an enterprise team, the Explain Error Plugin can make your CI/CD process smarter and more efficient.\nFinally # If you have any thoughts or questions during use, feel free to:\nSubmit an issue on GitHub Leave a comment in the comments section If you find this plugin helpful, please give it a Star ⭐️!\nThis will not only help more Jenkins users discover it but also motivate me to continue optimizing and expanding its features.\nIf you find this article helpful, feel free to share it with your friends who use Jenkins.\nWhen reprinting articles from this site, please credit the author and source, and do not use for any commercial purposes. Welcome to follow the official WeChat account \u0026ldquo;DevOps Engineer\u0026rdquo;.\n","date":"2025-10-01","externalUrl":null,"permalink":"/en/posts/2025/explain-error-plugin-support-ollama/","section":"Posts","summary":"This article introduces the new feature of Jenkins Explain Error Plugin, which is the support for Ollama local models, helping users more efficiently analyze and resolve build errors.","title":"Jenkins Explain Error Plugin Now Supports Ollama! 🤖","type":"posts"},{"content":"","date":"2025-09-30","externalUrl":null,"permalink":"/en/tags/cpp-linter/","section":"Tags","summary":"","title":"Cpp-Linter","type":"tags"},{"content":"As cpp-linter enters its 4th year of maintenance, we have also created quite a few related projects and tools. However, up until now, we\u0026rsquo;ve only provided a simple introduction on the GitHub organization\u0026rsquo;s homepage.\nBut for users, this is clearly not friendly enough. Additionally, a website can integrate Google Analytics for traffic analysis, helping us better understand user needs and behaviors.\nTherefore, I decided to create an official website to more systematically introduce cpp-linter related projects and tools.\nAfter several evenings of effort, cpp-linter\u0026rsquo;s official website is finally live: https://cpp-linter.github.io/\nWithout further ado, let\u0026rsquo;s look at the pictures!\nHomepage # The homepage is generally concise, highlighting the tool entry points.\nThe homepage also showcases renowned organizations and open-source communities that use our projects.\nThis includes industry giants like Microsoft, Samsung, and Apache, as well as top research institutions such as NASA, Stanford University, Cambridge University, and even Lawrence Livermore National Laboratory.\nThis makes me very proud and encouraged!\nHow to quickly use cpp-linter\u0026rsquo;s projects and tools.\nGet Started Page # Lists all related projects and tools, providing brief introductions and links.\nDiscussion Page # Finally, there\u0026rsquo;s the discussion section, inviting users to participate in discussions and exchanges.\nWhy This Design? # Many projects under our previous cpp-linter organization already had online documentation, mostly built using GitHub Pages + MkDocs + Material for MkDocs.\nTo maintain a consistent style, the cpp-linter.github.io official website also uses mkdocs-material as its theme.\nOverall, I am very satisfied with the final result, which has largely met the original goals.\nTip # If you also want to build a website, it\u0026rsquo;s best to first find a theme you like. A suitable theme can better carry and express your content, thus achieving the desired effect.\nFor this website, for example, I referenced the designs of some excellent open-source project official websites, such as astral.sh and mkdocs-material\u0026rsquo;s official website.\nOf course, it\u0026rsquo;s not a direct copy, but rather learning from their design concepts, layout methods, and classification logic.\nConclusion # When working on open-source projects, it\u0026rsquo;s not enough to just have good code; good documentation and a good website are also essential. This allows more people to discover and use your project.\nEveryone is welcome to use cpp-linter related projects and tools — providing you with a one-stop C/C++ code formatting and static analysis solution.\nIf you have any suggestions or feedback, feel free to submit an Issue or participate in Discussions!\nIf you found this article helpful, please share it with your C/C++ developer friends.\nPlease cite the author and source when reproducing articles from this site. Do not use for any commercial purposes. Welcome to follow the official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-09-30","externalUrl":null,"permalink":"/en/posts/2025/cpp-linter-website/","section":"Posts","summary":"The official website for cpp-linter related projects and tools is now live at https://cpp-linter.github.io/. Everyone is welcome to visit and use it.","title":"The Cpp Linter Used by Microsoft and NASA Finally Has an Official Website!","type":"posts"},{"content":"你是不是早已习惯了：\npip install，一行命令，依赖全都下载好了； npm install，几秒钟，拉上百 MB 包快到飞起； CI 跑一遍 pipeline，所有依赖都能在线获取； 用 AI 工具跑个自动化构建，分分钟拉几十次仓库，不眨眼。 可当有人说：这些东西统统都不是免费的。\n就在昨天，OpenSSF 联合多个开源基础设施，像 Maven Central、PyPI、crates.io、NuGet、Packagist、Open VSX 等的负责人发布了一份声明，标题很直白：\n“Open Infrastructure is Not Free”\n意思就是：开源基础设施不是免费的。\n啥意思？要付费了吗？\n先别紧张，目前还轮不到我们普通开发者来买单。\n为什么现在开始提这个事？ # 声明里提到，过去几年，开源基础设施的使用量暴增：\nAI 工具越来越多，自动化依赖拉取、频繁扫描、构建操作爆炸式增长； 很多工具默认配置完全不做缓存，疯狂地向公共服务发请求； 有些公司在产品里用这些公共资源，但从来没想过要反哺。 结果就是：需求越来越大，钱和人手却没有同步增加。\n那怎么办？ # 声明里给了一些方向：\n大厂和高流量用户要掏钱，别再只薅羊毛； 可以分层：普通开发者继续免费，但企业级 / 高频访问要收费； 提供增值服务（比如企业级 SLA、统计分析），用商业模式反哺开源； 工具和框架开发者要在设计上更友好，比如启用缓存、减少不必要的请求。 其实核心思想很简单：开源基础设施也得吃饭。\n我们普通开发者能做点啥？ # CI 流程里加缓存，别让每次构建都要做一次远程拉取； 在公司层面，支持或推动开源基金会的赞助； 如果你做工具，记得优化对公共资源的使用。 最后想说 # 很难想象如果有一天，如果有一天 PyPI、NPM、Maven 宣布“开始收费”或者“某些地区无法访问”，那将会怎样？\n现实是：若我们还想继续拥有这种无感、稳定、几乎免费的体验，就必须有人站出来，把这份责任公平地分担。\n那会是谁？\n原文链接：https://openssf.org/blog/2025/09/23/open-infrastructure-is-not-free-a-joint-statement-on-sustainable-stewardship/\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-09-24","externalUrl":null,"permalink":"/posts/2025/open-infrastructure-is-not-free/","section":"Posts","summary":"开源基础设施并非免费，自来水般的 pip/npm 安装背后是高昂的带宽、存储和运维成本。声明呼吁开发者与企业共同分担，优化工具、支持赞助，才能让我们习以为常的“免费”体验真正可持续。","title":"白嫖了这么多年，开源基础设施要开始收费了？","type":"posts"},{"content":"","date":"2025-09-16","externalUrl":null,"permalink":"/en/tags/conventional-branch/","section":"Tags","summary":"","title":"Conventional Branch","type":"tags"},{"content":"One year ago today, I officially released the Conventional Branch specification.\nOver the past year, the community response has been enthusiastic, with more and more teams adopting this specification to manage their Git branches.\nFor example, when searching \u0026ldquo;Conventional Branch\u0026rdquo; on Google, the specification page ranked first in search results for a considerable period.\nAccording to Google Analytics data, the specification homepage (https://conventional-branch.github.io/) has accumulated over 12K visits in the past year, with widespread adoption beginning earlier this year.\nCurrently, the Conventional Branch specification has been translated into multiple languages, including Chinese and Portuguese.\nBased on access data, users are distributed globally, with particularly active usage in the United States, Brazil, France, the United Kingdom, Germany, Russia, and Japan.\nHowever, there are some regrets: despite China\u0026rsquo;s large IT workforce, the number of visits is relatively limited, possibly due to the inconvenience of using Google Search and GitHub within the country.\nToday, the Conventional Branch specification is quite complete and frequently appears alongside Conventional Commits in the Contribution Guides of many open-source projects, guiding developers to better manage branches and commits.\nGoing forward, I hope it will be adopted in more projects and that we can continue to improve multilingual support, allowing more developers to benefit.\nIf you also find this specification valuable, please like and share it so more people can learn about it.\nFeel free to try it in your projects or suggest improvements; let\u0026rsquo;s work together to make Git branch management simpler and more efficient.\n","date":"2025-09-16","externalUrl":null,"permalink":"/en/posts/2025/conventional-branch-one-year-anniversary/","section":"Posts","summary":"12K+ visits, multilingual translations, and global developer adoption—the journey of the Conventional Branch specification’s first year","title":"Conventional Branch Specification One-Year Anniversary — From Zero to Global Developer Adoption","type":"posts"},{"content":"","date":"2025-09-16","externalUrl":null,"permalink":"/en/tags/git/","section":"Tags","summary":"","title":"Git","type":"tags"},{"content":"","date":"2025-08-29","externalUrl":null,"permalink":"/tags/overtime/","section":"标签","summary":"","title":"Overtime","type":"tags"},{"content":"最近在我常踢球的微信群里，大家晚上约踢球，有几个朋友因为“要加班”来不了。\n每次因为加班而错过中年人难得的运动机会时，心里总会有些感慨。\n看到这句话，我突然意识到：来欧洲工作一年多，我居然只加过一次班。\n唯一一次加班的经历 # 事情发生在一周前的周五晚上，正好是假期的前一天。\n晚上八点多，美国的领导突然发消息，说客户在用一个非常老旧的版本时遇到严重问题，Support 团队在线支持无果，现在急需一个 debug build 来定位问题。\n这个需求看似简单，实际难度却未知：\n这个旧版本我们团队接手后就没发布过，能否准备好构建环境？ 构建步骤是否和现在的版本不同？ 时间紧急，客户的应用已经宕机。 但正如那句老话：救急不救穷。\n但那天晚上我没带电脑回家，于是决定去公司取电脑，带着孩子一起去了公司。那时已是晚上九点多，办公室空荡荡的，只有我一个人在忙着折腾旧版本的编译环境。\n孩子很乖，陪我到晚上十一点。她困了想回家，我就带她先回去，回到家后继续干到凌晨一点半。\n终于，我把调试程序打包好交给了 Support，客户的问题得以缓解。周一收到了领导和 Support 同事的感谢和鼓励，那一刻，心里还是挺有成就感。\n为什么在欧洲加班很少？ # 对比国内公司，加班这种场景并不稀奇，甚至每天都在发生。\n但在欧洲，一年多只遇到一次。主要还是因为这里非常注重工作与生活的平衡。\n工作与生活边界清晰。 到点就走，没有人要求你加班。 信任机制。 领导更看重结果，而不是你在工位上坐了多久。 尊重个人生活。 周末、节假日、休假是“神圣不可侵犯”的。 当初也是因为外企撤离，觉得再找不到更好或能接受的外企，所以选择了来欧洲工作的机会。如果去到国内的公司，我会尝试去适应，但可能不会开心。\n我可以加班，曾经在京东工作时也经常加班，但现在已经习惯了健康的工作节奏，不太能接受“无意义的加班”。\n如果国内企业能提供更多健康的职场环境，可能很多人根本不会选择离开，去欧洲、去日本发展。毕竟，家人、朋友都在国内，绝大多数人谁不想待在熟悉的地方？\n写在最后 # 这篇文章原本没打算写，只是因为微信群里一句“加班来不了”，还是有些感触。\n在欧洲这一年多，最大的收获不是环境有多好，赚钱有多少，主要还是生活和工作的平衡。\n工作时间专注于任务和解决问题，下班后陪孩子、家人，或去运动健身，而不是被“无尽的加班”绑住。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-08-29","externalUrl":null,"permalink":"/posts/2025/work-overtime/","section":"Posts","summary":"在欧洲工作一年多，我只加过一次班，这让我深刻体会到东西方工作文化的差异。","title":"来欧洲工作一年多，我只加过一次班","type":"posts"},{"content":"Recently, while reviewing Google Analytics, I discovered an interesting phenomenon: My blog (shenxianpeng.github.io) has decent traffic from Google Search, but the primary language of visitors is surprisingly English, with Chinese coming in second.\nCome to think of it, it\u0026rsquo;s not surprising—I\u0026rsquo;ve written a few good English articles before, such as Using Gcov and LCOV for C/C++ Code Coverage, attracting many overseas readers.\nHowever, the problem is: I mainly write in Chinese, only occasionally in English. If readers want to see a version in another language, I have to manually translate, copy, paste, preview, and submit\u0026hellip; The whole process is tedious and time-consuming.\nSo I thought: Let\u0026rsquo;s automate this with AI.\nMy Solution # Using GitHub Actions + Gemini API to achieve automatic bilingual blog publishing.\nThe overall idea is simple:\nEvery time I write a new article and commit it to the repository, GitHub Actions will be automatically triggered; The workflow calls the Gemini API to translate the Chinese text into English; The translated article is committed to a new branch, and a PR is automatically created; Netlify deploys a preview; I can review it on GitHub and merge it with one click; After merging, the English version of the article will be published. This way, I only need to focus on writing in Chinese, and the English version will be generated automatically.\nCore Configuration # Here\u0026rsquo;s the core configuration of GitHub Actions (simplified version):\non: push: paths: - \u0026#39;content/posts/**/*.md\u0026#39; - \u0026#39;content/misc/**/*.md\u0026#39; schedule: - cron: \u0026#39;0 2 * * *\u0026#39; # Prevents missed translations and controls API call frequency workflow_dispatch: jobs: check-and-translate: runs-on: ubuntu-latest steps: - uses: actions/checkout@v5 - uses: actions/setup-python@v5 with: python-version: \u0026#39;3.13\u0026#39; - run: make install-deps - run: make translate env: GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }} GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} I put the dependencies and translation logic in the Makefile:\ninstall-deps: pip3 install -r requirements.txt translate: python3 .github/auto_translate.py Results and Inspiration # The final result is excellent:\nEffortless: No need for manual translation and copy-pasting; Efficient: An English version is generated almost simultaneously with a Chinese article; Cost-effective: Using Gemini API + Netlify, the cost is almost zero.\nMore importantly, many \u0026ldquo;seemingly troublesome little things\u0026rdquo; can be solved through AI + automation. Writing a blog is just one example; extending this idea, it can be applied to many scenarios: For example, bilingual support for technical documentation, internationalization of team knowledge bases, or even translation of internal company wikis.\nConclusion # Don\u0026rsquo;t waste time on \u0026ldquo;repetitive work\u0026rdquo;—let tools and automation handle it; save your energy for more creative things.\nIf you also have the habit of writing blogs or documents, I hope this article will inspire you.\nComplete code here 👉 GitHub Repository\n","date":"2025-08-24","externalUrl":null,"permalink":"/en/posts/2025/auto-translate-post/","section":"Posts","summary":"Discovered my blog has more English than Chinese readers?  So I used GitHub Actions + Gemini API to automatically translate articles into English, saving time and effort.","title":"Blog Bilingual Publishing Made Easy — GitHub Actions + Gemini API in Practice","type":"posts"},{"content":"","date":"2025-08-24","externalUrl":null,"permalink":"/en/tags/gemini-api/","section":"Tags","summary":"","title":"Gemini API","type":"tags"},{"content":"","date":"2025-08-24","externalUrl":null,"permalink":"/en/tags/github-actions/","section":"Tags","summary":"","title":"GitHub Actions","type":"tags"},{"content":"","date":"2025-08-22","externalUrl":null,"permalink":"/en/tags/career-development/","section":"Tags","summary":"","title":"Career Development","type":"tags"},{"content":"I\u0026rsquo;ve been asked: Why dedicate your free time to open source, especially after having children?\nFrankly, open source is more than a hobby for me; it\u0026rsquo;s a long-term investment in value.\nWhile direct financial returns are minimal, the rewards it brings far outweigh monetary gains.\nToday, I want to share three of them: these aren\u0026rsquo;t just for open-source contributors; every developer, and even every professional, can benefit from them.\n1. Making Your Work Truly Visible # In a company, our achievements are usually only known to colleagues or superiors.\nOpen-source projects are different; they showcase your abilities and accomplishments to a wider technical community.\nThis \u0026ldquo;visibility\u0026rdquo; is often more persuasive in career development than a few lines on a resume.\n👉 Key takeaway: Even without open source, you can create visibility by blogging, producing technical documentation, and sharing your work in communities. This is an intangible asset.\n2. Connecting with Exceptional People and Projects # Writing CRUD code in isolation might lead to career stagnation by age 35. Open source has exposed me to trends, best practices, and exceptional developers worldwide.\nFor example, I regularly watch the CPython repository, learning how they write PRs, propose PEPs, and implement CI/CD.\nLearning these processes alone has been invaluable. Participating in the English-speaking community has also improved my English.\n👉 Key takeaway: Even without contributing to open source, you can benefit by following excellent projects or large-company engineering practices and applying their lessons to your daily work.\n3. Long-Term Value Accumulation # Over the past four years, I\u0026rsquo;ve created several GitHub organizations and over ten projects, accumulating tens of thousands of users and hundreds of stars.\nThese accomplishments are the result of gradual accumulation during my free time.\nWhile there\u0026rsquo;s no direct financial return, this is a form of long-term asset.\nMy open-source experience and perspective help me grow faster at work, and my work experience, in turn, feeds back into my open-source contributions.\nMore importantly, it keeps me in a state of continuous learning and exploration.\n👉 Key takeaway: Whether it\u0026rsquo;s open source, writing articles, or side hustles, these can be seen as long-term investments. They may not immediately generate income, but they build reputation, skills, and opportunities.\nMy Open Source Story (Short Version) # In 2021, I took my first step into open source with cpp-linter-action. I was surprised when someone proactively offered contributions in an Issue; that moment ignited my open-source journey.\nLater, I collaborated with Brendan (2bndy5) on several projects, learning a great deal from him.\nWe also created cpp-linter, which has become one of the leading C/C++ linter projects on GitHub.\nI later developed commit-check and conventional-branch, gaining a significant user base.\nIn 2024, AI tools exploded. I initially worried about their potential to displace developers, but quickly realized that AI can rapidly generate runnable code, but is insufficient for building large, maintainable projects.\nFor example, when developing Jenkins Explain Error Plugin, AI helped me create a basic version quickly, but getting the project into the official Jenkins ecosystem required reviewer feedback and manual refinement.\nThis reinforced my belief that AI is a powerful tool, but ultimate value still comes from human thought and judgment.\nConclusion # Open source has:\nIncreased my visibility; Connected me with exceptional people and best practices; Kept me learning and growing. Even without direct financial rewards, I believe it\u0026rsquo;s a worthwhile endeavor.\nAs for whether AI will replace us? Perhaps someday.\nBut for now, it\u0026rsquo;s far from time to \u0026ldquo;lie flat.\u0026rdquo;\nContinuous learning and adaptation are key to maintaining value and competitiveness in this rapidly changing world.\n","date":"2025-08-22","externalUrl":null,"permalink":"/en/posts/2025/why-open-source/","section":"Posts","summary":"Some say open source is useless, yielding neither profit nor time savings.  But through four years of dedication, I’ve discovered three unexpected rewards: increased visibility for my work, connections with exceptional people and projects, and the accumulation of long-term value.  These rewards are applicable to every developer.","title":"Four Years of Open Source —— Three Unexpected Rewards","type":"posts"},{"content":"","date":"2025-08-22","externalUrl":null,"permalink":"/en/tags/open-source/","section":"Tags","summary":"","title":"Open Source","type":"tags"},{"content":"","date":"2025-08-22","externalUrl":null,"permalink":"/en/tags/technical-growth/","section":"Tags","summary":"","title":"Technical Growth","type":"tags"},{"content":"In daily work, we often talk about the buzzword DevOps. But have you ever wondered:\nWhat is our team\u0026rsquo;s actual DevOps maturity? How do we measure if one group is doing well and another needs improvement? Is there a unified standard for assessment? Based on these questions, we need a DevOps maturity model to help teams better evaluate and improve their practices.\nBut the assessment method itself is worth considering. If you just write a wiki or send out an Excel sheet for everyone to fill in, this \u0026ldquo;primitive\u0026rdquo; approach is not only inefficient, but also hard to track and accumulate results.\nThus, an open source project—DevOps Maturity—was born.\nDevOps Maturity Specification # This specification is based on the DevOps Maturity Model and Google DORA, combined with real-world implementation.\nIt defines the characteristics and metrics of different maturity levels, so teams clearly know:\nWhat stage are we currently at? What should we improve next? Key points:\nThe specification covers basics, quality, security, supply chain security, analytics and reporting, with actionable assessment items for each part:\nFor results, instead of complex percentages or scores, we use badge levels to motivate teams to keep improving:\nMore details can be found on the official site 👉 https://devops-maturity.github.io/en/\nDevOps Maturity Assessment Tool # With standards and specifications, we also need a tool for practical implementation.\nSo I developed the DevOps Maturity Assessment Tool, available in both Web and CLI versions.\nAssessment results are stored in a SQLAlchemy database and can be viewed and analyzed via Web or CLI.\nCLI example:\nCLI video:\nWeb video:\nFive Highlights # The DevOps Maturity project has five key highlights:\nStandardized Assessment: Based on industry-proven models, providing clear standards and metrics. Open Source Tools: Web and CLI options for easy use and integration. Visualized Results: Intuitive interfaces to quickly understand your team\u0026rsquo;s status. Continuous Improvement: Badge level incentives encourage ongoing DevOps practice improvement. Customizable Extension: Users can customize assessment items and metrics as needed, e.g., by editing criteria.yaml, without changing a single line of code to generate a unique assessment scheme. DevOps Maturity Enterprise Edition is also in development, with more enterprise features and support coming soon!\nJoin Us # DevOps Maturity is officially released—everyone is welcome to:\nAdopt it Share feedback Suggest improvements Contribute code All of these are the greatest support for open source 🙌\nIf you find it valuable, please give a ⭐ Star to help more people discover and benefit from it.\n","date":"2025-08-17","externalUrl":null,"permalink":"/en/posts/2025/devops-maturity/","section":"Posts","summary":"How to assess and improve your team’s DevOps maturity. Official release of the DevOps Maturity open source project, with assessment tools and practical guides.","title":"DevOps Maturity — From Reflection to Open Source Practice","type":"posts"},{"content":"","date":"2025-08-13","externalUrl":null,"permalink":"/tags/pre-commit/","section":"标签","summary":"","title":"Pre-Commit","type":"tags"},{"content":"昨天在网上冲浪，突然看到了一个仓库叫 prek，一看介绍是 —— ⚡ Better pre-commit, re-engineered in Rust。这就引起了我的兴趣，毕竟 pre-commit 作为非常广泛的预提交的工具，如果能改进，尤其是性能方面的改进，肯定是好事。\n最最有意思的是 pre-commit 的作者也来到这个项目的 Issue 里发帖了，他先表示要合作，然后提到该项目违反了版权（已修复），再后面说这是一个恶意的、不道德，以及抄袭。\n让我们一起来看看这个帖子是怎么回事，我这里直接用 Google 翻译，带大家过一遍。\n最终，在 Airflow Maintainer 留言之后，作者点了一个 ❤ 然后锁了这个帖子。（这个操作可谓恰到好处）\n链接在这里：https://github.com/j178/prek/issues/73\n这件事有点像 uv 取代 pip 的故事，只不过我没看到像这次一样的争议。原因或许在于 pip 是由众多社区志愿者共同维护的项目，而 pre-commit 更像是 Anthony Sottile 的“个人”项目。虽然它是开源的，但原作者对项目拥有绝对的控制权。\n此外，它的衍生项目 pre-commit.ci 对开源项目免费，但对私有仓库（$10/月）、初创公司（$20/月）和大型组织（$100/月）则收取费用。如果出现有竞争力的替代方案，可能会对其收入造成影响。\n这里简单介绍一下 Anthony Sottile —— 他是 pre-commit 的作者，同时也是 pytest-dev、tox-dev 的核心开发者，维护 flake8，PyCQA 成员，GitHub Star 等等。只要你使用 Python，就很可能接触到他参与的项目。此外，他还是一名 YouTuber，会进行编程直播。我最初是因为使用 pre-commit 才了解到他，也看过他的视频，专业能力确实很强。不过，正如上文所提到的，他在 pre-commit 社区的互动方式，确实让部分人感到不适甚至不愉快。\n我的看法 # 除非 pre-commit 原作者 Anthony Sottile 能够更积极、更开放地推进，并加快 pre-commit-rs 的开发进度，否则 prek 对它的威胁将持续增加。从目前的趋势来看，prek 已经展现出强劲的势头。\n基于以下几点，我认为它很有可能走得很远：\n作者影响力： prek 作者是活跃且有影响力的开源贡献者，参与并贡献了 encode/httpx、astral-sh/uv、astral-sh/rye 等知名项目，具备长期获得社区信任与背书的能力。 重量级背书： prek 已经获得知名贡献者如 Jarek Potiuk 的积极背书——他是 Apache Airflow 的贡献者与 PMC 成员，Airflow 正在积极筹备切换到 prek。 社群形象差异： 与 pre-commit 原作者相比，其“高冷”风格可能限制了外部贡献者的参与意愿；反观 prek，作者听取的社区的建议将这个项目名字从 prefligit 改为 prek，我认为这个一个更好的名字，即简短，从发音上也更容易获得好感，这样也为替代方案的崛起创造了空间。 社区需求： 社区需要一个像 prek 这样积极推动 Rust 重写的项目，来打破 pre-commit 的现状。 除非 Anthony Sottile 做出 180 度转变，主动邀请外部贡献者共同加速 pre-commit-rs 的开发，并改变现有的社区互动方式，否则这种趋势短期内难以逆转。综合来看，我对 prek 的未来持乐观态度。\n在我截稿时，还看到作者将上述对话内容发布在 V2EX 和 Twitter 上，引发了更多关注。\n这部分我不作过多评价——开源社区本就是一个不断交流、切磋和竞争的舞台。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-08-13","externalUrl":null,"permalink":"/posts/2025/pre-commit-competitors/","section":"Posts","summary":"昨天在网上冲浪，突然看到了一个仓库叫 prek，一看介绍是 —— ⚡ Better pre-commit, re-engineered in Rust。这就引起了我的兴趣，毕竟 pre-commit 作为非常广泛的预提交的工具，如果能改进，尤其是性能方面的改进，肯定是好事。","title":"被 Airflow Maintainer 一顿夸：Rust 重写版 pre-commit 项目 prek 的崛起","type":"posts"},{"content":"","date":"2025-08-10","externalUrl":null,"permalink":"/en/tags/blog/","section":"Tags","summary":"","title":"Blog","type":"tags"},{"content":"","date":"2025-08-10","externalUrl":null,"permalink":"/en/tags/blowfish/","section":"Tags","summary":"","title":"Blowfish","type":"tags"},{"content":"Eight years ago, I built my blog using Hexo + the landscape theme. From 2017 until today, it has hosted 236 posts, including technical articles and a small number of personal notes.\nOver time, Hexo no longer seemed like the best choice, and some limitations and inconveniences became more apparent, such as:\nPoor support for multiple languages The current theme couldn’t meet my needs, such as listing works or a resume Many features required third-party plugins, such as comments, reading time, and word count Content and images for posts were stored in different directories, making management inconvenient I had previously made a short-lived attempt to revamp the blog, but the results weren’t ideal, so I shelved the plan.\nRecently, however, I came across Hugo with the Blowfish theme, which immediately caught my eye. Combined with GitHub Copilot and my OpenAI Plus subscription, I decided to fully revamp my blog and migrate to Hugo.\nThat €30 per month AI subscription shouldn’t go to waste!\nEven with so many tools and resources at my disposal, it still took me three nights to complete a migration I was satisfied with. Here’s a comparison:\nHome Page Comparison # Previous home page (no localization applied)\nAbout Page Comparison # New Pages # Upgrade Notes # If you have a large number of posts like I do, create a new repository (e.g., new-blog), set it up with Hugo + Blowfish theme first. Then migrate your Hexo posts. I used Copilot to write a script to automatically convert Hexo posts into Hugo format. Use ChatGPT or other AI tools to generate post cover images. Use Copilot to copy the generated covers to the corresponding directories based on the post topics. Gains and Losses of the Upgrade # After the upgrade, my blog gained the following improvements:\nResolved the lack of multilingual support Added built-in post search functionality Provided better navigation and page layout Stored post content and images in the same directory for easier management The Blowfish theme offers many built-in features (comments, reading time, word count, etc.) without needing extra plugins Of course, there were also some losses:\nOld post URLs were not mapped to the new ones, impacting SEO (I can fix it if I want to) Lost previous comment data Time investment was needed for migration and adjustments Overall, the upgrade was definitely worth it — the new theme is a huge improvement over the old one.\nFinal Thoughts # Without the help of AI, this migration would have taken much more time and effort.\nAt the same time, with less reliance on search engines, the chances of technical articles being seen seem lower — perhaps now they exist more as training data for AI.\nRegardless, for IT professionals, having a personal writing space is still valuable, whether for recording, summarizing, or reflection.\nI will continue to share my learning and work experiences here, as well as record my thoughts, observations, and reflections as a programmer living in Eastern Europe. Perhaps that still holds some value.\n","date":"2025-08-10","externalUrl":null,"permalink":"/en/posts/2025/upgrade-blog-using-hugo/","section":"Posts","summary":"This article records the process of revamping my blog after eight years, from migrating from Hexo to Hugo, to the functional and design improvements of the new blog.","title":"Eight Years Later, I Finally Revamped My Blog","type":"posts"},{"content":"","date":"2025-08-10","externalUrl":null,"permalink":"/en/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"Last week I released the Jenkins Explain Error Plugin, aiming to help Jenkins users analyze and resolve errors in Jenkins builds faster through built-in AI.\nSome readers mentioned in the comments that they hoped the plugin would support the Google Gemini model for error analysis, as their company can only use Google\u0026rsquo;s AI services.\nToday, I\u0026rsquo;m excited to announce that the plugin now supports the Google Gemini model! 🎉\nPlugin Updates # Added support for the Google Gemini model. Optimized documentation and added an example video. How to Use Google Gemini # Before you begin, ensure the plugin is updated to the latest version. You can find the Explain Error Plugin in the Jenkins Plugin Manager and upgrade it to the latest version.\nAfter updating, you can choose to use the Google Gemini model for error analysis in the plugin configuration. Simply set the model to Google Gemini in the Explain Error Plugin Configuration section under Manage Jenkins → Configure System, and provide the corresponding API address and key.\nClick Test Configuration to ensure your Google Gemini API Key, URL, and Model are correctly filled in and accessible.\nExample Plugin Video # Considering that many users may not be familiar with using the plugin, I\u0026rsquo;ve recorded a short video demonstrating how to use the Explain Error Plugin for error analysis in Jenkins.\nYou can watch this video on YouTube.\nConclusion # If you have any questions or suggestions during use, please submit an issue on GitHub or leave a comment.\nRepository address: jenkinsci/explain-error-plugin\nWelcome to Star ⭐️ and support!\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2025-08-03","externalUrl":null,"permalink":"/en/posts/2025/explain-error-plugin-support-gemini/","section":"Posts","summary":"This article introduces a new feature of the Jenkins Explain Error Plugin that supports for Google Gemini model for error analysis. It provides configuration methods and an example video.","title":"Jenkins Explain Error Plugin Now Supports Google Gemini! 🤖","type":"posts"},{"content":"As the title says, I recently launched my first Jenkins plugin! 🎉\nThe main function of this plugin is to eliminate the need to copy error messages from Jenkins builds into AI tools like ChatGPT for analysis. Instead, it provides a button directly in the Jenkins build log. Clicking this button automatically sends the error information to OpenAI for analysis. You can also add an explainError() step in your pipeline to get error explanations, helping developers locate and solve problems faster.\nThis is my first plugin project in the Jenkins community. Previously, I hadn\u0026rsquo;t attempted this because I believed many functionalities could be implemented via pipeline scripts, making a separate plugin unnecessary.\nHowever, with the increasing popularity of AI, I discovered that the Jenkins Plugin Center surprisingly lacked a similar plugin. So, I decided to implement this functionality myself. With the help of AI, and utilizing my evenings for development and testing, along with thorough code reviews from the Jenkins Hoster, I finally submitted it to the Jenkins Plugin Center last weekend, and it\u0026rsquo;s officially online.\nPlugin Introduction # Explain Error Plugin is a Jenkins plugin based on OpenAI. It automatically parses build failure log information and generates readable error explanations, suitable for common Jenkins job types such as Pipeline and Freestyle.\n🔍 One-click analysis of errors in console output ⚙️ Usable in Pipelines with a simple explainError() step 💡 AI-powered intelligent explanations based on the OpenAI GPT model 🌐 Provides two Web UI options to display AI-generated analysis results 🎯 Customizable: Supports setting the model, API address, log filters, and more Plugin Highlights # Feature Description ✅ One-click console analysis Adds an \u0026ldquo;Explain Error\u0026rdquo; button to the top of the console page ✅ Pipeline support Provides the explainError() step, automatically triggered on failure ✅ Model configuration support Customize to use GPT-3.5 or other models ✅ Jenkins CasC support Supports Configuration as Code ✅ Log filtering Supports regular expression filtering of logs to focus on error content Prerequisites # Jenkins version ≥ 2.479.3 Java version ≥ 17 An OpenAI API Key (available at OpenAI website) Quick Installation # You can install it through the Jenkins Plugin Manager:\nManage Jenkins → Manage Plugins → Available → Search for \u0026ldquo;Explain Error Plugin\u0026rdquo;\nPlugin Configuration # After installation, find Explain Error Plugin Configuration under Manage Jenkins → Configure System to configure your OpenAI API key and model:\nSetting Description Default Value Enable Explanation Enable AI analysis functionality ✅ Enabled API Key Your OpenAI Key Required API URL OpenAI API address https://api.openai.com/v1/chat/completions AI Model Model to use gpt-3.5-turbo You can also manage these settings through Jenkins Configuration as Code, for example:\nunclassified: explainError: enableExplanation: true apiKey: \u0026#34;${AI_API_KEY}\u0026#34; apiUrl: \u0026#34;https://api.openai.com/v1/chat/completions\u0026#34; model: \u0026#34;gpt-3.5-turbo\u0026#34; How to Use # Using in Pipelines # It\u0026rsquo;s recommended to call explainError() within the post { failure { ... } } statement to automatically analyze errors on build failure:\npost { failure { explainError() } } You can also set the log length and matching keywords:\nexplainError( maxLines: 500, logPattern: \u0026#39;(?i)(error|failed|exception)\u0026#39; ) Using in the Console # Suitable for any type of Jenkins job:\nOpen the console output page of the failed build Click the \u0026ldquo;Explain Error\u0026rdquo; button at the top The AI analysis results will be displayed below the button Preview # After a build failure, a sidebar entry will appear on the job page. Clicking it will display the AI analysis results:\nOr directly click \u0026ldquo;Explain Error\u0026rdquo; on the console page to view the analysis:\nFuture Plans # Error caching: Avoid repeated calls to OpenAI when analyzing the same error multiple times to save calls. (Already implemented, awaiting merge) Multi-model support: Support other AI platforms, such as Google Gemini, Claude, DeepSeek, etc. These features are still under development, conception, and refinement. Your feedback and suggestions are highly welcome.\nIn Closing # The plugin is completely open-source. Feel free to try it out and contribute code!\nIf you encounter any problems during use or have suggestions, please submit an issue or pull request on GitHub:\nGitHub address: https://github.com/jenkinsci/explain-error-plugin\nIf you find this plugin helpful, please share this article; you can also give it a Star ⭐ on GitHub to show your support!\nThis is the most direct support for open-source projects 🙌\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-07-29","externalUrl":null,"permalink":"/en/posts/2025/explain-error-plugin/","section":"Posts","summary":"Introducing my first Jenkins plugin: Explain Error Plugin. It automatically analyzes build failure log information and generates readable error explanations, helping developers locate and solve problems faster.","title":"Jenkins Plugin Center Didn't Have an AI Plugin Yet? I Wrote One! 🤖","type":"posts"},{"content":"My child went to bed early tonight, so I\u0026rsquo;m sitting on the balcony sofa, sipping the leftover beer from yesterday\u0026rsquo;s dinner with colleagues, listening to the neighbors chatting in a language I don\u0026rsquo;t understand. It\u0026rsquo;s given me pause for thought.\nRecently, people have been asking me: \u0026ldquo;How did you go abroad? What\u0026rsquo;s the process? Will your child study there?\u0026rdquo;\nI\u0026rsquo;ve also learned that some friends are struggling with whether to live abroad, ultimately choosing to stay in China.\nI just signed a second-year lease with my landlord. Thinking about it, I\u0026rsquo;ve been living abroad alone for a year now, and I\u0026rsquo;ve accumulated some experience.\nIf I had to summarize my feelings in one sentence, it would be:\nWhether or not to go abroad is a very personal decision; there\u0026rsquo;s no right or wrong.\nBut for working professionals—hone a core skill, learn English well, maintain good health, and pursue long-term, valuable work.\nPerhaps due to the increasingly severe employment situation in recent years, more and more people are considering a question:\nAm I Suited to Living Abroad? # Some people do it for their children\u0026rsquo;s education; others want to escape involution and pursue a higher quality of life.\nBut between aspiration and reality, there are often obstacles like language, culture, education systems, efficiency, and lifestyle differences.\nAs a programmer working in Europe, I\u0026rsquo;m often asked these questions:\n“Is Europe suitable for raising children?” “Can I survive without speaking English?” “What about my child\u0026rsquo;s schooling? Is English enough?” “Can I save money on my salary?” Today, I want to share my observations from the perspective of both a \u0026ldquo;programmer\u0026rdquo; and a \u0026ldquo;father\u0026rdquo;:\nWho Thrives in Europe, and Who Might Struggle? # People Suited to Europe # 1. Adequate English Skills for Daily Communication # In Europe, especially in the tech field, English is the most common working language. You don\u0026rsquo;t need to be as fluent as a native speaker, but you should at least be able to understand emails, keep up in meetings, and communicate with colleagues.\n2. Willingness to Learn the Local Language and Culture # Europe isn\u0026rsquo;t like the UK, US, or Canada—purely English-speaking countries. Each country has its own official language. Mastering the local language will save you a lot of trouble when shopping, seeing a doctor, renting a house, and handling official business, and it will also help you integrate into society faster.\nOf course, some people don\u0026rsquo;t want to learn another language, which might lead to more inconveniences in daily life.\n3. Acceptance of a \u0026ldquo;Slower Pace + Higher Quality\u0026rdquo; of Life # Many European countries promote \u0026ldquo;work-life balance\u0026rdquo;:\nPunctual下班 (leaving work on time), no weekend overtime Vacation time is readily available; no one questions \u0026ldquo;what about the project?\u0026rdquo; Online shopping is slow, appointments are necessary for many things, but a slower pace doesn\u0026rsquo;t mean inefficiency—you just get used to it. If you\u0026rsquo;re chasing \u0026ldquo;a million-dollar annual income\u0026rdquo; or are passionate about \u0026ldquo;996 to launch projects,\u0026rdquo; you might find it unsuitable; but if you want a stable life, time with your family, and the energy to focus on personal growth, then Europe might be right for you.\n4. Families with Preschool-Aged Children # Children have a very strong ability to adapt to languages, especially between the ages of 0-5. Coming to Europe at this stage, language acquisition is almost \u0026ldquo;automatic,\u0026rdquo; the quality of education is good, and there are government subsidies.\nFor families, this is a \u0026ldquo;soft landing\u0026rdquo; golden period.\n5. Those Seeking Long-Term Development and Quality of Life # In China, programmers often face career anxiety after the age of 35; but in Europe, 35 is still a golden age in the workplace, with job hopping, salary increases, and stable growth being common.\nIf you are focused on long-term development, job stability, quality of life, education, and healthcare— Europe is indeed a worthwhile option.\nAnd for those who want to obtain an EU Green Card or long-term residency, Europe\u0026rsquo;s immigration policies are generally more favorable than those in the United States.\nPeople Who Might Not Adapt Well # 1. Poor English Skills and Unwillingness to Learn the Local Language # You might think, \u0026ldquo;I just write code,\u0026rdquo; but in reality:\nVisas, banks, hospitals, driver\u0026rsquo;s licenses, children\u0026rsquo;s schooling, daily life\u0026hellip; language is needed everywhere.\nGoogle Translate can help in a pinch, but relying on it for years can easily lead to anxiety and frustration.\n2. Impatient People Used to High Levels of Service # If you\u0026rsquo;re used to the domestic pace of \u0026ldquo;request today, launch tomorrow,\u0026rdquo; then the efficiency of European administration might drive you crazy.\nMany things here require appointments, waiting, and queuing, and many services simply don\u0026rsquo;t exist—like affordable takeout, massage, haircuts, or body scrubs. Either they are very expensive or they simply don\u0026rsquo;t exist.\n3. People Very Sensitive to Weather # The weather in many European countries isn\u0026rsquo;t exactly friendly to the Chinese:\nLong, cold, dark winters can easily lead to seasonal affective disorder Short summers, but very long days; sometimes it\u0026rsquo;s not dark until 10 pm Lots of rain, high humidity, clothes don\u0026rsquo;t dry easily If you absolutely need sunshine and distinct seasons, this could be a pain point.\n4. Children Already in Middle or High School # At this stage, children\u0026rsquo;s language learning ability decreases, but academic pressure increases.\nIn non-English-speaking countries (such as Germany, France, Lithuania, etc.), the courses are in the local language, and the exams are also localized, putting a lot of pressure on the children, and the parents also feel anxious.\n5. Primary Goal Is \u0026ldquo;Making Money\u0026rdquo; # If your goal is \u0026ldquo;accumulating initial capital\u0026rdquo; or \u0026ldquo;making quick money,\u0026rdquo; then:\nTaxes are high in Europe The cost of living is also not low Programmer salaries are decent, but far lower than in the United States, and often lower than in large Chinese companies Especially if you\u0026rsquo;re married, consider carefully the cost and benefits of coming to Europe alone for a few years.\nThe Child\u0026rsquo;s Age Is Crucial to Family Decisions: # Age Range Adaptability Recommendation 0–5 years old Highest Fast language learning, low educational pressure, recommended 6–11 years old Moderate Slightly higher language learning difficulty, extra tutoring needed 12+ years old Difficult High academic pressure, heavy language burden, careful consideration needed 18+ years old Easy University or postgraduate studies are a good choice; many educational resources, low fees Quick Assessment of Suitability # Characteristic Suitable for Europe Unsuitable for Europe English Ability Daily communication possible Poor English, unwilling to learn Personality Peaceful, willing to adapt to a slower pace Impatient, used to high efficiency and service Weather Tolerance Can accept long winters and less sunshine Extremely reliant on sunshine, afraid of the cold Family Stage Young children (0–5 years old) Children already in middle or high school Career Goals Seeking stable development and quality of life Primary goal is making money or rapid capital accumulation Cultural Attitude Willing to integrate, willing to learn the language Resistant to local culture, wants to stay in the \u0026ldquo;Chinese community\u0026rdquo; In Conclusion # There\u0026rsquo;s no standard answer to \u0026ldquo;should I go abroad?\u0026rdquo; Everyone\u0026rsquo;s background, aspirations, and resources are different.\nPersonally, I value the living environment, educational resources, healthcare system, and work-life balance. The biggest drawback is being far from home; I can only return to China once a year, and even a month feels too short.\nLanguage is a source of inconvenience in life, but I\u0026rsquo;m gradually adapting and learning.\nIf in the future I can achieve the freedom of \u0026ldquo;remote work + returning to China for a few months,\u0026rdquo; that would be ideal for me.\nLife abroad is a significant decision, especially for families with children. Embarking on a new life is not easy.\nHopefully, this article will help you more clearly determine whether you are suited to Europe. If you are considering relocation, feel free to leave a comment or share this with your friends!\nPlease specify the author and source when reprinting this article, and do not use it for any commercial purposes. Follow the public account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-07-13","externalUrl":null,"permalink":"/en/misc/run-or-not/","section":"Miscs","summary":"Who is suited to developing their career in Europe, and who might find it challenging? A programmer and father shares his experiences of life abroad.","title":"A Year Abroad — Not Everyone Is Cut Out for Europe","type":"misc"},{"content":"","date":"2025-07-13","externalUrl":null,"permalink":"/en/misc/","section":"Miscs","summary":"","title":"Miscs","type":"misc"},{"content":"","date":"2025-07-13","externalUrl":null,"permalink":"/en/tags/thoughts/","section":"Tags","summary":"","title":"Thoughts","type":"tags"},{"content":"大家好，我是DevOps攻城狮。\n最近读了一份挺有意思也挺震撼的报告：JFrog发布的《2025软件供应链现状报告》。这是JFrog连续多年基于其平台数据、CVE趋势分析、安全研究团队研究和1400位开发/安全从业者调查所形成的一份行业报告。\n我挑了一些对我们DevOps从业者、尤其是负责CI/CD和软件交付的人来说非常有参考价值的内容，分享如下：\n软件供应链，真的变了 # 报告开头就给出了几个数字，让人警觉：\n64% 的企业开发团队使用 7种以上编程语言，44% 使用10种以上； 一个普通组织 每年引入458个新包，也就是每月平均38个； Docker Hub 和 Hugging Face 上的镜像和模型数量仍在指数级增长； npm依然是恶意包的“重灾区”，但 Hugging Face 上的恶意模型增长了6.5倍。 如果说过去我们担心“你用的包有没有CVE”，现在可能得先问一句：\n你真的知道自己用了哪些包、拉了哪些模型吗？\n风险激增，不只来自漏洞 # 2024年，全球共披露了 33,000多个CVE，比2023年多了27%。但这只是“冰山一角”。\n报告揭示了一个更加令人担忧的趋势：“漏洞≠风险”，而“风险”正在从更多方向袭来：\n秘钥泄露：JFrog在公开仓库中扫描出 25,229个秘密token，其中 6,790个是“活的”； XZ Utils后门：攻击者假装是OSS维护者，潜伏多年后埋入后门，影响OpenSSH； AI模型的投毒：某些Hugging Face模型在加载时自动执行恶意代码（Pickle注入），悄无声息入侵机器； 误配置的代价：比如微软Power Pages因权限配置问题泄露大量用户数据，Volkswagen旗下公司因SaaS误配置泄露了80万台电动车的定位数据。 说实话，这些问题和“有没有扫描CVE”没什么关系。很多时候，是我们根本没意识到“这里也会出事”。\nAI爆发，风险也同步升级 # 今年Hugging Face新增了超过100万个模型和数据集，但同时，恶意模型也增长了 6.5倍。很多组织都开始将AI模型纳入业务，但：\n有 37%的企业靠“人工白名单”筛选模型使用； 很多AI模型使用的是Pickle格式，加载即执行，一不小心就中招； Hugging Face等平台上也出现了“挂羊头卖木马”的开源模型。 对于我们DevOps或者平台团队来说，这意味着：\n“模型”正在变成新的“依赖包”，也应该纳入供应链治理和安全扫描的范畴。\n安全实践现状：工具变多，人却更焦虑 # 报告里还有一个很现实的观察：\n安全工具越多，反而让人越看不清真相。\n73%的企业使用了 7个以上安全工具，但只有 43%同时扫描代码和二进制； 71%的组织允许开发者直接从公网拉包； 52%的组织一边允许公网拉包，一边又靠自动规则追踪来源； CVSS评分“虚高”问题越来越严重，JFrog分析后发现，88%的Critical CVE其实并不适用。 作为一线DevOps，我看到的是：工具越来越多，但我们似乎并没有真正“安心”下来。\n我们能做什么？ # 报告里没有提供“万无一失”的解决方案，但给出了一些务实建议，我结合自己的理解补充几点：\n治理从源头做起：不要再让开发者从公网自由拉包，使用内部代理仓库如 Artifactory/Nexus/Harbor； 扫描不止于代码：二进制扫描、容器镜像扫描和SBOM（软件物料清单）都需要纳入CI流程； 引入“适用性评估”：别光看CVE得分，更重要的是它是否真的适用于你的场景； 把AI模型当“依赖”管理：构建模型白名单、扫描模型安全性，甚至做模型SBOM； 限制新“匿名包”引入：不要因为某个库“突然火了”就引入，回顾XZ事件足够令人警醒。 写在最后 # 2025年的软件供应链比以往更大、更快、更复杂，也更脆弱。\n安全问题不是“有没有风险”，而是“你知道风险藏在哪里吗”。 一不小心，风险可能来自一个新同事引入的PyPI包，或者一位AI工程师下载的模型。\nJFrog这份报告虽然没有解决所有问题，但给我们敲了一个不小的警钟。\n如果你也在构建自己的DevOps流程、AI平台、或者仅仅是维护日常构建环境， 建议你认真想一想：\n你的“依赖”到底靠不靠谱？ 你的“扫描”真的能发现问题吗？ 你的“策略”是否跟得上变化了？\n欢迎在评论区聊聊你看到的“供应链怪现象”。也希望这篇分享对你有所启发。\n🧡 欢迎关注，一起做更好的技术实践。 📥 如果你需要这份《JFrog Software Supply Chain Report 2025》原报告， 可以在公众号后台回复关键词：“jfrog report 2025” 来获取报告下载链接。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-06-28","externalUrl":null,"permalink":"/posts/2025/jfrog-report/","section":"Posts","summary":"JFrog发布的《2025软件供应链现状报告》揭示了软件供应链的变化和风险，尤其是AI模型的安全问题。本文分享了报告中的关键发现和对DevOps从业者的启示。","title":"🧊2025软件供应链现状报告：开源时代，我们究竟在和谁打交道？","type":"posts"},{"content":" Issue # I have set up documentation publishing on Jenkins using the following Groovy code:\npublishHTML([ allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: \u0026#34;docs/build/html/\u0026#34;, reportFiles: \u0026#39;index.html\u0026#39;, reportName: \u0026#34;Documentation\u0026#34;, useWrapperFileDirectly: true ]) However, some badges from Shields.io do not display properly within the published documentation.\nHow to fix it # ✅ Working Script to Update Jenkins CSP in Script Console\nHere’s the correct and minimal Groovy script you should run in Manage Jenkins → Script Console:\nSystem.setProperty( \u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;default-src \u0026#39;self\u0026#39;; img-src * data:;\u0026#34; ) This allows images from any domain (img-src *), which includes Shields.io. If you want to restrict it more safely:\nSystem.setProperty( \u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;default-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; https://img.shields.io data:;\u0026#34; ) 🟡 Note: This change is temporary (in-memory only). It will reset if Jenkins restarts.\n✅ To Make It Permanent\nModify Jenkins startup arguments (depends on how you run Jenkins): If using /etc/default/jenkins (Debian/Ubuntu):\nJENKINS_JAVA_OPTIONS=\u0026#34;-Dhudson.model.DirectoryBrowserSupport.CSP=\\\u0026#34;default-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; https://img.shields.io data:;\\\u0026#34;\u0026#34; If using systemd service unit (CentOS/Red Hat-based or modern setups):\nEdit or override your jenkins.service file:\nEnvironment=\u0026#34;JAVA_OPTS=-Dhudson.model.DirectoryBrowserSupport.CSP=default-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; https://img.shields.io data:;\u0026#34; Then restart Jenkins: sudo systemctl restart jenkins Results # The issue with Shields.io badges not displaying on Jenkins has now been resolved.\nPlease credit the author and source when reposing this article. Commercial use is not permitted. You\u0026rsquo;re welcome to subscribe to my blog via RSS.\n","date":"2025-06-23","externalUrl":null,"permalink":"/en/posts/2025/jenkins-show-badge/","section":"Posts","summary":"How to temporarily fix it via the Script Console, and how to make it permanent by modifying Jenkins startup parameters. This method is suitable for internal Jenkins environments and has been tested on modern Jenkins installations.","title":"How to Fix Shields.io Badges Not Displaying in Jenkins","type":"posts"},{"content":"","date":"2025-06-12","externalUrl":null,"permalink":"/en/tags/bitbucket/","section":"Tags","summary":"","title":"Bitbucket","type":"tags"},{"content":" Background # Recently, I had a discussion with colleagues about a seemingly simple yet crucial issue:\nHow can we ensure that valuable information in PRs isn\u0026rsquo;t lost over time and with changes in tools?\nWhile we currently use Bitbucket for collaborative development, we might migrate to GitHub, GitLab, or other platforms in the future. These hosting platforms may change, but Git itself, as the core record of code history, is likely to remain for a long time.\nThis leads to the problem:\nPR page descriptions of change context, solutions, and key discussions, if only stored within the PR tool, are likely to \u0026ldquo;disappear\u0026rdquo; after platform migration. This information should be part of the commit message.\nSolutions We Considered: # Manually recording the solution in git commit -m — but this is easily overlooked or incomplete. Mimicking pip projects using a NEWS file to record each change — while this preserves information, it\u0026rsquo;s more suitable for generating release notes than recording the motivation or reasons for a change. Mandating that members write the content in Jira tickets — this creates tool silos and hinders quick understanding of history within the code context. Ultimately, we decided to: Use Bitbucket\u0026rsquo;s Commit Message Templates feature to directly write the PR description into the Git commit.\nBitbucket\u0026rsquo;s Commit Message Templates Feature # Bitbucket supports automatically generating commit messages when merging PRs, allowing useful information to be inserted via templates. The documentation is available here: 🔗 Pull request merge strategies - Bitbucket Server\nI\u0026rsquo;ve also seen similar functionality in GitLab, but GitHub seems to lack this feature.\nSee the GitLab Commit Templates official documentation\nYou can use the following variables in the template:\nVariable Name Description title PR Title id PR ID description PR Description approvers Current Approved Reviewers fromRefName / toRefName Source / Target Branch Name fromProjectKey / toProjectKey Source / Target Project fromRepoSlug / toRepoSlug Source / Target Repository crossRepoPullRequestRepo Source repository information for cross-repository PRs How We Used It? # In Bitbucket\u0026rsquo;s repository settings, you can configure the PR merge commit template. Find the settings:\nRepository settings -\u0026gt; Merge strategies -\u0026gt; Commit message template\nAfter configuration, when you merge a PR, Bitbucket will automatically write the PR title, description, and related ID into the final merge commit message.\nThis way, regardless of whether the team continues to use Bitbucket in the future, key PR information will forever be preserved in Git history.\nHere are the actual results:\n📥 Template Configuration Interface: 📤 Final Generated Git commit message: Summary # This small change helps us procedurally protect the context of code changes. It prevents PRs from being \u0026ldquo;temporary information containers\u0026rdquo; and instead makes them a natural part of Git history.\nIf you\u0026rsquo;re also using Bitbucket, give this feature a try.\nLet Git commit messages be not just code submissions, but records of decisions and evolution.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-06-12","externalUrl":null,"permalink":"/en/posts/2025/commit-message-template/","section":"Posts","summary":"Leverage Bitbucket’s Commit Message Templates feature to automatically write PR descriptions into Git commit messages, ensuring crucial information isn’t lost with tool changes.","title":"Enhancing Code Traceability — Automatically Including PR Descriptions in Git Commits","type":"posts"},{"content":" Background # In our Jenkins pipeline, we use the following configuration to manage resource consumption, especially for builds that typically take more than 30 minutes to complete:\ndisableConcurrentBuilds abortPrevious: true This setting prevents concurrent builds on the same branch or pull request. When a new build is triggered while a previous one is still running, the older build is automatically aborted.\nThis helps conserve resources and avoids redundant builds when developers push multiple updates shortly after opening a pull request.\nThe problem # But the problem is:\nSometimes, during merges, an ongoing build gets aborted midway or near completion because a new build was triggered for the same branch. They requested that if a build is already running, new builds triggered on the same branch should wait in the queue instead of aborting the current build.\nBefore the changes: # Let\u0026rsquo;s take a picture bellow of the release/x.x.x branch.\nJob #104 was aborted because a new merge was triggered on the release/x.x.x branch. Job #105 was also aborted for the same reason, due to another new merge on the release/x.x.x branch. After the changes: # Job #106 will continue running without being canceled, even if a new merge occurs. Job #107 will wait in the queue until Job #106 finishes before starting. Initial Thoughts # Initially, I believed that disableConcurrentBuilds was a global setting that applied uniformly across all branches and pull requests.\nAfter researching via ChatGPT and Google, I found that selectively applying this setting per branch is not straightforward and requires adding complexity to the existing pipeline.\nThe Solution # But there is a simple solution? Yes!\nI implemented and tested conditional logic that:\nRetains abortPrevious: true for pull request builds, Disables it for specific branches such as devel and release. Here the code snap about how to implement it.\n// vars/build.groovy def call() { def isAboutPrevious = true if (env.BRANCH_NAME == \u0026#39;devel\u0026#39; || env.BRANCH_NAME.startsWith(\u0026#39;release/\u0026#39;)) { isAboutPrevious = false // disable abortPrevious for devel and release branches. } pipeline { options { disableConcurrentBuilds abortPrevious: isAboutPrevious } stages{ // .... } } } The Result # The changes passed testing and have been merged into our shared pipeline library.\nNow, for the devel and release branches in multi-branch pipeline:\nJenkins no longer aborts running builds when new builds are triggered. Instead, it queues subsequent builds, improving stability and predictability for our QA workflows. Please credit the author and source when reposing this article. Commercial use is not permitted. You\u0026rsquo;re welcome to subscribe to my blog via RSS.\n","date":"2025-06-04","externalUrl":null,"permalink":"/en/posts/2025/jenkins-concurrent-build/","section":"Posts","summary":"In Jenkins, the disableConcurrentBuilds option is used to manage concurrent builds. This article explains how to conditionally set the abortPrevious value based on the branch being built, allowing for more flexible build management.","title":"How to Change abortPrevious Value in Jenkins?","type":"posts"},{"content":"","date":"2025-06-02","externalUrl":null,"permalink":"/en/tags/ci/","section":"Tags","summary":"","title":"CI","type":"tags"},{"content":"","date":"2025-06-02","externalUrl":null,"permalink":"/en/tags/ci/cd/","section":"Tags","summary":"","title":"CI/CD","type":"tags"},{"content":"This article stems from a WeChat Moments post I saw from a former colleague.\nI feel like he\u0026rsquo;s blatantly praising me :) (Sorry, had to brag a little~)\nBut honestly, for the past few years in the CI/CD field, I\u0026rsquo;ve been consistently doing two things:\nKeeping up with the latest industry practices; Selecting and implementing parts suitable for our business, or writing articles to share. However, I\u0026rsquo;ve noticed a common misconception:\nMany people think that once CI/CD is done, it\u0026rsquo;s done—a set-and-forget solution.\nThis is not the case.\nRome Wasn\u0026rsquo;t Built in a Day, Neither Was CI/CD # Whether it\u0026rsquo;s the CI/CD pipeline, or the underlying programs, tools, libraries, and platforms, they all belong to the infrastructure. A significant characteristic of infrastructure is:\nIt requires long-term investment and continuous maintenance.\nOtherwise, no matter how well you build it now, after a few years, it will become a bloated, uncontrolled \u0026ldquo;technical debt wasteland\u0026rdquo; due to lack of maintenance. Eventually, you\u0026rsquo;ll have to start over.\nA few examples will make this clear:\nRisks Quickly Emerge Without Maintenance # 1. Security Risks: # If you haven\u0026rsquo;t paid attention to CVE-2024-23897, you might not know that your Jenkins has an arbitrary file reading vulnerability, risking intrusion by attackers.\n2. Missing Functionality and Compatibility Issues: # Not checking Jenkins\u0026rsquo; update logs might cause you to miss critical fixes or new features; Not updating the CI/CD toolchain might suddenly cause your code to fail compilation or tests.\n3. Technical Debt Accumulation and Increased Maintenance Costs: # Without introducing new practices and tools, the pipeline will become increasingly complex and difficult to maintain, leading to a worse development experience.\nNow Let\u0026rsquo;s Look at the Python Project Side # 1. Still Using setup.py? # Then you might have missed the unified build ecosystem brought by pyproject.toml introduced by PEP 518.\n2. Still Using pip install to Install CLI Tools? # Then you might not know how pipx and uvx can manage tool dependencies more conveniently and efficiently.\n3. Unfamiliar with PEPs, Package Structure, and Release Specifications? # This easily leads to writing non-standard, hard-to-maintain, and non-reusable code.\nThese issues, seemingly minor on the surface, hide behind them maintenance costs, team collaboration efficiency, and the sustainability of the project\u0026rsquo;s future development.\nTherefore, refactoring is a daily task in software engineering.\nMany times, this work is not obvious, and is even easily considered \u0026ldquo;doing too much,\u0026rdquo; \u0026ldquo;fiddling around,\u0026rdquo; or \u0026ldquo;wasting time.\u0026rdquo;\nBut in reality, precisely these \u0026ldquo;hidden tasks\u0026rdquo; are the true foundation of software engineering.\nWithout refactoring, the system will only become more bloated; Without continuous evolution, CI/CD will eventually become a mess; Without continuous learning and exploration, you will drift further and further from best practices.\nSo, I hope this article can give you some inspiration:\nCI/CD is not a one-time construction project, but a continuously evolving, constantly refactored, and growing DevOps system.\nIf you are also doing this \u0026ldquo;invisible work,\u0026rdquo; don\u0026rsquo;t be discouraged. You are laying the foundation for the long-term sustainability of the entire system.\nHave you encountered any related pitfalls? Welcome to share your story in the comments.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-06-02","externalUrl":null,"permalink":"/en/posts/2025/code-refactor/","section":"Posts","summary":"In DevOps, CI/CD pipelines require continuous maintenance and refactoring. This article explores why CI/CD is not a one-time construction project, but a system that requires long-term investment and continuous evolution.","title":"CI/CD—Not a One-Time Project, but a Continuously Evolving System","type":"posts"},{"content":"","date":"2025-05-29","externalUrl":null,"permalink":"/en/tags/asdf/","section":"Tags","summary":"","title":"Asdf","type":"tags"},{"content":"Recently, I released a new repository under the cpp-linter organization called asdf-clang-tools. This project is a fork of amrox/asdf-clang-tools. Due to the original author\u0026rsquo;s lack of maintenance for many years, I have repaired, upgraded, and expanded its functionality, giving it a new lease on life. In short, asdf-clang-tools is an asdf plugin for installing and managing Clang Tools, such as clang-format, clang-tidy, clang-query, and clang-apply-replacements.\nA New Installation Method: asdf in Addition to pip # Previously, I released the clang-tools-pip toolkit, allowing users to install a complete set of Clang executable tools (including clang-format, clang-tidy, clang-query, and clang-apply-replacements) using pip install clang-tools.\nasdf-clang-tools provides another approach—using the asdf version manager to install these tools. In short, this gives developers who prefer using asdf to manage tool versions another option.\nThese two methods are not mutually exclusive: you can easily install and manage Clang tools using either pip or asdf. The choice depends on your workflow and personal preferences.\nWhat is the asdf Version Manager? # Many developers may not be familiar with asdf. asdf is a polyglot, multi-tool version manager.\nIt can manage versions of multiple runtime environments with a single command-line tool and supports a plugin mechanism.\nFor example, you can use asdf to manage versions of Python, Node.js, Ruby, and other languages, as well as Clang tools (like the asdf-clang-tools I introduced).\nAll tool version information is recorded in a shared .tool-versions file, allowing teams to easily synchronize configurations across different machines.\nIn short, the advantage of asdf is \u0026ldquo;one tool to manage all dependencies,\u0026rdquo; unifying the versions of various tools required by a project and eliminating the hassle of using different version managers for each tool.\nInstallation and Usage Example # Installing Clang tools using asdf-clang-tools is very simple. Assuming you have already installed asdf, simply follow the instructions in the official repository:\nFirst, add the plugin: Using clang-format as an example, run the following in your terminal:\nasdf plugin add clang-format https://github.com/cpp-linter/asdf-clang-tools.git Similarly, clang-query, clang-tidy, clang-apply-replacements, and other tools use the same repository address; just change the plugin name to the corresponding name.\nView available versions: After adding the plugin, you can run asdf list all clang-format to list all installable clang-format versions.\nInstall the tool: Choose a version (e.g., the latest latest), and execute:\nasdf install clang-format latest This will download and install the specified version of the clang-format binary.\nSet the global version: After installation, you can execute:\nasdf set clang-format latest This will write the version to the ~/.tool-versions file, making it globally available. After this, you can directly use commands like clang-format on the command line.\nAfter completing the above steps, clang-format, clang-tidy, and other tools will be integrated into asdf management. Refer to the official asdf documentation for more details.\nWelcome to Try and Provide Feedback # In summary, asdf-clang-tools provides a new installation method for developers who need Clang Tools.\nIt complements other tools from the cpp-linter organization (such as clang-tools-pip).\nI sincerely welcome everyone to try the entire C/C++ lint solution provided by cpp-linter and choose the tools that best suit their workflow.\nAlso, if you encounter any problems or have suggestions for improvement during use, please submit them through GitHub Issues, the discussion area, etc. Let\u0026rsquo;s work together to improve the Cpp Linter toolchain and make C/C++ formatting and static analysis more convenient and efficient!\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-05-29","externalUrl":null,"permalink":"/en/posts/2025/asdf-clang-tools/","section":"Posts","summary":"asdf-clang-tools is an asdf plugin for installing and managing Clang Tools, such as clang-format, clang-tidy, clang-query, and clang-apply-replacements.","title":"asdf-clang-tools — A New Way to Install Clang Tools Using asdf","type":"posts"},{"content":"","date":"2025-05-29","externalUrl":null,"permalink":"/en/tags/clang-tools/","section":"Tags","summary":"","title":"Clang-Tools","type":"tags"},{"content":" 分享两个最近的体会 # AI 让人感到“虚” # 老同事来欧洲出差，和他聊了很多 —— 从工作到生活，从技术到 AI。\n当我问他：“你对 AI 有什么感受？”\n他说了一个字：虚！\n这让我有些意外。他可是我心中的技术大牛，工作了十几年，积累了很多经验和技能。但如今，他却觉得自己在 AI 面前变得“虚”了。\n我也有同感。可能在一年前，我还觉得，自己在工作、博客、开源上的不断积累，会在下一份工作中起到很大的帮助；但 AI 的出现，让我开始怀疑：\n我过去四五年的努力是否还有意义？\nAI 的确让每个人的工作效率都有了显著提升。\n有同事开玩笑说：“我路过每个人的桌面，都看到 ChatGPT 开着。”；另一位同事接着说：“我都同时开好几个。”\n这样的对话一度让我感到沮丧。\n我开始怀疑：\n自己在工作中是否还有不可替代的价值？ 自己在开源社区的贡献是否还有意义？ 自己还在坚持写博客是否还有用处？ 直到我在 LinkedIn 上看到还有不少岗位在招聘，意识到手头的工作依然需要推进，这才慢慢把我拉回现实：AI 不会取代我们的职位，但一定会改变我们的做事方式。\n每天一小步 # 另外还有一个小的体会是：每天一小步。\n无论是工作还是生活，有目标感是好事。但如果目标太大，往往难以坚持，甚至会被拖垮。\n过去我常常给自己设定理想化的目标，比如“写一本书”或“翻译一本书”。但这样的目标需要投入大量时间和精力，最后常常因为现实的工作和生活节奏被搁置。\n后来我发现，其实偶尔写一篇小文章，或者维护一个小项目，更容易坚持。\n当一件事不再那么“重”，它也就没那么难开始了。\n所以，我们既要抬头看路，树立目标，也要低头走好每一步。\n这两点体会，虽然简单，却是我在日常工作和生活中逐渐领悟到的。\nAI 带来的冲击还在继续，我们每个人都要拥抱变化；而“每天一小步”的坚持，也在悄悄积累力量。\n愿我们都能在变化中找到节奏，在前行中保持方向。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-05-26","externalUrl":null,"permalink":"/posts/2025/ai/","section":"Posts","summary":"AI 的出现让很多人感到“虚”，但它不会取代我们的职位，而是改变我们的做事方式。本文分享了对 AI 的体会和每天一小步的坚持。","title":"ChatGPT 一开，谁还去“努力”？","type":"posts"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/en/tags/confluence/","section":"Tags","summary":"","title":"Confluence","type":"tags"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/en/tags/documentation/","section":"Tags","summary":"","title":"Documentation","type":"tags"},{"content":"I don\u0026rsquo;t know if you\u0026rsquo;ve used Confluence or similar Wiki tools in your enterprise. When I first encountered it, I thought it was great: powerful features, support for various formatting styles, ability to insert images, videos, icons, and view historical versions—the user experience was far easier than Git.\nBut slowly, I discovered a huge problem: everyone can create and maintain their own Wiki pages.\nInitially, this freedom seemed like an advantage. But over time, problems arose: the same topic might have multiple versions written by different people. Especially when projects or products are transferred from one team to another (common in multinational companies), new team members may not continue maintaining the original documents but instead prefer to start from scratch, recording their own understanding.\nAs a result, old Wikis become obsolete with personnel turnover (the original authors may have already left), and the new Wiki content is incomplete or even inaccurate. Knowledge sedimentation is not only ununified but even more chaotic.\nI\u0026rsquo;ve always believed that Wiki tools themselves are good, but without a unified management mechanism and a Pull Request approval process like Git, they easily become breeding grounds for garbage information.\nIn contrast, open-source communities do a much better job.\nTake the Python community as an example:\nThe content of the Python website https://www.python.org is maintained through the GitHub repository python/pythondotorg; The content of the developer\u0026rsquo;s guide https://devguide.python.org is hosted on the GitHub repository python/devguide. Anyone with suggestions for modifying these documents must submit them via PR, which undergoes review and CI checks before merging. Moreover, because it\u0026rsquo;s an open-source project, community users actively participate in feedback and improvement, helping to maintain high-quality documentation over the long term.\nBut in contrast, inside enterprises, it\u0026rsquo;s completely different:\nMultiple people write their own Wikis, resulting in inconsistent quality; Many content silos lack maintenance; once personnel turnover occurs, old documents become \u0026ldquo;obsolete\u0026rdquo;; More importantly, internal documentation lacks a public review mechanism and external feedback channels, making it difficult to detect or correct errors. Another point might be more realistic and brutal: within enterprises, employees lack the motivation to maintain documentation. Because a meticulously written, flawless \u0026ldquo;perfect document\u0026rdquo; might one day mean you can be \u0026ldquo;seamlessly replaced.\u0026rdquo; In contrast, keeping key details in your own head provides a greater sense of \u0026ldquo;job security.\u0026rdquo;\nTherefore, document governance is fundamentally not about tools but about people. Without cultural and process support, even the most advanced tools can become dumping grounds for information garbage.\nDocumentation and code are inseparable. My experience in the open-source community has taught me: truly excellent individuals often single-handedly support a team\u0026rsquo;s quality and rhythm. They are passionate about technology, take proactive responsibility, and are willing to share, driving healthy project development.\nIn contrast, problems in some enterprise projects often stem from the opposite direction. When team members lack a sense of ownership, personnel turnover is frequent, or individuals with mediocre abilities have the most opinions, the result is only more piling onto the legacy codebase.\nFinally, have you had similar experiences? How does your company manage internal documentation and code? Please share your thoughts in the comments.\nPlease indicate the author and source when reprinting this article. Do not use it for any commercial purposes. Follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2025-05-14","externalUrl":null,"permalink":"/en/posts/2025/docs-and-code/","section":"Posts","summary":"In enterprises, documentation tools like Wiki and Confluence, if lacking unified management and review mechanisms, can lead to information chaos and knowledge sedimentation failure. This article explores how to avoid this situation and draws on the successful experiences of open-source communities.","title":"Still Using Wiki/Confluence? You Might Be Producing Garbage","type":"posts"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/en/tags/wiki/","section":"Tags","summary":"","title":"Wiki","type":"tags"},{"content":"","date":"2025-05-05","externalUrl":null,"permalink":"/en/tags/pip/","section":"Tags","summary":"","title":"Pip","type":"tags"},{"content":"If you\u0026rsquo;re used to pip install, manually creating virtual environments, and managing requirements.txt yourself, you might be surprised by uv.\nThis is a Python package management tool developed by the Astral team and written in Rust. It not only replaces the functionality of pip, venv, and pip-tools, but also provides faster dependency resolution and a more modern project management approach.\nStart with uv init to create a project skeleton with one command # We won\u0026rsquo;t start with \u0026ldquo;how to install uv,\u0026rdquo; but with \u0026ldquo;how to use uv to create a project.\u0026rdquo;\nuv init After running this command, uv will help you:\nCreate a .venv virtual environment; Initialize the pyproject.toml configuration file; (Optional) Add dependencies; Generate a uv.lock lock file; Set .venv as the default environment for the current directory (no manual activation needed); The entire process only requires one command to complete what used to take multiple steps, making it a better starting point for building Python projects.\nInstall dependencies using uv add (instead of pip install) # The traditional way is:\npip install requests But in the uv world, adding dependencies looks like this:\nuv add requests The benefits are:\nAutomatically writes to pyproject.toml\u0026rsquo;s [project.dependencies]; Automatically installs into .venv; Automatically updates the uv.lock lock file; No need to maintain requirements.txt anymore. If you want to add development dependencies (such as testing or formatting tools), you can:\nuv add --dev pytest ruff Want to remove dependencies?\nuv remove requests Running project scripts or tools: uv venv + uvx # uv\u0026rsquo;s virtual environment is installed by default in .venv, but you don\u0026rsquo;t need to source activate every time. Just execute:\nuv venv This ensures that .venv exists and is automatically configured as the default Python environment for the current shell. After that, you can run scripts or tools without worrying about path issues.\nEven better, uv provides a uvx command, similar to pipx and Node.js\u0026rsquo;s npx, which allows you to directly run CLI tools installed in the project.\nFor example, let\u0026rsquo;s use ruff to check or format Python code:\nuvx ruff check . uvx ruff format . Now with uvx, you don\u0026rsquo;t need to install a bunch of global tools, nor do you need to use pre-commit to unify command calls—use it directly within the project, it\u0026rsquo;s cross-platform and convenient.\nExample Project Structure # After uv init and some uv add commands, a clean Python project structure might look like this:\nmy-project/ ├── .venv/ ← Virtual environment ├── pyproject.toml ← Project configuration (dependencies, metadata, etc.) ├── uv.lock ← Locked dependency versions ├── main.py ← Project entry script User Experience? # I\u0026rsquo;ve recently adopted uv as the default tool for new projects:\nNo more manually writing requirements.txt No more struggling with mixing poetry.lock and pyproject.toml Dependency installation is very fast, 3-5 times faster for large projects on the first installation. Combined with ruff as a Lint + Format tool, there\u0026rsquo;s no need to install black and flake8 separately. In CI, I\u0026rsquo;m also gradually replacing pip install -r requirements.txt with it, using the lock file to build the environment for stronger consistency.\nSummary # If you:\nAre dissatisfied with the slow speed of pip install; Don\u0026rsquo;t want to write a bunch of requirements files; Want a more modern, faster, and more automated Python project structure; Then you should try uv. It\u0026rsquo;s a faster and more modern package manager toolset.\nStarting with your next project, why not start with uv init?\nProject address: https://docs.astral.sh/uv/\n","date":"2025-05-05","externalUrl":null,"permalink":"/en/posts/2025/uv/","section":"Posts","summary":"uv is a Python package management tool developed by the Astral team. It replaces the functionality of pip, venv, and pip-tools, offering faster dependency resolution and a more modern project management approach.","title":"Still using pip and venv? You're outdated! Try uv!","type":"posts"},{"content":"","date":"2025-05-05","externalUrl":null,"permalink":"/en/tags/uv/","section":"Tags","summary":"","title":"Uv","type":"tags"},{"content":"Today is my third day attending PyCon LT 2025: AI and ML Day, focusing on LLMs and Neural Networks.\nThe full schedule can be found here: PyCon LT 2025\nAI is developing rapidly, and I wanted to take this opportunity to listen to discussions about it. Although I had to return to the company to handle work in the afternoon, and only attended the morning sessions, I still gained a lot.\nMy main takeaway is that AI has become a technology that everyone must learn and apply.\nA Simple Log of Today\u0026rsquo;s Experiences # I arrived at the venue again at around nine o\u0026rsquo;clock this morning. After grabbing a coffee, I went straight into the first talk.\nThe first talk was titled \u0026ldquo;Open-source Multimodal AI\u0026rdquo;. The speaker was from France and a member of the Hugging Face team. Many of you might have seen this emoji 🤗—they are the developers of the well-known Transformers and Diffusers open-source libraries. They also have the Hugging Face Hub website where users can upload, share, and download various AI models and datasets. This presentation mainly shared the content of multimodal AI, introducing some background knowledge, related libraries, basic APIs of multimodal and open-source AI, how to start using open-source models, and some currently popular open-source models.\nThe second talk was \u0026ldquo;Knowledge Bases \u0026amp; Memory for Agentic AI\u0026rdquo;, introducing the origins of Agent AI (Agentic AI). The presentation covered how to design prompts to guide LLMs to use tools and plan solutions for complex queries; learning function calls, and how to utilize this LLM feature as the foundation of an Agent. The speaker was also a woman, from the Netherlands.\nThe third talk was \u0026ldquo;EGTL data-processing model prototype using Python\u0026rdquo;, which discussed how to add a \u0026ldquo;generation\u0026rdquo; step supported by generative AI to the traditional ETL (Extract, Transform, Load) process, expanding it to EGTL (Extract, Generate, Transform, Load). It demonstrated how a Python pipeline based on a data warehouse can automatically extract data, generate new insights, and achieve optimized transformations. It also explored practical workflows, real-world use cases, and best practices to help data projects apply the EGTL method.\nThe final talk of the morning was \u0026ldquo;AI 360: From Theory to Transformation\u0026rdquo;, which shared the development history of AI. From the dual perspectives of data and models, it reviewed the evolution of artificial intelligence, recounting the development from early symbolic systems to today\u0026rsquo;s advanced data-driven techniques. Attendees learned how the growing datasets and increasingly complex model architectures interact to propel AI from a theoretical curiosity to a global innovation catalyst.\nI left after lunch because I had to return to the company for work in the afternoon.\nInspiration # I listened to several talks today, and the proportion of female speakers was still quite high. The first two speakers were particularly good, with clear logic and strong delivery.\nToday\u0026rsquo;s AI-focused topics made me realize one thing: AI development waits for no one. Anyone still watching and hesitating is already falling behind. The only way out is to embrace it as soon as possible, instead of wasting time resisting or doubting it.\nFrankly speaking, the most important thing to do now is not to learn how to \u0026ldquo;use\u0026rdquo; AI, but rather to ask the reverse question: What tasks should be redone by AI? Your entire past workflow may already be inefficient; it\u0026rsquo;s just that AI hasn\u0026rsquo;t had a chance to overhaul it yet.\nIncidentally, 99% of AI courses on the market are essentially a scam—they use DeepSeek, ChatGPT to extract some information, put it into a Notion template, and then sell it to you. It seems like they\u0026rsquo;re teaching you to use AI, but essentially they are \u0026ldquo;using AI to teach you to pay money\u0026rdquo;.\nWhat you should really be doing is focusing on your industry and job, and seriously considering two questions: Is AI replacing my job? Can I use AI to achieve better results than others?\nSummary # This is a simple record of my participation in the third day of PyCon.\nOverall, this PyCon LT trip was very worthwhile. Although I didn\u0026rsquo;t gain many specific technical details, hearing from my peers, whether from a technical perspective or in terms of interpersonal communication, was valuable.\nI also attended the conference with several colleagues, which allowed for more in-depth exchanges, a very rare opportunity.\nIf I have the opportunity to attend PyCon again in the future, I hope to focus more on core projects like CPython and PyPA, and also hope to learn more about DevOps and AI-related topics.\nI\u0026rsquo;ll share my PyCon notes here. Looking forward to seeing you next time!\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo; (DevOps Siege Lion)\n","date":"2025-04-29","externalUrl":null,"permalink":"/en/misc/pycon-lt-d3/","section":"Miscs","summary":"Today was the final day of PyCon LT 2025, themed AI and ML Day.  It featured several excellent talks by female speakers and provided new insights into data science and AI.","title":"Full Record｜PyCon LT 2025 Day Three - Can AI Replace You?","type":"misc"},{"content":"","date":"2025-04-29","externalUrl":null,"permalink":"/en/tags/lithuania/","section":"Tags","summary":"","title":"Lithuania","type":"tags"},{"content":"","date":"2025-04-29","externalUrl":null,"permalink":"/en/tags/pycon/","section":"Tags","summary":"","title":"PyCon","type":"tags"},{"content":"Today was my second day at PyCon LT 2025: Data Day, focusing on Dataframes, Databases, and Orchestration.\nThe full schedule can be viewed here: PyCon LT 2025\nThis is definitely an area I\u0026rsquo;m not very familiar with, but I still wanted to listen and see if I could gain anything, after all, the ticket included this day\u0026rsquo;s program. Unexpectedly, what truly moved me today were the female speakers among the technical speakers—not only were their talks informative, but they also had passion, style, and even led me to follow several blogs…\nJust a Running Log of Today\u0026rsquo;s Feelings # I arrived at the venue a little after 9 am, just in time for the first talk to begin. I grabbed a cup of coffee and went in.\nThe topic was \u0026ldquo;Build Your Own (Simple) Static Code Analyzer\u0026rdquo;. This speaker was from New York, a core developer of numpydoc, and the author of the book \u0026ldquo;Hands-On Data Analysis with Pandas\u0026rdquo;. It was truly insightful; she talked about how to build your own static code analyzer using AST (Abstract Syntax Tree). This was my first time learning about the working principles of these Linter tools, and it was very enlightening.\nI also felt that she not only had technical prowess but also enjoyed public speaking and building her personal influence. Her blog, https://stefaniemolin.com/, is also worth checking out.\nNext, I listened to a talk titled \u0026ldquo;Data Warehouses Meet Data Lakes,\u0026rdquo; presented by an Italian with a strong Italian accent. She discussed the challenges faced by data warehouses and the technologies they use to build data architectures. I\u0026rsquo;m not very familiar with this area, but I learned that, like DevOps, it ultimately requires building a pipeline to complete data collection and analysis.\nThen I listened to \u0026ldquo;Cutting the price of Scraping Cloud Costs,\u0026rdquo; which discussed the technologies they used to build pipelines for calculating cloud pricing.\nThe last talk of the morning was \u0026ldquo;cluster-experiments: A Python library for end-to-end A/B testing workflows,\u0026rdquo; primarily introducing his Python library for A/B testing. I suspect this talk was partly a promotion of his open-source project.\nBy the time I finished the talks, it was after 12:30 pm. Seeing many people lining up for lunch on the first floor, I went to the third floor to catch up on work.\nAfter finishing my work, a colleague messaged me to take a group photo, so I went downstairs to get lunch.\nThe lunch line was long again. After finishing lunch, it was almost 2 pm, and the afternoon talks were about to begin.\nThe first talk in the afternoon was \u0026ldquo;Accelerating privacy-enhancing data processing,\u0026rdquo; concerning the challenges of data processing for cancer research in the real world. The speaker also introduced their technology stack and thoughtfully prepared a Lego brick at the beginning because they used Lego elements in their PPT to illustrate points. In the Q\u0026amp;A session, they offered the Lego to whoever answered correctly, and my colleague got it.\nThe second talk I attended was \u0026ldquo;Working for a Faster World: Accelerating Data Science with Less Resources,\u0026rdquo; which introduced a tool called Panel for data exploration and building web applications. I didn\u0026rsquo;t gain much else from this talk.\nNext, I attended \u0026ldquo;Organize your data stack using Dagster,\u0026rdquo; introducing the open-source data orchestration tool Dagster. It was an excellent presentation. I think the Dagster developers should thank her. I bet many who hadn\u0026rsquo;t heard of this tool before now want to try it out and see what benefits it brings and how to use it in practice. It\u0026rsquo;s worth mentioning that this speaker is also a passionate female technologist who gave two talks at this PyCon—impressive!\nThe last talk I attended was \u0026ldquo;Top 5 Lessons from a Senior Data Scientist.\u0026rdquo; This one was more enlightening. The speaker was also a woman, currently a freelance data scientist. I found that there are indeed many women in the data field. Her talk didn\u0026rsquo;t involve technology, focusing purely on experience—the top five lessons from a senior data scientist. The points were applicable to those of us in the workplace. Her personal website is also well-maintained and worth learning from.\nInspiration # I noticed that the female speakers I heard today generally had better personal websites than the male speakers.\nBut, all the speakers were great; at least they were passionate, had ideas, and were willing to share their knowledge and thoughts with the audience on stage. This alone is worth learning from.\nEspecially those female speakers who are technically skilled, confident in their expression, and willing to share—they really impressed me and inspired me.\nThis is my \u0026ldquo;running log\u0026rdquo; of the second day of PyCon.\nI hope tomorrow\u0026rsquo;s AI and ML Day will bring more inspiration. See you tomorrow!\nPlease indicate the author and source when reprinting this article. Please do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-04-24","externalUrl":null,"permalink":"/en/misc/pycon-lt-d2/","section":"Miscs","summary":"Today was Data Day at PyCon LT 2025, focusing on Dataframes, Databases, and Orchestration. While not deeply familiar with these areas, I was deeply impressed by several female speakers and gained a lot.","title":"Full Record｜PyCon LT 2025 Day Two - Charmed by Several Female Developers","type":"misc"},{"content":"This was my first time attending a Python conference, and unexpectedly, it was abroad.\nAlthough the scale and the renown of the speakers weren\u0026rsquo;t as impressive as the upcoming PyCon Europe conference in June in the Czech Republic, it was still a great experience.\nPyCon LT 2025 spans three days:\nDay One (April 22nd): Python Day (Web, Cloud, IoT, Security) Day Two (April 23rd): Data Day (Dataframes, Databases, Orchestration) Day Three (April 24th): AI and ML Day (LLM, Neural Networks) The full schedule can be viewed here: PyCon LT 2025\nFirst Impressions: # I arrived at the venue just after 9 am. After scanning a QR code, a staff member gave me a badge with my name on it, and then I went to the opening ceremony.\nThe first presentation of the day was titled \u0026ldquo;Ethics, Privacy and a Few Other Words\u0026rdquo;. To be honest, I wasn\u0026rsquo;t very familiar with this topic. It mainly discussed ethical and privacy-related issues, and I didn\u0026rsquo;t quite understand everything. My colleague next to me, however, was listening intently and occasionally filled me in on some details. This made me realize my English shortcomings—I can understand technical content, but once the topic veers away from technology, I struggle to keep up.\nThe speaker was certainly outspoken; some of his opinions even made me worry about his ability to secure positions at some large companies or potential travel restrictions to certain countries.\nThen came the coffee break, where everyone gathered to drink water, coffee, and eat some desserts.\nFollowing this, multiple sessions with different topics ran concurrently in different rooms, allowing attendees to choose based on their interests.\nNext, I attended a session titled \u0026ldquo;Code Review the Right Way\u0026rdquo;. This presentation didn\u0026rsquo;t delve into many technical details or tools but shared best practices for code review and how to build a code review culture. I thought it was quite good, especially the emphasis on humility, tact, and politeness when reviewing others\u0026rsquo; code to avoid conflicts.\nThen I went to another session, \u0026ldquo;What We Can Learn from Exemplary Python Documentation\u0026rdquo;. This topic aligned with something I\u0026rsquo;ve been working on recently—converting Markdown documentation in Python projects to reStructuredText and using Sphinx to generate HTML documentation.\nAfter listening for a while, I felt the information density wasn\u0026rsquo;t very high, so I left for another session: \u0026ldquo;Using Trusted Publishing to Ansible Release\u0026rdquo;.\nThis was mainly because I\u0026rsquo;m familiar with Trusted Publishing and Ansible, and I wanted to see if there was anything new. The speaker was a researcher from PFS and the release manager for Ansible. However, the topic wasn\u0026rsquo;t particularly novel; perhaps because I already use these tools. She demonstrated a demo of releasing a Python package to PyPI via GitHub using Trusted Publishing—nothing groundbreaking.\nBy 12:30 pm, the session ended, just in time for lunch. Lunch was included in the conference; the organizers apparently booked a restaurant in the venue. Attendees could order from self-service kiosks, just like at McDonald\u0026rsquo;s or KFC. Nobody stopped you from ordering multiple meals; the kiosks were there for everyone\u0026rsquo;s use.\nAfter lunch, the next session started at 2 pm. With some time to kill, a colleague and I participated in a stamp-collecting activity.\nThe organizers and sponsors had set up some mini-games, such as Snake, and other small activities, allowing participants to collect stamps. You also needed to take photos with speakers to get specific stamps.\nI eventually collected four stamps, which qualified me for a spin of the prize wheel. I spun twice: the first time I won \u0026ldquo;another spin\u0026rdquo;, and the second time I won a PyCon T-shirt, much to the envy of my colleagues.\nThe first afternoon session I attended was \u0026ldquo;Python Containers: Best Practices\u0026rdquo;. It was well-presented, but I already knew most of the content because I\u0026rsquo;d previously written an article on Docker container best practices. It felt like I could have skipped it.\nNext, I attended a session called \u0026ldquo;Do Repeat Yourself\u0026rdquo;. This one was a bit misleading by the title, but the speaker had clearly put a lot of effort into the preparation. Instead of using a PPT, he created a website using FastAPI + CSS + HTML for his presentation, complete with music. It was very well-done and quite cool. Although the technical content wasn\u0026rsquo;t new, it showed me that there are other ways to deliver a presentation besides PPT, inspiring me to try building a web app using FastAPI.\nFollowing this was \u0026ldquo;Coding Aesthetics: PEP 8, Existing Conventions, and Beyond\u0026rdquo;, mainly about Linters, Pythonic principles, etc.—things I\u0026rsquo;m already quite familiar with, so I won\u0026rsquo;t elaborate.\nThen came the afternoon coffee break, where I chatted with my colleagues about the sessions they\u0026rsquo;d attended. I was also randomly interviewed by an organizer, so I might appear in a future video.\nThe final session was \u0026ldquo;Skip the Design Patterns: Architecting with Nouns and Verbs\u0026rdquo;. The speaker was a \u0026ldquo;heavyweight\u0026rdquo; guest, appearing to be in his fifties or sixties, and many people in the audience seemed to know him.\nThe speaker\u0026rsquo;s main point was that Python programmers rarely spend time considering design patterns and why design patterns seem less relevant in Python. He walked the audience through refactoring a Python project, highlighting three different ways of thinking and explaining how to apply them to architecture.\nThat\u0026rsquo;s my \u0026ldquo;daily log\u0026rdquo; from the first day of PyCon.\nHopefully, tomorrow\u0026rsquo;s Data Day will offer even more learning opportunities. See you tomorrow!\nPlease indicate the author and source when reprinting this article. Do not use it for any commercial purposes. Follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2025-04-23","externalUrl":null,"permalink":"/en/misc/pycon-lt-d1/","section":"Miscs","summary":"Sharing my experiences, presentation content, and thoughts on the Python community from the first day of PyCon LT 2025.","title":"Full Record｜PyCon LT 2025 Day One - My Experiences at an International Python Conference","type":"misc"},{"content":"","date":"2025-04-16","externalUrl":null,"permalink":"/en/tags/life/","section":"Tags","summary":"","title":"Life","type":"tags"},{"content":"Last month, I returned home for a vacation, transferring in Beijing before flying to Europe. Because my family of three had a lot of luggage, we booked a hotel near Capital Airport for convenient departure the next day. Taking advantage of the evening in Beijing, I arranged a get-together with a few university classmates.\nIt was a Tuesday, and I arrived at the hotel around 4 or 5 pm. I\u0026rsquo;m grateful for the warm hospitality of my Beijing classmates, and also felt a little apologetic for making them travel all the way to the airport after work to see me.\nOne small observation: people in Beijing really work late.\nThe earliest classmate arrived at 8 pm, and the latest at 10 pm. Because everyone had to work the next day, and with a child present needing early rest, we broke up around 11 pm.\nActually, I\u0026rsquo;ve been away from Beijing for over ten years, and I\u0026rsquo;ve almost forgotten that I also used to get off work at 9 or 10 pm, sometimes even working overtime until midnight.\nFor the past ten years, working mainly in foreign companies in Dalian, the work pace has been relatively relaxed. Generally, if I had evening plans, I would go to the office earlier, leave earlier, or take an hour\u0026rsquo;s leave. Even if I left early occasionally, nobody would calculate whether I had actually sat in the office for a full eight hours.\nNow, seeing the lifestyle of my Beijing classmates, I can hardly imagine how to balance such late working hours with family life, especially with children.\nOf course, everyone has their own coping mechanisms.\nFor example, some classmates have their parents come to help with childcare; others, due to school issues, have their children temporarily living in their hometown, only reuniting during holidays.\nThey also mentioned the issue of kindergarten enrollment in Beijing. A classmate said that different kindergartens in Beijing have a certain priority order during enrollment, such as whether the child is a local resident, whether they have property in the district, etc. The ranking considers factors like household registration, property ownership, and even residence permits, making the process quite complicated. This again made me realize that life in Beijing is indeed not easy, especially after becoming a parent; one can no longer afford the pace of life in Beijing.\n\u0026ldquo;Escape\u0026rdquo; is not an impossible choice. Avoiding these realistic pressures, making life simpler and purer.\nWhile chatting with my teammates, we didn\u0026rsquo;t think the grass was necessarily greener on the other side.\nHow should I put it? Maybe this is a characteristic of a populous country. Limited resources and a large population mean that even if you try your best, you often feel a sense of helplessness, as if you\u0026rsquo;re being pushed forward.\nThink about it, if a European country had such a large population, the competition would probably be just as fierce.\nAs individuals, all we can do is to stay prepared, continue moving forward, and meet and seize our opportunities at the right time.\nPlease indicate the author and source when reprinting this article. Do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2025-04-16","externalUrl":null,"permalink":"/en/misc/one-night-in-beijing/","section":"Miscs","summary":"A single night in Beijing, catching up with university classmates, revealed the late working hours and the pressures and challenges of life in a big city.","title":"Returning Home for Vacation — Reflections on a Beijing Night","type":"misc"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/en/tags/clang/","section":"Tags","summary":"","title":"Clang","type":"tags"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/en/tags/clang-format/","section":"Tags","summary":"","title":"Clang-Format","type":"tags"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/en/tags/clang-tidy/","section":"Tags","summary":"","title":"Clang-Tidy","type":"tags"},{"content":"Last week, the open-source project I created and maintain, cpp-linter-action, reached a small milestone:\n🌟 GitHub Star count surpassed 100!\nWhile this number isn\u0026rsquo;t huge, it\u0026rsquo;s a small milestone for me—it\u0026rsquo;s the first time one of my projects has received over 100 stars on GitHub. It\u0026rsquo;s a validation of the project and gives me the motivation to continue maintaining it.\nMy first commit to this project was on April 26, 2021, and almost 4 years have passed. Looking back, I\u0026rsquo;m glad I haven\u0026rsquo;t been idle and have left behind something useful for others.\nAs the project has evolved, so has its user base. I estimate that thousands of projects are currently using this Action.\nThese include several well-known organizations and open-source projects, such as:\nMicrosoft Apache NASA CachyOS nextcloud Jupyter Most importantly, this process has taught me many new skills and knowledge, and it\u0026rsquo;s maintained a \u0026ldquo;side hustle\u0026rdquo; habit for me:\nMy phone isn\u0026rsquo;t just for WeChat and Douyin (TikTok), but also for GitHub.\nThis project also became a springboard for my subsequent work. I later created the cpp-linter organization to maintain and release binary versions and Docker images of clang-tools with other developers. I also developed cpp-linter-hooks, providing users with clang-format and clang-tidy pre-commit hooks for easier use.\nWithout being immodest:\nIf your project is developed in C/C++, and you want to use clang-format and clang-tidy, then cpp-linter is an unavoidable option.\nFinally, I welcome your feedback and suggestions, and you\u0026rsquo;re welcome to contact me via Issue or Discussions!\nIf you find this project helpful, please leave a message on the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo; or give the project a star on GitHub to show your support!\n—— Written on 2025-04-15 12:49 AM\n","date":"2025-04-15","externalUrl":null,"permalink":"/en/posts/2025/cpp-linter-action-milestone/","section":"Posts","summary":"cpp-linter-action is a GitHub Action that provides C/C++ code formatting and static analysis capabilities. It uses clang-format and clang-tidy, supporting various configurations and custom rules.  Since its creation in 2021, the project has been used by several well-known organizations and open-source projects.","title":"Microsoft and NASA Use It? My 4-Year-Old Side Project Hit 100 Stars","type":"posts"},{"content":"Yesterday, some people may have encountered the inability to access GitHub.\nSome people joked online that it was because of increased tariffs from the US, and GitHub\u0026rsquo;s response code upgraded from 200 to 403.\nJokes aside, GitHub later provided an explanation—it was a configuration error, and the problem has now been fixed.\nI recently returned to China for a trip, and once again experienced the convenience and affordability of express delivery, takeout, transportation, and payment. The only inconvenience was the network.\nMore accurately, it\u0026rsquo;s the websites and services that many programmers rely on daily, such as GitHub, Docker Hub, Docker Desktop, and ChatGPT. Of course, now we also have alternatives like DeepSeek.\nIn my personal experience, GitHub is accessible in China, but its stability is poor. Often, it works when I first open it, but then completely fails to load after a while.\nOutside of work, I already have limited free time in the evenings, but I often have to spend that time troubleshooting network problems.\nWhile I know I can modify my hosts file or try other methods to circumvent network restrictions, setting it up on my computer has always been problematic.\nAll this troubleshooting is exhausting. Sometimes I don\u0026rsquo;t even want to turn on my computer anymore; I just want to lie down and scroll through my phone.\nMy patience with accessing GitHub is wearing thin.\nOh well, I\u0026rsquo;ll just take it as a break.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-04-13","externalUrl":null,"permalink":"/en/posts/2025/visit-github/","section":"Posts","summary":"Many programmers, especially those in China, are frustrated by access issues with GitHub. This article shares my personal experiences and reflections on network problems.","title":"My Patience with Accessing GitHub is Wearing Thin","type":"posts"},{"content":"","date":"2025-04-12","externalUrl":null,"permalink":"/en/tags/readthedocs/","section":"Tags","summary":"","title":"ReadTheDocs","type":"tags"},{"content":"","date":"2025-04-12","externalUrl":null,"permalink":"/en/tags/rst/","section":"Tags","summary":"","title":"RST","type":"tags"},{"content":"In daily open-source projects or team collaborations, we often need an easy-to-maintain, automatically deployable documentation system.\nRecently, while maintaining my own open-source project, I tried using Sphinx to generate documentation and ReadTheDocs to achieve automatic building and hosting. The overall experience was quite good.\nI\u0026rsquo;m documenting the configuration process here, hoping it can help others with similar needs.\nWhy Choose Sphinx and ReadTheDocs # Sphinx is a documentation generator written in Python, initially designed for the official Python documentation. It supports reStructuredText and Markdown (via plugins). ReadTheDocs is a documentation hosting platform that can automatically pull code from your Git repository, build, and publish documentation, supporting webhook auto-triggering. The combination of these two tools is ideal for continuously maintaining and updating documentation, and the community is mature with abundant resources.\nBasic Configuration Steps # Below is the complete process configured in a real project.\n1. Install Sphinx and Related Dependencies # It\u0026rsquo;s recommended to use a virtual environment, then install:\n# docs/requirements.txt sphinx==5.3.0 sphinx_rtd_theme==1.1.1 # If you need Markdown support, add: myst_parser==0.18.1 Install dependencies:\npip install -r docs/requirements.txt Notes:\nsphinx-rtd-theme is the default theme used by ReadTheDocs myst-parser is used to support Markdown 2. Initialize the Documentation Project Structure # In the project root directory, execute:\nsphinx-quickstart docs It is recommended to separate the source and build directories.\nAfter execution, the docs directory will generate files such as conf.py and index.rst.\n3. Modify the conf.py Configuration File # Several key settings are as follows:\n# Read the Docs configuration file # See https://docs.readthedocs.io/en/stable/config-file/v2.html for details from datetime import datetime project = \u0026#34;GitStats\u0026#34; author = \u0026#34;Xianpeng Shen\u0026#34; copyright = f\u0026#34;{datetime.now().year}, {author}\u0026#34; html_theme = \u0026#34;sphinx_rtd_theme\u0026#34; If you need Markdown support, you need to add the following to conf.py:\nextensions = [ \u0026#39;myst_parser\u0026#39;, # Support Markdown ] Configuring ReadTheDocs for Automatic Building # As long as the project structure is clear, ReadTheDocs can basically run with one click.\n1. Import the Project to ReadTheDocs # Log in to https://readthedocs.org/ Click \u0026ldquo;Import a Project\u0026rdquo; and select your GitHub or GitLab repository Ensure the repository contains docs/conf.py; the system will automatically recognize it. 2. Add the .readthedocs.yml Configuration File # To better control the build process, it is recommended to add .readthedocs.yml to the project root directory:\n# Read the Docs configuration file # See https://docs.readthedocs.io/en/stable/config-file/v2.html for details version: 2 build: os: ubuntu-24.04 tools: python: \u0026#34;3.12\u0026#34; sphinx: configuration: docs/source/conf.py python: install: - requirements: docs/requirements.txt After configuration, every time you submit a Pull Request, ReadTheDocs will automatically pull and build the latest documentation for preview, ensuring the documentation is as expected.\nFinal Result # After building, ReadTheDocs will provide a documentation address similar to https://your-project.readthedocs.io/, facilitating team collaboration and user consultation.\nMy current open-source project also uses this scheme, for example: GitStats documentation\nSummary # By following the above configuration, you can almost achieve \u0026ldquo;submit after writing documentation, and it goes live,\u0026rdquo; greatly improving the efficiency of documentation maintenance.\nIf you are writing open-source project documentation, or want to add a documentation system to a team project (especially Python projects), you might want to try Sphinx + ReadTheDocs.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the official account \u0026ldquo;DevOps攻城狮\u0026rdquo; (DevOps Engineer).\n","date":"2025-04-12","externalUrl":null,"permalink":"/en/posts/2025/sphinx-readthedoc/","section":"Posts","summary":"In open-source projects or team collaborations, Sphinx + ReadTheDocs provides an easy-to-maintain, automatically deployable documentation system. This article documents the configuration process and considerations.","title":"Setting up Sphinx + ReadTheDocs from Scratch - Rapidly Deploying Automated Documentation","type":"posts"},{"content":"","date":"2025-04-12","externalUrl":null,"permalink":"/en/tags/sphinx/","section":"Tags","summary":"","title":"Sphinx","type":"tags"},{"content":"","date":"2025-04-11","externalUrl":null,"permalink":"/en/tags/markdown/","section":"Tags","summary":"","title":"Markdown","type":"tags"},{"content":"In daily work, whether writing READMEs, blogs, or project documentation, we always need to choose a markup language to format the content.\nCurrently, there are two mainstream choices: Markdown and reStructuredText (RST).\nWhat are the differences between them? And which one should be chosen in what scenario?\nRecently, I converted the documentation of the gitstats project from Markdown to RST and published it on ReadTheDocs. This article will discuss some of my practical experiences.\nWhat is Markdown? # Markdown is a lightweight markup language, originally designed by John Gruber and Aaron Swartz in 2004, aiming to make documents as \u0026ldquo;readable, writable, and convertible to well-formed HTML\u0026rdquo; as possible.\nAdvantages:\nSimple syntax, easy to learn Wide community support (GitHub, GitLab, Hexo, Jekyll, etc., all support it) Fast rendering speed, good format consistency Common Uses:\nREADME.md Blog posts Simple project documentation What is RST? # RST is a markup language used more frequently in the Python community, maintained by the Docutils project. Compared to Markdown, its syntax is richer and stricter.\nAdvantages:\nNative support for footnotes, cross-references, automatic indexing, code documentation, and other advanced features The preferred format for Sphinx, suitable for large-scale project documentation More friendly to structured documents Common Uses:\nDocumentation for Python projects (such as official documentation) Technical manuals generated using Sphinx Multilingual documentation (in conjunction with gettext) Syntax Comparison Summary # Feature Markdown RST Heading # at the beginning ===== or ----- underline Bold / Italic **text** / *text* **text** / *text* Hyperlink [text](url) `text \u0026lt;url\u0026gt;`_ Table Simple tables (extension supported) Requires strict indentation, complex writing Footnote / Citation Unsupported / Highly limited Native support Cross-reference Unsupported Native support Markdown is easier, RST is more professional.\nWhen to Use Markdown? # For smaller projects requiring only simple documentation When team members are unfamiliar with RST and want to write documentation quickly For blogs and daily notes, Markdown is more recommended When the platform used (such as GitHub Pages, Hexo) supports Markdown by default In short: Markdown is the preferred choice for lightweight documents.\nWhen to Use RST? # When using Sphinx to generate API documentation or technical manuals When the project structure is complex and requires advanced features such as automatic indexing, cross-referencing, and module documentation When integration with the Python toolchain (such as autodoc, napoleon, etc.) is required When publishing to ReadTheDocs (although it now supports Markdown, the RST experience is better) In short: RST is recommended for Python projects or structured technical documents.\nMy Personal Suggestion # If you are:\nA development engineer, using Markdown for daily documentation is sufficient A Python developer, it\u0026rsquo;s recommended to learn RST, especially when using Sphinx to write documentation An open-source project maintainer, for small-scale projects, using Markdown for the README is fine; but for documentation sites, consider using RST + Sphinx for building Summary in One Sentence # Markdown is more like a convenient notepad for writing, while RST is more like a typesetting system for writing books.\nThe choice depends on whether your goal is to write \u0026ldquo;notes\u0026rdquo; or \u0026ldquo;manuals\u0026rdquo;.\nIf you have your own insights on Markdown, RST, or documentation building tools, please feel free to leave a comment and share.\nNext time, I plan to share my experience with Sphinx configuration and automatic publication to ReadTheDocs～\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-04-11","externalUrl":null,"permalink":"/en/posts/2025/md-vs-rst/","section":"Posts","summary":"Markdown and reStructuredText (RST) are two commonly used markup languages. This article compares their advantages and disadvantages and shares usage suggestions in different scenarios.","title":"Markdown — Not So Great Anymore? Why More and More Python Projects Use RST?","type":"posts"},{"content":"","date":"2025-03-12","externalUrl":null,"permalink":"/en/tags/nutanix/","section":"Tags","summary":"","title":"Nutanix","type":"tags"},{"content":"","date":"2025-03-12","externalUrl":null,"permalink":"/en/tags/vmware/","section":"Tags","summary":"","title":"VMware","type":"tags"},{"content":" Background # If you are an enterprise VMware user, you may be considering leaving VMware. Currently, many enterprise users are actively seeking alternatives to reduce costs and reduce dependence on the VMware ecosystem.\nMany companies are considering leaving VMware, mainly due to:\nThe impact of Broadcom\u0026rsquo;s acquisition of VMware. In 2023, Broadcom completed its acquisition of VMware and made a series of adjustments, including:\nCancellation of perpetual licenses for some products, pushing for a subscription model. Price increases, significantly increasing the costs for many companies. Cutting some non-core products and partnership programs, causing unease among some companies and partners. These changes have led many companies to start looking for alternatives to reduce costs and reduce their reliance on the VMware ecosystem.\nAlternatives # Today, I\u0026rsquo;ll share a potential alternative for enterprise users—Nutanix.\nIt is a hyper-converged infrastructure (HCI) (alternative to VMware vSAN / vSphere), suitable for enterprises that need to integrate computing, storage, and network resources and want to reduce their reliance on VMware vSAN or vSphere.\nNutanix is a major competitor to VMware vSphere, supporting KVM and providing enterprise-grade HCI solutions. It offers a simple management interface, similar to VMware vSAN, but at a lower cost. Suitable for companies that want to migrate from VMware but still want enterprise support. Advantages of Nutanix # Nutanix has several advantages over VMware in terms of architecture, management, scalability, and cost-effectiveness, as detailed below:\n1. Unified Hyper-Converged Architecture (HCI) # 🔹VMware： # Traditionally uses a vSphere + vSAN + NSX combination, requiring multiple components to work together. Requires separate configuration of vSAN storage, separated from computing and network resources, resulting in higher management complexity. 🔹Nutanix： # Uses a hyper-converged architecture (HCI), integrating computing, storage, and networking. Replaces VMware ESXi with Nutanix AHV (Acropolis Hypervisor), eliminating the need for additional hypervisor licensing fees. Storage-as-a-Service (Distributed Storage Fabric, DSF), storage performance scales linearly with cluster expansion. ✅ Advantages：\nSimplified management, eliminating the need for additional SAN/NAS storage devices, with unified management of storage and computing resources. Better scale-out, increasing computing and storage capacity by adding nodes. Reduced software licensing costs (VMware vSphere/vSAN costs are high, while Nutanix AHV is free). 2. More Flexible Hyper-Converged Storage # 🔹VMware： # Relies on vSAN for storage, requiring combined configuration with vSphere during expansion. vSAN requires additional licensing and has strict hardware compatibility requirements. 🔹Nutanix： # Built-in Nutanix AOS (Acropolis Operating System), providing distributed storage (Nutanix Files, Volumes, Objects). Higher storage elasticity, supporting hybrid cloud storage (AWS/Azure/Nutanix private cloud). Supports a combination of local NVMe SSDs and HDDs, automatically tiering storage based on data temperature. ✅ Advantages：\nNo additional storage licensing fees, lower cost compared to vSAN. Automatic data optimization, storage performance improves with cluster expansion. Supports external cloud storage, suitable for hybrid cloud deployments. 3. Built-in Free Hypervisor (AHV), Reduced Licensing Costs # 🔹VMware： # Requires paid use of ESXi and management through vCenter. Many companies using VMware need to purchase additional vSphere Enterprise Plus or vSAN licenses, resulting in high costs. 🔹Nutanix： # Provides Nutanix AHV (Acropolis Hypervisor), based on KVM, and is free to use. Managed directly through Prism Central, eliminating the need for vCenter. VMware VM compatible, supporting direct migration of existing VMware VMs to Nutanix AHV. ✅ Advantages：\nFree hypervisor, saving VMware vSphere/ESXi licensing fees. No vCenter needed, simpler management. VMware VM compatible, easier migration. 4. Higher Automation and Scalability # 🔹VMware： # Relies on vSphere and vSAN for VM resource management. Requires vRealize Automation (vRA) for automated operations, with high licensing costs. 🔹Nutanix： # Provides Prism Central for unified management of VMs, storage, and networking, making operations simpler. Provides Calm (automation orchestration tool), supporting one-click deployment of applications (K8s, Jenkins, DevOps-related tools). Supports direct API calls, allowing automation through the Nutanix Prism API. ✅ Advantages：\nHigher degree of automation, suitable for DevOps scenarios. Simpler management interface, better usability. Stronger API support, suitable for CI/CD automated integration. 5. Stronger Hybrid Cloud Support # 🔹VMware： # Requires additional purchase of VMware Cloud on AWS, Azure VMware Solution to extend to the public cloud. VMware vSAN and NSX require additional licensing for support in public cloud environments. 🔹Nutanix： # Built-in Nutanix Clusters, supporting hybrid cloud deployments (AWS, Azure, on-premises data centers). Automatic data synchronization, allowing data migration between on-premises Nutanix and cloud-based Nutanix. Provides Nutanix Xi as a SaaS solution, supporting cross-cloud management. ✅ Advantages：\nHybrid cloud support without additional licensing, lower cost compared to VMware. Simpler data migration, supporting automatic synchronization of on-premises and cloud data. Unified management of public and private clouds, reducing operational complexity. 6. Cost Advantages # 🔹VMware： # Requires a vSphere + vSAN + NSX combination, with high overall licensing costs. VMware uses CPU core billing, and VMware may further increase licensing costs in 2024 (due to the Broadcom acquisition). 🔹Nutanix： # Provides a free AHV Hypervisor, saving vSphere licensing fees compared to VMware. Pay-as-you-go, unlike VMware, which requires bundling multiple products. Integrated storage, computing, and networking, eliminating the need to purchase vSAN or NSX separately. ✅ Advantages：\nLower overall TCO (Total Cost of Ownership), potentially saving 30%-50% compared to VMware. Reduced hypervisor licensing fees, free AHV replacing vSphere. Fewer components, improved management efficiency. Summary: Nutanix vs VMware # Feature VMware Nutanix Hypervisor ESXi (Paid) AHV (Free) Management Tool vCenter (Additional Charge) Prism Central (Free) Storage vSAN (Additional Charge) Nutanix Files/Volumes (Built-in) Automation vRealize Automation (Paid) Calm (Built-in) Hybrid Cloud Requires VMware Cloud solutions Nutanix Clusters (Built-in support) Cost High cost of vSphere + vSAN + NSX Saves ESXi/vSAN costs Scalability Requires manual scaling of vSphere/vSAN HCI model, unified scaling of storage and computing When to choose Nutanix? # You want to reduce VMware licensing costs (eliminate vSphere/vSAN/NSX costs). You need simpler management and don\u0026rsquo;t want to configure vCenter, vSAN, NSX. You plan to use a hybrid cloud and want seamless migration between on-premises and cloud. You want stronger automation capabilities, suitable for DevOps scenarios. When to choose VMware? # You already have a large VMware ecosystem (vSphere, NSX, vSAN) and don\u0026rsquo;t want to switch architectures. Your enterprise applications rely on the VMware ecosystem, such as Horizon (desktop virtualization). You don\u0026rsquo;t mind additional licensing fees, and you already have a mature VMware operations team. If you are primarily focused on reducing costs, simplifying management, and enhancing hybrid cloud capabilities, Nutanix may be a better choice for enterprise users!\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2025-03-12","externalUrl":null,"permalink":"/en/posts/2025/nutanix/","section":"Posts","summary":"Following Broadcom’s acquisition of VMware, many enterprise users are seeking alternatives. Nutanix, as a hyper-converged infrastructure (HCI) solution, offers lower costs and a simpler management interface, making it a good option.","title":"Why Are More and More Enterprise Users Abandoning VMware?","type":"posts"},{"content":"","date":"2025-02-27","externalUrl":null,"permalink":"/tags/cpython/","section":"标签","summary":"","title":"CPython","type":"tags"},{"content":"昨晚，哄完女儿睡觉已经是午夜十二点了。我回到自己的屋里，打开 GitHub，看看当晚有没有什么可以贡献的项目。\n这次，我决定去 CPython 的 Issue 区找找有没有适合自己的贡献机会。\nCPython 就是大名鼎鼎的 Python 编程语言的官方代码仓库。\n其实，早就想找机会为 CPython 贡献代码，但一直没能迈出第一步。这次，我想用自己的方式寻找突破口。\n这种想法的启发，来自 Tian Gao（GitHub ID：gaogaotiantian），他是 Python 的 Core Developer（核心开发者），专注于维护 pdb，并曾跻身 Python 贡献排行榜 #94 名。他可能是唯一一个前 100 名的中国开发者。\n于是，我筛选了一些自己感兴趣的类别，当然就是 Infra 和 DevOps 相关的问题。很快，我找到了一张合适的 Issue，修改代码、测试、提交 Pull Request，然后就去睡觉了。\n今天早上醒来，我发现我的 PR 已经被 Merge 到 CPython 主分支了！\n虽然这算不上什么了不起的成就，但却是一个很好的学习过程。比如，通过参与优秀的开源项目，了解他们是如何管理 Issue 和 Pull Request 的，学习他们做得好的地方。这些经验都有可能应用到自己的工作或项目中。\n在贡献优秀开源项目的过程中，不仅能提升相关技能，还能与这些优秀的开发者交流，学到新的知识。\n从短期来看，或许不会带来直接的收益，但如果这是你真正热爱的事情，那么长期投入一定是值得的。\n假如拥有 Python Core Developer 这样的身份认可，在国内可能有助于获得更理想的工作机会。然而，并非所有公司都青睐这种“双时区开发者”（白天工作，晚上开源）。\n但如果你的目标是寻找远程工作，或者申请欧美国家的签证，这样的经历无疑会成为一个重要的加分项。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-02-27","externalUrl":null,"permalink":"/posts/2025/my-first-pr-to-cpython/","section":"Posts","summary":"在 CPython 的 Issue 区找到一个合适的 PR，修改代码、测试、提交 Pull Request，第二天醒来发现已经被 Merge 到主分支了！这是一个很好的学习过程，也是对开源社区的贡献。","title":"一觉醒来，我的 PR 已经被 Merge 到 CPython 主分支了！","type":"posts"},{"content":"","date":"2025-02-14","externalUrl":null,"permalink":"/en/tags/europython/","section":"Tags","summary":"","title":"EuroPython","type":"tags"},{"content":"","date":"2025-02-14","externalUrl":null,"permalink":"/en/tags/reviewer/","section":"Tags","summary":"","title":"Reviewer","type":"tags"},{"content":"Lately, I haven\u0026rsquo;t been contributing much code outside of work. I\u0026rsquo;ve mainly been spending my time reviewing proposals for EuroPython 2025 (the 2025 European Python Conference).\nWhile in China, I never considered participating in or volunteering for events. But since moving to Europe, I\u0026rsquo;ve felt a desire to participate more.\nBelow, I\u0026rsquo;ll discuss why I chose to volunteer and what I\u0026rsquo;ve learned during this week of activity.\nWhy I chose to participate # First, I wanted to challenge myself—to force myself to speak up, communicate with others, and actively listen.\nSecond, this experience is valuable for my resume and blog.\nThird, I don\u0026rsquo;t participate in just any event. For me, the ideal event falls into one of two categories: Python or DevOps.\nThis is the underlying logic behind my decision-making process.\nThe idea itself was the most important part. Once I had the idea, at the end of last year I searched for European events in 2025 and found EuroPython 2025, being held in Prague. I then applied to join the organizing committee.\nNo matter what, I really wanted to participate, to volunteer. It would be even better to attend in person—as long as the ticket is free. I would cover the flight and hotel myself, treating it as a family vacation, and I could also meet up with former colleagues. A win-win-win.\nWhat I\u0026rsquo;ve learned as a reviewer # First, I\u0026rsquo;ve gained insight into how such an event is organized, including the documentation, responsibilities, review process, requirements, and tools used. Overall, the organizers seem very experienced, and the event is progressing smoothly.\nSecond, as a reviewer, I\u0026rsquo;ve read hundreds of presentation proposals, shared my opinions, helped the organizing committee score them, and ultimately helped select which proposals will be presented at the conference in July 2025. This has broadened my horizons.\nA reflection # Open source is a powerful personal endorsement.\nWithin our company, sharing a Python linter tool like uv or pre-commit is fine as long as you\u0026rsquo;re knowledgeable.\nHowever, when presenting at a conference, you might have plenty of valuable content, but it might not be enough to get the judges\u0026rsquo; approval. If you are an author or maintainer of uv or pre-commit, your chances of getting selected are several times higher.\nFor example, while reviewing proposals, I saw a proposal from a PyPI member. Their proposal had great content, but being a PyPI member instantly boosted the value of the presentation.\nThere were also some Python Core Devs and PyPy Core Devs. Let\u0026rsquo;s just say, if these individuals want to share something about their contributions, it\u0026rsquo;s very easy—the organizers would be delighted.\nFinally # Although this requires a few extra hours each day, the rewards are significant. At the very least, it has broadened my horizons and exposed me to many new ideas.\nTherefore, I\u0026rsquo;ll try some projects I haven\u0026rsquo;t tackled before, such as PyPy, Pydantic, and FastAPI.\nI\u0026rsquo;ve completed 129 reviews, and there\u0026rsquo;s one day left. I hope I can finish close to 150-200, even though the committee only suggests 30 reviews per person.\nIn short, this has been a very positive learning experience. I\u0026rsquo;ve learned many new ideas, improved my English and communication skills, and gained experience beneficial for my career and resume.\nThis task was initially challenging, but not insurmountable—it\u0026rsquo;s the perfect level of challenge for growth.\nWhat do you think?\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Follow the \u0026ldquo;DevOps攻城狮\u0026rdquo; WeChat public account.\n","date":"2025-02-14","externalUrl":null,"permalink":"/en/posts/2025/euro-python-review/","section":"Posts","summary":"I haven’t contributed much code lately, focusing instead on reviewing proposals for EuroPython 2025.","title":"Why I chose to review for EuroPython 2025","type":"posts"},{"content":"Hello everyone! Since my last post announcing the start of maintaining gitstats, I\u0026rsquo;ve been continuously improving this project. Here are the major updates over the past two months:\n✨ New Features and Improvements # Support for Generating JSON Files In addition to the original HTML report, gitstats can now generate JSON files! Use Case: Facilitates secondary development or programmatic use for developers, meeting more customized needs. Origin: This feature was implemented quickly based on user feedback.\nCode Refactoring A significant amount of refactoring and optimization has been performed on the previously mixed code. Benefits: The code structure is clearer and easier to maintain, laying the foundation for writing unit tests.\nReplacing getopt The deprecated getopt has been replaced with the more modern argparse. Advantages: Improves code readability and maintainability.\nCross-Platform Support In addition to Linux, gitstats has now been fully tested on Windows and macOS. Testing: I\u0026rsquo;ve conducted thorough testing on all three platforms to ensure real-time availability.\n📅 Next Steps # Support for Theme Switching In addition to the default theme, I plan to add a Dark Mode, catering to different user visual preferences.\nUnit Testing and Coverage Unit tests will be added, and coverage will be increased to 100% (a small goal) to prevent regression bugs.\n💡 Your Needs Matter! # If you have any other needs or feature suggestions, feel free to visit the following repository address and submit an Issue: 👉 https://github.com/shenxianpeng/gitstats\n🌟 Welcome to Use and Support # If you find gitstats helpful, welcome to Star🌟 it! Your recognition is the motivation for my continuous improvement!\nWhat new features would you like to see added to gitstats? Feel free to leave a comment or directly submit an Issue on GitHub!\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2025-02-05","externalUrl":null,"permalink":"/en/posts/2025/gitstats-update/","section":"Posts","summary":"After two months of continuous improvement, gitstats now supports JSON output, code refactoring, argparse replacing getopt, and full compatibility with Windows and macOS. Welcome to use and Star support!","title":"gitstats Upgrade Arrives—JSON Output, Cross-Platform Compatibility, and Code Refactoring!","type":"posts"},{"content":"","date":"2025-01-25","externalUrl":null,"permalink":"/en/tags/cloud/","section":"Tags","summary":"","title":"Cloud","type":"tags"},{"content":"","date":"2025-01-25","externalUrl":null,"permalink":"/en/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"Recently, I am working on Jenkins instance migration, this time I started to use Jenkins Docker Cloud insead of use docker { ... } in Jenkinsfile.\nJenkins cloud plugin # First you need to install Jenkins Docker Cloud plugin https://plugins.jenkins.io/docker-plugin/\nJenkins Docker Cloud is a plugin that allows Jenkins to use Docker containers as build agents.\nSo you need to config a Docker Host with remote API as follows.\nEnable Docker Remote API # Jenkins controller connects to the docker host using REST APIs. To enable the remote API for docker host, please follow the steps below.\nStep 1: Spin up a VM, and install docker on it. You can follow the official documentation for installing docker. based on the Linux distribution you use. Make sure the docker service is up and running.\nStep 2: Log in to the server and open the docker service file /lib/systemd/system/docker.service. Search for ExecStart and replace that line with the following.\nExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock Step 3: Reload and restart docker service.\nsystemctl daemon-reload service docker restart Step 4: Validate API by executing the following curl commands. Replace myhostname with your hostname or IP.\ncurl http://localhost:4243/version curl http://myhostname:4243/version Create custom docker image # For me, I use launch via JNLP for my custom docker image.\nFor example, you docker image is based on jenkins/inbound-agent, you can use the following Dockerfile to create your custom image.\nFROM jenkins/inbound-agent RUN apt-get update \u0026amp;\u0026amp; apt-get install XXX COPY your-favorite-tool-here ENTRYPOINT [\u0026#34;/usr/local/bin/jenkins-agent\u0026#34;] How to use in Jenkinsfile # Once you have configed Docker Cloud, you can use it in Jenkinsfile like a normal agent.\nFor example:\n// Jenkinsfile (Declarative Pipeline) pipeline { agent { node { \u0026#34;docker\u0026#34; } } } Which is does not like if you use docker { ... } directly.\nFor example of using docker { ... }:\n// Jenkinsfile (Declarative Pipeline) pipeline { agent { docker { image \u0026#39;node:22.13.1-alpine3.21\u0026#39; } } stages { stage(\u0026#39;Test\u0026#39;) { steps { sh \u0026#39;node --eval \u0026#34;console.log(process.platform,process.env.CI)\u0026#34;\u0026#39; } } } } Here is more details about using Docker with pipeline: https://www.jenkins.io/doc/book/pipeline/docker/\nFeel free to comment if you have any questions.\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-01-25","externalUrl":null,"permalink":"/en/posts/2025/jenkins-docker-cloud/","section":"Posts","summary":"This article explains how to use Jenkins Docker Cloud for building and deploying applications, including setting up a Docker host and creating custom Docker images.","title":"How to use Jenkins Docker Cloud","type":"posts"},{"content":"","date":"2025-01-20","externalUrl":null,"permalink":"/tags/copyright/","section":"标签","summary":"","title":"Copyright","type":"tags"},{"content":"最近逛 CPython 的仓库发现了这个 Issue gh-126133:\nHugo van Kemenade 他作为 Python 3.14 \u0026amp; 3.15 发布经理提出是否可以停止更新 Copyright。\n在工作中，我其中的职责之一也是发布，因此我这个想法的提出也非常感兴趣，跟 CPython 项目一样，我们的项目在每年的年初都要更新 Copyright。\n下面我们就一起来看看 Hugo 提出的理由以及最终这个想法被 Python 项目法律团队采纳并最终合并到 CPython 的主分支的过程。\nHugo 提出问题的内容是这样的 # 每年一月，我们会在代码库中更新 PSF 的版权年份：\nCopyright © 2001-2024 Python Software Foundation. All rights reserved. 2025年即将到来。\n我们能否获得 PSF 的许可，采用一种无需每年更新的版权声明？\n我建议使用如下之一：\nCopyright © Python Software Foundation. All rights reserved. Copyright © 2001 Python Software Foundation. All rights reserved. Copyright © 2001-present Python Software Foundation. All rights reserved. 然后他列出了许多以前很多关于更新 Copyright 的 PR 的例子。\n其中很多还有重复的，浪费了开发者的时间。并表示不仅这些工作枯燥无趣，而且可能完全没有必要。\n许多大型项目已经停止更新版权年份，比如：\nCopyright (c) 2009 The Go Authors. All rights reserved. Copyright (c) 2015 - present Microsoft Corporation Copyright 2013 Netflix, Inc. Copyright (c) Meta Platforms, Inc. and affiliates. Copyright (c) Microsoft Corporation. All rights reserved. Copyright (c) .NET Foundation and Contributors 这些要么只有首年，要么完全省略年份，要么以“present”结尾。\n我们可以效仿类似的做法吗？\n这里面有其他人发表的观点。\nGregory P. Smith（Python 指导委员会成员）给出了很好的建议：\n他说：我不会从任何地方删除整个版权声明。Hugo 应该向 PSF（作为版权持有者）寻求建议。我们的目标是简化我们的生活，制定一项可参考的政策，减少劳累。我们应该将这些建议编入开发指南，并简单地回复任何试图修改版权声明的人，并提供该文档的链接。\n最终 Hugo 给 PFS 法律发了邮件，并得到如下回复 # 首先要理解的是，版权声明是可选的/信息性的。曾经有一段时间，版权所有者必须包含一份声明，以保护他们的版权；根据国际条约，这一要求已被废除。因此，版权声明的形式不会影响版权所有者的权利。\n然而，版权声明仍然可以发挥有用的信息作用。根据美国版权局的说法，有效的版权声明的要素包括：\n版权符号 ©；“copyright”一词；或缩写 “copr.”； 作品首次出版的年份；和 版权所有者的姓名。\n示例：© 2024 John Doe\n对于只出版过一次的作品，首次出版的年份很简单。另一方面，开源项目中的文件本质上是修订或衍生作品的累积，每个作品都有不同的首次出版年份。这就是为什么您会在相应的版权声明中看到多个日期或日期范围。例如，如果某个文件是在 2015 年创建的，并在 2017 年、2018 年、2019 年和 2022 年进行了修订，则您可以采用以下方式之一呈现版权声明：\n© 2015、2017-2019、2022 John Doe © 2015、2017、2018、2019、2022 John Doe 在这种情况下，使用“2015-2022”作为日期范围可能相当普遍，尽管这提供的有关各个修订的首次发布日期的信息较少，并且可能错误地暗示该文件在该范围内的每一年都进行了修订。\n无论相应文件是否最近更新过，在每个版权声明的日期范围末尾更新声明以添加当前年份都没有特别的意义。但它也不会影响任何人的权利，也不会使用“-present”来代替。\n所以我认为你提出的包含年份的两种格式都可以。我不建议完全省略首次出版的年份，因为它是有效版权声明的要素之一。\n我还要指出的是，“Python 软件基金会”可能不是 PSF 项目作者的准确陈述，因为 PSF 不需要（据我所知）版权转让。话虽如此，为文件的所有作者包含声明也是不切实际且容易出错的。一些常见的替代方案是 “FOO 项目贡献者”或“贡献者中列出的作者”。这些都不是理想的，因为它们要么实际上没有命名作者，要么随着作者的贡献随着时间的推移而消失，它们可能会变得不准确。\n简而言之，没有理想的解决方案，但幸运的是声明并不那么重要。选择一些合理的内容并尽力坚持下去。\n以上就是 PFS 法律的回复。\n期间还有人提议 Hugo 要争得 PFS 董事会的同意。\n但有人这样回复这个提议：PSF 法律列表由 PSF 员工（包括我自己）监控，是解决此类问题的正确途径。需要董事会全员参与的主题会被提交，不需要董事会全员参与的主题会在那里决定，根据主题的不同，是否需要法律顾问。在这个特定情况下，我们确实请来了律师，以确保 PSF 履行我们作为版权持有人的法律义务。\n此回答并获得了 Hugo 及其他人的点赞👍和❤️。\n最终 Hugo 的提议 “只包含首次出版的年份，删除结束年份” 修改被合并到了 CPython 的主分支了。详见：https://github.com/python/cpython/pull/126236\n总结 # 从这个问题我们可以学到，关于 Copyright 中的年份，我们可以仿照 Python，谷歌，微软和 Netflix，只包含首次出版的年份即可。\n我不是律师，所以不要把这当作我的法律建议。\n我只想说，如果这对 Python，谷歌、微软和 Netflix 的律师来说是可以的，那么对我来说也是可以的。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-01-20","externalUrl":null,"permalink":"/posts/2025/copyright/","section":"Posts","summary":"CPython 停止更新 Copyright 的原因和过程。了解 Python 项目法律团队的建议，以及如何处理开源项目中的版权声明。","title":"CPython 停止更新 Copyright 了，为什么？","type":"posts"},{"content":"Time flies, and 2024 is coming to an end. If I don\u0026rsquo;t record what happened this year, it will be difficult to recall it after a while. As usual, this year-end summary is here on schedule.\nLooking back at the goals I set at the beginning of the year, some were achieved, some were done but not very well, and some were not achieved at all.\nCareer Development: Hope to successfully navigate the career development phase and usher in a new and challenging beginning. ✅ Family Health: Family members remain healthy, achieve work-life balance, and take them out more often. ✅ Skill Enhancement: Address some shortcomings in the DevOps field and strive to improve through real-world projects. ⏳ Sharing and Growth: Continuously share knowledge through blogs and public accounts, learning by teaching. ✅ Maintain Exercise: Whether running or playing soccer, strive to lose weight. ❌ In summary, the goals for 2024 were generally well-met except for the weight loss goal.\nReviewing 2024 # Looking back at this year, the biggest change was my job, which also led to changes in my life. This is the biggest change I\u0026rsquo;ve experienced in the ten years since I returned to Dalian after leaving Shanghai and Beijing.\nCherishing Every Day # From December 2013, I knew I would be leaving the company I had worked for nearly 10 years at the end of June 2024, and I might be working in Europe.\nFrom that moment on, I began to cherish every day of my work and life. Whenever I had time, I would take my family out more often.\nHere\u0026rsquo;s a brief account, recording my incredibly cherished seven months in China:\nJanuary: I injured my ankle playing badminton, so I participated in fewer activities, but I still went to the company\u0026rsquo;s event at the Jinshitang Luneng Hot Spring Resort despite my injury. February: My child suffered an accidental burn, which was a dark moment for our whole family. We spent the entire Spring Festival in the hospital, and thinking about it still makes me feel heartbroken. March: Visited my sister\u0026rsquo;s house at the beginning of the month, went to the Dalian Forest Zoo in the middle of the month, and took my parents to Dandong at the end of the month, visiting the Yalu River Bridge and the Korean War Memorial Museum. April: Visited relatives and friends many times, went to the zoo, and participated in company-organized activities such as hiking up Xiaohéishan Mountain and picking blueberries and strawberries. May: My parents came to Dalian at the beginning of the month to celebrate my child\u0026rsquo;s birthday; at the end of the month, we went to Tokyo, Japan, to apply for a visa. June: Went camping with my child\u0026rsquo;s friends\u0026rsquo; parents at the beginning of the month; went back to my hometown in the middle of the month and took my parents for a trip to the provincial capital; went to the zoo again; officially ended my work at the Dalian company at the end of the month. July: Had a farewell dinner with friends at the beginning of the month; took my child back to my hometown alone, and I cried the moment I drove away from my parents; then went to Japan to collect the visa and visited Tokyo Disneyland; left for Europe at the end of the month, feeling heavyhearted. These little moments made up the precious memories of the first half of 2024.\nEnjoying Every Day of Work # In the first half of the year, I prepared for the handover while completing my regular work, consistently and diligently practicing DevOps best practices. Although the results are still unknown.\nBut I really enjoyed every day on the job, including coffee breaks, lunch walks, listening to my favorite content, occasional runs, and badminton time every Tuesday and Thursday.\nSpare Time: Adhering to Open Source and Writing # As in the previous year, I continued doing the same things in my spare time:\nOpen Source Projects cpp-linter-action: Has over a thousand users, including well-known projects such as Microsoft, Jupyter, and the Linux Foundation. commit-check: Optimized and added user requests, with a steady increase in the number of users. gitstats: This is a new open-source project I took over this year, hoping to help more people. Writing — Updated 27 blog posts and 41 WeChat official account articles throughout the year, significantly exceeding expectations. Learning — The plan to join well-known communities for learning and contribution continues, but taking care of my child after work has significantly reduced my free time. Looking Forward to 2025 # 2024 was a special year for many people. No matter where you are, you can feel the instability of the political situation and the pessimistic economic outlook.\nWill 2025 be better? Probably not, but I still have hope: I hope that in the future, I will have a job, food to eat, a place to live, and that my family will be healthy and safe.\nIf I have any wishes, they are:\nWish my family good health. Travel to other European countries and try to watch a game of my favorite team. Obtain Azure certification. Participate in a DevOps or Python conference. Join PyPA or the Python GitHub Organization. Avoid gaining weight and maintain exercise. Wishes I\u0026rsquo;m not confident about achieving are described using \u0026ldquo;strive to.\u0026rdquo; :)\nPast Year-End Summaries # 2023 Year-End Summary 2022 Year-End Summary 2020 Year-End Summary 2019 Year-End Summary 2018 From QA to Dev\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2024-12-28","externalUrl":null,"permalink":"/en/misc/2024-summary/","section":"Miscs","summary":"Time flies, and 2024 is coming to an end. If I don’t record what happened this year, it will be difficult to recall it after a while. As usual, this year-end summary is here on schedule.\n","title":"2024 Year-End Summary","type":"misc"},{"content":"My daughter is two years and seven months old, and I haven\u0026rsquo;t written a separate article about her yet.\nMy daughter is a very active little girl, full of energy every day, so much so that I often accompany her from 6:30 pm when I get home from work until after midnight before she finally falls asleep.\nTonight is Friday night, it\u0026rsquo;s 12:30 am, and she\u0026rsquo;s still bouncing around and doesn\u0026rsquo;t want to sleep. She finally went to bed at 1:00 am. But then she suddenly got out of bed and wanted an apple.\nIn the past, at 11 pm, I would try my best to satisfy her to get her to sleep, but it was too late tonight, so I sternly told her again: after the lights are off, you can only drink water, nothing else to eat or drink.\nShe clearly resisted my refusal and started crying and making a fuss! But this time I didn\u0026rsquo;t want to give in, sitting on the bed, no matter how she pushed or pulled me to the kitchen, I remained motionless.\nShe was so angry at my actions that she cried until she was out of breath.\nTo be honest, my heart ached, and I was also hesitant.\nMy heart ached because she was crying so hard; I hesitated because I wondered if what I was doing was right?\nIn the end, when I got the toilet paper to wipe her tears and nose, my two-year-old daughter, in her \u0026ldquo;sensitive period of order,\u0026rdquo; stopped crying and no longer thought about the apple.\nFinally, she lay on me, wouldn\u0026rsquo;t talk to me, and wouldn\u0026rsquo;t let me leave the kitchen. I held her like that for about five minutes.\nThen I asked her: Do you love Daddy? She didn\u0026rsquo;t speak.\nI asked her again: Do you love Mommy? She still didn\u0026rsquo;t speak.\nI don\u0026rsquo;t remember which sentence it was, but she agreed to go back to the bedroom with me to sleep.\nWhen she lay beside me, I held her close, listening to her breathing. When I moved my arm away from her, she said: Daddy hug!\nMy heart melted at that moment.\nI asked her again: Do you love Daddy? She said, \u0026ldquo;Love!\u0026rdquo; Do you love Mommy? She said, \u0026ldquo;Love!\u0026rdquo;\nI said Daddy and Mommy love you too!\nAs her breathing deepened, I knew she had fallen asleep.\nAt this moment, reflecting on everything that just happened, I felt I was wrong. From the beginning, I didn\u0026rsquo;t make it clear to her what she could and couldn\u0026rsquo;t do before bed.\nShe could eat apples before, but today Daddy suddenly wouldn\u0026rsquo;t let her eat apples, and she obviously couldn\u0026rsquo;t understand. (Although from my perspective, it was because it was too late).\nIn my heart, I silently said, Daughter, I\u0026rsquo;m sorry! Please give Daddy some patience, this is my first time being a father, and I still need to learn how to be a good father.\nAs I write this article, my daughter is fast asleep beside me. — Written at 2:41 am on December 28, 2024\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2024-12-28","externalUrl":null,"permalink":"/en/misc/my-daughter/","section":"Miscs","summary":"My daughter is two years and seven months old, and I haven’t written a separate article about her yet. Recording the bits and pieces of my life with my daughter.","title":"My Daughter","type":"misc"},{"content":"","date":"2024-12-28","externalUrl":null,"permalink":"/en/tags/summary/","section":"Tags","summary":"","title":"Summary","type":"tags"},{"content":"Recently, in the evenings (usually after my child has gone to sleep), I\u0026rsquo;ve been working on something: reviving the long-dormant GitStats project.\nPreviously, I wrote two articles about GitStats. If you\u0026rsquo;re interested, you can check them out:\nGit History Statistics Generator GitStats Automatically Providing Multi-Dimensional Code Analysis Reports to My Boss Regularly via Jenkins What is GitStats # GitStats is a Python-based tool for analyzing the history of a Git repository and generating an HTML report.\nHowever, it only supports Python 2, and the author has stopped maintaining it (the last commit was 9 years ago).\nIn current development environments, compatibility and ease of use are significantly limited, but its value remains undeniable.\nTherefore, I decided to modernize this project.\nCompleted Work # Migration to Python 3.9+: Refactored the code to support Python 3 versions. ✅ Creation of a Modern CI/CD Pipeline: Added CI/CD tools for easier continuous development and release. ✅ Publication to PyPI: Users can now install it via pip install gitstats. ✅ Docker Image Provided: Users no longer need to handle dependencies themselves; running gitstats is more convenient. ✅ Online Preview Provided: Created a demonstration page to allow users to intuitively understand GitStats\u0026rsquo; functionality. ✅ Special Thanks # Here, I would like to express my special thanks to Sumin Byeon (@suminb). From his introduction, he seems to be a programmer living in South Korea.\nThe original owner of GitStats on PyPI was him, so I couldn\u0026rsquo;t directly use that name. I tried other names, such as git-stats and gitstory, but they were rejected by PyPI due to similarity to other projects.\nSeeing that his project hadn\u0026rsquo;t been maintained for five years, I sent him an email on a whim, asking if he would be willing to transfer the GitStats name to me, as I was reviving the project.\nUnexpectedly, he replied quickly and eventually agreed to transfer the GitStats name to me. His only condition was that if I stopped maintaining GitStats in the future and someone else needed the name, I would do the same as he did and transfer the name to them.\nI agreed and promised to maintain GitStats long-term. (Hopefully, I can do it.)\nFuture Plans # Addressing Valuable Issues: Reviewing unresolved issues in the original repository and selecting valuable ones for fixing. Reviewing Existing Pull Requests: Evaluating PRs from the original repository and merging them into the current project as appropriate. Updating Documentation: Improving the documentation to make it clearer and easier to understand. Adding New Features: Adding features to make the project more powerful and useful. UI Optimization: Improving the visual appeal and user experience of the interface. How to Participate # If you are interested in improving GitStats, you are welcome to participate in this project! You can:\nSuggest Features: Propose ideas or feature requests to help the project better meet user needs. Contribute Code: Fix bugs or add features to directly contribute to the project. Share and Promote: Recommend GitStats to friends or communities who might be interested. Finally, let\u0026rsquo;s work together to bring GitStats back to life!\nWritten at 2:50 AM on November 28, 2024\nPlease indicate the author and source when reprinting this article. Please do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2024-11-28","externalUrl":null,"permalink":"/en/posts/2024/gitstats/","section":"Posts","summary":"This post details the revival of the GitStats project, including its migration to Python 3, the creation of a modern CI/CD pipeline, publication to PyPI and Docker, and future improvement plans.","title":"Reviving GitStats — Breathing New Life into Git History Analysis","type":"posts"},{"content":" pip vs pipx Differences # In the Python ecosystem, both pip and pipx are software tools used for package management, but they have different design goals and usage scenarios. Some developers may be confused about the differences between the two and how to choose.\n1. pip: The General-Purpose Python Package Manager # pip is the officially recommended package manager for Python, used to install and manage Python packages (libraries).\nMain Features:\nSuitable for any Python package: Can install libraries and command-line tools. Installation in global or virtual environments: Packages are installed by default in the global Python environment or in a virtual environment (such as venv, virtualenv). Simple commands: pip install package-name Use Cases:\nInstalling development dependencies (such as requests, flask). Creating project-specific environments (usually used in conjunction with virtual environments). Limitations:\nIf installed directly into the global environment, it can easily lead to version conflicts. Installation and management of command-line tools (CLI) is more cumbersome because they share the same environment. 2. pipx: Focused on Isolated Installation of Command-Line Tools # pipx is a tool specifically designed for Python command-line tools (CLIs), providing an isolated installation environment.\nMain Features:\nCreates an independent environment for each tool: Each CLI tool runs in its own virtual environment, avoiding conflicts. Automatic dependency management: When installing a tool, it automatically handles dependency version management. Simplified user experience: CLI tools are directly usable without extra path configuration. Simple commands: pipx install package-name Use Cases:\nInstalling and managing Python CLI tools (such as black, httpie, commit-check). Avoiding dependency conflicts between tools. Users with high requirements for the development tool or script runtime environment. Limitations:\nOnly suitable for CLI tools, not suitable for installing ordinary Python libraries. Requires installing the pipx tool first: python -m pip install pipx Comparison Summary # Feature pip pipx Purpose Install and manage all Python packages Install and manage CLI tools Installation Scope Global environment or virtual environment Independent virtual environment for each tool Dependency Isolation Requires manual management (better with virtual environments) Automatic isolation, tools do not affect each other Use Cases Development project dependency management Independent installation and use of CLI tools Example pip install flask pipx install black How to Choose? # If you are building a Python project and need to install project dependencies, use pip. If you need to install Python CLI tools, such as pytest or pre-commit, it is recommended to use pipx to ensure independence and stability. In short: pip is a general-purpose tool, pipx is a dedicated solution for CLI tools.\n","date":"2024-11-26","externalUrl":null,"permalink":"/en/posts/2024/pip-vs-pipx/","section":"Posts","summary":"This article introduces the differences between pip and pipx, helping developers choose the right tool to manage Python packages and command-line tools.","title":"pip vs pipx Differences","type":"posts"},{"content":"","date":"2024-11-18","externalUrl":null,"permalink":"/en/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"Recently, while setting up a new Windows Server 2022, I encountered an issue where my Ansible playbook, which previously worked without problems, failed to execute.\nHere’s the configuration I used for the Windows host in my Ansible inventory:\n[jenkins-agent-windows:vars] ansible_user= ansible_ssh_pass= ansible_connection=winrm ansible_winrm_transport=ntlm ansible_winrm_server_cert_validation=ignore However, when I ran the playbook, the following error occurred:\nwinrm send_input failed; stdout: stderr \u0026#39;PowerShell\u0026#39; is not recognized as an internal or external command, operable program or batch file. Cause of the Issue # This is usually the case when the SYSTEM\u0026rsquo;s PATH environment variable has been changed and is no longer able to find PowerShell.exe in the path.\nPlease verify the PATH environment contains the entry C:\\Windows\\System32\\WindowsPowerShell\\v1.0 in there.\nSolution # Right-click This PC \u0026gt; Properties \u0026gt; Advanced system settings \u0026gt; Environment Variables.\nAfter adding C:\\Windows\\System32\\WindowsPowerShell\\v1.0 to PATH, the error disappeared, and my Ansible playbook executed successfully.\n","date":"2024-11-18","externalUrl":null,"permalink":"/en/posts/2024/ansbile-playbook-issue/","section":"Posts","summary":"Explaining a recent issue with Ansible playbook execution on Windows Server 2022, where PowerShell was not recognized, and how to resolve it.","title":"PowerShell is not recognized as an internal or external command","type":"posts"},{"content":"很多人可能会好奇，作为一名 DevOps 工程师，每天究竟忙些什么呢？今天就来简单聊聊，作为 DevOps/Build/Release 工程师，我的日常工作节奏是怎样的。\n工作准备 # 每天早上九点半到公司，第一件事就是打开 Slack 和邮箱，优先处理那些紧急或容易回复的消息。遇到比较复杂的内容，就会设置提醒，以防漏掉。之后，会把当天的任务列入 To-Do List，再检查 Jenkins 上是否有失败的任务需要关注。这一系列动作大概会花半小时左右。\n咖啡时间 # 十点左右是咖啡时间——一天的正式开始。如果十点半有站会，那就是一个快速的回顾和计划环节，主要是分享昨天的进展、当天的任务安排，也和团队同步一下各自的状态。\n日常工作 # 开始工作后，我会打开 VSCode，接着前一天没完成的任务。平时常用的代码仓库包括 pipeline-library、ansible-playbook、docker-images 和 infra，它们分别负责管理流水线、自动化脚本、容器和基础设施。几乎每天都会对这些仓库进行一些更新或优化。\nBuild 和 Release 也是我的主要工作之一。构建任务已经实现了自动化，团队成员通过 Multibranch Pipeline 自行构建；我主要负责分支管理、发布时的合并和冲突解决，确保发布信息的准确和版本的合规性。\n此外，还有一些日常任务，比如：\n维护和升级构建环境 收集代码覆盖率，生成报告 升级编译器，处理相关问题 管理虚拟机分配，帮团队解决环境问题 上午的主要工作还是消息回复和问题处理，之后再逐一处理 To-Do List。\n午餐与休息 # 中午十二点半左右和同事一起午餐。吃饭时聊聊天，也是练习英语的机会。饭后，有时会和同事一起在附近散步，或者自己跑步运动一下。\n下午 # 下午是主要的产出时间。从一点半到四点半，专心处理 To-Do List 上的任务，尽量推进工作进度。四点半之后，美国同事上线，可能会有会议或讨论需求。\n晚间时光 # 晚上是家人时间。天气渐冷，不方便带孩子出门散步，我们偶尔会去超市采购。如果孩子自己看书或看动画片，我会利用时间给开源社区做点贡献，或是写文章。\n这就是我在 DevOps 岗位上的一天，一边忙工作，一边兼顾家庭和爱好。希望这个小分享能让大家更了解这个岗位的日常。\n​你更喜欢我分享一些技术，还是更偏爱这种工作、生活的日常？欢迎留言告诉我！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-10-27","externalUrl":null,"permalink":"/posts/2024/devops-everyday/","section":"Posts","summary":"本文介绍了作为 DevOps 工程师的日常工作节奏，从早上到晚上，涵盖工作准备、会议、代码管理、构建发布等环节。","title":"从早到晚，我的 DevOps 一天","type":"posts"},{"content":" Background # Recently, I’ve been working on migrating and upgrading Jenkins. The main motivation is the vulnerability CVE-2024-23897.\nJenkins 2.441 and earlier, LTS 2.426.2 and earlier does not disable a feature of its CLI command parser that replaces an \u0026lsquo;@\u0026rsquo; character followed by a file path in an argument with the file\u0026rsquo;s contents, allowing unauthenticated attackers to read arbitrary files on the Jenkins controller file system.\nTo address this, Jenkins needs to be upgraded to at least version 2.442 or LTS 2.426.3 or above. This was also an opportunity to rework parts of the setup that weren’t initially optimized.\nPre-Upgrade Jenkins # Before the upgrade, we were using Jenkins 2.346.3, the last version supporting Java 8. Because older operating systems don’t support Java 17, this blocked us from upgrading Jenkins.\nThat said, our initial setup was already well-structured:\nWe followed the Infrastructure as Code principle, deploying Jenkins through Docker Compose. We adhered to the Configuration as Code principle, managing all Jenkins Pipelines with a Jenkins Shared Library. We used Jenkins Multibranch Pipeline to build and test projects. However, there were some limitations, such as:\nThe Jenkins server didn’t have a fixed domain name like jenkinsci.organization.com, but instead had a format like http://234.345.999:8080. Whenever the IP or hostname changed, Webhook configurations for this Jenkins instance had to be updated manually in the Git repository. We hadn’t adopted Docker Cloud. While many tasks used Docker for builds, we weren’t utilizing Docker JNLP agents to create dynamic agents for builds that would automatically be destroyed upon completion. The naming and code structure of the Jenkins Shared Library needed refactoring, as it was initially created for a single team, which limited repository naming. We hadn’t yet used Windows Docker Containers. Some Jenkins plugins were likely outdated or unused but still present. Jenkins and its plugins weren’t regularly updated due to the Jenkins Agent version restrictions. Post-Upgrade Jenkins # Building on prior best practices, we made the following improvements:\nContinued following the Infrastructure as Code principle, and using Nginx as a reverse proxy, we deployed Jenkins with Docker Compose to ensure a stable domain name. Continued to follow the Configuration as Code principle. Continued using Jenkins Multibranch Pipeline for building and testing projects. Where possible, implemented Docker Cloud for builds. Renamed the Jenkins Shared Library to pipeline-library (aligning with Jenkins\u0026rsquo; naming conventions) and refactored many Jenkinsfiles and Groovy files. Introduced Windows Docker Containers to build Windows components. Utilized the Jenkins Configuration as Code plugin and scheduled regular configuration backups. Installed only necessary Jenkins plugins and exported the current plugin list using the plugin command. Attempted to automate plugin backups before upgrading, enabling quick rollback if the upgrade fails. Summary # I hope these efforts enable Infrastructure maintenance and Pipeline development to be managed through GitOps.\nThrough continuous exploration, experimentation, and application of best practices, I aim to establish CI/CD as a healthy, sustainable, self-improving DevOps system.\n","date":"2024-10-25","externalUrl":null,"permalink":"/en/posts/2024/jenkins-upgrade/","section":"Posts","summary":"This article discusses the optimizations made during the Jenkins upgrade, including using Docker Compose for deployment, refactoring the Jenkins Shared Library, introducing Windows Docker Containers, and more to enhance the efficiency and security of the CI/CD process.","title":"What Optimizations I Made During the Jenkins Upgrade","type":"posts"},{"content":"Late at night, I often wonder how I ended up where I am today. It all stems from a series of choices I made after graduation!\nI often reflect on how choices are often more important than effort. Looking back over the past ten years or so, these decisions have had a crucial impact on my journey to where I am today.\n1. Going to Shanghai After Graduation # In 2009, when I just graduated, my goal was simple: to find a job related to my major, regardless of location or specifics. I had no plan, just the need for a job that could support myself.\nAt that time, some classmates had already found mobile phone testing jobs and were sent to Shanghai on business trips. I learned that they were still hiring, and I applied.\nI remember that shortly after the interview, I got on the train home. As the train departed from Shenyang North Station, I received a call from the company informing me that I had passed the interview and could come to sign the contract. As the train arrived at Shenyang Station, I didn\u0026rsquo;t hesitate to get off, even refunded my train ticket, and embarked on the first truly significant choice of my life.\nThat\u0026rsquo;s how my first testing job began. I was really daring back then, just packing up and leaving, and then going to Shanghai alone.\nShanghai was a fresh and magical city for me. Even living in the company\u0026rsquo;s suburban dormitory, life was still happy and exciting.\nIn Shanghai, I visited the World Expo, the opening of the first Apple store, and of course, Nanjing Road, the Bund, and West Lake in Hangzhou. I also watched Liu Xiang run the 110-meter hurdles in the Diamond League and attended Jay Chou\u0026rsquo;s Shanghai concert. On weekend nights, I would have dinner with colleagues until midnight. Everyone was a fresh graduate back then, and the relationship between colleagues was more like that of classmates.\n2. From Shanghai Back to Shenyang # After more than a year of business trips in Shanghai, it was time to choose again: return to the Shenyang branch or resign and find a new job in Shanghai? I and some colleagues chose to return to the Shenyang branch.\nBack in Shenyang, the office, cafeteria, factory, and dormitory were all in a large factory area, surrounded by suburbs. I could only go out for a stroll on weekends.\nI felt like a cog in the factory in that suburb, seeing a life that was clearly predictable.\nAlthough life was stable, it wasn\u0026rsquo;t what a person in their twenties should be pursuing. I knew this wouldn\u0026rsquo;t last long, and if I didn\u0026rsquo;t adjust in time, I would be passive when I had no skills in the future. I started thinking: if I want to go to Dalian in the future, my current mobile phone testing job would be hard to find there, so I need to switch to Web testing, go wherever they need me, and all I need is relevant work experience.\nLater, I interviewed at Dongsoft Shenyang and got a position at Dongsoft Beijing with a salary of 3000 yuan, so I resolutely went to Beijing.\n3. From Shenyang to Beijing # That was March 2011. Before I went, I contacted my college classmate Gangge. I could stay at his place temporarily before I found a place to live.\nSo, I got on the train to Beijing.\nI finally got the real software testing job I wanted, and that\u0026rsquo;s when I started to learn some scripting and database-related knowledge.\nAt Dongsoft Beijing, I experienced two outsourced projects and found that I didn\u0026rsquo;t like this kind of outsourcing. Firstly, the work location was unstable, and secondly, I regularly changed teams and met new colleagues. In less than a year, I decided to change jobs.\nThe first resume I submitted was to Baidu. I had an interview, but I don\u0026rsquo;t know why I didn\u0026rsquo;t pass. Maybe I was too inexperienced back then; I was just a web testing newbie with less than a year of experience, and I wanted to interview at Baidu? It was unlikely.\nLater, I submitted a few more resumes, and by chance, I interviewed at JD.com and passed. I remember the salary was 6500 yuan, which was a huge sum of money for me, double what I earned when I first came to Beijing.\nThe two or three years I worked at JD.com were the years I truly entered the testing field. There were indeed some colleagues worth learning from, and I learned Python, Jenkins, performance testing, and functional testing from them.\nAlthough I learned these skills, I knew that if I wanted to find a good job in Dalian, just having some technical skills wasn\u0026rsquo;t enough. I also needed English, because only foreign companies in Dalian offered decent salaries.\nSo I started searching for foreign companies in Dalian while in Beijing, and I also started preparing to learn English.\nI enrolled in New Oriental, the tuition was several thousand yuan, and I remember each class cost tens or hundreds of yuan. I thought the cost-effectiveness was not high. After listening to a couple of trial lessons, I decisively refunded the tuition.\nI thought the best environment to learn English was to join a foreign company. I started trying to interview with foreign companies in Dalian from Beijing; one happened to have a branch in Beijing, so I went for an interview, but unfortunately, I didn\u0026rsquo;t pass.\n4. From Beijing Back to Dalian # There was no way to find a foreign company in Dalian from Beijing, so I resigned from JD.com and returned to Dalian to look for a job.\nAfter resting for a few days back in Dalian, I found that not having a job made me feel down, so I started looking for a job.\nI interviewed for an outsourced position at Citibank, but I didn\u0026rsquo;t succeed. At that time, I also had the idea of switching to development!\nThis was influenced by a colleague at JD.com. At that time, there was a testing colleague who could develop and had good skills, responsible for writing unit tests for the development team. I thought that was amazing, and I wanted to do that kind of technically demanding job.\nTherefore, my other plan at that time was: if any company was willing to hire me, someone with only testing experience, to train me to be a developer, I would be willing to take less money, or even work for free. My salary when I left JD.com was already 12,000 yuan, but I still wanted to switch to development, even starting as an intern. Unfortunately, the market didn\u0026rsquo;t give me that opportunity at the time. I needed a job, so I found a junior testing manager position with a salary of over 6000 yuan. I worked in this position for a few months and found that it wasn\u0026rsquo;t the environment I wanted to continue working in, and I started looking for a job at a foreign company while still working.\nLater, I accidentally applied to the foreign company I had applied to in Beijing before. This time, the interview went well, and I was officially hired. I started a ten-year career here, my most hardworking ten years.\n5. From Testing to Development and Then DevOps in a Foreign Company # As mentioned above, I wanted to switch to development. After joining the new company, there was an opportunity within the team to switch from testing to front-end development. I was extremely envious of this opportunity, but unfortunately, I was too late; they already had enough people, so I could only continue to shine in the testing position.\nUntil 2018, due to business adjustments, I had another opportunity to switch to development, but it required an assessment.\nAlthough it was a big challenge for me, I thought it was the right thing to do. Two reasons:\nIf my programming skills were strong enough, I could become the best technically skilled person in testing; Developers usually have more say and more opportunities than testers, such as technical immigration. I applied to become a developer without hesitation!\nIt was a very stressful period. I wrote about it in the #Programmer\u0026rsquo;s Musings series. But in the end, I successfully transitioned to development. Later, the team needed a Build Engineer. Honestly, it was the perfect role for me!\nThe role was previously called a Build Engineer, but because I was very passionate about doing this job well, I did a lot more than just Build work, so my responsibilities gradually expanded to the DevOps field, and later everyone called me a DevOps engineer, responsible for DevOps/Build/Release related tasks.\n6. From Dalian to Europe # Years ago, I started to have the idea of going abroad to see the world, looking for a DevOps job in Japan or Europe that could provide a visa. If I didn\u0026rsquo;t go abroad, I might not have any chance when I turned forty.\nThis idea may have come from my previous independent trips to Thailand, Japan, and the United States. After that, I enjoyed listening to audio programs like \u0026ldquo;Quietly Talking About Japan\u0026rdquo; and \u0026ldquo;Casually Talking About America.\u0026rdquo;\nI updated my LinkedIn profile, and occasionally someone contacted me about opportunities in Singapore. To be honest, it was uncertain whether I could go or not, but I wasn\u0026rsquo;t very interested in Singapore; it felt too competitive.\nAfter the pandemic and the birth of my child, this idea was put on hold, and there was no progress. After the well-known changes in the general environment, a sudden opportunity arose.\nBut honestly, at this time, I had far more concerns than when I went to Shanghai, returned to Shenyang, went to Beijing, or returned to Dalian.\nBecause of my family, my child, and my parents, making the decision wasn\u0026rsquo;t easy. But in all my past choices, there were no perfect opportunities. In this uncertain era, the only constant is change. Besides, such an opportunity would almost never come again.\nStaying might mean being forced to engage in meaningless competition, like adults competing for work and education, and children competing in their studies. Thinking about this, I knew it was time to choose again—to go abroad.\nFinally # Since graduation, especially during the past ten years working for a foreign company, I\u0026rsquo;ve worked hard. I worked hard, unyieldingly completing tasks; I persistently wrote blogs and public accounts for seven or eight years in my spare time, using teaching to promote learning and contributing to open-source projects.\nI believe in the value of hard work, but I also remind myself not to fall into the trap of \u0026ldquo;self-satisfaction.\u0026rdquo; Hard work is certainly important, but we shouldn\u0026rsquo;t let tactical diligence obscure strategic laziness. We must look up to see the road ahead, seize key opportunities, and make wise decisions. After all, if the direction is wrong, no matter how hard you try, it may be in vain.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps Engineer\u0026rdquo;.\n","date":"2024-09-27","externalUrl":null,"permalink":"/en/misc/power-of-choice/","section":"Miscs","summary":"Late at night, I often wonder how I ended up where I am today. It all stems from a series of choices I made after graduation!","title":"Choices — Often More Important Than Effort","type":"misc"},{"content":"想象一下，你是一名 DevOps 工程师，不论初级、中级还是高级，老板总有一天拍拍你的肩膀说：“加油干，兄弟，未来你就是我们的首席 DevOps 工程师了！”你可能会心想：“啥是首席 DevOps？这是个什么新饼？”\n今天就带你了解一下，所谓的“首席 DevOps 工程师”到底干啥，职责是什么？\n我们一起看看，顺便找准未来的职业发展方向。毕竟，谁都希望能进阶到高级角色嘛，对吧？\n首席 DevOps 工程师是干啥的？ # 在今天这个技术跑得比人快的世界里，首席 DevOps 工程师是关键角色，帮助企业搞定基础设施，让软件交付又快又稳。这可不光是架个服务器那么简单，真正的大活儿是当好开发和运营团队之间的“桥梁”，推动 DevOps 文化在公司生根发芽。\n那么他们的日常是啥呢？总结起来，有三个主要工作：\n设计并维护基础设施 —— 和开发团队配合，搭建弹性、可扩展的基础设施，满足业务需求。 自动化所有能自动化的事情 —— 减少手动操作，提升代码发布和测试的效率。 推动团队文化变革 —— 推广 CI/CD 最佳实践，优化大家的工作方式。 核心职责 # 1. 协调开发和运营 # 在你成为首席 DevOps 工程师后，你的头号任务就是让开发和运营两边配合得像一个人。持续集成、持续部署这些词你得说得像背诗一样顺溜，同时，基础设施得稳如老狗。\n2. 实现自动化和流程优化 # 自动化是 DevOps 的灵魂，首席 DevOps 工程师就是得把那些繁琐的手动任务尽可能自动化，不断优化，让所有事情跑得又快又稳。\n3. 保证系统可靠性和效率 # 系统跑不稳，CI/CD就得停摆，所以你要设计出能撑得住风浪的基数设施。遇到高负载或者故障，系统照样得稳住。定期监控、优化，是你的日常功课。\n成为首席 DevOps 工程师需要哪些技能？ # 1. 技术要硬 # 技术基础是标配，Python、Bash 这些脚本语言得熟悉，Docker 这种容器技术也得懂，Ansible、Chef 这些配置管理工具是你日常操作。最重要的是，云平台（比如 AWS 和 Azure）管理经验不能少。\n2. 领导力和管理能力 # 技术大牛不稀奇，领导力大牛才是硬通货。你得激励团队，帮他们成长，创造出协作的氛围。别忘了，技术再牛，不会跟不同层级的利益相关者沟通，那也白搭。\n3. 解决问题的能力 # 技术上碰到过问题可以迅速找到问题的根源，给出靠谱的解决方案。更重要的是，做决策时得能平衡技术需求和业务目标，让公司上下都买账。\n对公司有啥好处？ # 1. 加强沟通协作 # 作为首席 DevOps，你不仅仅写代码，还得跨团队沟通协调，让大家更默契，工作更顺畅。减少内部摩擦，提升效率。\n2. 加快产品交付 # 优化流程、自动化任务，你能让产品的交付速度快得像坐火箭。市场变化快，你得让公司跟得上，产品能快速迭代，企业才能有竞争力。\n3. 提高系统稳定性和安全性 # 稳定性和安全性很重要，你得建立起强大的监控体系，防止潜在的威胁，在安全和稳定方面给公司打下坚实基础。\n总结一下 # 首席 DevOps 工程师可不是单纯的技术专家，你要靠技术提升效率，推动合作，保证产品交付和系统的稳定性。这过程里，你不仅是一个技术领袖，更是团队文化的推动者。\n想成为首席 DevOps？那就不仅要技术过硬，还要培养领导力和解决问题的能力。\n希望这篇轻松的文章能帮你找到未来的努力方向，毕竟，我们都在为更高的目标努力着！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-26","externalUrl":null,"permalink":"/posts/2024/principal-devops-engineer/","section":"Posts","summary":"本文介绍了首席DevOps工程师的职责、核心技能和对公司的价值，帮助你了解如何在DevOps领域实现职业发展。","title":"DevOps进阶：揭秘首席DevOps工程师的职责与技能","type":"posts"},{"content":" Summary # Conventional Branch refers to a structured and standardized naming convention for Git branches which aims to make branch more readable and actionable. We\u0026rsquo;ve suggested some branch prefixes you might want to use but you can also specify your own naming convention. A consistent naming convention makes it easier to identify branches by type.\nKey Points # Purpose-driven Branch Names: Each branch name clearly indicates its purpose, making it easy for all developers to understand what the branch is for. Integration with CI/CD: By using consistent branch names, it can help automated systems (like Continuous Integration/Continuous Deployment pipelines) to trigger specific actions based on the branch type (e.g., auto-deployment from release branches). Team Collaboration : It encourages collaboration within teams by making branch purpose explicit, reducing misunderstandings and making it easier for team members to switch between tasks without confusion. Specification # Branch Naming Prefixes # The branch specification by describing with feature/, bugfix/, hotfix/, release/ and chore/ and it should be structured as follows:\n\u0026lt;type\u0026gt;/\u0026lt;description\u0026gt; main: The main development branch (e.g., main, master, or develop) feature/: For new features (e.g., feature/add-login-page) bugfix/: For bug fixes (e.g., bugfix/fix-header-bug) hotfix/: For urgent fixes (e.g., hotfix/security-patch) release/: For branches preparing a release (e.g., release/v1.2.0) chore/: For non-code tasks like dependency, docs updates (e.g., chore/update-dependencies) Basic Rules # Lowercase and Hyphen-Separated: Always use lowercase letters, and separate words with hyphens. For example, feature/new-login or bugfix/header-styling. Alphanumeric and Hyphens Only: Use only lowercase letters (a-z), numbers (0-9), and hyphens. Avoid special characters like spaces, punctuation, and underscores. No Consecutive Hyphens: Ensure that hyphens are used singly, with no consecutive hyphens (e.g., feature/new-login, not feature/new--login). No Trailing Hyphens: Do not add a hyphen at the end of the branch name. For instance, use feature/new-login instead of feature/new-login-. Clear and Concise: Make branch names descriptive but concise. The name should clearly indicate the work being done. Include Jira (or Other Tool) Ticket Numbers: If applicable, include the ticket number from your project management tool to make tracking easier. For example, for a ticket T-123, the branch name could be feature/T-123-new-login. Conclusion # Clear Communication: The branch name alone provides a clear understanding of its purpose the code change. Automation-Friendly: Easily hooks into automation processes (e.g., different workflows for feature, release, etc.). Scalability: Works well in large teams where many developers are working on different tasks simultaneously. In summary, conventional branch is designed to improve project organization, communication, and automation within Git workflows.\nFAQ # What tools can be used to automatically identify if a team member does not meet this specification? # You can used commit-check to check branch specification or commit-check-action if your codes are hosted on GitHub.\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-18","externalUrl":null,"permalink":"/en/posts/2024/conventional-branch/","section":"Posts","summary":"This article introduces the Conventional Branch specification, which provides a structured naming convention for Git branches to enhance readability and collaboration.","title":"Conventional Branch Specification Released!","type":"posts"},{"content":"PyPA（Python Packaging Authority）是管理和维护 Python 包相关工具的一个社区组织。PyPA 管理的知名项目包括 pip、packaging、setuptools、wheel、twine、build 等等。了解这些项目的关于有助于我们更好的了解 Python 的生态系统。\n以下是这些项目的介绍以及它们之间的关系：\npip 作用：pip 是 Python 的包管理工具，用于安装和管理 Python 库和依赖项。通过 pip，用户可以从 Python Package Index (PyPI) 或其他包源下载并安装所需的 Python 包。 关系：pip 依赖于 setuptools 和 wheel 来处理包的构建和安装。它是最常用的 Python 包管理工具，也是官方推荐的包安装方法。\nsetuptools 作用：setuptools 是一个用于打包 Python 项目的工具，可以创建分发包（distribution packages）并发布到 PyPI。它扩展了 Python 标准库中的 distutils，提供了更多功能，如依赖管理、插件系统等。 关系：setuptools 是创建 Python 包时常用的工具，pip 使用 setuptools 来安装源代码形式的 Python 包。setuptools 生成的分发包通常是 .tar.gz 或 .zip 文件格式。\npackaging 作用：packaging 提供了用于与 Python 包打包和分发相关的核心实用工具和标准实现。它实现了一些与包版本、依赖关系解析等有关的 PEP（Python Enhancement Proposals）。 关系：packaging 是 setuptools 和 pip 等工具的底层依赖，用于处理包的版本比较、依赖解析等低层次操作。\nwheel 作用：wheel 是一种 Python 包的打包格式，作为 setuptools 打包格式 .egg 的替代方案。它是目前推荐的发布格式，可以避免编译步骤，安装速度更快。 关系：pip 优先安装 wheel 格式的包，因为它可以直接安装，而不需要像 .tar.gz 那样进行编译。setuptools 可以生成 wheel 格式的包。\nvirtualenv 作用：virtualenv 用于创建独立的 Python 环境，可以避免不同项目之间的包依赖冲突。每个虚拟环境都有自己独立的 Python 可执行文件和库集合。 关系：pip 被用于管理 virtualenv 中的包。virtualenv 是创建隔离环境的工具，但近年来 Python 标准库中的 venv 模块也能提供类似功能。\ntwine 作用：twine 是用于将 Python 包上传到 PyPI 的工具。与 setuptools 的 python setup.py upload 方法不同，twine 更加安全，支持上传 wheel 文件和 .tar.gz 文件。 关系：twine 通常与 setuptools 或 wheel 一起使用，负责将构建好的包上传到 PyPI。\nbuild 作用：build 是一个简单的命令行工具，用于构建 Python 项目。它可以使用 PEP 517 接口构建包，而不依赖于 setuptools。 关系：build 提供了比 setuptools 更简洁的构建方式，但它依赖于 setuptools 或其他构建后端（如 flit、poetry）来实际完成构建过程。\npyproject.toml 作用：pyproject.toml 不是一个工具，而是一种配置文件格式。它被用来描述项目的构建需求，并支持使用不同的构建后端，如 setuptools、flit、poetry 等。 关系：pip 和 build 等工具会读取 pyproject.toml 文件，了解如何构建和安装项目。\n他们之间的关系总结 # pip 作为包管理工具，与所有这些项目都有交互关系，尤其依赖 setuptools 和 wheel 来安装包。 setuptools 负责包的创建和打包，并使用 packaging 处理版本和依赖。 wheel 是打包格式，pip 更倾向于安装 wheel 格式的包。 virtualenv 用来创建隔离的环境，pip 被用来在这些环境中安装依赖。 twine 用来安全地上传包到 PyPI，通常与 setuptools 和 wheel 结合使用。 build 是一个新兴的构建工具，使用 PEP 517 接口，可以使用 setuptools 作为构建后端。 这些工具共同构成了 Python 包管理和分发的完整生态系统，简化了 Python 开发者的工作流程。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-05","externalUrl":null,"permalink":"/posts/2024/pypa/","section":"Posts","summary":"本文介绍了 PyPA（Python Packaging Authority）下的知名项目，包括 pip、setuptools、wheel 等，并分析了它们之间的关系，帮助读者更好地理解 Python 包管理和分发的生态系统。","title":"初步了解 PyPA（Python Packaging Authority）下的知名项目和关系","type":"posts"},{"content":"Lately, I\u0026rsquo;ve been listening to Zhao Lei\u0026rsquo;s songs while driving, especially \u0026ldquo;I Remember.\u0026rdquo;\nI think this song is a symphony of memories; every time I hear it, it\u0026rsquo;s like watching an old movie. It\u0026rsquo;s a kind of sound, a touching sensation, that allows us to recall the time that has passed, to reflect on our lifestyles and the direction of our lives.\nThe lyrics and melody are full of nostalgic emotions, always reminding me of all the greetings and blessings I received during this period.\nWhile the memories are still fresh, I want to record them all, also titled \u0026ldquo;I Remember\u0026rdquo;:\nJune 14th (Friday): My 1024 football team organized a dinner for me. At that time, only my visa had been approved, which was a good start. That night, I drank all the beer I\u0026rsquo;d accumulated over the years. Thankfully, the alcohol content wasn\u0026rsquo;t very high, and I wasn\u0026rsquo;t hungover when I got home. June 27th (Thursday): I went to Qiankuli Buffet with colleagues and leaders. My feelings were mixed because I was graduating the next day. We took a cheerful group photo together. June 28th (Friday): On my graduation day, many colleagues came to say goodbye, and I remember them seeing me off from the company and building. Finally, Da Chao and I went down the mountain together; he went home, and I waited for the bus. June 30th (Sunday): I happened to meet my colleague, Dashi, at Xiaoping Island. I was sending shoes to the laundry, and he was buying groceries. We stood on the roadside and chatted for a long time, and took a photo together before parting ways. July 1st (Monday): I had lunch with my childhood friend and old classmate. We walked around Tengfei and then he accompanied me to the 4S shop for car maintenance and tire repair. We chatted for the afternoon; it had been a long time since I\u0026rsquo;d had a long, one-on-one conversation with someone. It felt good. We took a photo to commemorate the occasion. July 4th (Thursday): I took my daughter back to my hometown alone so my parents could see their granddaughter again. The entire round trip was 6 hours. My daughter sat in her car seat the whole time and behaved exceptionally well. July 7th (Sunday): My wife, daughter, and I had a delicious lunch at my father-in-law\u0026rsquo;s house. That same day, in the afternoon, I went swimming with my coach at the city stadium. After returning to Tengfei, I had a cup of coffee, a small snack, and walked half a circle around Tengfei, chatting until we ran out of things to say. July 8th (Monday): I joined my former colleagues for a badminton game. July 9th (Tuesday): I played my last football game here with the 1024 football team. The opposing team had an incredibly strong goalkeeper, which left a deep impression on me. Another memorable moment was probably my long-range shot from the backfield. July 10th (Wednesday): I went back to Japan to get my card and also visited Tokyo Disney. Although it rained that day, it was still a great experience. July 14th (Sunday): I returned from Japan. That afternoon, colleagues and friends went to a mutton soup restaurant for a meal and chatted for several hours. July 15th (Monday): I saw my childhood friend; he was fishing by the sea, and I went along. It was my first time using a fishing rod, and I even caught a tiny fish. July 16th (Tuesday): I started my journey. Although I felt a thousand regrets, it was time to leave. I knew the sadness I felt was real, and I also knew that I would definitely move on. That evening, I took the high-speed rail to Beijing, where I met a university classmate at the airport. We chatted for a while, but it was late, so our reunion was brief. July 17th (Wednesday): After a flight of more than ten hours, our family finally arrived in Weicun. Thus ended my brief graduation season. I cherish these memories, so I\u0026rsquo;ve written down these few words to record all the beautiful things that happened.\nThese memories will remind me that no matter how life changes, we will never forget the past, but we must also be full of hope and expectation for the future, experiencing the beauty and hope of life.\nAlthough life is full of difficulties and challenges, as long as we have memories of the past and expectations for the future, life is full of meaning and value.\nBeautiful music also touches our emotions, inspires our thoughts, and brings us strength and hope.\nPlease indicate the author and source when reprinting this article. Please do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2024-07-09","externalUrl":null,"permalink":"/en/misc/remember/","section":"Miscs","summary":"Lately, I’ve been listening to Zhao Lei’s songs while driving, especially “I Remember.” This article records my graduation season at Rocket’s China branch and the well wishes from my colleagues.","title":"I Remember","type":"misc"},{"content":"","date":"2024-07-09","externalUrl":null,"permalink":"/en/tags/work/","section":"Tags","summary":"","title":"Work","type":"tags"},{"content":"Today (June 28th) is my last day at Rocket\u0026rsquo;s China branch, marking the end of my ten years here. This is the longest I\u0026rsquo;ve worked at any company in my career, and I wanted to take this opportunity to write a few words to conclude this decade.\nTime flies. In the blink of an eye, ten years have quietly passed. Only by living earnestly and diligently can one avoid lamenting the passage of time. A favorite quote of mine is: \u0026ldquo;The best time to plant a tree was ten years ago, the second best time is today.\u0026rdquo;\nInfluenced by my colleagues, I fell in love with the software industry after joining the company and aspired to become a technical expert in the eyes of others. I transitioned from a test engineer to a developer and eventually dedicated myself to the DevOps field, reaping joy and accomplishment along the way.\nA Decade\u0026rsquo;s Journey—Thank you. During my ten years at Rocket China, I\u0026rsquo;ve met many excellent colleagues and friends from whom I\u0026rsquo;ve learned a great deal and been profoundly influenced.\nCountless team dinners, activities, trips, football games, badminton matches, swimming sessions, and more—thank you for accompanying me along the way, making this journey more exciting and unforgettable.\nThank you also to my family for their constant tolerance, understanding, support, and love. You have been my strong backing, allowing me to move forward fearlessly. Thank you for all the beautiful memories of these ten years.\nThirty Years, Five Cities. In my thirty years, I\u0026rsquo;ve lived in five cities: born in Zhuanghe, Liaoning; studied at university in Shenyang; my first job was in Shanghai; later, I worked in Beijing for three years. In 2014, I resigned from JD.com and returned to my hometown of Dalian.\nDalian is one of my favorite cities; it has beautiful scenery and a pleasant climate. Here, I bought a house, got married, and had a child, completing many important milestones in my life.\nAll good things must come to an end. As the last day approaches, my feelings are mixed.\nThere is excitement and anticipation for the future, but also worry about challenges and the unknown, as well as concern for my family. I look forward to my wife, child, and I becoming better and stronger on the road ahead; however, this will undoubtedly be full of challenges, both professionally and personally. I can\u0026rsquo;t be sure whether this process will be smooth or how long it will take.\nChoosing Europe—A New Chapter. Some may ask, \u0026ldquo;Can\u0026rsquo;t you support your family in China? Why go so far?\u0026rdquo; In reality, it\u0026rsquo;s not a matter of right or wrong, but a matter of choice. A person\u0026rsquo;s choices are often closely related to their experiences.\nPreviously, I never considered going abroad. It wasn\u0026rsquo;t until I joined Rocket China that I saw excellent colleagues taking their families and children to Australia, Europe, and Japan. Furthermore, I had the opportunity to travel to the US on business and to Japan for tourism, witnessing a wider and more exciting world.\nThese experiences gradually planted a seed in my mind: if I had the opportunity to go abroad, it would be a challenging and fun life experience.\nI admire those who proactively break out of their comfort zones, while I have passively waited for opportunities to arise. If an opportunity presents itself, I\u0026rsquo;m willing to try bravely.\nIt just so happens that the company is adjusting its business, and the project is moving to Europe. For others, this might be bad news, but for me, it\u0026rsquo;s a rare opportunity. While it\u0026rsquo;s not a perfect opportunity, when is life ever perfect?\nThis opportunity will allow me to improve my English in a fully English-speaking work environment; my two-year-old daughter will also have the chance to receive a different education and begin her journey learning a second, or even third, language.\nFor our family, this is a completely new challenge and beginning. Although we are over 35, I still believe we are young and can continue to learn and explore, maintaining our curiosity about the world.\nAnother reason for choosing Europe is that it\u0026rsquo;s comparatively easier for programmers to extend their careers to their forties, fifties, or even sixties in European countries than in China.\nFurthermore, as I\u0026rsquo;ve gotten older, my definition of a \u0026ldquo;good company\u0026rdquo; has changed.\nFor instance, when I first graduated and went to work in Shanghai, I thought SIMCom was the best company; later, when I worked at Neusoft in Beijing, I thought Neusoft was pretty good too; then, I joined JD.com, thinking it was probably the best company I would ever join; until I joined this foreign company, and for the past ten years, I\u0026rsquo;ve felt that this is truly a good company.\nIf I were to use my current standards to find a new job in Dalian or other cities, I\u0026rsquo;m afraid it would only be possible in my dreams.\nFinally, thank you to every friend and colleague I\u0026rsquo;ve met at Rocket China over the past ten years.\nI sincerely appreciate the farewell greetings and well wishes from each of my colleagues.\nI hope we can meet again on life\u0026rsquo;s journey in the future.\nGoodbye! 👋\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Please follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2024-06-28","externalUrl":null,"permalink":"/en/misc/farewell/","section":"Miscs","summary":"June 28, 2024, marks my last day at Rocket’s China branch.  Looking back on a decade’s journey and forward to a new chapter.","title":"Farewell Rocket China — A Decade's Journey and a New Beginning","type":"misc"},{"content":"软件真是个有趣又深奥的东西，它由看似神奇的代码片段组成，这些代码运行在最终的终端上，本身却并非生命体，但拥有自己的生命周期。\n软件最初是源代码的形式，仅仅是存放在某个仓库的文本文件，然后通过独特的构建过程，这些源代码会转变为其他形式。例如交付到 web 服务器的压缩 JavaScript 代码块、包含框架代码和业务逻辑的容器镜像，或者针对特定处理器架构编译的原始二进制文件。\n这种最终的形态转变，也就是源代码生成的其他形式，通常被称为“软件制品”。在创建之后，制品通常会处于休眠状态，等待被使用。它们可能会被放置在软件包注册表（例如 npm、RubyGems、PyPI 等）或容器注册表（例如 GitHub Packages、Azure Container Registry、AWS ECR 等）中，也可能作为附加到 GitHub 版本发布的二进制文件，或者仅仅以 ZIP 文件的形式存储在某个 Blob 存储服务中。\n最终，有人会决定拾取该制品并使用它。他们可能会解压缩包、执行代码、启动容器、安装驱动程序、更新固件 - 无论采用何种方式，构建完成的软件都将开始运行。\n这标志着生产生命周期的顶峰，该周期可能需要大量人力投入、巨额资金，并且鉴于现代世界依赖软件运行，其重要性不言而喻。\n然而，在许多情况下，我们并不能完全保证所运行的制品就是我们构建的制品。制品经历的旅程细节要么丢失，要么模糊不清，很难将制品与其来源的源代码和构建指令联系起来。\n这种缺乏对制品生命周期的可见性是当今许多最严峻的安全挑战的根源。在整个软件开发生命周期 (SDLC) 中，有机会保护代码转换为制品的流程 - 如此一来，可以消除威胁行为者破坏最终软件并造成严重后果的风险。一些网络安全挑战似乎难以成功应对，但这种情况并非如此。让我们深入了解一些背景知识。\n哈希值和签名 # 假设你的目录中有一个文件，并且你想要确保它明天与今天完全相同。你该怎么做？一个好的方法是通过安全的哈希算法生成文件的哈希值。\n以下是如何使用 OpenSSL 和 SHA-256 算法完成此操作：\nopenssl dgst -sha256 ~/important-file.txt 现在，你拥有了一个哈希值（也称为散列值），它是一个由字母和数字组成的 64 字符字符串，代表该文件的唯一指纹。只要更改该文件中的任何内容，然后再次运行哈希函数，你就会得到不同的字符串。你可以将哈希值写在某个地方，然后第二天回来尝试相同的过程。如果你两次没有得到相同的哈希值字符串，则文件中的某些内容已发生更改。\n到目前为止，我们可以确定某个文件是否被篡改。如果我们想要对制品进行声明怎么办？如果我们想说“我今天看到了这个制品，我（系统或人）保证这个东西就是我看到的东西”，该怎么办？此时，你需要的是软件制品签名；你需要将哈希值字符串通过加密算法进行处理，以生成另一个字符串，代表使用唯一密钥“签名”该指纹的过程。 如果你随后希望其他人能够确认你的签名，则需要使用非对称加密：使用你的私钥签名哈希值，并提供相应的公钥，以便任何获取你文件的人都可以进行验证。\n你可能已经知道，非对称加密是互联网上几乎所有信任的基础。它可以帮助你安全地与银行互动，也可以帮助 GitHub 安全地交付你的存储库内容。我们使用非对称加密来支持诸如 TLS 和 SSH 等技术，以创建可信赖的通信通道，但我们也使用它通过签名来创建信任软件的基础。\nWindows、macOS、iOS、Android 等操作系统都具有用于确保可执行软件制品的可信来源的机制，方法是强制要求存在签名。这些系统是现代软件世界中极其重要的组件，构建它们非常困难。\n不仅仅是签名 - 还要证明 # 当我们思考如何展示关于软件制品的更多可信赖信息时，签名是一个好的开端。它表示“某个可信赖的系统确实看到了这个东西”。但是，如果你真的想在整个软件开发生命周期 (SDLC) 的安全性方面取得重大进步，那么你就需要超越简单的签名，而是要考虑证明。\n证明是一种事实断言，是对制品或制品所做的声明，并由可被认证的实体创建。之所以可以进行认证，是因为声明已签名，并且用于签名的密钥是可信的。\n最重要和最基础的证明类型之一是断言有关制品来源和创建的事实 - 它来自的源代码和将源代码转换为制品的构建指令，我们称之为来源证明。\n我们选择的来源证明规范来自 SLSA 项目。SLSA 是考虑软件供应链安全性的绝佳方式，因为它为软件的生产者和消费者提供了一个通用的框架，用于推理安全保证和边界，而这与特定的系统和技术栈无关。\nSLSA 基于 in-toto 项目的工作，提供了一种用于生成软件制品来源证明的标准化架构。in-toto 是一个 CNCF 毕业项目，其存在目的之一是提供一系列有关供应链和构建过程的相关信息的标准化元数据架构。\n构建这样的东西需要什么？ # GitHub 作为托管大量代码和构建管道的全球最大软件开发平台，对此进行了大量的思考。构建认证服务需要许多活动部件。\n这样做意味着有一种方法可以：\n颁发证书（本质上是绑定到某个经过身份验证的身份的公钥）。 确保这些证书不会被滥用。 在众所周知的上下文中启用工件的安全签名。 以最终用户可以信任的方式验证这些签名。 这意味着设置证书颁发机构 (CA) 并拥有某种客户端应用程序，你可以使用它来验证与该颁发机构颁发的证书相关联的签名。\n为了防止证书被滥用，你需要 (1) 维护证书吊销列表或 (2) 确保签名证书是短期的，这意味着需要某种时间戳机构的反签名（可以提供权威印章，表明证书仅在其有效期内用于生成签名）。\n这就是 Sigstore 的作用所在，它是一个开源项目，提供 X.509 CA 和基于 RFC 3161 的时间戳机构。它还允许你使用 OIDC 令牌进行身份验证，许多 CI 系统已经生成了令牌并将其与其工作负载相关联。\nSigstore 对软件签名的作用与 Let\u0026rsquo;s Encrypt 对 TLS 证书的作用相同：使其简单、透明且易于采用。\nGitHub 通过在技术指导委员会中的席位帮助监督 Sigstore 项目的治理，是服务器应用程序和多个客户端库的维护者，并且（与来自 Chainguard、Google、RedHat 和 Stacklok 的人员一起）组成了 Sigstore 公共物品实例的运营团队，该团队的存在是为了支持 OSS 项目的公共证明。\nSigstore 需要符合更新框架 (TUF) 规定的标准的安全信任根。这允许客户端跟上 CA 底层密钥的轮换，而无需更新其代码。TUF 的存在是为了缓解在现场更新代码时可能出现的大量攻击媒介。许多项目都使用它来更新长期运行的遥测代理、提供安全的固件更新等。\n有了 Sigstore，就可以创建防篡改的纸质跟踪，将工件与 CI 联系起来。这一点非常重要，因为以无法伪造的方式签署软件和捕获出处细节，意味着软件消费者有办法执行他们自己的规则，以确定他们正在执行的代码的来源。\n原文：https://github.blog/2024-04-30-where-does-your-software-really-come-from/\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-06-13","externalUrl":null,"permalink":"/posts/2024/where-does-your-software-come-from/","section":"Posts","summary":"本文介绍了软件制品的来源证明，强调了在软件开发生命周期中保护代码转换为制品的流程的重要性，并介绍了 SLSA 项目和 Sigstore 的作用。","title":"你的软件究竟从哪里来？","type":"posts"},{"content":"In my previous article on Code Signing, I mentioned GaraSign, another code signing tool I use at work.\nGiven the limited Chinese resources on GaraSign, this article will introduce some practical aspects of GaraSign, hoping it will be helpful to you.\nCode Signing # Let\u0026rsquo;s reiterate what code signing is. Code signing certificates are used to digitally sign applications, drivers, executables, and software programs. This allows customers to verify that the code they receive has not been tampered with or corrupted by cybercriminals and hackers. The signed delivery product combines an encryption token and a certificate, allowing users to verify it before installation. Code signing confirms the software author\u0026rsquo;s identity and proves that the code has not been modified or tampered with after signing.\nGarasign Solution # GaraSign is a SaaS-based security orchestration platform that allows for centralized management of enterprise infrastructure, services, and data. GaraSign integrates with native clients for all major operating systems, platforms, and tools, ensuring that existing workflows are not disrupted while improving their overall security posture and compliance.\nGaraSign consists of the following components: # Encryption Token — An encrypted device that stores signing keys (typically one or more HSMs — Hardware Security Modules) GaraSign Signing Server — A REST server located in front of the encryption token that stores the signing keys GaraSign Signing Client — A client that allows integrating signing tools to locally hash data and offload signature generation to the GaraSign Signing server. Garasign code signing hashing method — significantly improves speed\nInstalling GaraSign # I won\u0026rsquo;t go into detail about installing GaraSign here; you can find relevant installation documentation on the official website. Note that GaraSign currently has high operating system version requirements, such as:\nWindows minimum requirement is Windows 2019, Win10 and Win11 Linux minimum requirement is RHEL 7.9, 8.0, 9.0, CentOS 7.9, 8.0, 9.0, Rocky 8.0 If your build environment is older or doesn\u0026rsquo;t meet the supported versions, I recommend setting up a dedicated GaraSign machine (Linux recommended), as I did.\nIf you use Jenkins for building, you can set this machine as a Jenkins agent. By creating a Jenkins pipeline, all packages requiring publication can be signed through this pipeline.\nHow to Sign Independently # If you have set up the GaraSign environment, for example, on Linux, you can use the following command to sign.\nNote: The commands used to sign different file types vary between Windows and Linux; therefore, signing on Linux is recommended for simplicity.\nopenssl dgst -sign \u0026lt;private key file\u0026gt; -keyform PEM -sha256 -out \u0026lt;signature-file-name.sig\u0026gt; -binary \u0026lt;binary file to sign\u0026gt; Specific Implementation # Assuming your artifacts are stored on Artifactory, here\u0026rsquo;s a Jenkins example of an automated signing pipeline:\nDownload the artifacts to be signed from Artifactory Sign using GaraSign Verify GaraSign success Upload the signature file and public key to the same directory on Artifactory pipeline{ agent { node { label \u0026#39;garasign\u0026#39; } } parameters { string( name: \u0026#39;REPO_PATH\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Repository Path on Artifactory. eg. generic-stage/test_repo/devel/54/mybuild_1.1.0_752d0821_64bit.exe\u0026#39; ) } environment { BOT = credentials(\u0026#34;BOT-credential\u0026#34;) ART_URL = \u0026#34;https://my.org.com/artifactory\u0026#34; } stages { stage(\u0026#39;GaraSign\u0026#39;){ steps { script { if (! params.REPO_PATH){ error \u0026#34;REPO_PATH can not empty, exit!\u0026#34; } // Update Job description def manualTrigger = true currentBuild.upstreamBuilds?.each { b -\u0026gt; currentBuild.description = \u0026#34;Triggered by: ${b.getFullDisplayName()}\\n${REPO_PATH}\u0026#34; manualTrigger = false } if (manualTrigger == true) { currentBuild.description = \u0026#34;Manual sign: ${REPO_PATH}\u0026#34; } sh \u0026#39;\u0026#39;\u0026#39; # download artifacts curl -u${BOT_USR}:${BOT_PSW} -O ${ART_URL}/${REPO_PATH} file_name=$(basename ${REPO_PATH}) repo_folder=$(dirname ${REPO_PATH}) # garasign openssl dgst -sign grs.privkey.pem -keyform PEM -sha256 -out $file_name.sig -binary $file_name # verify grs.pem.pub.key output=$(openssl dgst -verify grs.pem.pub.key -keyform PEM -sha256 -signature $file_name.sig -binary $file_name) if echo \u0026#34;$output\u0026#34; | grep -q \u0026#34;Verified OK\u0026#34;; then echo \u0026#34;Output is Verified OK\u0026#34; else echo \u0026#34;Output is not Verified OK\u0026#34; exit 1 fi # upload signature file (.sig) and public key (.pem.pub.key) curl -u${BOT_USR}:${BOT_PSW} -T $file_name.sig ${ART_URL}/${repo_folder}/ curl -u${BOT_USR}:${BOT_PSW} -T grs.pem.pub.key ${ART_URL}/${repo_folder}/ \u0026#39;\u0026#39;\u0026#39; } } } } } How to Verify an Independent Signature # Again, using Linux as an example, the following command can be used to verify the signature.\nopenssl dgst -verify \u0026lt;public key file\u0026gt; -signature \u0026lt;signature\u0026gt; \u0026lt;file to verify\u0026gt; Once your artifacts have been signed, when providing them to customers, you need to provide not only the published package but also the signature file (.sig) and the public key (.pem.pub.key).\nFor example, the following CLI product provides installation packages for Windows, Linux, and AIX platforms. Customers can refer to the following for signature verification.\n# Download the installation package, signature file, and public key $ ls cli.pem.pub.key CLI_AIX_1.1.0.zip CLI_AIX_1.1.0.zip.sig CLI_LINUXX86_1.1.0.zip CLI_LINUXX86_1.1.0.zip.sig CLI_WINDOWS_1.1.0.zip CLI_WINDOWS_1.1.0.zip.sig # Verify the signature openssl dgst -verify cli.pem.pub.key -signature CLI_AIX_1.1.0.zip.sig CLI_AIX_1.1.0.zip Verified OK openssl dgst -verify cli.pem.pub.key -signature CLI_LINUXX86_1.1.0.zip.sig CLI_LINUXX86_1.1.0.zip Verified OK openssl dgst -verify cli.pem.pub.key -signature CLI_WINDOWS_1.1.0.zip.sig CLI_WINDOWS_1.1.0.zip Verified OK # Verification fails when the package and signature file do not match openssl dgst -verify cli.pem.pub.key -signature CLI_AIX_1.1.0.zip.sig CLI_LINUXX86_1.1.0.zip Verification Failure This concludes the GaraSign implementation sharing. For any questions or suggestions, please leave a comment.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2024-06-10","externalUrl":null,"permalink":"/en/posts/2024/garasign/","section":"Posts","summary":"This article introduces the installation, usage, and verification methods of the GaraSign code signing tool, helping developers achieve secure code signing.","title":"Code Signing — GaraSign","type":"posts"},{"content":"","date":"2024-06-10","externalUrl":null,"permalink":"/en/tags/devsecops/","section":"Tags","summary":"","title":"DevSecOps","type":"tags"},{"content":"","date":"2024-06-10","externalUrl":null,"permalink":"/en/tags/slsa/","section":"Tags","summary":"","title":"SLSA","type":"tags"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/infrastructure/","section":"标签","summary":"","title":"Infrastructure","type":"tags"},{"content":"Python 软件基金会 (PFS) 或许大家比较熟知，它是开源 Python 编程语言背后的组织，致力于为 Python 和 Python 社区的发展壮大创造条件。\n继上次我们看完了 Apache 的基础设施介绍，本篇文章我们一起来看看 Python 软件基金会 (PFS) 的基础设施，看看可以从中学到哪些。\nPSF 基础设施概述 # PSF 运行各种基础设施服务来支持其使命，从 PyCon 站点 到 CPython Mercurial 服务器。本页的目标是列举所有这些服务，它们在哪里运行，以及主要联系人是谁。\n基础架构团队 # 基础架构团队最终负责维护 PSF 基础设施。但是，它不需要成为运行 PSF 服务的基础设施。事实上，大多数的日常运营服务由不在基础设施团队中的人员处理。这基础设施团队可以协助建立新服务并为维护人员提供建议的个别服务。其成员通常还处理对敏感的更改全球系统，例如 DNS。目前的团队成员是：\nAlex Gaynor (has no responsibilities) Benjamin Peterson Benjamin W. Smith Donald Stufft Ee Durbin (PSF Director of Infrastructure) Noah Kantrowitz 联系基础架构团队的最佳方式是发送邮件 infrastructure-staff@python。也经常有人在 Libera 的 #python-infra 频道联系他们。\n基础设施提供商 # PSF 为其基础架构使用多个不同的云提供商和服务。\nFastly Fastly 慷慨捐赠其内容分发网络（CDN）到 PSF。我们最高的流量服务（即 PyPI, www.python.org, docs.python.org）使用此 CDN 来改善最终用户延迟。\nDigitalOcean DigitalOcean 是当前的主要托管对于大多数基础设施，此处部署的服务由 Salt 管理。\nHeroku Heroku 托管了许多 CPython 核心工作流机器人，短暂的或概念验证的应用程序，以及其他适合部署在 Heroku 的 Web 应用程序。\nGandi Gandi 是我们的域名注册之星。\nAmazon Route 53 Amazon Route 53 托管所有域的 DNS，它目前由基础设施人员手动管理。\nDataDog DataDog 提供指标、仪表板和警报。\nPingdom Pingdom 提供监控，当服务中断时向我们投诉。\nPagerDuty PagerDuty 用于 PSF 的待命轮换基础设施员工在一线，志愿者作为后援。\nOSUOSL 俄勒冈州立大学开源实验室举办一个 PSF 的硬件服务器，speed.python.org 用于运行基准测试，此主机是使用 Chef 和他们的配置管理位于 PSF-Chef Git 存储库中。\n数据中心 # PSF DC Provider Region ams1 Digital Ocean AMS3 nyc1 Digital Ocean NYC3 sfo1 Digital Ocean SFO2 各种服务的详细信息 # 本部分列举了 PSF 服务、有关其托管的一般情况以及所有者的联系信息。\nBuildbot buildbot master 是由 python-dev@python 运行的服务。特别是 Antoine Pitrou and Zach Ware.\nbugs.python.org bugs.python.org 由 PSF 在 DigitalOcean 上托管，由 Roundup 提供支持。它还部署了 bugs.jython.org 和 issues.roundup-tracker.org。\ndocs.python.org Python 文档托管在 DigitalOcean 上，通过 Fastly 提供。负责人是 Julien Palard。\nhg.python.org CPython Mercurial 存储库托管在 Digital Ocean VM 上。负责人是 Antoine Pitrou 和 Georg Brandl。\nmail.python.org python.org Mailman 实例托管在 https://mail.python.org 和 SMTP（Postfix）上。所有查询都应定向到 postmaster@python。\nplanetpython.org 和 planet.jython.org 它们托管在 DigitalOcean VM 上。Planet 代码和配置托管在 GitHub 上，并由团队在 planet@python。\npythontest.net pythontest.net 托管 Python 测试套件。python-dev@python 对其最终负责维护。\nspeed.python.org speed.python.org 是一个跟踪 Python 性能的 Codespeed 实例。Web 界面托管在 DigitalOcean VM 上，基准测试在 strongfy 上运行机器在 OSUOSL 上，由 Buildbot 主节点调度。由 speed@python 和 Zach Ware 维护。\nwiki.python.org 它托管在 DigitalOcean VM 上。负责人是 Marc-André Lemburg。\nwww.jython.org 这是从 Amazon S3 存储桶托管的。设置非常简单，不应该需要很多调整，但基础设施工作人员如有需要可以对它进行调整。\nwww.python.org 主要的 Python 网站是一个 Django 应用程序，托管在 Heroku。它的源代码可以在 GitHub 上找到，并且该网站的问题可能是报告给 GitHub 问题跟踪器。 Python 下载 （即 https://www.python.org/ftp/ 下的所有内容）都托管在单独的 DigitalOcean 虚拟机。整个网站都在 Fastly 后面。还有用于测试站点的 https://staging.python.org。http://legacy.python.org 是从静态镜像托管的旧网站。\nPyCon PyCon 网站托管在 Heroku 上。联系地址是 pycon-site@python。\nPyPI Python 包索引的负载最多 任何 PSF 服务。它的源代码可在 GitHub 上找到。 它的所有基础设施都在 由 pypi-infra 配置的 AWS，它以 Fastly 为首。基础设施是由 Ee Durbin, Donald Stufft, 和 Dustin Ingram 维护的，联系地址是 admin@pypi。\nPyPy properties PyPy 网站托管在 DigitalOcean VM 上并进行维护作者：pypy-dev@python。\n如需要参看原文。可访问地址。\n最后 # 从 PFS 基础设施来看，他们更多的使用了 Cloud 服务商和技术来部署他们的应用。因此想要参与到 PFS 基础设施管理用，需要对 CDN，DigitalOcean，Heroku，Amazon Route 53，Amazon S3，DataDog，Pingdom 这些技术有比较深入的使用经验。\n我们羡慕那些可以为开源软件全职工作的人，他们拿着薪水做着很多程序员都羡慕的事情。\n但拥有这样的工作需要我们自己的技能可以匹配的上，并且积极主动的去寻求这些机会，才有可能得到一份全职开源软件工程师的职位。共勉！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-05-28","externalUrl":null,"permalink":"/posts/2024/psf-infra/","section":"Posts","summary":"本文介绍了 Python 软件基金会 (PFS) 的基础设施，包括其服务、提供商和团队成员，帮助读者了解 PFS 如何支持 Python 社区。","title":"Python 软件基金会 (PFS) 基础设施概览","type":"posts"},{"content":"When it comes to software development and security, Code Signing is a crucial concept. In this article, we will explore what code signing is, why it\u0026rsquo;s important, and compare two code signing tools.\nWhat is Code Signing? # Code signing is a digital signature technique used to verify the authenticity and integrity of software or code. It uses cryptography to attach a digital signature to a software file, proving that the file was created by a specific developer and has not been tampered with.\nThe code signing process typically involves the following steps:\nGenerate a digital certificate: Developers use a digital certificate to create a digital signature. Certificates are usually issued by a trusted third-party Certificate Authority (CA). Sign the software: Developers use specialized tools, such as Microsoft\u0026rsquo;s SignTool or Apple\u0026rsquo;s codesign tool, to digitally sign the software. Distribute the signed software: Digitally signed software can be distributed to user devices. Why is Code Signing Important? # The importance of code signing is reflected in several aspects:\nIntegrity verification: Code signing ensures that the software has not been tampered with during distribution. Users can verify the signature to confirm the software\u0026rsquo;s integrity and ensure it comes from a trusted source.\nAuthentication: The digital certificate accompanying the signature can be used to verify the identity of the software publisher. Users can view the certificate to learn about the software manufacturer and assess its trustworthiness.\nEnhanced security: Digital signatures prevent malicious software from being inserted into legitimate software packages, ensuring that the software downloaded by users is safe and reliable.\nOperating system trust: Most operating systems and app stores require developers to code-sign software before publishing. Unsigned software may be considered insecure or untrusted.\nCode Signing Tools # In my work, I mainly use two code signing tools: Code Signing Certificates Code Signing Certificates and GaraSign, which are representative of two different types of tools.\nHere\u0026rsquo;s a brief comparison of Code Signing Certificates and GaraSign:\nCode Signing Certificates and GaraSign are both solutions for verifying software integrity and origin, but they have some key differences in how they work and their functionality.\nFeature Code Signing Certificates GaraSign Issuer Trusted Certificate Authority (CA) GaraSign Form Digital Certificate Cloud Service Verification Method Cryptographic Hash Cryptographic Hash Functionality Verify software integrity, ensure software hasn\u0026rsquo;t been tampered with Verify software integrity, ensure software hasn\u0026rsquo;t been tampered with, provide centralized management, support audit and compliance Cost $100 to thousands of dollars per year Pay-as-you-go Management Requires purchasing and managing certificates No need to manage certificates Scalability Suitable for organizations that need to sign a large amount of software Suitable for organizations of any size In general, both Code Signing Certificates and GaraSign are effective solutions for verifying software integrity and origin. The best choice for you will depend on your specific needs and budget. Here are some factors to consider:\nCost: If you need to sign a large amount of software, GaraSign may be more cost-effective. Scalability: If you need to sign a large amount of software, GaraSign may be a better choice. Audit and compliance: If you need to meet stringent audit and compliance requirements, GaraSign may be a better choice. Ease of use: Based on my current experience, Code Signing Certificates are easier to set up and use. GaraSign requires setting up services and installing and configuring clients, which is very cumbersome1. Other signing tools include Microsoft\u0026rsquo;s SignTool.exe, Docker trust sign, etc., which will not be introduced here.\nConclusion # Through code signing, developers can increase the security and trustworthiness of software, while helping users avoid malicious software and tampering risks. In today\u0026rsquo;s digital environment, code signing is an important part of ensuring software integrity and security, improving software supply chain security.\nFor more information on software supply chain security, see this series of articles.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\nI am still in the early stages of using GaraSign. If necessary, I will write separate articles about my user experience with it.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-04-29","externalUrl":null,"permalink":"/en/posts/2024/code-signing/","section":"Posts","summary":"This article introduces the concept and importance of code signing, along with a comparison of two common code signing tools, emphasizing its role in software supply chain security.","title":"Code Signing","type":"posts"},{"content":"今天翻译一篇我在 Jenkins Contributors 页面上看到的一篇文章。\n其作者是 Hervé Le Meur，我早在关注 Jenkins-Infra 的项目的时候就关注到他，他是一个法国人。以下是关于他如何通过 Jenkins-X 社区最终进入到 Jenkins 基础设施团队成为 SRE 的经历。\n说实话有点羡慕。希望这个分享能给每一个想加入开源、并且在开源组织中找到一份全职工作带来一点启发。\n以下是我对原文的翻译：\nHervé Le Meur 是一名 SRE（网站可靠性工程师），目前是 Jenkins 基础设施团队的成员。他是通过 Jenkins X 进入开源社区的，随后转到 Jenkins 基础设施工作。\nHervé 的父亲是一名木匠，母亲是一名室内装潢师，他出身于一个模拟技术背景的家庭，但在六岁时就通过 Amstrad CPC 464 电脑第一次真正接触到了技术。\n如今，在不从事 Jenkins 任务的时候，Hervé 喜欢和家人一起到户外散步、阅读和观看自己喜欢的节目。\n在加入 Jenkins 之前，你的背景是什么？ # 大学毕业后，我在一家小型 B2B 营销咨询公司工作了 10 年，当时我是开发我们所用工具的万事通，但那里既没有 CI/CD，也没有开源。\n之后，我进入一家建筑信息建模（BIM）软件公司，从软件开发人员做起。有些团队在使用 Jenkins，但对他们来说，当时的 Jenkins 看起来既麻烦又缓慢。\n随着我逐渐成长为软件架构师，我的任务是基于 Jenkins X 建立一个新的 CI/CD，这花了我几个月的时间。 由于 Jenkins X 刚刚出炉，而我又是 Kubernetes 的新手，这项任务比预期的要困难得多，尤其是 Jenkins X 进入测试阶段后，我不得不多次重做大部分工作。\n通过这项工作，我学到了很多关于 Kubernetes 和 CI/CD 的知识，同时也为 Jenkins X 做出了不少贡献。 被这份工作解雇后，我联系了 James Strachan 和 James Rawlings，他们给了我一个链接，让我从 CloudBees 公司的 Oleg Nenashev 那里获得一个职位空缺，也就是我现在的职位。\n在我的脑海中，我是一名程序员，而不是系统管理员。因此，当 Mark Waite 解释说最后一轮面试将与人际网络有关时，我有点害怕。 我以为我会因此失去机会，因为这可能是不可能完成的任务。然而，当我与 Mark、Damien Duportal 和 Olivier Vernin 面谈时，他们却问我如何将 CI/CD 与 Jenkins X 集成：这真是一次奇妙的经历。我们进行了有意义的讨论，这让我感觉更舒服，也让我更容易做出决定。\n面试前15分钟，我收到了另一家公司（ Damien 和 Jean-Marc Meessen 之前恰好在这家公司工作过）的最终录用通知，当时我犹豫了一下，但结果是最好的，因为我现在仍然是 Jenkins 项目的一员，这可以说是我梦寐以求的工作。\n我还有过主持在线论坛的经验，所以社区部分的工作对我来说很熟悉。\n你使用 Jenkins 多久了？ # 我从 Jenkins X 开始，但从未接触过 Jenkins 本身。除了共享 Jenkins 的名称外，它们没有任何共同之处。 我对 Jenkins 的预想是负面的。我认为它是一个笨重、过时、复杂的 Java 程序。这些印象都是我在以前的公司里从其他使用它的人那里听来的。然而，当我开始使用 Jenkins 后，这种对比简直是天壤之别。 我的先入之见是，与其他程序相比，它既笨重又缓慢。我并不是唯一一个认为 Jenkins 不一定是最好、最快或最新的项目的人，但事实证明，一旦我开始使用这个项目，我就错了。\n为什么选择 Jenkins 而不是其他项目？ # 我并不一定选择 Jenkins，因为它是我工作的主要部分。当我开始查看 Tyler、Olivier、Damien 和 Mark 为 Jenkins 基础设施所做的工作时，我意识到 Jenkins 比我想象的要完善和高效得多。 我还喜欢我们使用 Jenkins 开发和发布 Jenkins 的事实。这种用法是独一无二的，因为大多数开源工具都不具备转发成功的能力。 Jenkins 花费了大量的时间和精力，以配合开发人员的流程和功能。在我看来，这是 Jenkins 成功的主要原因之一。Jenkins 还集成了 Terraform、Puppet、Kubernetes 和 Helmfile 等其他工具，但 Jenkins 仍然是这些工具的协调者。\n对我来说，为 Jenkins 工作是我的最高成就，因为我一直喜欢创建和开发工具，即使不是开发 Jenkins 的核心。\n加入 Jenkins 项目后，你看到 Jenkins 有哪些增长？ # 我们已经有越来越多的实例被定义为代码。因此，我们可以重新创建任何实例，而无需手动配置设置，这大大提高了我们的恢复能力。我们还在慢慢实现将 ci.jenkins.io 定义为代码并进行管理。\n你对 Jenkins 的哪方面特别感兴趣？ # 现在，我正在重构 Jenkins 控制器和代理的官方 Docker 镜像的构建过程，这让我感到非常有趣。 我也喜欢在贡献者方面工作，因为这就像一个谜题，知道我需要达到什么目标以及我的起点会让我更愉快。\n什么样的贡献最成功或最有影响力？ # Basil Crow 提出了使用 SQLite 代替文件系统的有趣想法。改用 JDK 17 非常成功，随着 JDK 21 的推出，Jenkins 可以在更新的平台上运行，并跟上时代的进步。 由于我们喜欢让 Jenkins 基础架构保持领先（例如始终运行最新的 Jenkins Weekly），下一步将引入 JDK22。插件健康分数标识对于可视化整个插件生态系统的健康状况非常有用。\n给新开发人员和开源社区新成员的建议 # 首先要提醒我的是项目的庞大性，并指示我一开始要选择一件事来专注。\n不要犹豫，大胆尝试；开源意味着对所有人开放。不要害怕提交 pull request，它并不需要完美无缺。\n你可能最终会喜欢它，并继续提交贡献！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-04-21","externalUrl":null,"permalink":"/posts/2024/jenkins-contributors/","section":"Posts","summary":"本文介绍了 Hervé Le Meur 如何通过 Jenkins-X 社区的贡献，最终成为 Jenkins 基础设施团队的一名 SRE，并分享了他的经历和对 Jenkins 的看法。","title":"通过 Jenkins-X 社区最终进入到 Jenkins 基础设施团队成为 SRE 的经历","type":"posts"},{"content":"相信大家最近都总会看到这样或那样的新闻：哪个科技巨头又裁员了。裁员潮似乎成为了这个时代的常态，让许多打工人感到焦虑和不安。\n身在大连的我确实深有感触，外企和私企都有在裁员，与前两年相比，岗位越来越少，失业的人越来越多，因此想找到一个满意的岗位将会变得越来越难。\n再加上随着人工智能（AI）的发展，作为 DevOps 打工人常常在想，需要掌握哪些关键技能和能力才能让自己保持竞争力。\n以下是我认为在 2024 年至关重要的关键技能和能力：\n深入理解 DevOps 理念和工具：\n熟练掌握持续集成/持续交付（CI/CD）工具和流程。如 Jenkins，GitLab CI/CD，GitHub Actions。 能够设计和优化自动化部署流程，包括自动化测试、构建和发布。 精通容器化技术，如 Docker，以及容器编排工具，如 Kubernetes，Helm。 云计算和基础设施：\n对主流云服务提供商（如 AWS、Azure、Google Cloud）的基础设施和服务有深入了解。 能够进行云原生架构设计和实施，包括使用云原生服务和技术。 自动化和编程能力：\n精通至少一种编程语言（如 Python、Go、Java 等），能够编写脚本和工具来实现自动化。 对基础架构即代码（IaC）工具有熟练掌握，例如 Terraform、Ansible 等。 监控和日志管理：\n熟悉监控和日志管理工具，能够建立完善的监控系统和日志分析平台。 掌握应用性能监控和故障排除技术。如 Prometheus，Grafana，ELK Stack。 安全和合规性：\n了解容器和云安全最佳实践，能够设计安全的部署架构。 理解数据隐私和合规性要求，能够实施符合法规的解决方案。如 HashiCorp Vault，Chef InSpec。 持续学习和技术更新：\n持续关注新技术和行业趋势，参与培训和研讨会，多于同行交流。 不断学习和提升自身的技能，保持适应快速变化的技术环境。 团队协作和沟通能力：\n良好的团队合作和沟通能力，能够与开发团队、运维团队和其他利益相关者有效地协作。 熟练使用版本控制系统和协作工具。 问题解决和创新思维：\n具备快速定位和解决问题的能力，善于思考创新解决方案。 鼓励并参与团队中的持续改进和创新活动。 业务理解和领导能力（对于高级岗位）：\n具备对业务需求的理解和洞察，能够为业务提供技术支持和解决方案。 如果担任领导职务，需要具备领导团队和推动项目的能力。 只有通过不断学习和拓展技能，保持对最新技术的了解，注重团队协作和创新，才能够在市场不好，AI崛起的环境中继续保持竞争力。\n最后，希望大家都能在 2024 年工作顺利，不被裁员；裁员 N+x (x\u0026gt;=1)，并顺利过渡到下一份更好的工作 💪\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-04-08","externalUrl":null,"permalink":"/posts/2024/devops-skills-2024/","section":"Posts","summary":"本文介绍了在2024年DevOps工程师需要掌握的关键技能和能力，以应对裁员潮和人工智能的挑战，保持竞争力。","title":"2024年如何保持竞争力：DevOps工程师的关键技能","type":"posts"},{"content":" What are Reusable Workflows # If you\u0026rsquo;ve used GitHub Actions, you must know about the Reusable Workflows feature. It allows you to define workflows and reuse them across multiple repositories.\nGitHub Actions is GitHub\u0026rsquo;s own CI/CD tool. Other mainstream CI/CD tools include Jenkins, Azure DevOps, Travis CI, etc.\nWith GitHub Reusable Workflows, you can define common workflows in a separate Git repository and then reference these workflows in other repositories without having to redefine them in each repository. The benefits include:\nConsistency: Ensures your team or organization uses the same standardized workflows across different repositories, maintaining consistency. Maintainability: Changes or updates to the workflow only need to be made in one place, eliminating the need to modify code in multiple repositories. Reusability: Separates common workflows, allowing reuse in any project as needed, improving code reusability and maintainability. In general, GitHub Reusable Workflows makes managing and organizing workflows in GitHub Actions more flexible and maintainable.\nHow to Use Reusable Workflows # GitHub Reusable Workflows allows you to create a workflow in .github or another repository and then call that workflow from other repositories.\nHere are the general steps for using GitHub Reusable Workflows:\nCreate a Reusable Workflow:\nCreate a new repository under your GitHub account to store your reusable workflows. Create a directory named .github/workflows in the repository (if it doesn\u0026rsquo;t exist). Create a YAML file in this directory to define your workflow. Define a Parameterized Workflow (Optional):\nIf you want your workflow to be parameterized, you can define parameters using the inputs keyword in your workflow. Commit the Workflow to the Repository:\nCommit your created workflow YAML file to the repository, ensuring it\u0026rsquo;s located in the .github/workflows directory. Use the Workflow in Other Repositories:\nOpen the other repository where you want to use this workflow. Create a YAML file in the .github/workflows directory that points to the YAML file of your previously created reusable workflow. Commit Changes and Trigger the Workflow:\nCommit the changes to the repository and push them to the remote repository. GitHub will automatically detect the new workflow file and trigger workflow execution based on triggers (e.g., push, pull_request, etc.). Here\u0026rsquo;s a simple example demonstrating how to create and use a reusable workflow:\nSuppose you create a workflow file named build.yml in the .github/workflows directory of the reuse-workflows-demo repository to build your project. The file contents are as follows:\nIf not in the .github/workflows directory, you will encounter this error invalid value workflow reference: references to workflows must be rooted in '.github/workflows'\nname: Build on: workflow_call: inputs: target: required: true type: string default: \u0026#34;\u0026#34; jobs: build: strategy: matrix: target: [dev, stage, prod] runs-on: ubuntu-latest steps: - name: inputs.target = ${{ inputs.target }} if: inputs.target run: echo \u0026#34;inputs.target = ${{ inputs.target }}.\u0026#34; - name: matrix.targe = ${{ matrix.target }} if: matrix.target run: echo \u0026#34;matrix.targe = ${{ matrix.target }}.\u0026#34; Then, in the .github/workflows directory of your other repository, you can create a workflow build.yml pointing to this file, for example:\nname: Build on: push: pull_request: workflow_dispatch: jobs: call-build: uses: shenxianpeng/reuse-workflows-demo/.github/workflows/build.yml@main with: target: stage For more real-world examples of Reusable Workflows, refer to the .github repository under the cpp-linter organization.\n# cpp-linter/.github/.github/workflows . ├── codeql.yml ├── main.yml ├── mkdocs.yml ├── pre-commit.yml ├── py-coverage.yml ├── py-publish.yml ├── release-drafter.yml ├── snyk-container.yml ├── sphinx.yml └── stale.yml 7 Best Practices for GitHub Reusable Workflows: # Modular Design: Break down workflows into small, reusable modules, each focusing on a specific task. This improves workflow flexibility and maintainability, making them easier to reuse and combine. Parameterized Configuration: Allow workflows to accept parameters for configuration with each use. This makes workflows more versatile, adapting to different project and environment needs. Version Control: Ensure your reusable workflows are version-controlled and updated regularly to reflect changing project requirements. Use GitHub branches or tags to manage different workflow versions and easily switch or roll back when needed. Documentation and Comments: Provide clear documentation and comments for workflows to help other developers understand their purpose and steps. Comment on the purpose and implementation details of key steps so others can easily understand and modify the workflow. Security: Carefully handle workflow files containing sensitive information (such as credentials, keys, etc.) to prevent accidental leaks. Store sensitive information in GitHub Secrets and use Secrets to access this information in the workflow. Testing and Validation: Test and validate reusable workflows before introducing them to projects to ensure they integrate and execute correctly. Simulate and test workflows in a separate test repository to ensure correctness and reliability. Performance Optimization: Optimize workflow performance, minimizing unnecessary steps and resource consumption to ensure workflows complete tasks within a reasonable time and minimize system resource usage. Following these best practices helps you better utilize GitHub Reusable Workflows and provides your projects and teams with more efficient and maintainable automated workflows.\nDifferences and Similarities between Reusable Workflows and Jenkins Shared Library # Finally, let\u0026rsquo;s discuss my understanding and summary of GitHub Reusable Workflows and Jenkins Shared Library. There are similarities, but also differences.\nSimilarities:\nReusability: Both aim to provide a mechanism to define common automated workflows as reusable components, sharing and reusing them across multiple projects. Organization: Both help better organize and manage automated workflows, making them easier to maintain and update. Parameterization: Both support parameterization, allowing workflows to be customized and configured in different contexts as needed. Differences:\nPlatform: Reusable Workflows are part of GitHub Actions, while Shared Library is a Jenkins feature. They run on different platforms with different ecosystems and working principles. Syntax: Reusable Workflows use YAML syntax to define workflows, while Shared Library uses Groovy to define shared libraries. Ease of Use: Reusable Workflows are relatively simple to use on the GitHub platform, especially for projects already hosted on GitHub. Shared Library requires configuring the Jenkins server and related plugins and requires some understanding of the Jenkins build process. In summary, although GitHub Reusable Workflows and Jenkins Shared Library both aim to provide reusable automated workflows and share some similarities, they have significant differences in platform, syntax, and ease of use.\nThe specific choice depends on your needs, workflow, and the platform you need to use.\n","date":"2024-03-25","externalUrl":null,"permalink":"/en/posts/2024/reusable-workflows/","section":"Posts","summary":"This article introduces the Reusable Workflows feature of GitHub Actions, helping developers and teams manage and reuse CI/CD processes more efficiently.","title":"Must-Know GitHub Action Feature - Reusable Workflows","type":"posts"},{"content":"最近看到一篇非常有信息量的关于人工智能、云原生、开源的趋势报告，出自于GitHub，翻译并分享给大家，以下是报告全文。\n英文原文在这里：https://github.blog/2023-11-08-the-state-of-open-source-and-ai/?utm_source=banner\u0026amp;utm_medium=github\u0026amp;utm_campaign=octoverse\n新技术成为主流意味着什么？\nGit 于 2005 年首次发布，当我们创建 GitHub 时，Git 还是一个新的开源版本控制系统。如今，Git 已成为现代开发人员体验的基本元素 — 93% 的开发人员使用它在各地构建和部署软件。\n2023 年，GitHub 数据凸显了另一种技术如何迅速开始重塑开发者体验：人工智能。去年，越来越多的开发人员开始使用人工智能，同时也尝试构建人工智能驱动的应用程序。 Git 从根本上改变了当今的开发人员体验，现在人工智能正在为软件开发的下一步奠定基础。\n在 GitHub，我们知道开发人员喜欢边做边学，开源可以帮助开发人员更快地采用新技术，将其集成到他们的工作流程中，并构建下一代技术。开源还为几乎所有现代软件提供动力——包括大部分数字经济。当我们探索技术如何成为主流时，GitHub 在弥合开源技术实验与广泛采用之间的差距方面继续发挥着关键作用，这些技术支撑着我们软件生态系统的基础。\n在今年的报告中，我们将研究围绕人工智能、云和 Git 的开源活动如何改变开发人员体验，并日益增强对开发人员和组织的影响。\n我们发现了三大趋势:\n开发人员正在大量使用生成式人工智能进行构建。 我们看到越来越多的开发人员尝试使用 OpenAI 和其他 AI 参与者的基础模型，开源生成式 AI 项目甚至会在 2023 年进入按贡献者数量计算的前 10 个最受欢迎的开源项目。几乎所有开发人员 (92%) 都在使用或试验借助 AI 编码工具，我们期望开源开发人员能够在 GitHub 上推动下一波 AI 创新浪潮。 开发人员正在大规模运营云原生应用程序。 我们看到使用基于 Git 的基础设施即代码 (IaC) 工作流程的声明性语言有所增加，云部署的标准化程度更高，开发人员使用 Dockerfile 和容器、IaC 和其他云原生的速度急剧增加技术。 2023 年首次开源贡献者数量最多。 我们继续看到商业支持的开源项目在首次贡献者和总体贡献中占据最大份额，但今年，我们还看到生成式 AI 项目进入了首次贡献者最受欢迎的前 10 个项目。我们还看到 GitHub 上的私人项目显著增长，同比增长 38%，占 GitHub 上所有活动的 80% 以上。 在 GitHub 上构建的全球开发者社区 # 在全球范围内，开发人员正在使用 GitHub 来构建软件并进行比以往更多的协作，而且涉及公共和私人项目。这不仅证明了 Git 在当今开发者体验中的基础价值，也展示了全球开发者社区使用 GitHub 构建软件的情况。\n美国拥有 2020 万开发者，过去一年开发者增长 21%，继续拥有全球最大的开发者社区。\n但自 2013 年以来，我们不断看到其他社区在整个平台上实现了更多增长，我们预计这种情况会持续下去。 GitHub 上开发人员的全球分布显示了哪些地区拥有最多的开发人员。\n亚太地区、非洲、南美洲和欧洲的开发者社区逐年扩大，其中印度、巴西和日本处于领先地位。\n预测未来五年排名前 10 的开发者社区 # 为了了解哪些开发者社区将在未来五年内增长最快，我们根据当前的增长率进行了预测。在此标题下，我们预计到 2027 年印度将取代美国成为 GitHub 上最大的开发者社区。\n亚太地区发展最快的开发者社区 # 我们继续看到，在印度、日本和新加坡等经济中心的推动下，亚太地区出现了可观的增长。\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 表 1：2023 年开发者总数增长，较 2022 年增长百分比。\n印度的开发者社区继续实现同比大幅增长 # 在去年的 Octoverse 中，我们预测印度的开发者总数将超过美国。这仍然有望发生。印度的开发者人数同比增长 36%，2023 年有 350 万新开发者加入 GitHub。\n作为联合国支持的数字公共产品联盟的一部分，印度一直在利用开放材料（从软件代码到人工智能模型）建设数字公共基础设施，以改善数字支付和电子商务系统。以下是印度开发人员在 GitHub 上构建并贡献的开源软件 (OSS) 项目列表。\n新加坡今年是亚太地区开发者人数增长最快的国家 # 并且以开发者占总人口的比例最高而位居全球第一。新加坡国立大学计算机学院将 GitHub 纳入其课程，高增长也可能归因于该国在东南亚的监管重要性。\n由于对技术和初创公司的投资，我们还可能在明年看到日本的开发人员持续增长。\n非洲发展最快的开发者社区 # 非洲地区拥有世界上增长最快的人口和不断增加的开发人员，已被认为是有前途的科技公司中心。（例如，在肯尼亚，小学和中学必须教授编程。）\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 表 2：2023 年开发者总数增长，较 2022 年增长百分比。\n尼日利亚是 OSS 采用和技术投资的热点，其 45% 的同比增长率（全球增幅最高）反映了这一点。GitHub 上还有至少 200 个由尼日利亚开发者制作的项目集合，可以在“非洲制造”集合下找到。\n南美洲发展最快的开发者社区 # 南美洲的开发者增长率与亚太和非洲一些增长最快的开发者社区相当。\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 表 3：2023 年开发者总数增长，较 2022 年增长百分比。\n2023年，巴西的开发者人数是该地区最多的，并继续以两位数增长，同比增长30%。此前，巴西的私人和公共组织持续投资。查看巴西开发人员在 GitHub 上创建和贡献的 OSS 项目列表。\n我们还看到阿根廷和哥伦比亚的持续增长，这两个国家在过去几年中已成为组织的热门投资目标。\n欧洲发展最快的开发者社区 # 整个欧洲的社区开发人员总数继续增加，但他们的发展现在更接近于美国的总体发展，因为南美洲、非洲和亚太地区的社区增长超过了他们。\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 值得注意的是，法国的增长是在政府推动吸引更多科技初创企业之后实现的。我们还看到西班牙和意大利的增长有所上升，这说明这两个地区为支持其国内技术市场所做的努力。\n2023 年生成式 AI 爆发式增长 # 虽然生成式人工智能在 2023 年引起了轰动，但对于 GitHub 上的开发者来说，它并不是全新的。事实上，过去几年我们已经在 GitHub 上看到了几个生成式 AI 项目的出现，以及许多其他专注于 AI 的项目。\n但 2023 年的 GitHub 数据反映了这些人工智能项目如何从更面向专业的工作和研究发展到更主流的采用，开发人员越来越多地使用预先训练的模型和 API 来构建由人工智能驱动的生成应用程序。\n就在去年过半的时候，我们看到 2023 年的生成式 AI 项目数量是 2022 年全年的两倍多。 我们知道这只是冰山一角。\n随着越来越多的开发人员尝试这些新技术，我们期望他们能够推动软件开发中的人工智能创新，并继续将该技术快速发展的功能带入主流。\n开发人员越来越多地尝试人工智能模型。 在过去的几年里，我们看到开发人员使用 tensorflow/tensorflow、pytorch/pytorch 等机器学习库构建项目，而现在我们看到更多的开发人员尝试使用 AI 模型和LLM（例如 ChatGPT API）。\n保持聪明： 我们预计企业和组织也将利用预先训练的人工智能模型，特别是随着越来越多的开发人员熟悉如何使用它们进行构建。\n开源人工智能创新多种多样，顶级人工智能项目由个人开发者拥有。 分析 GitHub 上排名前 20 的开源生成式 AI 项目，其中一些顶级项目归个人所有。这表明 GitHub 上的开源项目继续推动创新，并向我们所有人展示行业的未来发展，社区围绕最令人兴奋的进步而构建。\n生成式人工智能正在推动生成式人工智能项目的个人贡献者在全球范围内大幅增长，同比增长 148%，生成式人工智能项目总数也同比增长 248%。 值得注意的是，美国、印度和日本在开发者社区中处于领先地位，其他地区（包括香港特别行政区）、英国和巴西紧随其后。\n了解生成式人工智能的开发人员数量大幅增加将对企业产生影响。 随着越来越多的开发人员熟悉构建基于人工智能的生成式应用程序，我们预计不断增长的人才库将支持寻求开发自己的基于人工智能的产品和服务的企业。\n底线： 在过去的一年里，我们看到基于基础模型（例如 ChatGPT）构建的应用程序呈指数级增长，因为开发人员使用这些 LLM 来开发面向用户的工具，例如 API、机器人、助手、移动应用程序和插件。全球开发人员正在帮助为主流采用奠定基础，而实验正在帮助组织建立人才库。\n最流行的编程语言 # 自从我们在 2019 年看到云原生开发的巨大增长以来，IaC 在开源领域也持续增长。 2023 年，Shell 和 Hashicorp 配置语言 (HCL) 再次成为开源项目中的顶级语言，这表明运维和 IaC 工作在开源领域越来越受到重视。\nHCL 采用率同比增长 36%，这表明开发人员正在为其应用程序利用基础设施。 HCL 的增加表明开发人员越来越多地使用声明性语言来指示他们如何利用云部署。 JavaScript 再次夺得第一大最受欢迎语言的桂冠，并且我们继续看到 Python 和 Java 等熟悉的语言逐年保持在前五名语言之列。\nTypeScript 越来越受欢迎。 今年，TypeScript 首次取代 Java，成为 GitHub 上 OSS 项目中第三大最受欢迎的语言，其用户群增长了 37%。 TypeScript 是一种集语言、类型检查器、编译器和语言服务于一体的语言，它于 2012 年推出，标志着渐进类型的到来，它允许开发人员在代码中采用不同级别的静态和动态类型。\n用于数据分析和操作的流行语言和框架显著增加。 T-SQL 和 TeX 等古老语言在 2023 年不断发展，这凸显了数据科学家、数学家和分析师如何越来越多地使用开源平台和工具。\n底线： 编程语言不再仅仅局限于传统软件开发领域。\n与 GitHub 上使用的总体最流行语言相比，我们发现 2023 年创建的项目中使用的最流行语言具有显著的一致性。一些值得注意的异常值包括 Kotlin、Rust、Go 和 Lua，它们在 GitHub 上的新项目中出现了更大的增长。\nRust 和 Lua 都以其内存安全性和效率而闻名，并且都可以用于系统和嵌入式系统编程，这可以归因于它们的增长。Go 最近的增长是由 Kubernetes 和 Prometheus 等云原生项目推动的。\n开发者活动是新技术采用的领头羊 # 2023 年初，我们庆祝了超过 1 亿开发者使用 GitHub 的里程碑 —— 自去年以来，我们看到 GitHub 上的全球开发者帐户数量增长了近 26%。更多的开发人员跨时区协作并构建软件。私人和公共存储库中的开发人员活动强调了哪些技术正在被广泛采用，以及哪些技术有望得到更广泛的采用。\n开发人员正在自动化更多的工作流程。 在过去的一年里，开发人员使用 GitHub Actions 分钟数增加了 169%，用于自动化公共项目中的任务、开发 CI/CD 管道等。\n平均而言，开发人员在公共项目中每天使用 GitHub Actions 的时间超过 2000 万分钟。随着 GitHub Marketplace 中的 GitHub Actions 数量在 2023 年突破 20,000 个大关，社区不断发展。 这凸显了开源社区对 CI/CD 和社区管理自动化的认识不断增强。 超过 80% 的 GitHub 贡献都贡献给私有存储库。 其中，私人项目贡献超过 42 亿美元，公共和开源项目贡献超过 3.1 亿美元。这些数字显示了通过免费、团队和 GitHub Enterprise 帐户在公共、开源和私人存储库中发生的活动的巨大规模。大量的私人活动表明了内部源代码的价值，以及基于 Git 的协作不仅有利于开源代码的质量，而且也有利于专有代码的质量。\n事实上，在最近 GitHub 赞助的一项调查中，所有开发人员都表示他们的公司至少采用了一些内部源实践，超过一半的开发人员表示他们的组织中有活跃的内部源文化。\nGitHub 是开发人员操作和扩展云原生应用程序的地方。 2023 年，430 万个公共和私有存储库使用 Dockerfile，超过 100 万个公共存储库使用 Dockerfile 来创建容器。过去几年，我们看到 Terraform 和其他云原生技术的使用量不断增加。 IaC 实践的增加也表明开发人员正在为云部署带来更多标准化。\n生成式人工智能进入 GitHub Actions。 人工智能在开发者社区中的早期采用和协作能力在 GitHub Marketplace 中的300 多个人工智能驱动的 GitHub Actions和40 多个 GPT 支持的 GitHub Actions中显而易见。开发人员不仅继续尝试人工智能，还通过 GitHub Marketplace 将其带入开发人员体验及其工作流程的更多部分。\n底线： 开发人员尝试新技术并在公共和私人存储库中分享他们的经验。这项相互依赖的工作凸显了容器化、自动化和 CI/CD 在开源社区和公司之间打包和发布代码的价值。\n开源的安全状况 # 今年，我们看到开发人员、OSS 社区和公司等通过自动警报、工具和主动安全措施更快地响应安全事件，这有助于开发人员更快地获得更好的安全结果。我们还看到 GitHub 上共享了负责任的 AI 工具和研究。\n更多的开发人员正在使用自动化来保护依赖关系。 与 2022 年相比，2023 年开源开发人员合并的针对易受攻击包的自动化Dependabot拉取请求数量增加了 60%，这凸显了共享社区对开源和安全性的奉献精神。得益于 GitHub 上的免费工具（例如 Dependabot、代码扫描和密钥扫描），开源社区的开发人员正在修复更多易受攻击的软件包并解决代码中的更多漏洞。\n**更多的开源维护者正在保护他们的分支机构。**受保护的分支为维护者提供了更多方法来确保其项目的安全性，我们已经看到超过 60% 的最受欢迎的开源项目 使用它们。自从我们今年早些时候在 GA 中在 GitHub 上推出存储库规则以来，大规模管理这些规则应该会变得更加容易。\n开发人员正在 GitHub 上分享负责任的 AI 工具。 在实验性生成人工智能时代，我们看到人工智能信任和安全工具的发展趋势。开发人员正在围绕负责任的人工智能、人工智能公平、负责任的机器学习和道德人工智能创建和共享工具。\n乔治城大学安全与新兴技术中心也在确定哪些国家和机构是值得信赖的人工智能研究的顶级生产者，并在 GitHub 上分享其研究代码。\n底线： 为了帮助 OSS 社区和项目保持更加安全，我们投资了 Dependabot、受保护的分支、CodeQL 和秘密扫描，免费向公共项目提供。2023 年的新采用指标显示这些投资如何成功帮助更多开源项目提高整体安全性。我们还看到软件开发人员和机构研究人员对创建和共享负责任的人工智能工具感兴趣。\n开源状态 # 2023 年，开发者为 GitHub 上的开源项目做出了总计 3.01 亿的贡献，其中包括 Mastodon 等热门项目到 Stable Diffusion 和 LangChain 等生成式 AI 项目。\n商业支持的项目继续吸引一些最开源的贡献，但 2023 年是生成式 AI 项目也进入 GitHub 上十大最受欢迎项目的第一年。说到生成式 AI，几乎三分之一拥有至少一颗星的开源项目都有一位使用 GitHub Copilot 的维护者。\n商业支持的项目继续领先。 2023 年，贡献者总数最大的项目获得了压倒性的商业支持。这是去年以来的持续趋势，microsoft/vscode、flutter/flutter 和 vercel/next.js 在 2023 年再次跻身前 10 名。\n生成式人工智能在开源和公共项目中快速发展。 2023 年，我们看到基于 AI 的生成式 OSS 项目，如 langchain-ai/langchain 和 AUTOMATIC1111/stable-diffusion-webui，在 GitHub 上按贡献者数量跃居榜首。越来越多的开发人员正在使用预先训练的人工智能模型构建法学硕士应用程序，并根据用户需求定制人工智能应用程序。\n开源维护者正在采用生成式人工智能。 几乎三分之一拥有至少一颗星的开源项目都有使用 GitHub Copilot 的维护者。这是我们向开源维护人员免费提供 GitHub Copilot 的计划，并表明生成式 AI 在开源领域的采用日益广泛。\n开发人员看到了组合包和容器化的好处。 正如我们之前指出的，2023 年有 430 万个存储库使用了 Docker。另一方面，Linux 发行版 NixOS/nixpkgs 在过去两年中一直位居贡献者开源项目的榜首。\n首次贡献者继续青睐商业支持的项目。 去年，我们发现，与其他项目相比，围绕流行的、商业支持的项目的品牌认知度吸引了更多的首次贡献者。这种情况在 2023 年继续出现，一些在 Microsoft、Google、Meta 和 Vercel 支持的首次贡献者中最受欢迎的开源项目。\n但社区驱动的开源项目（从 home-assistant/core 到 AUTOMATIC1111/stable-diffusion-webui、langchain-ai/langchain 和Significant-Gravitas/Auto-GPT）也见证了首次贡献者的活动激增。这表明，基础模型的开放实验增加了生成人工智能的可及性，为新的创新和更多合作打开了大门。\n2023 年，首次为开源项目做出贡献的贡献者数量最多。 新的开发人员通过 freeCodeCamp、First Contributions 和 GitHub Education 等计划参与到开源社区中。我们还看到大量开发人员参与了 Google 和 IBM 等公司的在线开源教育项目。\n**底线是：**开发人员正在为开源生成式人工智能项目做出贡献，开源维护者正在采用生成式人工智能编码工具，而公司则继续依赖开源软件。这些都表明，公开学习并分享新技术实验的开发人员提升了整个全球开发人员网络 - 无论他们是在公共存储库还是私人存储库中工作。\n总结 # 正如 Git 已成为当今开发人员体验的基础一样，我们现在也看到了人工智能成为主流的证据。仅在过去一年，就有高达 92% 的开发人员表示在工作内外使用基于人工智能的编码工具。在过去的一年里，GitHub 上托管的各种开源项目的人工智能实验也出现了爆炸性增长。\n我们给您留下三个要点：\nGitHub 是生成式 AI 的开发者平台。 生成式 AI 将于 2023 年从专业领域发展成为主流技术，开源活动的爆炸式增长反映了这一点。随着越来越多的开发人员构建和试验生成式 AI，他们正在使用 GitHub 进行协作和集体学习。 开发人员正在 GitHub 上大规模运行云原生应用程序。 2019 年，我们开始看到开源中使用基于容器的技术的开发人员数量大幅增加，并且越来越多的开发人员使用基于 Git 的 IaC 工作流程、容器编排和其他云原生技术的速度急剧增加2023 年。如此大量的活动表明开发人员正在使用 GitHub 来标准化他们将软件部署到云的方式。 GitHub 是开源社区、开发人员和公司构建软件的地方。 2023 年，我们看到私有存储库的数量增加了 38%，占 GitHub 上所有活动的 81% 以上。但我们看到开源社区持续增长，他们使用 GitHub 来构建未来并推动行业向前发展。数据显示新的开源开发人员的增加以及开放社区可能实现的快速创新步伐，很明显开源从未如此强大。 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-02-22","externalUrl":null,"permalink":"/posts/2024/open-source-state/","section":"Posts","summary":"本文介绍了 GitHub 发布的 2023 年开源状况和人工智能的崛起报告，分析了开发者社区的增长、生成式 AI 的应用以及云原生技术的发展趋势。","title":"2023 年开源状况和人工智能的崛起（GitHub）","type":"posts"},{"content":"As the creator and contributor of cpp-linter, I am pleased to announce that cpp-linter-action now supports Pull Request Review functionality starting from version v2.9.0 👏\nHere are the release notes for cpp-linter-action.\nAmong them, Bump cpp-linter from 1.6.5 to 1.7.1 by @dependabot in #191 is the most significant change. Note: the cpp-linter package is the core dependency of cpp-linter-action.\nWhat is cpp-linter-action # If you are unfamiliar with cpp-linter-action, you can check out my previous article.\nIn short, cpp-linter-action is a GitHub Action under the cpp-linter organization, designed for C/C++ code to format code, diagnose, and fix typical programming errors.\nCurrently, cpp-linter-action is used by over 500 open-source projects (closed-source projects cannot be counted), including well-known organizations or projects such as Microsoft, Linux Foundation, CachyOS, nextcloud, and Jupyter.\nIt can be said that cpp-linter-action is currently the best Linter choice for C/C++ projects on GitHub.\nAbout the Pull Request Review Feature # The newly added Pull Request Review feature allows you to provide review suggestions directly after cpp-linter-action completes the check, eliminating the need for developers to manually modify and push the detected errors to the remote repository.\nInstead, you can directly click the Commit suggestions button on GitHub to merge the suggested changes into the current pull request, saving the effort of manual modification and pushing.\nOnce all suggestions have been addressed, the GitHub Actions bot will automatically approve your pull request.\nOther Features Supported by cpp-linter-action # Besides the Pull Request Review feature, cpp-linter-action currently supports three other options: Annotations, Thread Comment, and Step Summary.\nGitHub Annotations # This displays the execution results at the specified code lines that need modification.\nThread Comment # This adds the execution results as comments on the Pull Request.\nStep Summary # This adds and displays the execution results in the GitHub Action job execution interface.\nThe Story Behind This Release # I finally found some time to sit down and write an article about the story behind this release on the evening of the eighth day of the Lunar New Year after my child fell asleep.\nThis release is particularly thanks to the contribution of cpp-linter co-creator @2bndy5. I have never met him in person, yet we have \u0026ldquo;worked\u0026rdquo; together for three years. We met because of his proactive contribution, but our communication has been limited to discussions on GitHub issues and pull requests, with only non-public information communicated via email.\nAs @2bndy5\u0026rsquo;s self-introduction states: passionate about programming, enjoys DIY electronics, and insists on writing easy-to-understand documentation. He is one of the few people I know who is technically comprehensive and has incredibly user-friendly documentation—a true geek.\nNot long ago, I received an email from him saying that due to a family emergency, he needed to take a break and had lost the motivation to code. He told me that all the changes for the Pull Request Review seemed to have passed the tests. If I wanted to lead the release, he could provide support.\nHere, I want to express my deepest sympathy and condolences 🙏 for what happened to him.\nTo continue his work, I needed to carefully read his changes and understand this functionality, but I didn\u0026rsquo;t have enough time to do so at the end of the year. I wanted to wait until the Lunar New Year holiday to catch up.\nHowever, before the Lunar New Year holiday, my child fell ill on the 27th day of the 12th lunar month and needed to be hospitalized. Because we discovered it early, the illness wasn\u0026rsquo;t serious, and the child recovered well on New Year\u0026rsquo;s Eve. We hoped that after a couple of days of observation, they could be discharged. Alas, an accident happened—the child accidentally burned their arm. As parents, we were heartbroken. This is the darkest moment I never want to recall. It was just one setback after another. So we stayed in the hospital from the 27th day of the 12th lunar month until the 6th day of the first lunar month—10 days in total. The day after the child was discharged, my wife and I both fell ill, probably because we relaxed and became exhausted.\nDuring this time, @2bndy5 completed the testing, documentation updates, and release of the Pull Request Review feature. He said in a comment that spending time coding allowed him to temporarily escape reality.\nFinally, the day before I returned to work, I had almost recovered my energy and eagerly opened GitHub to review and test the contribution code from another contributor. This contributor\u0026rsquo;s title came from an astrophysicist from the University of Dortmund in Germany, which was quite surprising.\nBut that\u0026rsquo;s the fun part of open source; it allows you to have the opportunity to freely spar with any expert at close range. If you submit code to the Linux kernel, you might even get guidance from Linus himself :)\nFinally, welcome to use any project under the cpp-linter organization and provide your valuable opinions, suggestions, or contribute code.\n———— February 17, 2024, 23:34\nPlease indicate the author and source when reprinting this article and do not use it for any commercial purposes. Follow the \u0026ldquo;DevOps攻城狮\u0026rdquo; WeChat official account.\n","date":"2024-02-17","externalUrl":null,"permalink":"/en/posts/2024/cpp-linter-action/","section":"Posts","summary":"This article introduces the new feature of cpp-linter-action Pull Request Review, allowing developers to directly submit code modification suggestions on GitHub, improving code quality and collaboration efficiency.","title":"cpp-linter-action—Latest Version Now Supports Pull Request Review Functionality 👏","type":"posts"},{"content":"","date":"2024-01-21","externalUrl":null,"permalink":"/tags/apache/","section":"标签","summary":"","title":"Apache","type":"tags"},{"content":"本篇介绍的是大名鼎鼎的开源软件基金会 Apache 所使用的服务(Services)和工具(Tools)，这或许能帮助你打开视野，在选择工具的时候提供参考。如果你是一名 DevOps、SRE 或是 Infra 工程师，通过本篇文章内容结果帮助你更好的展示团队所提供的服务有哪些，以及窥探到 Apache Infra 是怎样组织和管理他们的。\nApache 是谁 # 如果你不太了解 Apache，下面是关于 Apache 的简要介绍。\nApache 是一个开源软件基金会（Apache Software Foundation，简称 ASF）的缩写。ASF 是一个非营利性的组织，致力于支持和发展开源软件项目。Apache 软件基金会通过提供法律、财务和基础设施支持，帮助开发者共同合作创建和维护开源软件。其中，Apache 软件基金会最为著名的项目之一是 Apache HTTP 服务器，也称为 Apache Web 服务器。此外，ASF 还托管了许多其他流行的开源项目，像 ECharts，Superset，Dubbo，Spark，Kafka 等等。\n服务与工具 # Apache Infra 团队维护着供 PMC（项目管理委员会）、项目提交者和 Apache 董事会使用的各种工具。这些工具中的部分工具只提供给有特定职责或角色的人员使用。其他工具，如显示 Apache 基础设施各部分状态的监控工具，则向所有人开放。\n为顶级项目（TLP）提供的服务 网站 电子邮件 ASF 自助服务平台 ASF 账户管理 支持 LDAP 的服务 项目孵化服务 ASF 项目工具 版本控制 问题跟踪和功能请求 将版本库与 Jira 票据集成 源码库发布者/订阅者服务 构建服务 产品命名 代码签名 代码质量 代码分发 虚拟服务器 在线投票 其他工具 DNS URL 短缩器 共享片段 机器列表 奇思妙想 为顶级项目（TLP）提供的服务 # 网站 # www.apache.org 这是 Apache 的主要网站。 Apache 项目网站检查器 会定期检查所有为顶级项目（TLP）提供的网站，并报告它们是否符合 Apache 的 TLP 网站政策。 这里只列出了几个挺有意思的连接，比如项目网址检查器，它会检查顶级项目是否有 License, Donate, Sponsors, Privacy 等正确的连接。\n电子邮件 # 所有新建电子邮件列表的申请都应通过自助服务系统进行。 电子邮件服务器 - QMail/QSMTPD ASF自助服务平台 # Infra 的目标之一是让 ASF 成员、PMC 和提交者有能力完成他们需要做的大部分工作，而无需向 Infra 求助。例如，自助服务平台提供了许多方便的工具，拥有 Apache 电子邮件地址的人（基本上是项目提交者、PMC 成员和 ASF 成员）可以使用这些工具：\n创建 Jira 或 Confluence 项目、Git 仓库或电子邮件列表（PMC 主席和 Infra 成员）。 编辑你的 ASF 身份或更新你的 ASF 密码。如果要更新密码，则需要访问与 Apache 帐户相关联的电子邮件帐户。重置密钥的有效期只有 15 分钟，因此请务必在收到密钥后立即使用。 同步 Git 仓库。 使用 OTP 计算器为 OTP 或 S/Key 一次性密码系统生成一次性密码（一般用于 PMC 成员）。 将 Confluence Wiki 空间存档并设置为只读。 不属于 ASF 社区但希望提交有关 ASF 项目产品的 Jira 票据的人员可使用该平台申请 Jira 账户。\nASF账户管理 # 如果你想更新账户详情或丢失了账户访问权限，ASF 账户管理可为你提供指导。\n支持LDAP的服务 # Infra 支持许多 LDAP 的 ASF 服务。你可以使用 LDAP 凭据登录这些服务。\n孵化项目服务 # Infra 支持孵化项目。\nInfra 孵化器介绍，展示了建立孵化项目的步骤。 项目或产品名称选择指南 ASF项目工具 # Infra 支持一系列工具和服务，以帮助项目开发和支持其应用程序及其社区，包括\n每个项目都可以在 Confluence 维基上使用专用空间。 如何管理项目维基空间的用户权限。 如何授予用户编辑维基空间的权限。 Reporter 提供有关项目的活动统计和其他信息，并提供编辑工具，帮助你撰写和提交项目的季度董事会报告。 你可以创建并运行项目博客。 你可以建立一个 Slack 频道，用于团队实时讨论。一旦你建立了 Slack 频道，Infra 就可以建立 Slack-Jira 桥接，这样你就可以在频道中收到新的或更新的 Jira 票据通知。 团队可以使用 ASFBot 通过 Internet Relay Chat (IRC) 进行并记录会议。不过，你必须按照 Apache 投票流程，在相应的项目电子邮件列表中对决策进行正式投票。 本地化工具。 Apache 发布审核工具 (RAT) 可帮助你确认所提议的产品发布符合 ASF 的所有要求。 ASF OAuth 系统为希望使用身份验证的服务提供了一个协调中心，而不会对存储敏感用户数据造成安全影响。许多 Apache 服务使用它来验证请求访问的用户是否是项目中的提交者，以及是否拥有对相关系统的合法访问权限。了解更多有关 Apache OAuth 的信息。 版本控制 # Apache 提供并由 Infra 维护代码库，Apache 项目可使用这些代码库来保证项目代码的安全、团队成员的可访问性以及版本控制。\n关于使用 【Git 的信息](https://infra.apache.org/git-primer.html)\nSVN 代码库的只读 Git 镜像 可写的 Git 代码库 Apache 与 GitHub GitHub 仓库的访问角色 关于使用 Subversion 的信息\nSubversion (SVN) 版本库 ViewVC（SVN 主版本库的浏览器界面） 问题跟踪和功能请求 # ASF 支持以下用于跟踪问题和功能请求的选项： * Jira * GitHub 问题跟踪功能\n由于历史原因，一些项目使用 Bugzilla。我们将继续支持 Bugzilla，但不会为尚未使用它的项目设置。\nApache Allura 是另一个问题跟踪选项。如果你觉得它可以满足你的项目需求，请通过 users@allura.apache.org 邮件列表直接咨询 Allura 项目。\n请参阅 issues.apache.org，查看各项目使用的问题列表。\n以下是为你的项目申请 bug 和问题跟踪器的方法。\n以下是撰写优秀错误报告的指南。\n将你的版本库与Jira票据集成 # Infra 可以为你的项目激活 Subversion 和 Git 与 Jira 票据的集成。\n源码库发布者/订阅者服务 # SvnPubSub PyPubSub 构建服务 # Apache 支持并模拟持续集成和持续部署（或 CI/CD）。ASF 构建和支持的服务页面提供了有关 ASF 提供和/或支持的 CI 服务的信息和链接。\n其他可考虑的工具:\nTravis CI Appveyor 产品命名 # 请参阅产品名称选择指南\n代码签名 # 数字证书源码库发布者/订阅者服务\n请求访问 Digicert 代码签名服务 使用 Digicert 通过苹果应用程序商店发布\n关于代码签名和发布的更多信息\n代码质量 # SonarCloud 是一款代码质量和安全工具，开源项目可免费使用。它允许对代码质量进行持续检查，因此你的项目可以通过对代码进行静态分析来执行自动审查，以检测 20 多种编程语言中的错误、代码气味和安全漏洞。\n你可以检查许多 Apache 项目软件源的状态。\n有关在 ASF 项目中使用 SonarCloud 的指导，请点击此处。\n代码分发 # 使用 ASF Nexus Repository Manager 浏览和审查 ASF 项目的代码发布。\n发布 # 当前发布 历史发布存档 Rsync 分发镜像 Nexus 虚拟服务器 # Infra 可为项目提供 Ubuntu 虚拟机。\n虚拟机策略 申请虚拟机的流程 使用nightlies.a.o # nightlies.a.o 如其名称所示，是一种 \u0026ldquo;短期 \u0026ldquo;存储解决方案。请参阅 nightlies 使用政策。\n在线投票 # 项目可使用 Apache STeVe 投票系统实例（不使用时离线）。工具名称指的是作为投票选项之一的单一可转移投票系统。为 Infra 开立 Jira 票单，以便为你的项目使用 STeVe 做好准备。\n其他工具 # DNS # Infra 管理在 Namecheap 注册的 ASF DNS。\nURL短缩器 # URL 短缩器\n分享代码片段 # Paste 是一项服务，ASF 成员可以发布代码片段或类似的文件摘要，以说明代码问题或供重复使用，通常是与其他项目成员共享。你可以以纯文本形式发布内容，也可以以多种编码和格式发布内容。\n机器列表 # 主机密钥和指纹\n奇思妙想 # Apache Whimsy 自称为 \u0026ldquo;以易于使用的方式提供有关 ASF 和我们项目的组织信息，并帮助 ASF 实现企业流程自动化，使我们众多的志愿者能够更轻松地处理幕后工作\u0026rdquo;。\nWhimsy 有许多对项目管理委员会和个人提交者有用的工具，例如提交者搜索。\n总结 # 以上就是 Apache 开源软件基金会用到的一些服务和工具，总体的感觉就是写的很全面，并且每个连接都对应着完整的文档，这也是这种开源协作方式最重要的地方：通读文档。另外这种组织方式对于想参与的人来说很清晰，值得学习。\n另外我们看到了一些常见的服务和工具，像是 Jira，Confluence，Slack，Git，GitHub，SonarCloud，Digicert，Nexus。 也看到了不太常见的工具，像在 CI 工具上的选择是 Travis CI 和 Appveyor。 还有一些有意思的工具，像是 URL缩短器，代码片段分享，奇思妙想等工具，从访问的网址来看它们是部署在内部。 由于历史原因，还有项目还在使用 Bugzilla 和 SVN 等工具。 以上 Apache 所使用的服务和工具，借用理财中风险评估等级划分是属于稳健型，而非一味的追求“新”、“开源”和“免费”。\n为了文章的可读性，本文做了部分修改和删减。原文在这里。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-01-21","externalUrl":null,"permalink":"/posts/2024/apache-services-and-tools/","section":"Posts","summary":"本篇介绍的是大名鼎鼎的开源软件基金会 Apache 所使用的服务(Services)和工具(Tools)，这或许能帮助你打开视野，在选择工具的时候提供参考。","title":"看看顶级的开源组织都在用哪些服务和工具","type":"posts"},{"content":"Time flies, and 2023 has passed in a flash. If I don\u0026rsquo;t record what happened this year, it will be difficult to recall what happened soon.\nTherefore, as usual, I will first review 2023 and then look forward to 2024.\nReviewing 2023 # Looking back on this year, I would use three keywords to describe my life, work, and leisure time.\nLife: \u0026ldquo;Dad\u0026rdquo; # Since the birth of my child, my teammate and I have been independently taking care of the baby. During the day I work, and she takes care of the child; in the evening, I rush home after work, play with the child, do housework until the child goes to sleep. On weekends, we\u0026rsquo;ll take the child out for a stroll at least one day.\nTaking care of the child can be tiring and long, but looking back on this year, I\u0026rsquo;ve found that the child has grown up in the blink of an eye. At the beginning of the year, the child could only crawl and stand with support; by the end of the year, the child can climb tables and run, with endless energy.\nNext is a simple record of the small events in this ordinary year:\nApril: Went to Discovery Kingdom, climbed Tongniuling, played San Guo Sha (a card game), and had a seafood buffet at Yu Gong Wharf with the company. May: Held birthday parties for the child in Xinghai and Zhuanghe. September: Took the child to a hotel for the first time, stayed at the Hilton Jin Shi Tan for one night, and had a hotel buffet breakfast. October: Returned to Zhuanghe for one night during the National Day holiday, had a family barbecue; went to Hengshan Temple, had a vegetarian buffet; played a soccer game during the holiday; bought a membership card for Dalian Forest Zoo and started visiting the zoo. November: Visited Dalian Forest Zoo, Lushun Sun Gully, Dalian Natural Museum, and attended a company dinner in Qianku. December: Received very sudden news (I will elaborate on this later), participated in team building, visited Dalian Museum, accompanied my father to the hospital for a checkup, and visited Lushun Museum. Work: Upholding \u0026ldquo;Best Practices\u0026rdquo; # This year, I continued to uphold DevOps best practices:\nContinued expanding the application scope of the Ansible Playbook repository. Created an Infrastructure as Code repository to manage infrastructure with code. Created a Docker Image repository to containerize products and applied many of my DevOps practices in the Bitbucket repository. Undertook more product build and release work. Proposed and implemented Software Supply Chain Security ideas. Shared DevOps practices within the team, sharing them with other teams in the form of a Jenkins shared library. Leisure Time: Enthusiast of \u0026ldquo;Open Source and Writing\u0026rdquo; # As with last year, I continued doing the same things in my spare time:\nOpen Source Projects: cpp-linter-action: Currently has over 400 users, including well-known open-source projects such as Microsoft, Jupyter, Waybar, libvips, desktop, and chocolate-doom. I hope to have more ideas to improve it in the future. commit-check: Currently in its early stages of development, with few users. It still needs optimization and the introduction of more good ideas. Writing: In 2023, I updated 20 blog posts + 6 WeChat official account articles, falling slightly short of my initial goal of 24 (blog) + 12 (WeChat). Learning: I did not achieve my goal of learning and contributing to well-known communities. Having a child has really limited my free time. I hope that in 2024, the child will go to bed earlier so I can have more time for myself. Looking Forward to 2024 # I still need to set some wishes or flags for each year, just in case they come true!\nHope to smoothly pass through the career development period, hoping it will be a fresh and challenging start. Family health, work-life balance, and taking them out for more trips. Make up for some shortcomings in DevOps, and strive to participate in practical projects to improve. Teach and learn, continue sharing on blogs and WeChat official accounts, and maintain personal growth. Maintain exercise, whether running or playing soccer, and lose weight. Past Year-End Summaries # 2022 Year-End Summary 2020 Year-End Summary 2019 Year-End Summary 2018 From QA to Dev\nPlease indicate the author and source when reprinting this article. Please do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2023-12-31","externalUrl":null,"permalink":"/en/misc/2023-summary/","section":"Miscs","summary":"Time flies, and 2023 has passed in a flash. If I don’t record what happened this year, it will be difficult to recall what happened soon.\nTherefore, as usual, I will first review 2023 and then look forward to 2024.\n","title":"2023 Year-End Summary","type":"misc"},{"content":"If you\u0026rsquo;ve used GitHub to release projects, you\u0026rsquo;ll know that GitHub can automatically generate Release Notes.\nHere\u0026rsquo;s an example of GitHub\u0026rsquo;s automatically generated Release Notes.\nThe Release Notes in this screenshot are concise and easy to read. However, if the content is extensive, as in the case of the configuration-as-code-plugin project under the Jenkinsci organization, you\u0026rsquo;ll see that the Release Notes are categorized by title. If this content were mixed together, the experience would be significantly worse. (Don\u0026rsquo;t mistake this for manual categorization; programmers wouldn\u0026rsquo;t want to do that 😅)\nThis article will share two methods for automatically categorizing the content of GitHub Release Notes based on their titles.\nMethod One: Using GitHub\u0026rsquo;s Built-in Functionality # Method one uses GitHub\u0026rsquo;s built-in functionality to automatically categorize Release Notes. This involves creating a configuration file, .github/release.yml, in your repository. This functionality is similar to GitHub\u0026rsquo;s Issue Template and Pull Request Template. Specific configuration options can be found in the official documentation.\nBelow is the configuration from the commit-check-action project configuration:\nchangelog: exclude: labels: - ignore-for-release categories: - title: \u0026#39;🔥 Breaking Changes\u0026#39; labels: - \u0026#39;breaking\u0026#39; - title: 🏕 Features labels: - \u0026#39;enhancement\u0026#39; - title: \u0026#39;🐛 Bug Fixes\u0026#39; labels: - \u0026#39;bug\u0026#39; - title: \u0026#39;👋 Deprecated\u0026#39; labels: - \u0026#39;deprecation\u0026#39; - title: 📦 Dependencies labels: - dependencies - title: Other Changes labels: - \u0026#34;*\u0026#34; With the .github/release.yml configuration file added, the next time Release Notes are generated, the content will be automatically categorized (the 📦 Dependencies title in the image below is automatically added).\nMethod Two: Using Release Drafter # Method two uses Release Drafter, requiring a configuration file named .github/release-drafter.yml in your repository.\nThe configuration options provided by the Release Drafter project show that it offers more features and is more complex to use. It also supports placing the configuration file in a central repository\u0026rsquo;s .github directory to enable unified configuration and sharing across multiple repositories.\nCurrently, method one (.github/release.yml) does not support unified configuration via a central .github repository; see this discussion.\nHere\u0026rsquo;s an example of the .github/release-drafter.yml configuration from jenkinsci/configuration-as-code-plugin.\n_extends: .github The _extends: .github configuration inherits settings from the central repository\u0026rsquo;s .github/.github/release-drafter.yml configuration.\n# Configuration for Release Drafter: https://github.com/toolmantim/release-drafter name-template: $NEXT_MINOR_VERSION tag-template: $NEXT_MINOR_VERSION # Uses a more common 2-digit versioning in Jenkins plugins. Can be replaced by semver: $MAJOR.$MINOR.$PATCH version-template: $MAJOR.$MINOR # Emoji reference: https://gitmoji.carloscuesta.me/ # If adding categories, please also update: https://github.com/jenkins-infra/jenkins-maven-cd-action/blob/master/action.yaml#L16 categories: - title: 💥 Breaking changes labels: - breaking - title: 🚨 Removed labels: - removed - title: 🎉 Major features and improvements labels: - major-enhancement - major-rfe - title: 🐛 Major bug fixes labels: - major-bug - title: ⚠️ Deprecated labels: - deprecated - title: 🚀 New features and improvements labels: - enhancement - feature - rfe - title: 🐛 Bug fixes labels: - bug - fix - bugfix - regression - regression-fix - title: 🌐 Localization and translation labels: - localization - title: 👷 Changes for plugin developers labels: - developer - title: 📝 Documentation updates labels: - documentation - title: 👻 Maintenance labels: - chore - internal - maintenance - title: 🚦 Tests labels: - test - tests - title: ✍ Other changes # Default label used by Dependabot - title: 📦 Dependency updates labels: - dependencies collapse-after: 15 exclude-labels: - reverted - no-changelog - skip-changelog - invalid template: | \u0026lt;!-- Optional: add a release summary here --\u0026gt; $CHANGES replacers: - search: \u0026#39;/\\[*JENKINS-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[JENKINS-$1](https://issues.jenkins.io/browse/JENKINS-$1) - \u0026#39; - search: \u0026#39;/\\[*HELPDESK-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[HELPDESK-$1](https://github.com/jenkins-infra/helpdesk/issues/$1) - \u0026#39; # TODO(oleg_nenashev): Find a better way to reference issues - search: \u0026#39;/\\[*SECURITY-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[SECURITY-$1](https://jenkins.io/security/advisories/) - \u0026#39; - search: \u0026#39;/\\[*JEP-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[JEP-$1](https://github.com/jenkinsci/jep/tree/master/jep/$1) - \u0026#39; - search: \u0026#39;/CVE-(\\d{4})-(\\d+)/g\u0026#39; replace: \u0026#39;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-$1-$2\u0026#39; - search: \u0026#39;JFR\u0026#39; replace: \u0026#39;Jenkinsfile Runner\u0026#39; - search: \u0026#39;CWP\u0026#39; replace: \u0026#39;Custom WAR Packager\u0026#39; - search: \u0026#39;@dependabot-preview\u0026#39; replace: \u0026#39;@dependabot\u0026#39; autolabeler: - label: \u0026#39;documentation\u0026#39; files: - \u0026#39;*.md\u0026#39; branch: - \u0026#39;/docs{0,1}\\/.+/\u0026#39; - label: \u0026#39;bug\u0026#39; branch: - \u0026#39;/fix\\/.+/\u0026#39; title: - \u0026#39;/fix/i\u0026#39; - label: \u0026#39;enhancement\u0026#39; branch: - \u0026#39;/feature\\/.+/\u0026#39; body: - \u0026#39;/JENKINS-[0-9]{1,4}/\u0026#39; This is the central repository\u0026rsquo;s .github/.github/release-drafter.yml configuration. The Jenkins official configuration uses many features, such as templates, replacements, and automatic label addition. A thorough reading of the Release Drafter documentation is recommended for a better understanding and utilization.\nSummary # Both methods can automatically categorize titles when generating Release Notes, but they have some key differences. Understanding these differences will help you make a better choice.\nGitHub\u0026rsquo;s built-in method is easier to understand and configure and meets the needs of most projects. Its main drawback is the lack of support for reading .github/release.yml from a central .github repository. Release Drafter provides more powerful features, such as templates, sorting, replacements, and automatic label addition to pull requests. Importantly, it allows the creation of a template in a central repository for inheritance by other projects. For large open-source organizations, Release Drafter is a better choice due to its powerful features and support for inheriting central repository configurations. For personal projects, GitHub\u0026rsquo;s built-in method is generally sufficient.\nThis concludes my discussion of the two methods for automatically generating and categorizing GitHub Release Notes.\nPlease feel free to leave any questions or suggestions in the comments section.\nPlease cite the author and source when reprinting this article. Do not use it for any commercial purposes. Follow the \u0026ldquo;DevOps攻城狮\u0026rdquo; WeChat official account.\n","date":"2023-12-27","externalUrl":null,"permalink":"/en/posts/2023/automatic-categorize-release-notes/","section":"Posts","summary":"This article shares two methods for automatically categorizing the content of GitHub Release Notes based on titles.","title":"How to Automatically Categorize GitHub Release Notes by New features, Bug Fixes…","type":"posts"},{"content":"Sometimes you don\u0026rsquo;t want Jenkins pipeline failed for a specific error occurs. so you can use catchError to catch error and update stage or build result to SUCCESSFUL or UNSTABLE or FAILURE (if you want)\nHere is the Jenkinsfile of pipeline\npipeline { agent { node { label \u0026#39;linux\u0026#39; } } stages { stage(\u0026#39;Catch Error\u0026#39;) { steps { catchError(buildResult: \u0026#39;UNSTABLE\u0026#39;, stageResult: \u0026#39;UNSTABLE\u0026#39;, message: \u0026#39;abc: command not found\u0026#39;) { sh \u0026#34;abc\u0026#34; } } } } } Here is the output of pipeline\n17:14:07 [Pipeline] Start of Pipeline 17:14:08 [Pipeline] node 17:14:08 Running on linux in /agent/workspace/Stage-Job/catch-error 17:14:08 [Pipeline] { 17:14:08 [Pipeline] stage 17:14:08 [Pipeline] { (Catch Error) 17:14:08 [Pipeline] catchError 17:14:08 [Pipeline] { 17:14:08 [Pipeline] sh 17:14:08 + abc 17:14:08 /agent/workspace/Stage-Job/catch-error@tmp/durable-303b03ca/script.sh: line 1: abc: command not found 17:14:08 [Pipeline] } 17:14:08 ERROR: abc: command not found 17:14:08 ERROR: script returned exit code 127 17:14:08 [Pipeline] // catchError 17:14:08 [Pipeline] } 17:14:08 [Pipeline] // stage 17:14:08 [Pipeline] } 17:14:08 [Pipeline] // node 17:14:08 [Pipeline] End of Pipeline 17:14:08 Finished: UNSTABLE 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-12-15","externalUrl":null,"permalink":"/en/posts/2023/jenkins-catch-error/","section":"Posts","summary":"Introducing the catchError step in Jenkins pipeline to handle specific errors without failing the entire build, allowing for more flexible error management.","title":"How to make Jenkins pipeline not fail if a specific error occurs","type":"posts"},{"content":" Why Software Supply Chain Security is important? # Software supply chain security is the act of securing the components, activities, and practices involved in creating software.\nAttacks in the software supply chain have become more and more frequent in recent years, SonaType reported more than 700% of attacks in open-source software from 2019 to 2022.\nIn this Google Security Blog, there are many real examples of software supply chain attacks that pose growing threats to users and Google proposed a solution called SLSA in 2021.\nAlso, some well-known organizations such as Linux Foundation and CNCF have created standards and tools to address the issue of how to produce trusted software and attestations.\nBased on this background, many organizations want to incorporate best practices from the open-source community into our CICD pipeline.\nHow to adopt Supply Chain Security for GitHub and Non-GitHub projects # Next, I will show you how to adopt on both GitHub and Rocket Bitbucket as an example to show you how we integrate software supply chain security\nGitHub projects # On GitHub, the easiest and most popular option is to use slsa-github-generator, a tool provided by the official slsa-framework for native GitHub projects to create attestations during the build process and upload signed attestations to Rekor a transparency log system created by Sigstore. Here is the demo reposistory for reference.\nBefore installing your product package, the user can download the package and verify the provenance file at the end of .intoto.jsonl first, then run the following command manually or in their CI pipeline to verify whether the artifact is tampered with or not\nbash-4.4$ slsa-verifier verify-artifact test-1.0.0-py3-none-any.whl --provenance-path test-1.0.0-py3-none-any.whl.intoto.jsonl --source-uri github.com/shenxianpeng/slsa-provenance-demo Verified signature against tlog entry index 49728014 at URL: https://rekor.sigstore.dev/api/v1/log/entries/24296fb24b8ad77af7063689e8760fd7134f37e17251ec1d5adc16af64cb5cb579493278f7686e77 Verified build using builder \u0026#34;https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.9.0\u0026#34; at commit fb7f6df9f8565ed6fa01591df2af0c41e5573798 Verifying artifact test-1.0.0-py3-none-any.whl: PASSED PASSED: Verified SLSA provenance Non-GitHub projects # However, there are many organizations\u0026rsquo; codes are hosted on Non-GitHub SCM, so you can use the Witness, a tool from CNCF in-toto, which can help us generate and verify attestations.\nIt’s easy to scale Witness to your products, just integrate witness command into the existing build command it will generate proof of the software build and release execution process and can be verified.\nYou can follow this demo to integrate with witness, then will generate the build package along with attestations file, policy-signed.json file, and a public key.\nBefore user installing your product package, they can run the following command manually or in their CI pipeline to verify whether the artifact is tampered or not.\nwitness verify -f dist/witness_demo-1.0.0-py3-none-any.whl -a witness-demo-att.json -p policy-signed.json -k witness-demo-pub.pem INFO Using config file: .witness.yaml INFO Verification succeeded INFO Evidence: INFO 0: witness-demo-att.json ","date":"2023-12-02","externalUrl":null,"permalink":"/en/posts/2023/supply-chain-security/","section":"Posts","summary":"This article introduces how to implement software supply chain security using SLSA and Witness for both GitHub and non-GitHub projects, enhancing the security of software development and deployment processes.","title":"How to adopt Supply Chain Security for GitHub and Non-GitHub projects","type":"posts"},{"content":"","date":"2023-12-02","externalUrl":null,"permalink":"/en/tags/witness/","section":"Tags","summary":"","title":"Witness","type":"tags"},{"content":"Due to the increasing frequency of attacks targeting software supply chains in recent years, Google proposed a solution in 2021: Supply chain Levels for Software Artifacts (\u0026ldquo;SLSA\u0026rdquo;).\nThis article will describe how to generate and verify the provenance of software artifacts outside the GitHub ecosystem to improve your project\u0026rsquo;s SLSA Level.\nWitness is a pluggable software supply chain risk management framework that automates, standardizes, and verifies software artifact provenance. It is a CNCF project under CNCF, originally authored by Testifysec and later donated to in-toto.\nWhat is Witness # Witness is a pluggable supply chain security framework that creates a provenance trace throughout the software development lifecycle (SDLC), ensuring the integrity of software from source code to target. It supports most major CI and infrastructure providers, offering a versatile and flexible solution for securing software supply chains.\nThe use of a secure Public Key Infrastructure (PKI) system and the ability to verify Witness metadata further enhance the security of the process and help mitigate many software supply chain attack vectors.\nWitness works by encapsulating commands executed within a continuous integration process, providing a provenance trace for each operation in the software development lifecycle (SDLC). This allows for a detailed and verifiable record of how the software was built, by whom, and what tools were used.\nThis evidence can be used to assess policy compliance, detect any potential tampering or malicious activity, and ensure that only authorized users or machines can complete a given step in the process.\nSummary - What Witness can do\nVerify who built the software, how it was built, and what tools were used Detect any potential tampering or malicious activity Ensure that only authorized users or machines can complete each step in the process Distribute Attestations and Policies How to use Witness # It mainly consists of three steps:\nwitness run - Run the provided command and record an attestation about the execution. witness sign - Sign the provided file using the provided key. witness verify - Verify the witness policy. Quick Start # This is a Witness Demo repository I created to demonstrate the Witness workflow. You can follow these steps.\nPrepare the environment # Install Witness and download the demo project\nbash \u0026lt;(curl -s https://raw.githubusercontent.com/in-toto/witness/main/install-witness.sh) Latest version of Witness is 0.1.14 Downloading for linux amd64 from https://github.com/in-toto/witness/releases/download/v0.1.14/witness_0.1.14_linux_amd64.tar.gz expected checksum: f9b67ca04cb391cd854aec3397eb904392ff689dcd3c38305d38c444781a5a67 file checksum: f9b67ca04cb391cd854aec3397eb904392ff689dcd3c38305d38c444781a5a67 witness v0.1.14-aa35c1f Witness v0.1.14 has been installed at /usr/local/bin/witness git clone https://github.com/shenxianpeng/witness-demo.git Create key pair # openssl genpkey -algorithm ed25519 -outform PEM -out witness-demo-key.pem openssl pkey -in witness-demo-key.pem -pubout \u0026gt; witness-demo-pub.pem Prepare the Witness configuration file .witness.yaml # run: signer-file-key-path: witness-demo-key.pem trace: false verify: attestations: - \u0026#34;witness-demo-att.json\u0026#34; policy: policy-signed.json publickey: witness-demo-pub.pem Integrate the build step into the Attestation # witness run --step build -o witness-demo-att.json -- python3 -m pip wheel --no-deps -w dist . INFO Using config file: .witness.yaml INFO Starting environment attestor... INFO Starting git attestor... INFO Starting material attestor... INFO Starting command-run attestor... Processing /tmp/witness-demo Building wheels for collected packages: witness-demo Running setup.py bdist_wheel for witness-demo: started Running setup.py bdist_wheel for witness-demo: finished with status \u0026#39;done\u0026#39; Stored in directory: /tmp/witness-demo/dist Successfully built witness-demo INFO Starting product attestor... That is, add the witness run command to the previous build command.\nView the verification data of the signed Attestation # Because the Attestation data is base64 encoded, it needs to be decoded before viewing.\ncat witness-demo-att.json | jq -r .payload | base64 -d | jq # Partial output below { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/command-run/v0.1\u0026#34;, \u0026#34;attestation\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;pip\u0026#34;, \u0026#34;wheel\u0026#34;, \u0026#34;--no-deps\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;dist\u0026#34;, \u0026#34;.\u0026#34; ], \u0026#34;stdout\u0026#34;: \u0026#34;Processing /tmp/witness-demo\\nBuilding wheels for collected packages: witness-demo\\n Running setup.py bdist_wheel for witness-demo: started\\n Running setup.py bdist_wheel for witness-demo: finished with status \u0026#39;done\u0026#39;\\n Stored in directory: /tmp/witness-demo/dist\\nSuccessfully built witness-demo\\n\u0026#34;, \u0026#34;exitcode\u0026#34;: 0 }, \u0026#34;starttime\u0026#34;: \u0026#34;2023-11-29T05:15:19.227943473-05:00\u0026#34;, \u0026#34;endtime\u0026#34;: \u0026#34;2023-11-29T05:15:20.078517025-05:00\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/product/v0.1\u0026#34;, \u0026#34;attestation\u0026#34;: { \u0026#34;dist/witness_demo-1.0.0-py3-none-any.whl\u0026#34;: { \u0026#34;mime_type\u0026#34;: \u0026#34;application/zip\u0026#34;, \u0026#34;digest\u0026#34;: { \u0026#34;gitoid:sha1\u0026#34;: \u0026#34;gitoid:blob:sha1:b4b7210729998829c82208685837058f5ad614ab\u0026#34;, \u0026#34;gitoid:sha256\u0026#34;: \u0026#34;gitoid:blob:sha256:473a0f4c3be8a93681a267e3b1e9a7dcda1185436fe141f7749120a303721813\u0026#34;, \u0026#34;sha256\u0026#34;: \u0026#34;471985cd3b0d3e0101a1cbba8840819bfdc8d8f8cc19bd08add1e04be25b51ec\u0026#34; } } }, \u0026#34;starttime\u0026#34;: \u0026#34;2023-11-29T05:15:20.078579187-05:00\u0026#34;, \u0026#34;endtime\u0026#34;: \u0026#34;2023-11-29T05:15:20.081170078-05:00\u0026#34; } Create the Policy file # policy.json is used to define or require that a step has specific attributes or meets certain values, thereby requiring verification of the Attestation for successful verification. For example, if the expires field expires (is less than the current time), the execution of witness verify will fail.\n{ \u0026#34;expires\u0026#34;: \u0026#34;2033-12-17T23:57:40-05:00\u0026#34;, \u0026#34;steps\u0026#34;: { \u0026#34;build\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;attestations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/material/v0.1\u0026#34;, \u0026#34;regopolicies\u0026#34;: [] }, { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/command-run/v0.1\u0026#34;, \u0026#34;regopolicies\u0026#34;: [] }, { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/product/v0.1\u0026#34;, \u0026#34;regopolicies\u0026#34;: [] } ], \u0026#34;functionaries\u0026#34;: [ { \u0026#34;publickeyid\u0026#34;: \u0026#34;{{PUBLIC_KEY_ID}}\u0026#34; } ] } }, \u0026#34;publickeys\u0026#34;: { \u0026#34;{{PUBLIC_KEY_ID}}\u0026#34;: { \u0026#34;keyid\u0026#34;: \u0026#34;{{PUBLIC_KEY_ID}}\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;{{B64_PUBLIC_KEY}}\u0026#34; } } } More information about policy attributes and settings can be found here: https://github.com/in-toto/witness/blob/main/docs/policy.md\nSign the Policy file # Before signing, you need to replace the variables in the Policy file.\nid=`sha256sum witness-demo-pub.pem | awk \u0026#39;{print $1}\u0026#39;` \u0026amp;\u0026amp; sed -i \u0026#34;s/{{PUBLIC_KEY_ID}}/$id/g\u0026#34; policy.json pubb64=`cat witness-demo-pub.pem | base64 -w 0` \u0026amp;\u0026amp; sed -i \u0026#34;s/{{B64_PUBLIC_KEY}}/$pubb64/g\u0026#34; policy.json Then use witness sign to sign.\nwitness sign -f policy.json --signer-file-key-path witness-demo-key.pem --outfile policy-signed.json INFO Using config file: .witness.yaml Verify that the binary file meets policy requirements # witness verify -f dist/witness_demo-1.0.0-py3-none-any.whl -a witness-demo-att.json -p policy-signed.json -k witness-demo-pub.pem INFO Using config file: .witness.yaml INFO Verification succeeded INFO Evidence: INFO 0: witness-demo-att.json Finally # This is a demonstration of using Witness for Non-GitHub projects.\nIf your project code is on GitHub, the easiest and most popular way is to use slsa-github-generator, a tool provided by the SLSA Framework, and then use slsa-verifier to verify the Provenance. Refer to my previous article Python and SLSA 💃 for details.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2023-11-30","externalUrl":null,"permalink":"/en/posts/2023/witness-and-slsa/","section":"Posts","summary":"This article introduces the concept and working mechanism of Witness, and how to use Witness to generate and verify the provenance of software artifacts, emphasizing its importance in improving software supply chain security.","title":"Witness and SLSA 💃","type":"posts"},{"content":"由于近些年针对软件的供应链的攻击越来越频繁，据 SonaType 的统计从 2019 年到 2022 年针对开源软件的攻击增长了 742%，因此 2021 年 Google 提出的解决方案是软件工件供应链级别（Supply chain Levels for Software Artifacts，\u0026ldquo;SLSA\u0026rdquo;）\n本篇将介绍在 Python 生态系统中，我们如何使用 SLSA 框架来生成和验证 Python 工件的来源，从而让你的 SLSA Level 从 L0/L1 到 L3。\n注意：本文介绍的是针对托管在 GitHub 上的 Python 项目。SLSA 框架可通过 GitHub Actions 来实现开箱即用，只需较少的配置即可完成。\n对于托管在非 GitHub 上的项目（例如 Bitbucket）可以尝试 Witness，下一篇我将更新关于如何使用 Witness。\n内容 # 构建纯净的Python包 生成出处证明 上传到PyPI 验证Python包的来源 文中用到的项目 下面是从维护人员到用户的端到端工作流程：从构建 Wheel package -\u0026gt; 生成出处 -\u0026gt; 验证出处 -\u0026gt; 发布到 PyPI -\u0026gt; 以及用户验证出处 -\u0026gt; 安装 wheel。接下来让我们一起来完成这其中的每一步。\n如果你想了解 Python 打包的流程或是术语可以参见Python 打包用户指南。\n构建纯净的Python包 # 构建纯 Python 包通常只有两个工件：即纯 Python Wheel Package 和源代码 distribution。可以使用命令 python3 -m build 从源代码构建。\n下面是 GitHub Actions job 定义来构建 Wheel Package 和源代码 distribution，并为每个工件创建 SHA-256 哈希值：\njobs: build: steps: - uses: actions/checkout@... - uses: actions/setup-python@... with: python-version: 3.x - run: | # 安装 build，创建 sdist 和 wheel python -m pip install build python -m build # 收集所有文件的哈希值 cd dist \u0026amp;\u0026amp; echo \u0026#34;hashes=$(sha256sum * | base64 -w0)\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - uses: actions/upload-artifacts@... with: path: ./dist 这里将 build 完的 wheel package 上传到 GitHub Artifacts 存起来，用作后续在 “上传到PyPI” job 中使用。另外还将 dist 下的所有文件的哈希值存储在 hashes 用作后续的 provenance job 的输入。\n注意： SLSA 使用 sha265sum 的输出作为出处证明中 subject-base64 字段的输入。sha256sum 的输出是一个或多个对散列 + 名称。\n生成出处证明 # 现在我们已经构建了 sdist 和 wheel，我们可以从文件哈希生成来出处证明。\n因为我们需要将 Build 阶段的的输出作为这里生成出处的输入，因此这里使用了 needs 选项来作为 provenance job 执行的前提条件。可以看到上面生成的哈希值在这里被 subject-base64 所使用。\njobs: provenance: needs: [build] uses: slsa-framework/slsa-github-builder/.github/workflows/generator_generic_slsa3.yml@v1.9.0 permissions: # 需要检测 GitHub 操作环境 actions: read # 需要通过 GitHub OIDC 创建出处 id-token: write # 需要创建并上传到 GitHub Releases contents: write with: # 生成的 package SHA-256 哈希值 subject-base64: ${{ provenance.needs.build.output.hashes }} # 将出处文件上传到 GitHub Release upload-assets: true 你会注意到 SLSA builders 使用可重用工作流功能来证明给定的 builders 行为不能被用户或其他进程修改。\n出处证明文件是 JSON lines，以 .intoto.jsonl 结尾。*.intoto.jsonl 文件可以包含多个工件的证明，也可以在同一文件中包含多个出处证明。该 .jsonl 格式意味着该文件是一个 “JSON lines” 文件，即每行一个 JSON 文档。\n注意：这里有一点令人困惑的是 GitHub job 中的 id-token 需要 write 权限才能读取 GitHub OIDC 令牌。read 不允许你读取 OIDC\u0026hellip;🤷。有关 id-token 权限的更多信息，请参阅 GitHub 文档。\n上传到PyPI # 我们使用官方 pypa/gh-action-pypi-publish GitHub Action 将 wheel 包上传到 PyPI。\n注意：publish job 需要在 build 和 provenance 都完成后开始执行，这意味着我们可以假设 provenance job 已经为我们起草了 GitHub Release（因为 upload-assets: true 的设置），并且我们可以假设该 job 已成功。如果不先创建来 provenance 文件，我们不想将这些 wheel 包上传到 PyPI，因此我们最后上传到 PyPI。\npublish: needs: [\u0026#34;build\u0026#34;, \u0026#34;provenance\u0026#34;] permissions: contents: write runs-on: \u0026#34;ubuntu-latest\u0026#34; steps: # 下载已构建的 distributions - uses: \u0026#34;actions/download-artifact@...\u0026#34; with: name: \u0026#34;dist\u0026#34; path: \u0026#34;dist/\u0026#34; # 上传 distributions 到 GitHub Release - env: GITHUB_TOKEN: \u0026#34;${{ secrets.GITHUB_TOKEN }}\u0026#34; run: gh release upload ${{ github.ref_name }} dist/* --repo ${{ github.repository }} # 发布 distributions 到 PyPI - uses: \u0026#34;pypa/gh-action-pypi-publish@...\u0026#34; with: user: __token__ password: ${{ secrets.PYPI_TOKEN }} 验证Python包的来源 # 让我们使用一个真正的 Python 项目来验证它的出处。以 urllib3 项目为例，它在 GitHub Releases 发布了版本中包含出处证明，这里演示的是使用它的最新版本 2.1.0 。\n首先我们需要下载 slsa-verifier 用来验证出处。下载完 slsa-verifier 工具后，让我们从 PyPI 获取 urllib3 wheel 包，而不使用 pip download. 我们使用该 --only-binary 选项强制 pip 下载 wheel。\npython3 -m pip download --only-binary=:all: urllib3 Collecting urllib3 Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB) Downloading urllib3-2.1.0-py3-none-any.whl (104 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.6/104.6 kB 761.0 kB/s eta 0:00:00 Saved ./urllib3-2.1.0-py3-none-any.whl Successfully downloaded urllib3 下载软件包后，我们需要从 GitHub 版本下载出处证明。我们需要使用与包版本相同的 GitHub Release 来确保获得正确的出处证明，因此 tag 也是 2.1.0。\ncurl --location -O https://github.com/urllib3/urllib3/releases/download/2.1.0/multiple.intoto.jsonl 该出处文件的名称为 multiple.intoto.jsonl，这是一个包含多个工件证明的出处证明的标准名称。\n此时，我们当前的工作目录中应该有两个文件：wheel 和出处证明，ls 浏览一下确保已经准备好了：\nls multiple.intoto.jsonl urllib3-2.1.0-py3-none-any.whl 从这里我们可以使用 slsa-verifier 来验证出处。我们可以验证最重要的事情，即哪个 GitHub 仓库实际构建了 wheel，以及其他信息，例如 git 标签、分支和建造者 ID：\n源存储库 (--source-uri) 建造者 ID (--builder-id) Git 分支 (--source-branch) git 标签 (--source-tag)\n# 这里仅验证 wheel package 的 GitHub 仓库 slsa-verifier verify-artifact --provenance-path multiple.intoto.jsonl --source-uri github.com/urllib3/urllib3 urllib3-2.1.0-py3-none-any.whl Verified signature against tlog entry index 49513169 at URL: https://rekor.sigstore.dev/api/v1/log/entries/24296fb24b8ad77a08c2f012d69948ed5d12e8e020852bb7936ea9208d684688e5108cca859a3302 Verified build using builder \u0026#34;https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.9.0\u0026#34; at commit 69be2992f8a25a1f27e49f339e4d5b98dec07462 Verifying artifact urllib3-2.1.0-py3-none-any.whl: PASSED PASSED: Verified SLSA provenance 成功了！🥳 我们已经验证了这个 wheel 的出处，所以现在我们可以放心的安装它，因为我们知道它是按照我们的预期构建的：\npython3 -m pip install urllib3-2.1.0-py3-none-any.whl Defaulting to user installation because normal site-packages is not writeable Processing ./urllib3-2.1.0-py3-none-any.whl Installing collected packages: urllib3 Attempting uninstall: urllib3 Found existing installation: urllib3 2.0.5 Uninstalling urllib3-2.0.5: Successfully uninstalled urllib3-2.0.5 Successfully installed urllib3-2.1.0 文中用到的项目 # 以下这些是本文使用的所有项目和工具：\nSLSA GitHub Builder slsa-framework/slsa-verifier pypa/gha-action-pypi-publish pypa/build urllib3/urllib3 英文原文：https://sethmlarson.dev/python-and-slsa\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-11-15","externalUrl":null,"permalink":"/posts/2023/python-and-slsa/","section":"Posts","summary":"本文介绍了如何在 Python 生态系统中使用 SLSA 框架来生成和验证 Python 工件的来源，从而提升软件供应链的安全性。","title":"Python 和 SLSA 💃","type":"posts"},{"content":"","date":"2023-10-08","externalUrl":null,"permalink":"/en/tags/aix/","section":"Tags","summary":"","title":"AIX","type":"tags"},{"content":"In this article, I would like to document the problems encountered when upgrading from IBM XLC 10.1 to XLC 17.1 (IBM Open XL C/C++ for AIX 17.1.0) and how to fix the following 12 errors.\nIf you\u0026rsquo;ve encountered any other errors, feel free to share your comments with or without a solution.\n1. Change cc to ibm-clang # First you need to change all the related cc to ibm-clang in the the global Makefile. for example:\n- CC=cc - CXX=xlC_r - XCC=xlC_r - MAKE_SHARED=xlC_r + CC=ibm-clang + CXX=ibm-clang_r + XCC=ibm-clang_r + MAKE_SHARED=ibm-clang_r And check following link of Mapping of options to map new Clang options if any.\n2. error: unknown argument: \u0026lsquo;-qmakedep=gcc\u0026rsquo; # - GEN_DEPENDENTS_OPTIONS=-qmakedep=gcc -E -MF $@.1 \u0026gt; /dev/null + GEN_DEPENDENTS_OPTIONS= -E -MF $@.1 \u0026gt; /dev/null 3. should not return a value [-Wreturn-type] # - return -1; + return; 4. error: non-void function \u0026lsquo;main\u0026rsquo; should return a value [-Wreturn-type] # - return; + return 0; 5. error: unsupported option \u0026lsquo;-G\u0026rsquo; for target \u0026lsquo;powerpc64-ibm-aix7.3.0.0\u0026rsquo; # - LIB_101_FLAGS := -G + LIB_101_FLAGS := -shared -Wl,-G 6. Undefined symbol (libxxxx.so) # - LIB_10_FLAGS := -bexport:$(SRC)/makefiles/xxxx.def + LIB_10_FLAGS := -lstdc++ -lm -bexport:$(SRC)/makefiles/xxxx.def 7. unsupported option -qlongdouble # - drv_connect.c.CC_OPTIONS=$(CFLAGS) -qlongdouble -brtl + drv_loadfunc.c.CC_OPTIONS=$(CFLAGS) $(IDIR) -brtl 8. Undefined symbol: ._Z8u9_closei # - extern int u9_close(int fd) ; + extern \u0026#34;C\u0026#34; int u9_close(int fd) ; 9. ERROR: Undefined symbol: .pow # - CXXLIBES = -lpthread -lC -lstdc++ + CXXLIBES = -lpthread -lC -lstdc++ -lm 10. \u0026lsquo;main\u0026rsquo; (argument array) must be of type \u0026lsquo;char **\u0026rsquo; # - d_char *argv[]; + char *argv[]; 11. first parameter of \u0026lsquo;main\u0026rsquo; (argument count) must be of type \u0026lsquo;int\u0026rsquo; # - int main(char *argc, char *argv[]) + int main(int argc, char *argv[]) 12. ERROR: Undefined symbol: ._ZdaPv # - LIB_3_LIBS\t:= -lverse -llog_nosig + LIB_3_LIBS\t:= -lverse -llog_nosig -lstdc++ ","date":"2023-10-08","externalUrl":null,"permalink":"/en/posts/2023/upgrade-xlc-10-to-xlc-17.1/","section":"Posts","summary":"This article documents the problems encountered when upgrading from IBM XLC 10.1 to XLC 17.1 (IBM Open XL C/C++ for AIX 17.1.0) and how to fix the errors.","title":"Problems and solutions when upgrading XLC from 10.1 to IBM Open XL C/C++ for AIX 17.1.0","type":"posts"},{"content":"","date":"2023-10-08","externalUrl":null,"permalink":"/en/tags/xlc/","section":"Tags","summary":"","title":"XLC","type":"tags"},{"content":"","date":"2023-09-11","externalUrl":null,"permalink":"/en/tags/artifactory/","section":"Tags","summary":"","title":"Artifactory","type":"tags"},{"content":"","date":"2023-09-11","externalUrl":null,"permalink":"/en/tags/java/","section":"Tags","summary":"","title":"Java","type":"tags"},{"content":"Recently, I encountered an issue where artifacts could not be uploaded from a Jenkins agent to Artifactory. The specific error is as follows:\n[2023-09-11T08:21:53.385Z] Executing command: /bin/sh -c git log --pretty=format:%s -1 [2023-09-11T08:21:54.250Z] [consumer_0] Deploying artifact: https://artifactory.mycompany.com/artifactory/generic-int-den/my-project/hotfix/1.2.0.HF5/3/pj120_bin_opt_SunOS_3792bcf.tar.Z [2023-09-11T08:21:54.269Z] Error occurred for request GET /artifactory/api/system/version HTTP/1.1: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target. [2023-09-11T08:21:54.282Z] Error occurred for request PUT /artifactory/generic-int-den/my-project/hotfix/1.2.0.HF5/3/pj120_bin_opt_SunOS_3792bcf.tar.Z;build.timestamp=1694418199972;build.name=hotfix%2F1.2.0.HF5;build.number=3 HTTP/1.1: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target. [2023-09-11T08:21:54.284Z] [consumer_0] An exception occurred during execution: [2023-09-11T08:21:54.284Z] java.lang.RuntimeException: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target [2023-09-11T08:21:54.284Z] at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:44) [2023-09-11T08:21:54.284Z] at org.jfrog.build.extractor.producerConsumer.ConsumerRunnableBase.run(ConsumerRunnableBase.java:11) [2023-09-11T08:21:54.284Z] at java.lang.Thread.run(Thread.java:745) [2023-09-11T08:21:54.285Z] Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This problem occurs because the Java security certificate does not validate the HTTPS connection when uploading files. The solution is to regenerate the certificate file and import it. The steps are as follows.\nGenerating the Security Certificate File # Steps:\nFirst, open your Artifactory URL in a browser. There should be a lock icon on the left side of the URL. Click on \u0026ldquo;Connection is secure\u0026rdquo; -\u0026gt; \u0026ldquo;Certificate is valid\u0026rdquo; -\u0026gt; \u0026ldquo;Details\u0026rdquo; -\u0026gt; \u0026ldquo;Export\u0026rdquo;. Select \u0026ldquo;DER-encoded binary, single certificate (*.der)\u0026rdquo; to generate the certificate file. For example, I named my security certificate file: artifactory.mycompany.der (the name can be arbitrary, as long as the extension remains unchanged).\nImporting the Security Certificate via Command Line # Log in to the problematic Solaris agent, upload artifactory.mycompany.der to a specified directory, find the path to cacerts, and execute the following command:\nroot@mysolaris:/# keytool -import -alias example -keystore /usr/java/jre/lib/security/cacerts -file /tmp/artifactory.mycompany.der # Then select yes You will be prompted to enter the password. The default password is changeit. Enter it and restart your JVM or VM. Once you retry uploading artifacts through this agent, everything should return to normal.\nReference: https://stackoverflow.com/questions/21076179/pkix-path-building-failed-and-unable-to-find-valid-certification-path-to-requ\nPlease indicate the author and source when reprinting this article. Do not use for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2023-09-11","externalUrl":null,"permalink":"/en/posts/2023/unable-to-find-valid-certification-path/","section":"Posts","summary":"This article describes how to resolve SSL certificate validation issues when uploading artifacts from a Jenkins agent to Artifactory, including generating a security certificate file and importing it into Java’s cacerts.","title":"Resolving Jenkins Artifactory Plugin Artifact Upload Failure \"unable to find valid certification path to requested target\"","type":"posts"},{"content":"DevOps 运动仍然是一个不断发展的领域，受到技术进步、行业趋势和组织需求等多种因素的影响。这使得很难对 DevOps 工程的未来做出具体预测。然而我认为有一些趋势可能会在来年继续影响 DevOps 领域的发展。\n云原生技术的持续采用 # 容器化、微服务和无服务器计算等云原生技术会继续在 DevOps 环境中广泛采用，这些技术为各种项目提供了许多好处，包括：\n提高可扩展性和可靠性：云原生技术可以让 DevOps 团队更轻松地构建、部署和管理可扩展且有弹性的应用程序。 更快的部署和迭代：云原生技术可以使团队更快地部署和迭代应用程序，帮助组织更快地行动并响应不断变化的业务需求。 更大的灵活性和敏捷性：云原生技术可以为 DevOps 团队提供更大的灵活性和敏捷性，使他们能够使用各种工具和平台构建和部署应用程序。 总体而言，随着组织寻求提高数字时代的效率和竞争力，采用云原生技术可能会继续成为未来几年 DevOps 的趋势。 Kubernetes 和类似的编排平台仍将是这一过程的重要组成部分，因为它们为构建、部署和管理容器化提供了一致的、基于云的环境、应用程序和基础设施。\n更加关注安全性和合规性 # 随着安全性和合规性的重要性不断增长，组织会更加重视将安全性和合规性最佳实践纳入其流程和工具中。\n未来几年，安全性和合规性在一些特定领域尤为重要：\n云安全：随着越来越多的组织采用基于云的基础设施和服务，DevOps 团队将更加需要关注云安全性和合规性，包括确保云环境配置安全、数据在传输和静态时加密以及对云资源的访问进行控制和监控。 应用程序安全性：DevOps 团队需要专注于构建安全的应用程序并确保它们以安全的方式进行测试和部署，包括实施安全编码实践、将安全测试纳入开发过程以及使用安全配置部署应用程序。 遵守法规和标准：团队需要确保他们的流程和工具符合相关法规和行业标准，包括实施控制措施来保护敏感数据，确保系统配置为以合规的方式，通过审计和评估证明合规性，以及在适用的情况下将部分责任推给云提供商。 总而言之，随着组织寻求保护其系统、数据和客户免受网络威胁并确保遵守相关法规和标准，DevOps 中对安全性和合规性的关注肯定会增加，它还将使 DevSecOps 专家 在 DevOps 团队中发挥更大的作用。\n开发和运营团队之间加强协作 # DevOps 运动的理念是将开发和运营团队聚集在一起，以便更紧密、更有效地工作。未来几年，开发和运营团队之间的协作在一些关键领域可能会特别重要：\n持续集成和交付 (CI/CD)：开发和运营团队需要密切合作，以确保有效地测试、部署和监控代码更改，并快速识别和解决任何问题。 事件管理：开发和运营团队需要合作识别和解决生产环境中出现的问题，并制定策略以防止将来发生类似问题。 容量规划和资源管理：开发和运营团队需要共同努力，以确保系统拥有必要的资源来满足需求，并规划未来的增长。 DevOps 的成功取决于开发和运营团队之间的强有力合作，这可能仍然是 2023 年的重点。\n自动化和人工智能的持续发展 # 自动化和人工智能 (AI) 可能在 DevOps 的发展中发挥重要作用。自动化工具可以帮助 DevOps 团队对重复性任务进行编程、提高效率并降低人为错误的风险，而人工智能可用于分析数据、识别模式并协助做出更明智的决策。\n人工智能可能对 DevOps 产生重大影响的一个潜在领域是预测分析领域。通过分析过去部署和性能指标的数据，人工智能算法可以识别模式并预测未来的结果，从而使团队能够更好地优化其流程并提高整体性能。 人工智能可能产生影响的另一个领域是事件管理领域。人工智能算法可用于分析日志数据并在潜在问题发生之前识别它们，从而使团队能够在出现重大事件之前主动解决出现的问题。 总体而言，DevOps 中自动化和人工智能的发展可能会带来更高效、更有效的流程，并提高应用程序和系统的性能和可靠性。然而，组织必须仔细考虑这些技术的潜在影响，并确保它们的使用方式符合其业务目标和价值观。 自动化和人工智能的实施应该成为战略的一部分：包括从一开始就需要集成到业务流程中，调整期望和目标，估计成本以及相关的风险和挑战。仅仅为了实现两者而实施并不一定会从中获益，相反，从长远来看，它可能会因维护它们而导致其他问题。\n基础设施即代码的多云支持 # 基础设施即代码 (IaC) 正在成为一种越来越流行的实践，涉及使用与管理代码相同的版本控制和协作工具来管理基础设施。这使得组织能够将其基础设施视为一等公民，并且更容易自动化基础设施的配置和管理。\n多云基础设施是指在单个组织内使用多个云计算平台，例如 AWS、Azure 和 Google Cloud。这种方法可以为组织提供更大的灵活性和弹性，因为它们不依赖于单个提供商。\n结合这两个概念，对 IaC 的多云支持是指使用 IaC 实践和工具来管理和自动化跨多个云平台的资源调配和配置的能力。这可以包括使用 IaC 定义和部署基础设施资源，例如虚拟机、网络和存储，以及管理这些资源的配置。\n使用 IaC 管理多云基础设施可以带来多种好处。它可以帮助组织在其环境中实现更高的一致性和标准化，并降低在多个云平台中管理资源的复杂性。它还可以使组织能够更轻松地在云平台之间迁移工作负载，并利用每个平台的独特功能。\n总体而言，对于寻求优化多个云平台的使用并简化多云基础设施管理的组织来说，对 IaC 的多云支持可能仍然是一个重要因素。\n更加强调多样性和包容性 # 随着技术行业继续关注多样性和包容性，DevOps 团队可能会更加重视建立多元化和包容性的团队并创造更具包容性的工作环境 - 包括：\n为团队提供具有新的非技术技能的人员，以填补深刻的技能差距。根据 《提高 IT 技能 2022》报告，IT 需要七种关键技能：流程和框架技能、人员技能、技术技能、自动化技能、领导技能、数字技能、业务技能和高级认知技能。不幸的是，大多数 IT 经理首先追求的是技术技能，而忽视了“软技能” —— 这可能是阻碍 DevOps 在组织中进一步发展的最大因素。 整个团队以及单个 DevOps 成员的技能发展。近年来，每个人都面临着考验（新冠等流行病、不确定的经济形势），并使越来越多的 IT 专业人员不确定自己的技能，并感到需要进一步的培训和发展。与此同时，根据《2022 年 IT 技能提升》报告，接受调查的 IT 组织中只有 52% 制定了正式的技能提升计划。这实际上导致全球 IT 团队面临的最大挑战是缺乏技能。 向外部团队开放，保证能力和预期质量的快速提高。无论是外包以及其他形式的合作。这还可以在优化生产成本方面带来切实的好处（这无疑是当今开展业务最重要的因素之一）。 概括 # 上述 6 个方面是我们认为 2023 年 DevOps 的主要趋势。当然，每个人都可以从自己的角度添加其他要点，例如：无服务器计算、低代码和无代码解决方案、GitOps 等。\n过往 DevOps 趋势文章 # 2022 年最值得关注的 DevOps 趋势和问答 参考 # Trends for DevOps engineering in 2023 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-09-01","externalUrl":null,"permalink":"/posts/2023/devops-trends-2023/","section":"Posts","summary":"本文介绍了2023年DevOps领域的主要趋势，包括云原生技术的持续采用、加强安全性和合规性、开发与运营团队协作、自动化和人工智能的发展等。","title":"2023 年最值得关注的 DevOps 趋势","type":"posts"},{"content":"","date":"2023-09-01","externalUrl":null,"permalink":"/tags/kubernetes/","section":"标签","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"2023-08-29","externalUrl":null,"permalink":"/en/tags/troubleshooting/","section":"Tags","summary":"","title":"Troubleshooting","type":"tags"},{"content":"Recently my CI pipeline suddenly does not work on AIX 7.1 with error Caused by: javax.net.ssl.SSLHandshakeException: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath..\nClick to see more details about the failure log. 22:13:30 Executing command: /bin/sh -c git log --pretty=format:%s -1 22:13:36 [consumer_0] Deploying artifact: https://artifactory.company.com/artifactory/generic-int-den/myproject/PRs/PR-880/1/myproject_bin_rel_AIX_5797b20.tar.Z 22:13:36 Error occurred for request GET /artifactory/api/system/version HTTP/1.1: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error. 22:13:36 Error occurred for request PUT /artifactory/generic-int-den/myproject/PRs/PR-880/1/cpplinter_bin_rel_AIX_5797b20.tar.Z;build.timestamp=1693273923987;build.name=PR-880;build.number=1 HTTP/1.1: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error. 22:13:36 [consumer_0] An exception occurred during execution: 22:13:36 java.lang.RuntimeException: javax.net.ssl.SSLHandshakeException: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:44) 22:13:36 at org.jfrog.build.extractor.producerConsumer.ConsumerRunnableBase.run(ConsumerRunnableBase.java:11) 22:13:36 at java.lang.Thread.run(Thread.java:785) 22:13:36 Caused by: javax.net.ssl.SSLHandshakeException: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.jsse2.j.a(j.java:3) 22:13:36 at com.ibm.jsse2.as.a(as.java:213) 22:13:36 at com.ibm.jsse2.C.a(C.java:339) 22:13:36 at com.ibm.jsse2.C.a(C.java:248) 22:13:36 at com.ibm.jsse2.D.a(D.java:291) 22:13:36 at com.ibm.jsse2.D.a(D.java:217) 22:13:36 at com.ibm.jsse2.C.r(C.java:373) 22:13:36 at com.ibm.jsse2.C.a(C.java:352) 22:13:36 at com.ibm.jsse2.as.a(as.java:752) 22:13:36 at com.ibm.jsse2.as.i(as.java:338) 22:13:36 at com.ibm.jsse2.as.a(as.java:711) 22:13:36 at com.ibm.jsse2.as.startHandshake(as.java:454) 22:13:36 at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:436) 22:13:36 at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384) 22:13:36 at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) 22:13:36 at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376) 22:13:36 at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) 22:13:36 at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) 22:13:36 at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) 22:13:36 at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) 22:13:36 at org.apache.http.impl.execchain.ServiceUnavailableRetryExec.execute(ServiceUnavailableRetryExec.java:85) 22:13:36 at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) 22:13:36 at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) 22:13:36 at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) 22:13:36 at org.jfrog.build.client.PreemptiveHttpClient.execute(PreemptiveHttpClient.java:76) 22:13:36 at org.jfrog.build.client.PreemptiveHttpClient.execute(PreemptiveHttpClient.java:64) 22:13:36 at org.jfrog.build.client.JFrogHttpClient.sendRequest(JFrogHttpClient.java:133) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.JFrogService.execute(JFrogService.java:112) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.artifactory.services.Upload.execute(Upload.java:77) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.artifactory.ArtifactoryManager.upload(ArtifactoryManager.java:267) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.artifactory.ArtifactoryManager.upload(ArtifactoryManager.java:262) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:39) 22:13:36 ... 2 more 22:13:36 Caused by: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.jsse2.util.f.a(f.java:107) 22:13:36 at com.ibm.jsse2.util.f.b(f.java:143) 22:13:36 at com.ibm.jsse2.util.e.a(e.java:6) 22:13:36 at com.ibm.jsse2.aA.a(aA.java:99) 22:13:36 at com.ibm.jsse2.aA.a(aA.java:112) 22:13:36 at com.ibm.jsse2.aA.checkServerTrusted(aA.java:28) 22:13:36 at com.ibm.jsse2.D.a(D.java:588) 22:13:36 ... 29 more 22:13:36 Caused by: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.engineBuild(PKIXCertPathBuilderImpl.java:422) 22:13:36 at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:268) 22:13:36 at com.ibm.jsse2.util.f.a(f.java:120) 22:13:36 ... 35 more 22:13:36 Caused by: java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.security.cert.BasicChecker.\u0026lt;init\u0026gt;(BasicChecker.java:111) 22:13:36 at com.ibm.security.cert.PKIXCertPathValidatorImpl.engineValidate(PKIXCertPathValidatorImpl.java:199) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.myValidator(PKIXCertPathBuilderImpl.java:749) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.buildCertPath(PKIXCertPathBuilderImpl.java:661) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.buildCertPath(PKIXCertPathBuilderImpl.java:607) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.engineBuild(PKIXCertPathBuilderImpl.java:368) 22:13:36 ... 37 more 22:13:36 Caused by: java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.security.cert.CertPathUtil.findIssuer(CertPathUtil.java:316) 22:13:36 at com.ibm.security.cert.BasicChecker.\u0026lt;init\u0026gt;(BasicChecker.java:108) 22:13:36 ... 42 more 22:13:36 I have tried to download certificate.pem from my Artifactory and run this command, but the issue still there on my AIX 7.3.\n/usr/java8_64/jre/bin/keytool -importcert -alias cacertalias -keystore /usr/java8_64/jre/lib/security/cacerts -file /path/to/your/certificate.pem It investing it can not reproduce on my Windows, Linux and AIX 7.3 build machines.\nWhat\u0026rsquo;s the different between them? the only different is Java runtime. On my problematic AIX 7.1 build machine, I used a shared runtime which is a link to path /tools/AIX-7.1/Java8_64-8.0.0.401/usr/java8_64/bin/java\nbash-5.0$ /tools/AIX-7.1/Java8_64-8.0.0.401/usr/java8_64/bin/java -version java version \u0026#34;1.8.0\u0026#34; Java(TM) SE Runtime Environment (build pap6480sr4fp1-20170215_01(SR4 FP1)) IBM J9 VM (build 2.8, JRE 1.8.0 AIX ppc64-64 Compressed References 20170209_336038 (JIT enabled, AOT enabled) J9VM - R28_20170209_0201_B336038 JIT - tr.r14.java.green_20170125_131456 GC - R28_20170209_0201_B336038_CMPRSS J9CL - 20170209_336038) JCL - 20170215_01 based on Oracle jdk8u121-b13 And I have anther Java runtime installed my that machine which is /usr/java8_64/bin/java\nbash-5.0$ /usr/java8_64/bin/java -version java version \u0026#34;1.8.0_241\u0026#34; Java(TM) SE Runtime Environment (build 8.0.6.5 - pap6480sr6fp5-20200111_02(SR6 FP5)) IBM J9 VM (build 2.9, JRE 1.8.0 AIX ppc64-64-Bit Compressed References 20200108_436782 (JIT enabled, AOT enabled) OpenJ9 - 7d1059c OMR - d059105 IBM - c8aee39) JCL - 20200110_01 based on Oracle jdk8u241-b07 Actually the versions of these two java versions are different. I just changed configuration of JavaPath from /tools/AIX-7.1/Java8_64-8.0.0.401/usr/java8_64/bin/java to /usr/java8_64/bin/java in the Jenkins node and disconnect then launch agent again, it works.\nI don\u0026rsquo;t why it works, I don\u0026rsquo;t know much about Java certificate, if you know the reason please leave comments and let me know. Thank you.\n","date":"2023-08-29","externalUrl":null,"permalink":"/en/posts/2023/upload-artifacts-failed-on-aix/","section":"Posts","summary":"This article explains how to resolve an SSL certificate verification issue on AIX when uploading artifacts to Artifactory via Jenkins, including updating Java’s cacerts file.","title":"Upload artifacts failed to Artifactory from AIX","type":"posts"},{"content":"Actually, there\u0026rsquo;s nothing much to say about creating an account on a package management platform, but recently, when I was preparing to create an Organization on https://www.nuget.org before releasing a product, I encountered a problem.\nHere\u0026rsquo;s What Happened # As a company employee, when I first opened the NuGet website (www.nuget.org) and clicked \u0026ldquo;Sign in,\u0026rdquo; I saw \u0026ldquo;Sign in with Microsoft.\u0026rdquo; I clicked, went through the next steps, and successfully registered my first NuGet account using my company email address.\nWhen I tried to create an Organization, entering my company email address resulted in a message saying the email address was already in use. What???\nOK. So I tried entering my colleague\u0026rsquo;s company email address.\nAfter receiving the email notification, my colleague followed the steps: opened NuGet.org, clicked \u0026ldquo;Sign in with Microsoft,\u0026rdquo; and filled in their account details. However, after completing this process and trying to register an Organization using their email address, they also received the message that the email was already in use. What\u0026rsquo;s going on!!! I was baffled\u0026hellip;\nHow It Was Solved # At this critical juncture, while anxiously awaiting the release, I suddenly had a flash of inspiration. With a \u0026ldquo;desperate measures\u0026rdquo; mentality, I changed the company email address associated with my personal NuGet account to my Gmail address. Then, when creating the Organization, I used my company email address, and the Organization was successfully created!\nThis document records the problem I encountered while registering with NuGet. I don\u0026rsquo;t know if it will be helpful to you, but if it does, please let me know.\nPlease indicate the author and source when reprinting this article. Do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2023-08-25","externalUrl":null,"permalink":"/en/posts/2023/nuget/","section":"Posts","summary":"This article documents the problems and solutions encountered when creating a NuGet Organization, especially regarding the use of corporate email addresses.","title":"Creating a NuGet Organization — Pitfalls Encountered","type":"posts"},{"content":"","date":"2023-08-25","externalUrl":null,"permalink":"/en/tags/nuget/","section":"Tags","summary":"","title":"NuGet","type":"tags"},{"content":"With the increasing popularity and application scenarios of containerization technology, building and managing multi-platform images has become increasingly important. Docker Buildx is an extension of the official Docker CLI that provides Docker users with more powerful and flexible build capabilities. These include:\nMulti-platform Builds: Docker Buildx allows users to build container images for multiple different platforms in a single build command. This allows you to build images for multiple CPU architectures, such as x86, ARM, etc., all at once, so that the same image can run on different hardware devices. Build Cache Optimization: Docker Buildx improves the caching mechanism during the build process by automatically identifying which parts of the Dockerfile are cacheable, thus reducing redundant builds and speeding up the build process. Parallel Builds: Buildx allows parallel building of multiple images, improving build efficiency. Multiple Output Formats: Buildx supports different output formats, including Docker images, OCI images, and rootfs. Build Strategies: By supporting multiple build strategies, users can better control the build process, such as building on multiple nodes, using remote builds, etc. Using docker buildx requires a Docker Engine version of 19.03 or higher.\nDocker Buildx Bake is a subcommand of Buildx, and this article will focus on its concept, advantages, usage scenarios, and how to use this feature to accelerate the building and management of multi-platform images.\nWhat is Docker Buildx Bake? # Docker Buildx Bake is a feature of Docker Buildx designed to simplify and accelerate the image building process. Bake is a declarative build definition method that allows users to define multiple build configurations and target platforms in a single command, enabling automated batch building and publishing of cross-platform images.\nWhy Use Docker Buildx Bake? # 1. Improved Build Efficiency # Bake improves build efficiency through parallel builds and caching mechanisms. Using Bake, you can define and build multiple images at once without having to execute the build process separately for each image, which can significantly reduce build time and improve work efficiency.\n2. Support for Multiple Platforms and Architectures # Another advantage of Docker Buildx Bake is its ability to build images for multiple platforms and architectures. By specifying different platform parameters in the Bake configuration, you can easily build images suitable for different operating systems and architectures. This is very useful for the development and deployment of cross-platform applications.\n3. Consistent Build Environment # Using a docker-bake.hcl file (in addition to HCL configuration files, JSON or YAML files can also be used) to describe the build process ensures a consistent build environment, so that different build configurations and target platforms have the same build process and results. This consistency helps reduce errors in the build process and makes build configurations easier to maintain and manage.\nHow to Use Docker Buildx Bake? # Here are the basic steps for using Docker Buildx Bake for efficient builds. First, ensure you have Docker Engine or Docker Desktop version 19.03 or higher installed.\nThen your docker command will become docker buildx bake. The following is the help output of docker buildx bake --help:\ndocker buildx bake --help Usage: docker buildx bake [OPTIONS] [TARGET...] Build from a file Aliases: docker buildx bake, docker buildx f Options: --builder string Override the configured builder instance -f, --file stringArray Build definition file --load Shorthand for \u0026#34;--set=*.output=type=docker\u0026#34; --metadata-file string Write build result metadata to the file --no-cache Do not use cache when building the image --print Print the options without building --progress string Set type of progress output (\u0026#34;auto\u0026#34;, \u0026#34;plain\u0026#34;, \u0026#34;tty\u0026#34;). Use plain to show container output (default \u0026#34;auto\u0026#34;) --provenance string Shorthand for \u0026#34;--set=*.attest=type=provenance\u0026#34; --pull Always attempt to pull all referenced images --push Shorthand for \u0026#34;--set=*.output=type=registry\u0026#34; --sbom string Shorthand for \u0026#34;--set=*.attest=type=sbom\u0026#34; --set stringArray Override target value (e.g., \u0026#34;targetpattern.key=value\u0026#34;) Next, let\u0026rsquo;s try how to use docker buildx bake.\n1. Create a Bake Configuration File # For example, create a Bake configuration file named docker-bake.dev.hcl and define the build context, target platforms, and other build options in it. Here is a simple example:\n# docker-bake.dev.hcl group \u0026#34;default\u0026#34; { targets = [\u0026#34;db\u0026#34;, \u0026#34;webapp-dev\u0026#34;] } target \u0026#34;db\u0026#34; { dockerfile = \u0026#34;Dockerfile.db\u0026#34; tags = [\u0026#34;xianpengshen/docker-buildx-bake-demo:db\u0026#34;] } target \u0026#34;webapp-dev\u0026#34; { dockerfile = \u0026#34;Dockerfile.webapp\u0026#34; tags = [\u0026#34;xianpengshen/docker-buildx-bake-demo:webapp\u0026#34;] } target \u0026#34;webapp-release\u0026#34; { inherits = [\u0026#34;webapp-dev\u0026#34;] platforms = [\u0026#34;linux/amd64\u0026#34;, \u0026#34;linux/arm64\u0026#34;] } 2. Run Bake Build # Run the following command to start building images using Bake:\n$ docker buildx bake -f docker-bake.dev.hcl db webapp-release\n3. Print Build Options # You can also print the build options without building, using --print to see if a particular target build meets expectations. For example:\n$ docker buildx bake -f docker-bake.dev.hcl --print db [+] Building 0.0s (0/0) { \u0026#34;group\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;db\u0026#34; ] } }, \u0026#34;target\u0026#34;: { \u0026#34;db\u0026#34;: { \u0026#34;context\u0026#34;: \u0026#34;.\u0026#34;, \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile.db\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;xianpengshen/docker-buildx-bake-demo:db\u0026#34; ] } } } 4. Publish Built Images # By adding the --push option, you can push the built images to the image repository in one step. For example, $ docker buildx bake -f docker-bake.dev.hcl --push db webapp-release\nThe demo in the above example is located here: https://github.com/shenxianpeng/docker-buildx-bake-demo\n5. Advanced Buildx Bake Usage # Buildx Bake has many more usage tips, such as variable, function, matrix, etc., which will not be introduced here. For details, please refer to the official Buildx Bake reference documentation.\nSummary # Docker Buildx Bake is a powerful build tool that provides a way to simplify and accelerate the build process. By using Bake, you can efficiently build and test multiple images and build across multiple platforms and architectures. Bake is an important tool for developers and build engineers; mastering the use of Docker Buildx Bake will help you better address the challenges of multi-image builds and speed up application delivery.\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2023-07-17","externalUrl":null,"permalink":"/en/posts/2023/buildx-bake/","section":"Posts","summary":"This article introduces the concept, advantages, use cases, and how to use Docker Buildx Bake to accelerate the building and management of multi-platform images.","title":"Docker Buildx Bake—A Powerful Tool for Accelerating Builds and Managing Multi-platform Images","type":"posts"},{"content":"","date":"2023-06-10","externalUrl":null,"permalink":"/en/tags/sbom/","section":"Tags","summary":"","title":"SBOM","type":"tags"},{"content":" What is SBOM # SBOM stands for Software Bill of Materials. It is a detailed inventory of all components, libraries, and dependencies used in the software building process.\nAn SBOM is similar to a product\u0026rsquo;s ingredient list. It lists the various elements that make up a software application, including open-source software components, third-party libraries, frameworks, and tools. Each element in the SBOM will have detailed information such as name, version number, license information, and dependencies.\nThe purpose of SBOM is to increase the visibility and transparency of the software supply chain and provide better risk management and security. It helps software developers, vendors, and users understand the components and dependencies used in their software, allowing for better management of potential vulnerabilities, security risks, and compliance issues. Through SBOM, users can identify and track any potential vulnerabilities or known security issues in the software and take appropriate remedial measures in a timely manner.\nSBOM can also be used for software audits, compliance requirements, and regulatory compliance. Some industry standards and regulations (such as the Software Supply Chain Security Framework (SSCF) and the EU Network and Information Security Directive (NIS Directive)) already require software vendors to provide SBOMs to improve the security and trustworthiness of the software supply chain.\nIn short, an SBOM is a record of all components and dependencies used in the software building process. It provides visibility into the software supply chain, helps manage risks, improves security, and meets compliance requirements.\nRelationship and Differences Between SBOM and SLSA # SBOM (Software Bill of Materials) and SLSA (Supply Chain Levels for Software Artifacts) are two different but related concepts.\nSBOM is a software bill of materials that provides visibility into the software supply chain, including component versions, license information, vulnerabilities, etc. SBOM aims to help organizations better manage and control the software supply chain, identifying and addressing potential vulnerabilities, compliance issues, and security risks. SLSA is a supply chain security framework that defines different levels of security requirements and practices to ensure the security of the software supply chain. SLSA aims to strengthen the credibility and security of software and prevent the spread of malicious code, supply chain attacks, and vulnerabilities. SLSA focuses on the security of the entire software supply chain, including the origin, verification, build process, and release mechanism of components. Regarding the differences:\nDifferent Perspectives: SBOM focuses on the inventory and visibility of software building materials, providing details on components and dependencies. SLSA focuses on supply chain security, defining security levels and practices, emphasizing ensuring the credibility and security of the software supply chain. Different Uses: SBOM is used to identify and manage components, vulnerabilities, and compliance issues in software builds. It provides a tool for managing software supply chain risks. SLSA provides a security framework that helps organizations ensure the security of their software supply chain by defining security levels and requirements. Correlation: SLSA can utilize SBOM as part of its implementation. SBOM provides the details of components and dependencies required by SLSA, which helps to verify and audit the security of the supply chain. SLSA practices may include requiring the generation and verification of SBOMs to ensure the visibility and integrity of the software supply chain. SBOM and SLSA are both key concepts in software supply chain security. They can be used in conjunction with each other and complement each other to strengthen software supply chain security and management.\nDifferences Between SBOM and Black Duck # SBOM (Software Bill of Materials) and Synopsys Black Duck are two related but distinct concepts. Here\u0026rsquo;s a comparison:\nSBOM:\nDefinition: An SBOM is a document or inventory that records all components and dependencies used in the software building process. It provides visibility and transparency into the software supply chain. Content: An SBOM lists details for each component, including name, version number, author, license information, etc. It helps track and manage software components, dependencies, vulnerabilities, and license compliance. Uses: SBOM is used for software supply chain management, security audits, compliance verification, and risk management. It helps organizations understand the components used in software builds, identify potential vulnerabilities and risks, and ensure compliance. Synopsys Black Duck:\nFunctionality: Synopsys Black Duck is a supply chain risk management tool. It can scan software projects, identify the open-source components and third-party libraries used, and analyze their license compliance, security vulnerabilities, and other potential risks. Features: Black Duck has an extensive vulnerability database and license knowledge base, integrates with development processes and CI/CD tools, and provides vulnerability alerts, license compliance reports, and risk analysis. Purpose: Black Duck helps organizations manage and control software supply chain risks, providing real-time security and compliance information on open-source components and third-party libraries to support decision-making and appropriate actions. In summary, SBOM records the components and dependencies used in software builds, providing visibility and management of the software supply chain. Black Duck is a supply chain risk management tool that provides license compliance, security vulnerability, and risk analysis by scanning and analyzing open-source components and third-party libraries in software projects. Black Duck can be used to generate SBOMs and provide more comprehensive risk and compliance analysis. Therefore, Black Duck is a specific tool, while SBOM is a concept for recording and managing software supply chain information.\nSBOM Best Practices # Automated Generation: Use automated tools to generate SBOMs, avoiding manual creation and maintenance to ensure accuracy and consistency. Include Detailed Information: Include as much detail as possible in the SBOM, such as component name, version number, author, license information, dependencies, and vulnerability information. Regular Updates: Regularly update the SBOM to reflect the latest build materials, ensuring its accuracy and completeness. Version Control: Establish and manage corresponding SBOM versions for each software version to track software versions and their corresponding build materials. Integration into the Software Lifecycle: Integrate SBOM into the entire software lifecycle, including development, build, test, deployment, and maintenance phases. Vulnerability Management and Risk Assessment: Utilize vulnerability information in the SBOM, integrate with vulnerability databases, and perform vulnerability management and risk assessments. Vendor Collaboration: Share and obtain SBOM information with vendors and partners, ensuring they also provide accurate SBOMs and continuously monitor their vulnerability management and compliance measures. SBOM Generation Tools # CycloneDX: CycloneDX is an open software component description standard used to generate and share SBOMs. It supports multiple languages and build tools and has a broad ecosystem and tool integrations. SPDX: SPDX (Software Package Data Exchange) is an open standard for describing software components and related license information. It provides a unified way to generate and exchange SBOMs. OWASP Dependency-Track: Dependency-Track is an open-source supply chain security platform that can generate and analyze SBOMs, providing vulnerability management, license compliance, and supply chain visualization. WhiteSource: WhiteSource is a supply chain management tool that provides automated open-source component identification, license management, and vulnerability analysis, generating SBOMs and performing risk assessments. JFrog Xray: JFrog Xray is a software supply chain analysis tool that can scan and analyze bills of materials, providing vulnerability alerts, license compliance, and security analysis. Microsoft sbom-tool: A highly scalable, enterprise-ready tool for creating SPDX 2.2-compliant SBOMs for various artifacts. trivy: Supports finding vulnerabilities, misconfigurations, secrets in containers, Kubernetes, code repositories, Cloud, etc., and generating SBOMs. In addition to these, several other tools provide SBOM generation, management, and analysis capabilities. You can choose the appropriate tool based on your specific needs to implement SBOM best practices.\nSummary # This article aims to help you understand the concept of SBOM, its relationship and differences with SLSA and Black Duck, best practices, and available generation tools to better manage software supply chain security.\n","date":"2023-06-10","externalUrl":null,"permalink":"/en/posts/2023/sbom/","section":"Posts","summary":"This article introduces the definition of SBOM, its relationship and differences with SLSA and Black Duck, best practices, and available generation tools, helping readers better understand and apply SBOM.","title":"Understanding SBOM - Definition, Relationships, Differences, Best Practices, and Generation Tools","type":"posts"},{"content":"想必你也见到过很多开源项目中的 CONTRIBUTION.md 文档中通常都会让贡献者 Fork 仓库，然后做修改。\n那么如果你是该开源项目中的成员是否需要 Fork 仓库进行修改呢？\n以前我没有认真去想过这个问题，对于项目成员感觉 Fork 或不 Fork 好像差不多，但仔细想想 Fork 仓库与不 Fork 仓库其实是有以下几个主要的差别的：\n修改权限 # 在原始仓库中，你可能没有直接修改代码的权限，当你 Fork 一个仓库时，会创建一个属于你自己的副本，你可以在这个副本中拥有完全的修改权限，你可以自由地进行更改、添加新功能、解决bug等，而不会对原始仓库产生直接影响。\n做实验性的工作 # 如果你计划进行较大的修改或实验性工作，并且不希望直接影响原始仓库，那么 fork 仓库并在 fork 的中进行修改更为合适。\n比如你需要实验性的去大量清理现有仓库里的一些垃圾文件或是代码，如果你需要需要多次尝试，并将多次修改直接 git push 到推送原始仓库进行保存或是测试，这大大增加原始仓库的存储空间，如果你的修改是大型文件，那么对原始仓库的存储空间影响则会更大；如果你是 Fork 仓库则不会造成原始仓库的影响，直到你完成修改通过 Pull Request 合并到原始仓库时才会产生新的存储空间。\n代码审查和协作 # 当你 Fork 一个仓库并在自己的副本中进行修改后，你必须通过 Pull Request（PR）向原始仓库合并修改，有助于确保代码质量和功能正确性。（当然不 Fork 也可以这样做或不做，但 Fork 了就必须这样做了）\n版本控制和历史记录 # Fork 一个仓库后，你可以在自己的副本中维护独立的版本控制历史。你可以跟踪自己的更改、回溯历史、管理代码版本，而不会影响到原始仓库的版本控制。同时，你可以从原始仓库同步最新的更改，保持你的副本与原始仓库的同步。\n总结 # Fork 仓库与不 Fork 仓库的主要差别在于修改权限、做实验性的工作、代码审查和协作，以及版本控制和历史记录。\n个人认为只要一个仓库的贡献者超过 3 人，都建议所有人都 Fork 原始仓库，通过 Pull Request 方式合并代码。\n但也有例外情况可能不适合 Fork：项目在 Fork 之后 CI/CD 无法独立工作，但是你需要它们。比如 Fork 后的仓库因为环境等原因不支持独立的运行 CI/CD 而你需要在提交 Pull Request 之前通过自动化对分支进行测试。\n另外还要为原始仓库需要做适当的分支权限设置，以防止就算两个人的团队另外一个人不熟悉 Git 使用了非常危险的操作，比如强制推送（Force Push），变基（Rebasing），强制检出（Force Checkout）可能导致代码丢失、数据损坏或版本控制问题。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-06-09","externalUrl":null,"permalink":"/posts/2023/fork-or-unfork/","section":"Posts","summary":"本文讨论了在开源项目中，作为项目成员是 Fork 原始仓库还是直接在原始仓库中修改代码的利弊，帮助开发者做出更合适的选择。","title":"如果你是项目成员，是 Fork 原始仓库还是直接原始仓库中修改代码？","type":"posts"},{"content":"Git commit message and Git branch naming conventions are a very important part of team collaboration. They can make the codebase more standardized, easier to maintain, and easier to understand.\nWe need to use tools to help implement Git commit message and branch creation conventions. This article will introduce how to use the Commit Check tool to verify commit messages, branch names, committer usernames, and committer email addresses to ensure they conform to specifications.\nFor more information on Git commit messages and branch creation conventions, please refer to my previous article, “Programmer\u0026rsquo;s Self-Cultivation—Git Commit Message and Branch Creation Conventions”. I will not reiterate them here.\nCommit Check Introduction # Commit Check is a tool that can check Git commit messages, branch names, committer usernames, committer emails, and more. It is an open-source alternative to Yet Another Commit Checker.\nCommit Check Configuration # Using Default Settings # Without custom settings, Commit Check will use the default settings. Specific settings are available here.\nBy default, commit messages follow Conventional Commits, and branch naming follows the branch model details.\nUsing Custom Configuration # You can create a configuration file .commit-check.yml in your Git repository to customize the settings. For example:\nchecks: - check: message regex: \u0026#39;^(build|chore|ci|docs|feat|fix|perf|refactor|revert|style|test){1}(\\([\\w\\-\\.]+\\))?(!)?: ([\\w ])+([\\s\\S]*)|(Merge).*|(fixup!.*)\u0026#39; error: \u0026#34;The commit message should be structured as follows:\\n\\n \u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt;\\n [optional body]\\n [optional footer(s)]\\n\\n More details please refer to https://www.conventionalcommits.org\u0026#34; suggest: please check your commit message whether matches above regex - check: branch regex: ^(bugfix|feature|release|hotfix|task)\\/.+|(master)|(main)|(HEAD)|(PR-.+) error: \u0026#34;Branches must begin with these types: bugfix/ feature/ release/ hotfix/ task/\u0026#34; suggest: run command `git checkout -b type/branch_name` - check: author_name regex: ^[A-Za-z ,.\\\u0026#39;-]+$|.*(\\[bot]) error: The committer name seems invalid suggest: run command `git config user.name \u0026#34;Your Name\u0026#34;` - check: author_email regex: ^\\S+@\\S+\\.\\S+$ error: The committer email seems invalid suggest: run command `git config user.email yourname@example.com` You can modify the values of regex, error, and suggest according to your needs.\nCommit Check Usage # Commit Check supports multiple usage methods.\nRunning with GitHub Actions # For example, create a GitHub Actions workflow file .github/workflows/commit-check.yml:\nname: Commit Check on: push: pull_request: branches: \u0026#39;main\u0026#39; jobs: commit-check: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: commit-check-action@v1 with: message: true branch: true author-name: true author-email: true dry-run: true job-summary: true For details, please refer to https://github.com/commit-check-action\nRunning with pre-commit hook # First, you need to install pre-commit.\nThen add the following settings to your .pre-commit-config.yaml file.\n- repo: https://github.com/commit-check rev: the tag or revision hooks: # support hooks - id: check-message - id: check-branch - id: check-author-name - id: check-author-email Running from the Command Line # Install via pip:\npip install commit-check Then run the commit-check --help command to see how to use it. Details can be found in the documentation.\nRunning with Git Hooks # To configure Git Hooks, you need to create a new script file in the .git/hooks/ directory of your Git repository.\nFor example, .git/hooks/pre-push, the file contents are as follows:\n#!/bin/sh commit-check --message --branch --author-name --author-email Make it executable with chmod +x .git/hooks/pre-push. Then, when you run the git push command, this push hook will execute automatically.\nCommit Check Failure Example # Commit message check failure:\nCommit rejected by Commit-Check. (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) / ._. \\ / ._. \\ / ._. \\ / ._. \\ / ._. \\ __\\( C )/__ __\\( H )/__ __\\( E )/__ __\\( C )/__ __\\( K )/__ (_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._) || E || || R || || R || || O || || R || _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ (.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.) `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ Invalid commit message =\u0026gt; test It doesn\u0026#39;t match regex: ^(build|chore|ci|docs|feat|fix|perf|refactor|revert|style|test){1}(\\([\\w\\-\\.]+\\))?(!)?: ([\\w ])+([\\s\\S]*) The commit message should be structured as follows: \u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt; [optional body] [optional footer(s)] More details please refer to https://www.conventionalcommits.org Suggest: please check your commit message whether matches above regex Branch name check failure:\nCommit rejected by Commit-Check. (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) / ._. \\ / ._. \\ / ._. \\ / ._. \\ / ._. \\ __\\( C )/__ __\\( H )/__ __\\( E )/__ __\\( C )/__ __\\( K )/__ (_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._) || E || || R || || R || || O || || R || _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ (.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.) `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ Commit rejected. Invalid branch name =\u0026gt; test It doesn\u0026#39;t match regex: ^(bugfix|feature|release|hotfix|task)\\/.+|(master)|(main)|(HEAD)|(PR-.+) Branches must begin with these types: bugfix/ feature/ release/ hotfix/ task/ Suggest: run command `git checkout -b type/branch_name` This concludes the introduction to Commit Check. For more information, please refer to https://github.com/commit-check\nPlease indicate the author and source when reprinting this article. Do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2023-05-09","externalUrl":null,"permalink":"/en/posts/2023/commit-check/","section":"Posts","summary":"This article introduces how to use the Commit Check tool to verify whether Git commit messages, branch names, committer usernames, and committer email addresses conform to specifications.","title":"Programmer's Self-Cultivation — Git Commit Message and Branch Creation Conventions (Tools)","type":"posts"},{"content":"With the increasing number of attacks targeting software supply chains in recent years, Google has released a series of guidelines to ensure the integrity of software packages, aiming to prevent unauthorized code modifications from affecting the software supply chain.\nGoogle\u0026rsquo;s SLSA framework (Supply-chain Levels for Software Artifacts) provides recommendations for achieving more secure software development and deployment processes by identifying and mitigating issues in CI/CD pipelines.\nTable of Contents # What is SLSA Problems in the Software Supply Chain 2.1 What are Supply Chain Attacks? 2.2 Real-world Examples SLSA Levels 3.1 Detailed Explanation 3.2 Limitations SLSA Implementation Other Tools What is SLSA # SLSA stands for Supply chain Levels for Software Artifacts, or SLSA (pronounced \u0026ldquo;salsa\u0026rdquo;).\nSLSA is an end-to-end framework, a standard and checklist of controls to ensure the security of software building and deployment processes, preventing threats arising from tampering with source code, build platforms, and component repositories.\nProblems in the Software Supply Chain # Any software supply chain can introduce vulnerabilities. As systems become increasingly complex, it becomes crucial to implement best practices to ensure the integrity of delivered artifacts. Without certain standards and systematic development plans, it is difficult to cope with the next hacker attack.\nWhat are Supply Chain Attacks? # A Submitting unauthenticated modifications B Leaking the source code repository C Building from modified source code D Leaking the build process E Using compromised dependencies F Uploading modified packages G Leaking the package repository H Using compromised packages\nReal-world Examples # Integrity Threat Known Example How SLSA Helps A Submitting unauthenticated modifications Researchers attempting to introduce vulnerabilities into the Linux kernel via patches on the mailing list. Research Two-person review caught most (but not all) vulnerabilities. B Leaking the source code repository PHP: Attackers compromised PHP\u0026rsquo;s self-hosted git server and injected two malicious commits. A better-protected source code platform would make it more difficult for attackers to succeed. C Building from modified source code Webmin: Attackers modified the build infrastructure to use source files that didn\u0026rsquo;t match source control. SLSA compliant build servers generate provenance to identify the actual sources used, enabling consumers to detect such tampering. D Leaking the build process SolarWinds: Attackers compromised the build platform and installed implants that injected malicious behavior during each build. Higher SLSA levels require stronger security controls on the build platform making compromise and gaining persistence more difficult. E Using compromised dependencies event-stream: Attackers added a seemingly harmless dependency, then updated that dependency to add malicious behavior. The update didn\u0026rsquo;t match the code submitted to GitHub (i.e., attack F). Recursively applying SLSA to all dependencies blocks this specific vector because provenance would show it wasn\u0026rsquo;t built by the proper builder or the source wasn\u0026rsquo;t from GitHub. F Uploading modified packages CodeCov: Attackers used leaked credentials to upload malicious artifacts to Google Cloud Storage (GCS) from which users could download directly. Provenance of artifacts in GCS showed the artifacts weren\u0026rsquo;t built from the expected source code repository in the expected way. G Leaking the package repository Attacks on package mirrors: Researchers ran mirrors for several popular package repositories, which could be used to serve malicious packages. Similar to (F) above, the provenance of malicious artifacts would show they weren\u0026rsquo;t built as expected nor from the expected source code repository. H Using compromised packages Browserify typosquatting: Attackers uploaded a malicious package with a similar name to the original. SLSA doesn\u0026rsquo;t directly address this threat, but linking provenance back to source control enables and enhances other solutions. SLSA Levels # Level Description Example 1 Documentation of the build process Unsigned provenance 2 Tamper-resistant build service Hosted source/build, signed provenance 3 Additional resistance to specific threats Security controls on the host, unforgeable provenance 4 Highest level of confidence and trust Two-person review + hermetic build Detailed Explanation # Level Requirements 0 No guarantees. SLSA 0 indicates a lack of any SLSA level. 1 The build process must be fully scripted/automated and generate provenance. Provenance is metadata about how an artifact was built, including the build process, top-level source, and dependencies. Understanding provenance allows software consumers to make risk-based security decisions. SLSA 1 Provenance doesn\u0026rsquo;t prevent tampering, but it provides a basic level of code origin identification and helps with vulnerability management. 2 Requires version control and a hosted build service that generates authenticated provenance. These additional requirements give software consumers more confidence in the software\u0026rsquo;s origin. At this level, provenance is tamper-resistant to the degree that the build service is trusted. SLSA 2 also provides an easy path to upgrade to SLSA 3. 3 Source and build platforms meet specific standards to guarantee source auditability and provenance integrity, respectively. We envision a certification process where auditors can attest that platforms meet requirements, which consumers can then trust. SLSA 3 provides stronger tamper resistance than earlier levels by preventing specific classes of threats (e.g., cross-build contamination). 4 Requires two-person review of all changes and a hermetic, reproducible build process. Two-person review is an industry best practice for finding errors and preventing malicious behavior. Hermetic builds guarantee the dependency list of the provenance is complete. Reproducible builds, while not strictly required, provide numerous auditability and reliability benefits. Overall, SLSA 4 gives consumers high confidence that the software hasn\u0026rsquo;t been tampered with. Limitations # SLSA can help reduce supply chain threats in software artifacts, but it also has limitations.\nMany artifacts have a large number of dependencies in the supply chain, and the complete dependency graph can be very large. Teams actually doing security work need to identify and focus on important components in the supply chain. This can be done manually, but the workload can be significant. The SLSA level of an artifact is not transitive and dependencies have their own SLSA rating, meaning that an SLSA 4 artifact can be built from SLSA 0 dependencies. Therefore, while the main artifact has strong security, there may still be risks elsewhere. The sum of these risks will help software consumers understand how and where to use SLSA 4 artifacts. While automation of these tasks would help, a full review of the entire graph for every software artifact for every consumer isn\u0026rsquo;t practical. To bridge this gap, auditors and certification bodies can verify and attest that something meets SLSA requirements. This might be particularly valuable for closed-source software. As part of the SLSA roadmap, the SLSA team will also continue to explore how to identify important components, how to determine the overall risk of the entire supply chain, and the role of certification.\nSLSA Implementation # SLSA is a standard, but how do we implement it?\nWe can use the summary table of SLSA Requirements to check one by one and see which security level our current CI/CD workflow is at.\nAre there tools that can better help us check and guide us on how to improve the security level?\nCurrently, there are only a few tools that can achieve this, and the vast majority are limited to GitHub.\nOpenSSF Scorecard is an automated tool from the Open Source Security Foundation (OpenSSF) for checking security metrics of open-source software. It helps open-source maintainers improve their security best practices and helps open-source consumers judge whether their dependencies are secure.\nIt does this by assessing many important projects related to software security and assigning a score of 0-10 to each check. You can use these scores to understand specific areas that need improvement to strengthen the security posture of the project. It can also assess risks introduced by dependencies and make informed decisions about accepting those risks, assessing alternative solutions, or collaborating with maintainers to make improvements.\nOther Tools # slsa-verifier - Verifies build provenance compliant with SLSA standards Sigstore - A new standard for signing, verifying, and securing software Please indicate the author and source when reprinting this article. Please do not use it for any commercial purposes. Welcome to follow the WeChat public account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\n","date":"2023-03-23","externalUrl":null,"permalink":"/en/posts/2023/slsa/","section":"Posts","summary":"This article introduces the concept, purpose, and levels of the SLSA framework, and how to apply SLSA in the software supply chain to improve security. It helps readers understand the importance of SLSA in software development and deployment.","title":"The SLSA Framework and Software Supply Chain Security Protection","type":"posts"},{"content":"","date":"2023-02-26","externalUrl":null,"permalink":"/tags/chatgpt/","section":"标签","summary":"","title":"ChatGPT","type":"tags"},{"content":"随着 DevOps 的流行，越来越多的开发团队正在寻找一些工具来帮助他们更好地完成任务。ChatGPT 是一款基于人工智能的自然语言处理工具，它可以用来帮助开发团队在 DevOps 任务中更加高效地工作。\n本文将探讨如何在 DevOps 任务中使用 ChatGPT。\n一、ChatGPT 简介 # ChatGPT 是一款由 OpenAI 开发的人工智能自然语言处理工具。它可以用于许多不同的应用程序，例如语音识别、自然语言处理、文本生成等。 ChatGPT 使用深度学习技术，可以生成与输入内容相关的文本。它是一款非常强大的工具，可以帮助开发团队更加高效地工作。\n二、ChatGPT 在 DevOps 中的应用 # 在 DevOps 中，开发团队通常需要快速解决问题，并与团队成员和客户进行有效沟通。ChatGPT 可以用来帮助解决这些问题。\n自动化代码审查 开发团队通常需要花费大量时间来进行代码审查。ChatGPT 可以用来自动化这个过程。它可以根据代码库中的样本代码，生成与样本代码风格相似的代码，并对新代码进行审查。这可以帮助开发团队更快地进行代码审查，并减少人为错误的可能性。\n自动化测试 测试是 DevOps 中不可或缺的一部分。ChatGPT 可以用来自动化测试。它可以根据测试用例生成相应的测试代码，并对测试结果进行评估。这可以帮助开发团队更快地进行测试，并减少人为错误的可能性。\n自动化部署 部署是 DevOps 中不可或缺的一部分。ChatGPT 可以用来自动化部署。它可以根据部署规则生成相应的部署代码，并对部署结果进行评估。这可以帮助开发团队更快地进行部署，并减少人为错误的可能性。\n自动化文档生成 文档是 DevOps 中不可或缺的一部分。ChatGPT 可以用来自动化文档生成。它可以根据项目的代码库和测试用例生成相应的文档，并对文档的质量进行评估。这可以帮助开发团队更快地生成文档，并减少人为错误的可能性。\n三、如何使用 ChatGPT # 要使用 ChatGPT，开发团队需要进行以下步骤：\n收集数据 收集数据是使用 ChatGPT 的第一步。开发团队需要收集开发团队需要收集与其任务相关的数据，例如代码库、测试用例、部署规则和文档。这些数据将用于训练 ChatGPT 模型，以生成与任务相关的文本。\n训练 ChatGPT 模型 训练 ChatGPT 模型是使用 ChatGPT 的第二步。开发团队可以使用已有的数据来训练模型，也可以使用迁移学习技术，将已有的 ChatGPT 模型进行微调，以适应其任务的需求。训练好的 ChatGPT 模型将用于生成与任务相关的文本。\n集成 ChatGPT 模型 集成 ChatGPT 模型是使用 ChatGPT 的第三步。开发团队可以将 ChatGPT 模型集成到其 DevOps 工具链中。例如，可以将 ChatGPT 模型集成到自动化代码审查工具、自动化测试工具、自动化部署工具和自动化文档生成工具中。这将使这些工具更加智能化，并帮助开发团队更加高效地工作。\n优化 ChatGPT 模型 优化 ChatGPT 模型是使用 ChatGPT 的第四步。开发团队需要定期监控 ChatGPT 模型的性能，并对其进行优化。例如，可以增加更多的训练数据、调整模型的超参数、增加正则化约束等。这将有助于提高 ChatGPT 模型的准确性和性能，并使其更加适应任务需求。\n四、结论 # 在 DevOps 任务中使用 ChatGPT 可以帮助开发团队更加高效地工作。ChatGPT 可以用来自动化代码审查、自动化测试、自动化部署和自动化文档生成等任务。开发团队需要收集与其任务相关的数据，训练 ChatGPT 模型，并将其集成到其 DevOps 工具链中。开发团队还需要定期监控 ChatGPT 模型的性能，并对其进行优化。这将有助于提高 ChatGPT 模型的准确性和性能，并使其更加适应任务需求。\n对了，本篇文章是由 ChatGPT 生成的，你觉得它写的怎么样，像个油腻的中年人？\n","date":"2023-02-26","externalUrl":null,"permalink":"/posts/2023/chatgpt-for-devops/","section":"Posts","summary":"本文探讨如何在 DevOps 任务中使用 ChatGPT，包括自动化代码审查、测试、部署和文档生成等方面的应用。","title":"如何在 DevOps 任务中使用 ChatGPT?","type":"posts"},{"content":"As the title says, the reason why your Jenkins Controller is getting slower might be due to not following some best practices in Jenkins pipeline writing.\nTherefore, this article mainly introduces some best practices for Jenkins pipelines, aiming to show pipeline authors and maintainers some \u0026ldquo;anti-patterns\u0026rdquo; they might not have been aware of in the past.\nI will try to list all possible Pipeline best practices and provide some concrete examples from practice.\nGeneral Issues # Ensure using Groovy code as glue in your pipeline # Use Groovy code to connect a set of actions rather than as the main functionality of the pipeline.\nIn other words, instead of relying on pipeline features (Groovy or pipeline steps) to drive the build process forward, use individual steps (e.g., sh) to accomplish multiple parts of the build.\nPipelines, as they increase in complexity (amount of Groovy code, number of steps used, etc.), require more resources (CPU, memory, storage) on the controller. Think of the Pipeline as a tool to accomplish the build, not the core of the build.\nExample: Drive the build through its build/test/deploy process using a single Maven build step.\nRunning shell scripts in Jenkins pipeline # Using shell scripts in Jenkins Pipeline can help simplify builds by merging multiple steps into a single stage. Shell scripts also allow users to add or update commands without having to modify each step or stage individually.\nUsing shell scripts in Jenkins Pipeline and its benefits:\nAvoid complex Groovy code in pipelines # For pipelines, Groovy code is always executed on the controller, meaning it consumes controller resources (memory and CPU).\nTherefore, it is crucial to reduce the amount of Groovy code executed by the Pipeline (this includes any methods called on classes imported within the Pipeline). Here are examples of the most common Groovy methods to avoid:\nJsonSlurper: This function (and several other similar functions like XmlSlurper or readFile) can be used to read data from a file on disk, parse the data in that file into a JSON object, and then use JsonSlurper().parseText(readFile(\u0026quot;$LOCAL_FILE\u0026quot;)). This command loads the local file into the controller\u0026rsquo;s memory twice, which will require a lot of memory if the file is large or the command is executed frequently.\nSolution: Instead of using JsonSlurper, use a shell step and return standard output. This shell would look something like this: def JsonReturn = sh label: '', returnStdout: true, script: 'echo \u0026quot;$LOCAL_FILE\u0026quot;| jq \u0026quot;$PARSING_QUERY\u0026quot;'. This will use agent resources to read the file, and $PARSING_QUERY will help parse the file into a smaller size.\nHttpRequest: This command is frequently used to fetch data from external sources and store it in variables. This is not ideal because not only is the request made directly from the controller (which might give erroneous results for things like HTTPS requests if the controller doesn\u0026rsquo;t have certificates loaded), but the response to that request is stored twice.\nSolution: Use a shell step to perform the HTTP request from the agent, for example using tools like curl or wget, depending on the context. If the result must be in the Pipeline afterwards, try filtering the result on the agent side so that only the minimal necessary information has to be sent back to the Jenkins controller.\nReduce repetition of similar pipeline steps # Combine pipeline steps into single steps as much as possible to reduce the overhead caused by the pipeline execution engine itself.\nFor example, if you run three shell steps consecutively, each step must start and stop, requiring the creation and cleanup of connections and resources on the agent and controller.\nHowever, if you put all commands into a single shell step, only one step needs to be started and stopped.\nExample: Instead of creating a series of echo or sh steps, combine them into one step or script.\nAvoid calling Jenkins.getInstance # Using Jenkins.instance or its accessor methods in a Pipeline or shared library is indicative of code abuse within that Pipeline/shared library.\nUsing the Jenkins API from a non-sandboxed shared library means the shared library is both a shared library and a kind of Jenkins plugin.\nGreat care must be taken when interacting with the Jenkins API from a pipeline to avoid serious security and performance issues. If you must use the Jenkins API in your build, the recommended approach is to create a minimal plugin in Java that implements a secure wrapper around the Jenkins API that you want to access using the Pipeline’s Step API.\nUsing the Jenkins API directly from a sandboxed Jenkinsfile means you might have to whitelist methods allowed by the sandbox protection, which can be bypassed by anyone who can modify the pipeline, which is a major security risk. Whitelisted methods run as the system user, with overall administrator permissions, which might give developers more permissions than expected.\nSolution: The best solution is to address the calls being made, but if these calls must be done, then it’s best to implement a Jenkins plugin that can collect the data needed.\nClean up old Jenkins builds # As a Jenkins administrator, deleting old or unnecessary builds can allow the Jenkins controller to run efficiently.\nResources for updates and related versions are reduced when you don\u0026rsquo;t delete old builds. buildDiscarder can be used in each pipeline job to keep a specific number of historical builds.\nUsing Shared Libraries # Don\u0026rsquo;t override built-in pipeline steps # Stay away from custom/overridden pipeline steps as much as possible. Overriding built-in pipeline steps is the process of overriding standard pipeline APIs (like sh or timeout) using shared libraries. This practice is dangerous because pipeline APIs can change at any time, causing custom code to break or give different results than expected.\nTroubleshooting is difficult when custom code breaks due to Pipeline API changes, because even if the custom code hasn\u0026rsquo;t changed, it might not work after an API update.\nTherefore, even if the custom code hasn’t changed, it doesn’t mean it will remain unchanged after an API update.\nFinally, due to the widespread use of these steps throughout the pipeline, if something is coded incorrectly/inefficiently, the results can be catastrophic for Jenkins.\nAvoid large global variable declaration files # Having large variable declaration files can require significant memory with little to no benefit, since the file is loaded for every pipeline regardless of whether the variables are needed.\nIt is recommended to create small variable files containing only the variables relevant to the current execution.\nAvoid very large shared libraries # Using large shared libraries in Pipelines requires checking out a very large file before the Pipeline starts and loading the same shared library for every job currently executing, which leads to increased memory overhead and slower execution times.\nAnswering Other Common Questions # Handling concurrency in pipelines # Try to avoid sharing workspaces across multiple pipeline executions or multiple different pipelines. This practice might lead to unexpected file modifications or workspace renaming in each pipeline.\nIdeally, the shared volume/disk is mounted in a separate location, files are copied from that location to the current workspace, and then, when the build completes, if any updates are done, files can be copied back.\nBuild different containers, creating the needed resources from scratch (cloud-type agents are very suitable for this). Building these containers will ensure the build process starts from scratch every time and is easily repeatable. If building containers doesn’t work, disable concurrency on the pipeline or use a lockable resources plugin to lock the workspace at runtime so that other builds can’t use it while it’s locked.\nWarning: Disabling concurrency at runtime or locking the workspace might cause pipelines to be blocked while waiting for the resources if these resources are arbitrarily locked.\nAlso, note that both these approaches take longer to get build results compared to using unique resources for each job.\nAvoiding NotSerializableException # Pipeline code undergoes CPS transformation to allow pipelines to resume after a Jenkins restart. That is, you can shut down Jenkins or lose connection to the agent while the pipeline is running your script. When it comes back, Jenkins remembers what it was doing, and your pipeline script will resume execution as if it had never been interrupted. An execution technique called \u0026ldquo;Continuous Pipe Style (CPS)\u0026rdquo; plays a key role in resuming the pipeline. However, due to CPS transformation, some Groovy expressions will not work correctly.\nBehind the scenes, CPS relies on being able to serialize the current state of the pipeline, as well as the remainder of the pipeline to be executed. This means using unserializable objects in the pipeline will trigger a NotSerializableException being thrown when the pipeline tries to persist its state.\nFor more details and some examples that might be problematic, see Pipeline CPS Method Mismatches.\nTechniques to ensure pipelines run as expected will be described below.\nEnsuring persistent variables are serializable # During serialization, local variables are captured as part of the pipeline state. This means storing unserializable objects in variables during pipeline execution will result in a NotSerializableException being thrown.\nDon\u0026rsquo;t assign unserializable objects to variables # One strategy is to leverage the fact that unserializable objects always infer their values “just in time” instead of computing their values and storing that value in a variable.\nUsing @NonCPS # If necessary, you can use the @NonCPS annotation to disable CPS transformation for specific methods, whose body will not execute correctly if it is CPS transformed. Note that this also means the Groovy function will have to restart completely, since it is not transformed.\nAsynchronous pipeline steps (such as sh and sleep) are always CPS-transformed and cannot be used inside methods annotated with @NonCPS. Generally, you should avoid using pipeline steps inside methods annotated with @NonCPS.\nPipeline Durability # It is worth noting that changing the durability of the pipeline might result in NotSerializableException not being thrown where they would otherwise be thrown. This is because lowering the pipeline\u0026rsquo;s durability via PERFORMANCE_OPTIMIZED means the pipeline\u0026rsquo;s current state is persisted far less frequently. As a result, the pipeline never attempts to serialize unserializable values, and thus, no exception is thrown.\nThe presence of this note is to inform the user of the root cause of this behavior. Setting the pipeline\u0026rsquo;s durability to performance optimized is not recommended purely to avoid serialization problems.\nhttps://www.jenkins.io/doc/book/pipeline/pipeline-best-practices/ https://www.cloudbees.com/blog/top-10-best-practices-jenkins-pipeline-plugin https://github.com/jenkinsci/pipeline-examples/blob/master/docs/BEST_PRACTICES.md https://devopscook.com/jenkinsfile-best-practices/ ","date":"2023-02-06","externalUrl":null,"permalink":"/en/posts/2023/jenkins-pipeline-best-practices/","section":"Posts","summary":"This article introduces some best practices for Jenkins pipelines, aiming to help developers and operations personnel optimize Jenkins’ performance and maintainability.","title":"Why is my Jenkins Controller getting slower—Possible mistakes you might be making...","type":"posts"},{"content":"Time flies, another year has passed.\nThis year, I want to write a summary to review the important events that happened to me in the past year.\nSince I didn\u0026rsquo;t write a year-end summary in 2021, my memories of 2021 have become blurry. I can only recall some things based on photos and diaries. It seems I cannot miss year-end summaries in the future.\nReviewing 2021 # My personal keyword for 2021 is \u0026ldquo;Final Splendor\u0026rdquo;.\nAt the beginning of the year, I moved. My new home is only a dozen minutes\u0026rsquo; drive from the company. This saved a lot of commuting time, and I was more willing to go to the company on weekends.\nIn April, I forked and started maintaining the open-source project cpp-linter-action for the first time, and attracted another developer to maintain it with me.\nIn July, I traveled to Guanglu Island.\nIn August, my wife became pregnant. Looking back at the photos from 2021, our life as a couple was so carefree and splendid; two people could easily support themselves.\nIn November, I was informed that I was a secondary close contact and was required to quarantine at a hotel. So we were taken to a hotel for a week of quarantine, and then we had to quarantine for another week at home after returning. Looking back now, it\u0026rsquo;s unbelievable!\nReviewing 2022 # My personal keyword for 2022 is \u0026ldquo;Responsibility\u0026rdquo;.\nIn May, with the birth of my daughter, almost all my time outside of work was spent taking care of my family, leaving me with little time for learning and output.\nThe birth of my child made me directly feel the weight of responsibility. Raising a child requires not only time but also money. I truly became a middle-aged man with aging parents and a young child, and I can\u0026rsquo;t afford to collapse for a moment.\nI used to think I could do anything, and the future was bright. Now I feel the heavy responsibility on my shoulders, and I can no longer lie flat.\nFrom June to September, I took paternity leave. As a full-time stay-at-home dad, I sacrificed some rest time during my child\u0026rsquo;s sleep to read and contribute to open source.\nFrom October to December, I returned to my post, and due to the epidemic, I worked from home most of the time. Working during the day, taking care of the child at night, I no longer had large chunks of time for learning and output.\nTherefore, I produced very few articles this year. I published a total of 19 articles on my blog, and only 11 articles on my WeChat official account. Most of this output occurred in the first half of the year before my child was born.\nIn my spare time, my top priority was still open-source projects, which allowed me to learn more. I spent a lot of my spare time on the cpp-linter project. Currently, cpp-linter-action is used (depended on) by more than 100 other projects. I hope to make cpp-linter the preferred code formatter and static analysis tool for all C/C++ projects.\nOutlook for 2023 # Balance work and family. I hope my daughter grows up healthy, sleeps through the night soon, so that Dad can work and study at night. Make progress in English and technology, such as passing the TOEIC Tests, joining a well-known Python organization, and learning cloud technologies. Exceed the number of articles published on my blog and WeChat official account in 2022, aiming for 24 (blog) + 12 (WeChat official account) – not a high bar, right? Maintain good health, resume swimming, football, and exercise, and get my weight back below 160 jin (approximately 160 pounds). Past Year-End Summaries # 2020 Year-End Summary 2019 Year-End Summary 2018 From QA to Development in Five Months\nPlease indicate the author and source when reprinting this article, and do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo; (DevOps Engineer)\n","date":"2022-12-28","externalUrl":null,"permalink":"/en/misc/2022-summary/","section":"Miscs","summary":"Time flies, another year has passed.\nThis year, I want to write a summary to review the important events that happened to me in the past year.\nSince I didn’t write a year-end summary in 2021, my memories of 2021 have become blurry. I can only recall some things based on photos and diaries. It seems I cannot miss year-end summaries in the future.\n","title":"2022 Year-End Summary","type":"misc"},{"content":"When I want to implement [skip ci] or [ci skip] for Jenkins multi-branch pipeline, the existing plugin seems broken.\nJENKINS-35509\nJENKINS-34130\nMy advice: try not to use the Jenkins plugin if possible.\nGood, it\u0026rsquo;s time to implement [skip ci] myself.\nIf you like me used Jenkins shared library, you can create a function like SkipCI from src/org/cicd/utils.groovy, then other jobs can reused this function.\n// src/org/cicd/utils.groovy def SkipCI(number = \u0026#34;all\u0026#34;){ def statusCodeList = [] String[] keyWords = [\u0026#39;ci skip\u0026#39;, \u0026#39;skip ci\u0026#39;] // add more keywords if need. keyWords.each { keyWord -\u0026gt; def statusCode = null if (number == \u0026#34;all\u0026#34;) { statusCode = sh script: \u0026#34;git log --oneline --all | grep \\\u0026#39;${keyWord}\\\u0026#39;\u0026#34;, returnStatus: true } else { statusCode = sh script: \u0026#34;git log --oneline -n ${number} | grep \\\u0026#39;${keyWord}\\\u0026#39;\u0026#34;, returnStatus: true } statusCodeList.add(statusCode) } if (statusCodeList.contains(0)) { return true } else { return false } } Then I can call this function from other jobs.\n// The following is not the complete code, it is just sample code and may not be run successfully. import org.cicd.utils def call(){ pipeline { agent { node { label \u0026#39;linux\u0026#39; } } parameters { booleanParam defaultValue: true, name: \u0026#39;Build\u0026#39;, summary: \u0026#39;Uncheck to skip build.\u0026#39; } def utils = new org.cicd.utils() stage(\u0026#34;Checkout\u0026#34;) { checkout scm // just check the latest commit message. SkipCI = utils.SkipCI(\u0026#39;1\u0026#39;) } stage(\u0026#34;Build\u0026#34;){ when { beforeAgent true expression { return params.Build \u0026amp;\u0026amp; !SkipCI } } steps { script { sh \u0026#34;make build\u0026#34; } } } } } Please let me know if any questions or suggestions.\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-10-09","externalUrl":null,"permalink":"/en/posts/2022/jenkins-skip-ci/","section":"Posts","summary":"This article explains how to implement [skip ci] functionality in Jenkins multi-branch pipelines, allowing you to skip builds based on commit messages.","title":"How to implement [skip ci] for Jenkins multi-branch pipeline","type":"posts"},{"content":" Problem # I have encountered a problem when I ping google.com failed and return some error like \u0026ldquo;Temporary failure in name resolution\u0026rdquo;\nHow to fix # Inside WSL2, create or append file: /etc/wsl.conf\nPut the following lines in the file in order to ensure the your DNS changes do not get blown away\nsudo tee /etc/wsl.conf \u0026lt;\u0026lt; EOF [network] generateResolvConf = false EOF In a cmd windows (!!), run wsl --shutdown\nStart WSL2\nRun the following inside WSL2 (line with search is optional)\nsudo rm -rf /etc/resolv.conf sudo tee /etc/resolv.conf \u0026lt;\u0026lt; EOF search yourbase.domain.local nameserver 8.8.8.8 nameserver 1.1.1.1 EOF In my case, I can remove /etc/resolv.conf and error is \u0026ldquo;rm: cannot remove \u0026lsquo;/etc/resolv.conf\u0026rsquo;: Operation not permitted\u0026rdquo;\n# use following command instead fixed. sudo chattr -a -i /etc/resolv.conf Reference links # https://askubuntu.com/questions/1192347/temporary-failure-in-name-resolution-on-wsl https://askubuntu.com/questions/125847/un-removable-etc-resolv-conf\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-09-27","externalUrl":null,"permalink":"/en/posts/2022/fix-wsl-networking-issue/","section":"Posts","summary":"This article explains how to resolve the “Temporary failure in name resolution” issue in WSL by configuring DNS settings and ensuring persistent changes.","title":"How to fix \"Temporary Failure in name resolution\" in WSL","type":"posts"},{"content":"","date":"2022-09-27","externalUrl":null,"permalink":"/en/tags/wsl/","section":"Tags","summary":"","title":"WSL","type":"tags"},{"content":"","date":"2022-09-16","externalUrl":null,"permalink":"/en/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"If you have a critical machine like your team\u0026rsquo;s CI server that runs on Linux, so you don\u0026rsquo;t want every members in your group to access it.\nModifying this setting /etc/security/access.conf on Linux can do it.\nHow to setup # I commented out the access settings for TEAM A, and add some user accounts can access.\n#+ : (SRV_WW_TEAM_A_CompAdmin) : ALL + : shenx, map, xiar : ALL Be careful not to restrict everyone including yourself.\nIt would be best to allow several people can also access it to prevent any issues to log in with your account or you leave the organization.\nLet\u0026rsquo;s test # Then when I try to use another account not in the list to access this machine and the connection shows closed.\n$ ssh test@devciserver.organization.com test@devciserver.organization.com\u0026#39;s password: Connection closed by 10.84.17.119 port 22 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-09-16","externalUrl":null,"permalink":"/en/posts/2022/restrict-connect-server/","section":"Posts","summary":"This article explains how to restrict access to a critical Linux machine by modifying the /etc/security/access.conf file, allowing only specific users to log in.","title":"Restrict others from login your important Linux machine","type":"posts"},{"content":"This article shares practical experience with C/C++ code formatting and static analysis.\nCurrently, the most widely used tools for C/C++ code formatting and checking are Clang-Format and Clang-Tidy from the LLVM project.\nThe LLVM project is a collection of modular and reusable compiler and toolchain technologies.\nFor C/C++ code formatting and static analysis, we use clang-format and clang-tidy from the LLVM project; together, we call them clang-tools.\nAlthough we have the tools, how to better integrate the tools into our workflow is the focus of this article.\nThe cpp-linter organization was created to provide a one-stop workflow for C/C++ code formatting and static analysis, including:\nEasy download of clang-tools, providing both Docker images and binaries; Easy integration with workflows, including integration with CI and git hooks. Below is how to use clang-tools, download tools, and integrate them into your workflow.\nclang-tools Docker images # If you want to use clang-format and clang-tidy via Docker, the clang-tools project provides Docker images specifically for this purpose.\nJust download the clang-tools Docker image, and you can use clang-format and clang-tidy. For example:\n# Check clang-format version $ docker run xianpengshen/clang-tools:12 clang-format --version Ubuntu clang-format version 12.0.0-3ubuntu1~20.04.4 # Format code (helloworld.c is in the demo directory of the repository) $ docker run -v $PWD:/src xianpengshen/clang-tools:12 clang-format --dry-run -i helloworld.c # Check clang-tidy version $ docker run xianpengshen/clang-tools:12 clang-tidy --version LLVM (http://llvm.org/): LLVM version 12.0.0 Optimized build. Default target: x86_64-pc-linux-gnu Host CPU: cascadelake # Diagnose code (helloworld.c is in the demo directory of the repository) $ docker run -v $PWD:/src xianpengshen/clang-tools:12 clang-tidy helloworld.c \\ -checks=boost-*,bugprone-*,performance-*,readability-*,portability-*,modernize-*,clang-analyzer-cplusplus-*,clang-analyzer-*,cppcoreguidelines-* clang-tools binaries # If you need to use clang-tools binaries, taking Windows as an example, downloading a specific version of clang-tools usually requires installing the large LLVM package first to obtain tools like clang-format \u0026amp; clang-tidy; it\u0026rsquo;s much easier on Linux, where you can use commands to download, but downloading specific versions of clang-format \u0026amp; clang-tidy might require manual download and installation.\nclang-tools-pip provides and supports downloading any specified version of clang-tools executables on Windows, Linux, and macOS via the command line.\nSimply install clang-tools using pip (i.e., pip install clang-tools), and then you can install any version of the executable using the clang-tools command.\nFor example, to install clang-tools version 13:\n$ clang-tools --install 13\nYou can also install it to a specific directory:\n$ clang-tools --install 13 --directory .\nAfter successful installation, you can check the installed version:\n$ clang-format-13 --version clang-format version 13.0.0 $ clang-tidy-13 --version LLVM (http://llvm.org/): LLVM version 13.0.0 Optimized build. Default target: x86_64-unknown-linux-gnu Host CPU: skylake The clang-tools CLI also provides other options, such as automatically creating links for you. Check its CLI documentation for help.\nIntegrating clang-tools into your workflow # We\u0026rsquo;ve introduced two convenient ways to download clang-tools: Docker images and binaries. How to integrate them into your workflow is our ultimate concern.\nMainstream IDEs can use clang-format and clang-tidy via plugins, but this has problems:\nDifferent developers may use different IDEs, requiring a high learning cost to install plugins on different IDEs; It cannot guarantee that all developers will run Clang-Format or Clang-Tidy when submitting code. So how can we ensure that Clang-Format or Clang-Tidy is run every time code is submitted?\ncpp-linter-action provides CI checks. If unformatted code or diagnostic errors are found, the CI will fail, preventing code that hasn\u0026rsquo;t passed code checks from being merged into the main branch; cpp-linter-hooks uses git hooks to automatically run clang-format and clang-tidy when code is submitted. If the code doesn\u0026rsquo;t meet the standards, the submission fails, and a prompt appears with automatic formatting. cpp-linter-action for Automatic Checks Before Code Merge # If you\u0026rsquo;re using GitHub, we highly recommend using the cpp-linter-action GitHub Action.\nCurrently, cpp-linter does not have API integration with SCMs other than GitHub.\nHere are some of its key features:\nResults are displayed using Annotations and Thread Comments. Supports GitHub\u0026rsquo;s public and private repositories. Supports most Clang versions. Many other optional-inputs are available. To use this Action, simply create a cpp-linter.yml file under .github/workflows/, with the following content:\nOf course, you can also add the following configuration to an existing Workflow, such as build.\nname: cpp-linter on: pull_request: types: [opened, reopened] push: jobs: cpp-linter: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: cpp-linter-action@v1 id: linter env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: style: file - name: Fail fast?! if: steps.linter.outputs.checks-failed \u0026gt; 0 run: | echo \u0026#34;Some files failed the linting checks!\u0026#34; exit 1 If unformatted code or static code analysis errors are found, the CI workflow will fail, and the following comments will appear; annotations are enabled by default.\nIf the Thread Comment option (thread-comments: true) is enabled, the following error comments will be automatically added to the Pull Request.\nMany well-known projects already depend on this Action, and its ranking in the GitHub Marketplace is very high; you can use it with confidence.\nNote: The annotations and comment features currently only support GitHub. This project plans to support other SCMs like Bitbucket and GitLab in the future.\ncpp-linter-hooks for Automatic Checks on Code Submission # cpp-linter-hooks uses git hooks for automatic checks on code submission; this method is not limited to any SCM.\nJust add a .pre-commit-config.yaml configuration file to your project repository, then add the cpp-linter-hooks hook to .pre-commit-config.yaml, as follows:\n.pre-commit-config.yaml is the default configuration file for the pre-commit framework.\nInstall pre-commit\npip install pre-commit Create the .pre-commit-config.yaml configuration file, setting it as follows:\nrepos: - repo: https://github.com/cpp-linter-hooks rev: v0.2.1 hooks: - id: clang-format args: [--style=file] # to load .clang-format - id: clang-tidy args: [--checks=.clang-tidy] # path/to/.clang-tidy Here, file refers to .clang-format. clang-format supports LLVM, GNU, Google, Chromium, Microsoft, Mozilla, and WebKit encoding formats by default. If you need special settings, you can create a .clang-format configuration file in the root directory of your repository. Similarly, if the default static analysis settings do not meet your requirements, you can create a .clang-tidy configuration file in the root directory of the repository.\nFor more configurations, see the README.\nInstall the git hook script\n$ pre-commit install pre-commit installed at .git/hooks/pre-commit After this, every git commit will automatically run clang-format and clang-tidy.\nIf unformatted code or static analysis errors are detected, the following error message will appear:\nclang-format output\nclang-format.............................................................Failed - hook id: clang-format - files were modified by this hook And it will automatically format your code:\n--- a/testing/main.c +++ b/testing/main.c @@ -1,3 +1,6 @@ #include \u0026lt;stdio.h\u0026gt; -int main() {for (;;) break; printf(\u0026#34;Hello world!\\n\u0026#34;);return 0;} - +int main() { + for (;;) break; + printf(\u0026#34;Hello world!\\n\u0026#34;); + return 0; +} clang-tidy output\nclang-tidy...............................................................Failed - hook id: clang-tidy - exit code: 1 418 warnings and 1 error generated. Error while processing /home/ubuntu/cpp-linter-hooks/testing/main.c. Suppressed 417 warnings (417 in non-user code). Use -header-filter=.* to display errors from all non-system headers. Use -system-headers to display errors from system headers as well. Found compiler error(s). /home/ubuntu/cpp-linter-hooks/testing/main.c:3:11: warning: statement should be inside braces [readability-braces-around-statements] for (;;) break; ^ { /usr/include/stdio.h:33:10: error: \u0026#39;stddef.h\u0026#39; file not found [clang-diagnostic-error] #include \u0026lt;stddef.h\u0026gt; ^~~~~~~~~~ Conclusion # CI or git hooks?\nIf your team is already using pre-commit, we recommend using git hooks. Just add cpp-linter-hooks. If you don\u0026rsquo;t want to introduce pre-commit, you can use CI for checking. Of course, you can also choose both. The cpp-linter organization is an open-source project I created and maintained by Brendan Doherty and me as the primary contributors. We are developers who pursue code quality and strive to build the best software. I\u0026rsquo;ve spent a lot of my free time on it, but I\u0026rsquo;ve also learned a lot. I\u0026rsquo;ll share some interesting implementation methods later.\nCurrently, cpp-linter provides the best C/C++ Linter Action and clang-tools on GitHub. We welcome your use, and you can provide feedback on any suggestions or problems through Issues.\n","date":"2022-08-23","externalUrl":null,"permalink":"/en/posts/2022/cpp-linter/","section":"Posts","summary":"This article introduces tools and workflows for C/C++ code formatting and static analysis, focusing on the use and integration of clang-tools.","title":"C/C++ Code Formatting and Static Analysis—A One-Stop Workflow with Cpp Linter","type":"posts"},{"content":"","date":"2022-07-28","externalUrl":null,"permalink":"/en/tags/gpg/","section":"Tags","summary":"","title":"GPG","type":"tags"},{"content":" First, List your GPG key # # If folders does not exist will create be related automatically $ gpg --list-keys gpg: directory \u0026#39;/home/ubuntu/.gnupg\u0026#39; created gpg: keybox \u0026#39;/home/ubuntu/.gnupg/pubring.kbx\u0026#39; created gpg: /home/ubuntu/.gnupg/trustdb.gpg: trustdb created $ gpg --list-key Second, generate GPG key # $ gpg --gen-key gpg (GnuPG) 2.2.19; Copyright (C) 2019 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Note: Use \u0026#34;gpg --full-generate-key\u0026#34; for a full featured key generation dialog. GnuPG needs to construct a user ID to identify your key. Real name: shenxianpeng Email address: xianpeng.shen@gmail.com You selected this USER-ID: \u0026#34;shenxianpeng \u0026lt;xianpeng.shen@gmail.com\u0026gt;\u0026#34; Change (N)ame, (E)mail, or (O)kay/(Q)uit? O We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. gpg: key 5F72A7D009FC935A marked as ultimately trusted gpg: directory \u0026#39;/home/ubuntu/.gnupg/openpgp-revocs.d\u0026#39; created gpg: revocation certificate stored as \u0026#39;/home/ubuntu/.gnupg/openpgp-revocs.d/F0F32CB8C65536ECE0187EAD5F72A7D009FC935A.rev\u0026#39; public and secret key created and signed. pub rsa3072 2022-07-28 [SC] [expires: 2024-07-27] F0F32CB8C65536ECE0187EAD5F72A7D009FC935A uid shenxianpeng \u0026lt;xianpeng.shen@gmail.com\u0026gt; sub rsa3072 2022-07-28 [E] [expires: 2024-07-27] Third, get your public key content # # get with your email gpg --armor --export xianpeng.shen@gmail.com # or with your pub key id pg --armor --export F0F32CB8C65536ECE0187EAD5F72A7D009FC935A # # public key content output below # Fourth, add the public key content (GPG keys) to GitHub # Open GitHub, Settings -\u0026gt; SSH and GPG keys -\u0026gt; New GPG key\nThen when you commit with command git commit -S -m \u0026quot;Your commit message\u0026quot;, then a verified signature will show on GitHub\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-07-28","externalUrl":null,"permalink":"/en/posts/2022/git-create-gpg-keys/","section":"Posts","summary":"This article explains how to create GPG keys, export the public key, and add it to GitHub for signing commits.","title":"How to create GPG keys and add to GitHub","type":"posts"},{"content":" Hi! I\u0026#39;m Xianpeng Shen Welcome Welcome to my personal site I write blog posts, develop open source software, and contribute to open source projects. Based in Vilnius, Lithuania Vilnius, Lithuania Explore · Learn · Share Languages I use daily: Chinese · English · Lithuanian (beginner) Open Source Open Source Contributions Welcome I maintain several open-source organizations and tools and they are widely used by developers and teams around the world. All kinds of contributions and feedback are welcome. View Projects. Get in Touch Let\u0026#39;s Connect Collaboration · Writing · Discussion xianpeng.shen@gmail.com ","date":"2022-06-13","externalUrl":null,"permalink":"/en/about/","section":"","summary":" Hi! I'm Xianpeng Shen Welcome Welcome to my personal site I write blog posts, develop open source software, and contribute to open source projects. Based in Vilnius, Lithuania Vilnius, Lithuania Explore · Learn · Share Languages I use daily: Chinese · English · Lithuanian (beginner) Open Source Open Source Contributions Welcome I maintain several open-source organizations and tools and they are widely used by developers and teams around the world. All kinds of contributions and feedback are welcome. View Projects. Get in Touch Let's Connect Collaboration · Writing · Discussion xianpeng.shen@gmail.com ","title":"About","type":"page"},{"content":"有幸赶上了公司的政策变化，我有 12 周的陪产假来做全职奶爸，照顾家人的同时希望挤出时间来学习，毕竟在职期间很有有机会能有近 3 个月的假期。\n照顾孩子兼顾学习真不是一件轻松的事情，我尽力兼顾了两者，做了如下的流水账记录。\n计划 # 我知道 12 周会很快过去，就在已经快要过去了 2 周时我决定有计划的来完成一些任务，比如：\n完成《代码整洁之道》、《重构》以及《动手学习深度学习这三本书》的阅读和豆瓣评论 为 pre-commit 写一个 clang-format 和 clang-tidy 的 cpp-linter-hooks 完成每个月 15 节英语课以及 3~4 的体育锻炼（游泳和足球） 找一个可以作为长期业余参与的开源项目，例如 pytest，tox，pypa。 也就是从休假的第 2 周开始，我开始记录每周的完成的小任务。\n周报 # 第 11~12 周（8.15 - 8.28）- 最后两周\n时间过得太快了，不知不觉就是假期的最后两周了。\nValidate VMs 更新关于 setuptools_scm 的使用。 在 cpp-linter Org 中做一些项目的修改和代码评审 第 10 周（8.8 - 8.14）- 最后三周\nValidate VMs 总结关于 setuptools_scm 的使用。 第 9 周（8.1 - 8.7）- 时间过得真快，转眼就到了陪产假的最后 4 周了\nDraft 一篇关于参与开源的文章 做了一些工作，以及 Troubleshooting support；参与开源项目和 Code Review。 游泳以及完成《重构》书评 第 8 周（7.25 - 7.31）\n做 cpp-linter 里的一些项目的更新，做 Artifactory 的迁移和测试。 这周计划做的事情世纪没完成几样，比如写文章和看书。 游泳 第 7 周（7.18 - 7.24）\n发现 Python 真是入门容易学好难\u0026hellip; 初步学了一下 tox 和 mypy，会之后的 project 中尝试使用。 重构了代码，本打算并把 Code Coverage 写到 100 %，但没实现，pytest 还需要继续学。 在琢磨一个有意思的可以作为长期业余时间来做的项目，目前有个模糊的雏形，先试试看 《重构》书没怎么读，周六游泳可以继续 第 6 周（7.11 - 7.17）\n完成了 cpp-linter-hooks 功能的开发并把它迁移到 cpp-linter org 下面。 创建了 .github 仓库，对于 org 这是一个很神奇的残酷，玩法很多，还在陆续探索中。 终于把读 Code Clean 的书评交了，还需继续读完成任务。 周五发了一篇公众号文章，是之前写的，整理终于发出了，这是 3 个月以来的第一次更新。 周日去游一次泳。 第 5 周（7.4 - 7.10）\n上周主要是抽空写 clang-tools-pip 和 cpp-linter-hooks 这两个功能，目前完成大概 70%，预计本周可以基本结束。 工作上也花了点时间，修复了之前写的 pipeline 的几个问题 上周开始读《重构》了，但没多少时间花在读书上，没读几页。一致想更新公号文章，可惜挺花时间的 上周日游泳也没游，因为脖子坏了，可能是喂奶低头造成的 :( 第 3 - 4 周：\n《代码整洁之道》 P56 - P130 在实现 cpp-linter-hooks 之前需要实现 install clang-tools with pip, 因此我创建了 clang-tools-pip 去市游泳馆游了一次泳，第二次本已约好但临时有事取消了 第 2 周：\n《代码整洁之道》 P26 - P56 创建了 cpp-linter-hooks 仓库，学习别人的代码 计划本周末和乔教练去游泳 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-06-13","externalUrl":null,"permalink":"/posts/2022/vacation-weekly-report/","section":"Posts","summary":"本文记录了我在陪产假期间的学习和工作安排，包括阅读书籍、参与开源项目、体育锻炼等，分享了如何在照顾家庭的同时保持学习和成长。","title":"12 周的陪产假我一刻都没闲着","type":"posts"},{"content":"For a long time, many programmers have encountered various inconveniences in developing on Windows:\nFor example, setting up a development environment isn\u0026rsquo;t as simple as entering a single command to install various commands and packages like on Linux and Mac. Therefore, some programmers switched to Mac for development, while others simply used Linux as their development machine. Only those who had to use Windows as their development environment remained on Windows, making do.\nUntil the arrival of WSL, or more accurately, WSL2.\nThe combination of WSL + VS Code + Docker Desktop, these three musketeers, has made developing on Windows a truly enjoyable experience for me.\nWhat is WSL # WSL stands for Windows Subsystem for Linux. It\u0026rsquo;s a feature of the Windows 10 operating system that allows you to run a Linux file system, Linux command-line tools, and GUI applications directly on Windows, running alongside traditional Windows desktop and applications.\nThe minimum required version of WSL is Windows 10 version 1903 or later.\nWSL is specifically designed for developers who need to use Linux, such as web developers, those working on open-source projects, and developers who need to deploy to Linux server environments.\nWSL is suitable for those who prefer using Bash, common Linux tools (sed, awk, etc.), and Linux-first frameworks (Ruby, Python, etc.), while also enjoying using Windows as their productivity tool.\nLet\u0026rsquo;s take a look at the advantages of WSL compared to virtual machines.\nAdvantages of Using WSL # Compared to a full virtual machine, WSL requires fewer resources (CPU, memory, and storage). You can use Windows and Linux simultaneously and access your Windows files from Linux, providing a better interactive experience. Most importantly, using WSL combined with VS Code + Docker provides the perfect Linux experience while also offering the productivity of Windows. This is something that virtual machines or Linux operating systems alone cannot achieve. Macs can, but not everyone is suitable for Macs. Now let\u0026rsquo;s talk about how to install WSL and use it with VS Code + Docker.\nInstalling WSL # wsl --install This command will enable the required optional components, download the latest Linux kernel, set WSL 2 as your default, and install a Linux distribution for you (Ubuntu by default).\n# View the list of available distributions C:\\Users\\xshen\u0026gt;wsl --list --online The following is a list of valid distributions that can be installed. Install using \u0026#39;wsl --install -d \u0026lt;Distro\u0026gt;\u0026#39;. NAME FRIENDLY NAME Ubuntu Ubuntu Debian Debian GNU/Linux kali-linux Kali Linux Rolling openSUSE-42 openSUSE Leap 42 SLES-12 SUSE Linux Enterprise Server v12 Ubuntu-16.04 Ubuntu 16.04 LTS Ubuntu-18.04 Ubuntu 18.04 LTS Ubuntu-20.04 Ubuntu 20.04 LTS To install other distributions, such as Debian:\nwsl --install -d Debian For more details, please refer to the official documentation\nWSL + VS Code Demonstration # The following uses Ubuntu as an example to demonstrate downloading code and opening the code directory using VS Code.\nI have already opened the installed Ubuntu operating system via WSL.\nFirst, download the code:\nubuntu@CN-L-2680:~$ git clone https://github.com/cue-lang/cue.git --depth 1 Cloning into \u0026#39;cue\u0026#39;... remote: Enumerating objects: 1833, done. remote: Counting objects: 100% (1833/1833), done. remote: Compressing objects: 100% (1502/1502), done. remote: Total 1833 (delta 238), reused 1161 (delta 148), pack-reused 0 Receiving objects: 100% (1833/1833), 1.53 MiB | 5.39 MiB/s, done. Resolving deltas: 100% (238/238), done. Then go to the downloaded code directory and enter code .\nubuntu@CN-L-2680:~$ cd cue/ ubuntu@CN-L-2680:~/cue$ code . # The VS Code Server will only be installed the first time Installing VS Code Server for x64 (dfd34e8260c270da74b5c2d86d61aee4b6d56977) Downloading: 100% Unpacking: 100% Unpacked 2341 files and folders to /home/ubuntu/.vscode-server/bin/dfd34e8260c270da74b5c2d86d61aee4b6d56977. The first time, it will automatically download and install the VS Code Server. After installation, it will automatically launch the VS Code on your local machine and open the code directory on Ubuntu. The whole process is very smooth.\nAfter that, you can use the command-line apt-get command in VS Code to install any software you need. It\u0026rsquo;s really awesome!\nYou need to install the Microsoft-made Remote - WSL extension on your local VS Code; Additionally, if you need to use Docker in WSL, you need to pre-install Docker Desktop on Windows.\nPlease indicate the author and source when reprinting this article. Please do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2022-04-19","externalUrl":null,"permalink":"/en/posts/2022/wsl/","section":"Posts","summary":"This article introduces how to develop on Windows using WSL, VS Code, and Docker Desktop. It provides detailed installation and configuration steps, as well as the advantages and experience of using these tools.","title":"Developing on Windows Just Got Awesome using WSL + VS Code + Docker Desktop - Worth a Try","type":"posts"},{"content":"","date":"2022-04-19","externalUrl":null,"permalink":"/en/tags/vscode/","section":"Tags","summary":"","title":"VSCode","type":"tags"},{"content":"","date":"2022-04-19","externalUrl":null,"permalink":"/en/tags/windows/","section":"Tags","summary":"","title":"Windows","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/en/tags/containerd/","section":"Tags","summary":"","title":"Containerd","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/en/tags/cri/","section":"Tags","summary":"","title":"CRI","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/en/tags/cri-o/","section":"Tags","summary":"","title":"CRI-O","type":"tags"},{"content":"Since Docker ignited the explosive growth of container usage, more and more tools and standards have emerged to help manage and utilize this containerization technology. Simultaneously, this has led to a plethora of confusing terminology.\nFor example, Docker, containerd, CRI, CRI-O, OCI, and runc. This article will introduce these terms you\u0026rsquo;ve heard of but may not understand, and explain how the container ecosystem works together.\nThe Container Ecosystem # The container ecosystem is composed of many exciting technologies, a large amount of specialized terminology, and large companies vying for dominance.\nFortunately, these companies occasionally find common ground, cooperating to agree on standards that improve the ecosystem\u0026rsquo;s interoperability across different platforms and operating systems, reducing reliance on any single company or project.\nThis diagram shows how Docker, Kubernetes, CRI, OCI, containerd, and runc fit together in this ecosystem.\nThe workflow is simple:\nTools like Docker and Kubernetes call a container runtime (CRI) such as containerd or CRI-O when running a container. The container runtime handles the actual work of creating, running, and destroying containers. Docker uses containerd as its runtime; Kubernetes supports multiple container runtimes, including containerd and CRI-O. These container runtimes adhere to the OCI specification and use runc to interact with the operating system kernel to create and run containers. Let\u0026rsquo;s introduce the terms and specifications mentioned in the diagram.\nDocker # Let\u0026rsquo;s start with the familiar Docker, as it\u0026rsquo;s the most popular tool for managing containers. For many, the name \u0026ldquo;Docker\u0026rdquo; is synonymous with \u0026ldquo;container.\u0026rdquo;\nDocker initiated the container revolution, creating a user-friendly tool for handling containers, also called Docker. The key points to understand here are:\nDocker is not the only container contender. Containers are no longer inextricably linked to the name Docker. In the current landscape of container tools, Docker is just one among many. Other notable container tools include: Podman, LXC, containerd, and Buildah.\nTherefore, believing containers are solely about Docker is inaccurate and incomplete.\nDocker Components # Docker simplifies building container images, pulling images from Docker Hub, and creating, starting, and managing containers. In reality, when you run a container with Docker, it\u0026rsquo;s actually run through the Docker daemon, containerd, and runc.\nTo achieve this, Docker comprises these projects (among others, but these are the major ones):\ndocker-cli: A command-line interface (CLI) for interacting with commands like docker pull, build, run, and exec. containerd: A daemon that manages and runs containers. It pushes and pulls images, manages storage and networking, and supervises container operation. runc: A low-level container runtime (the actual component that creates and runs containers). It includes libcontainer, a Go-based native implementation for creating containers. Docker Images # Many refer to Docker images, which are actually images packaged in the Open Container Initiative (OCI) format.\nTherefore, if you pull an image from Docker Hub or another registry, you should be able to use it with Docker commands, on a Kubernetes cluster, with the Podman tool, or with any other tool supporting the OCI image format specification.\nDockershim # Kubernetes included a component called dockershim to support Docker. Because Docker predates Kubernetes and didn\u0026rsquo;t implement CRI, dockershim was necessary to integrate Docker into Kubernetes. As containerization became an industry standard, the Kubernetes project added support for additional runtimes, supporting container execution via the Container Runtime Interface (CRI). Thus, dockershim became an anomaly in the Kubernetes project. The dependence on Docker and dockershim permeated various tools and projects within the Cloud Native Computing Foundation (CNCF) ecosystem, resulting in fragile code.\nIn April 2022, dockershim was completely removed from Kubernetes 1.24. Kubernetes discontinued direct Docker support, opting to utilize only container runtimes implementing its Container Runtime Interface. This likely means using containerd or CRI-O. This doesn\u0026rsquo;t imply Kubernetes cannot run Docker-formatted containers. containerd and CRI-O can both run Docker-formatted (actually OCI-formatted) images; they simply don\u0026rsquo;t require the docker commands or the Docker daemon.\nContainer Runtime Interface (CRI) # The CRI (Container Runtime Interface) is a Kubernetes API for controlling different runtimes used to create and manage containers. It makes Kubernetes more adaptable to various container runtimes. It\u0026rsquo;s a plugin interface, meaning any compliant container runtime can be used by Kubernetes.\nThe Kubernetes project doesn\u0026rsquo;t need to manually add support for each runtime. The CRI API describes how Kubernetes interacts with each runtime; the runtime determines how to actually manage containers, provided it adheres to the CRI API.\nYou can use containerd or CRI-O to run your containers because both runtimes implement the CRI specification.\ncontainerd # containerd is a high-level container runtime from Docker that implements the CRI specification. It was separated from the Docker project and later donated to the Cloud Native Computing Foundation (CNCF) to provide the container community with a foundation for creating new container solutions.\nDocker itself internally uses containerd; it\u0026rsquo;s installed when you install Docker.\ncontainerd implements the Kubernetes Container Runtime Interface (CRI) via its CRI plugin. It manages the entire container lifecycle, from image transfer and storage to container execution, monitoring, and networking.\nCRI-O # CRI-O is another high-level container runtime that implements the Container Runtime Interface (CRI) and can use OCI (Open Container Initiative)-compliant runtimes. It\u0026rsquo;s an alternative to containerd.\nCRI-O originated from RedHat, IBM, Intel, SUSE, Hyper, and others. It was created from scratch as a container runtime for Kubernetes, providing the ability to start, stop, and restart containers, similar to containerd.\nOpen Container Initiative (OCI) # The Open Container Initiative (OCI) is a group of technology companies aiming to create open industry standards around container images and runtimes. They maintain specifications for the container image format and how containers should run.\nThe idea behind OCI is that you can choose different runtimes that comply with the specification, each with different underlying implementations.\nFor instance, you might have an OCI-compliant runtime for your Linux hosts and another for your Windows hosts. This is the advantage of having a standard that can be implemented by many different projects. This \u0026ldquo;one standard, multiple implementations\u0026rdquo; approach is widely used, from Bluetooth devices to Java APIs.\nrunc # runc is a lightweight, universal runtime for containers. It adheres to the OCI specification and is the lowest-level component implementing the OCI interface; it interacts with the kernel to create and run containers.\nrunc provides all low-level functionality for containers, interacting with existing low-level Linux features like namespaces and cgroups, using these to create and run container processes.\nSeveral alternatives to runc exist:\ncrun: A container runtime written in C (in contrast, runc is written in Go). kata-runtime from the Katacontainers project, which implements the OCI specification as separate lightweight virtual machines (hardware virtualization). Google\u0026rsquo;s gVisor, which creates containers with their own kernel. It implements OCI in its runtime, called runsc. runc is a tool for running containers on Linux, meaning it can run on Linux, bare metal, or within a virtual machine.\nOn Windows, it\u0026rsquo;s slightly different. The equivalent of runc is Microsoft\u0026rsquo;s Host Compute Service (HCS), which includes a tool called runhcs, itself a fork of runc, also implementing the Open Container Initiative specification.\nSummary # This article demonstrates that Docker is just a small part of the container ecosystem. A set of open standards allows for interchangeable implementations.\nThis is why standards like CRI and OCI exist, along with projects like containerd, runc, and CRI-O.\nNow you understand the intriguing and slightly complex world of containers. Next time, avoid saying you\u0026rsquo;re using \u0026ldquo;Docker containers\u0026rdquo;! :)\nReferences # The differences between Docker, containerd, CRI-O and runc\n","date":"2022-03-29","externalUrl":null,"permalink":"/en/posts/2022/container-ecosystem/","section":"Posts","summary":"This article introduces the key components and standards in the container ecosystem, such as Docker, containerd, CRI, CRI-O, OCI, and runc, explaining their relationships and how they work together.","title":"Docker, containerd, CRI, CRI-O, OCI, runc Explained and How They Work Together","type":"posts"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/en/tags/runc/","section":"Tags","summary":"","title":"Runc","type":"tags"},{"content":" Introduction # In organizations, using LDAP login very common way for users to log in with their credentials.\nHow to configure LDAP # Preparation: Installed LDAP Jenkins plugin\nAbout how to configure it, you can refer to Jenkins LDAP Plugin documentation https://plugins.jenkins.io/ldap/\nThis is my LDAP configuration just for testing.\nCan not login with LDAP? # Sometimes, for some reason, there is a problem with your organization\u0026rsquo;s LDAP server and you can\u0026rsquo;t log in to Jenkins using LDAP, but you need to use Jenkins now.\nYou can disable LDAP authentication by changing config.xml.\n# Login, cd to jenkins server folder $ cd /var/lib/jenkins/ # Highly rememend you to backup config.xml before making any change !!! # If you don\u0026#39;t backup config.xml, you\u0026#39;ll lost your LDAP configration after reboot service. $ cp config.xml config.xml.bak # Modify config.xml from \u0026lt;useSecurity\u0026gt;true\u0026lt;/useSecurity\u0026gt; # To \u0026lt;useSecurity\u0026gt;false\u0026lt;/useSecurity\u0026gt; # Restart Jenkins server sudo service jenkins restart Then you can log into the Jenkins server again.\nOnce your organization\u0026rsquo;s LDAP works again, you can replace config.xml with your backup config.xml file. Then your users can continue to log in via LDAP.\n","date":"2022-03-15","externalUrl":null,"permalink":"/en/posts/2022/jenkins-ldap-configuration/","section":"Posts","summary":"This article explains how to enable and configure LDAP authentication in Jenkins, including how to disable it temporarily if needed.","title":"How to enable, configure and disable Jenkins LDAP","type":"posts"},{"content":"","date":"2022-03-15","externalUrl":null,"permalink":"/en/tags/ldap/","section":"Tags","summary":"","title":"LDAP","type":"tags"},{"content":"","date":"2022-03-09","externalUrl":null,"permalink":"/en/tags/fork/","section":"Tags","summary":"","title":"Fork","type":"tags"},{"content":" Background # Developers, and even companies, may encounter the following problems:\nA repository was initially forked, and subsequently underwent significant modifications, diverging from the parent repository in both functionality and programming language. Because it\u0026rsquo;s a forked repository, every Pull Request defaults to the parent repository\u0026rsquo;s branch, leading to accidental PRs to the parent repository. Contributors have contributed to and used the forked repository, but their contributions and downstream usage are not visible, hindering project growth. Due to these issues, developers may consider separating from the parent repository. However, GitHub currently doesn\u0026rsquo;t provide an Unfork/Detach function.\nWhile deleting and recreating the project achieves separation, it results in the loss of crucial information such as Issues, Wikis, and Pull Requests.\nUnforking is fundamentally different from leveraging Apache SkyWalking through a certain engine under a certain section. It\u0026rsquo;s more akin to the divergence of Hudson and Jenkins.\nSolution # After investigation and testing, the most viable solution is to contact GitHub Support. The specific steps are as follows:\nOpen this link: https://support.github.com/contact?tags=rr-forks Select your account or organization, then enter \u0026ldquo;unfork\u0026rdquo; in the Subject field. A virtual assistant will automatically appear; select the virtual assistant. Follow the virtual assistant\u0026rsquo;s prompts and select the appropriate answers (partial screenshot below). The conversation will be automatically transcribed. Send the request and wait for Support to process it (it shouldn\u0026rsquo;t take too long). It\u0026rsquo;s important to note that if your repository has been forked by others and you want to retain the fork history of your child repository after separating from the parent repository, you should select \u0026ldquo;Bring the child forks with the repository\u0026rdquo;.\nAlternatively, using commands like git clone --bare and git push --mirror preserves the complete Git history, but not Issues, Wikis, or Pull Requests.\nHopefully, this helps those who need it.\nReferences # Delete fork dependency of a GitHub repository Unfork a Github fork without deleting Please indicate the author and source when reprinting this article. Do not use it for any commercial purposes. Welcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;\n","date":"2022-03-09","externalUrl":null,"permalink":"/en/posts/2022/unfork-github-repo/","section":"Posts","summary":"This article describes how to separate a forked repository from its parent repository using GitHub Support, avoiding data loss from deletion and reconstruction, and helping developers better manage forked repositories.","title":"Reliably Unforking a GitHub Repository Without Deletion and Reconstruction","type":"posts"},{"content":"","date":"2022-03-06","externalUrl":null,"permalink":"/tags/groovy/","section":"标签","summary":"","title":"Groovy","type":"tags"},{"content":"在使用 Jenkins 和 Groovy 越久，我就这样的疑问：\nGroovy 到底是什么语言？ Groovy 有哪些特性？ Groovy 和 Java 有什么区别？ Groovy 和 Java 如何选择？ Groovy 在整个开发语言中占什么位置？要不要学？\n本篇我的学习结果的分享，希望也能帮助你解答以上的这些问题。\n什么是 Groovy # Apache Groovy 是一种强大的、可选类型的动态语言，具有静态类型和静态编译功能，适用于 Java 平台，旨在通过简洁、熟悉且易于学习的语法提高开发人员的工作效率。 它与任何 Java 程序顺利集成，并立即为你的应用程序提供强大的功能，包括脚本功能、特定领域语言创作、运行时和编译时元编程和函数式编程。\nGroovy 的特性 # 翻译官方的说法，Groovy 有以下六大特性。\n平坦的学习曲线 - 简洁、易读且富有表现力的语法，Java 开发人员易于学习 强大的功能 - 闭包、构建器、运行时和编译时元编程、函数式编程、类型推断和静态编译 流畅的 Java 集成 - 与 Java 和任何第三方库无缝、透明地集成和互操作 领域特定语言 - 灵活可延展的语法，先进的集成和定制机制，在你的应用程序中集成可读的业务规则 充满活力和丰富的生态系统 - Web 开发、响应式应用程序、并发/异步/并行库、测试框架、构建工具、代码分析、GUI 构建 脚本和测试胶水 - 非常适合编写简洁和可维护的测试，以及所有构建和自动化任务 Groovy 和 Java 的区别 # Groovy 是一种编程语言，也支持脚本语言；Java 是一种面向对象的编程语言。 Groovy 支持多方法，运行方法的选择将在运行时选择；Java 提供多方法的声明，在编译时而不是运行时选择。 Groovy 中，自动资源管理机制是不存在的，静态的、匿名的内部类；Java 从 Java7 版本开始就提供了自动资源管理，在内部静态类或匿名类方面占上风。 Groovy 中，有一些函数式编程特性，如 Lambda 函数，函数式接口；而 Java 从 JDK 8 版本开始就有 Lambda 函数、函数式接口和许多其他的流和并行操作功能。 Groovy 可以用单引号或双引号格式定义和声明字符串和字符字面；Java 只有双引号格式来声明和定义字符串字面或字符字面。 Groovy 中所有东西都是一个对象，并且只使用对象。因此，不存在自动装箱或拆箱的概念，也不存在基元的转换；相反，Java 有基元数据类型和 Wrapper 类，可以显式或隐式地进行自动装箱和自动拆箱。 Groovy 中，数据类型的自动拓宽和缩小有很多宽广的范围，有很多转换；而Java在数据类型的缩小或拓宽方面有限制。 Groovy 对其所有类型的类成员或数据都有一个默认的访问修饰符；而Java的默认访问级别是包级，取决于类成员的类型。 Groovy 在其类中自动生成 getters 和 setter 来访问和修改类的成员；而在 Java 中，它们必须在类中明确提到访问修饰符。 Groovy 有 Groovy beans；而Java有Java beans。 Groovy 也被称为 Java 的超集，因为 Java 程序可以在 Groovy 环境中运行。反过来并不一定。 Groovy 在定义类型时有更简单的语法，只需使用 def 来声明一个变量；Java有不同类型的类型名称来声明变量或类的任何方法或成员。 Groovy 不要求任何主方法或方法的入口点来运行类或任何程序；而 Java 则要求类中的 main 方法来运行程序。 Groovy 和 Java 如何选择 # 如果可扩展性和性能至关重要并且公司开发 Web 应用程序，当然是 Java。比如电商、银行、金融、国防、医疗保健等领域的大公司都会选择 Java，因为它经过时间证明，通常用于开发复杂的企业项目。 Groovy 可以用来编排一些 Pipeline，自动化，测试任务，因为它既是编程语言也是一种出色的脚本语言，功能强大且易于学习。 Groovy 目前流行排名 # 我们从这张图看到 2022 年 Groovy 语言的排行有一个非常大的下滑，从之前的排名 12 直接跌倒了 20。\n从数据上 2022 年 2 月 Groovy 有小幅减少（从 0.76，%0.74%），但这不是主要原因，主要是很可能是因为 TIOBE index 从 Alexa 更换为 Similarweb 网络流量引擎导致的波动。\n这些语言的排名上升包括：Assembly language，Go，Swift, MATLAB, Delphi/Object Pascal, Classic Visual Basic, Objective-C 的增长抢占了 Groovy 原有的位置。\n另外从 Groovy 的流行历史来看，它目前还是有很多人在使用的。开发者或许不会把它当作第一语言，但作为脚本语言学习一下还是可以的。\n对于使用 Jenkins Shared Libraries 的 DevOps 工程师，需要学习 Groovy。\n参考 # Groovy vs Java: https://flyoutsourcing.com/blog/groovy-vs.-java-what-suits-your-needs.html Differences with Java：https://groovy-lang.org/differences.html TIOBE index：https://www.tiobe.com/tiobe-index/ ","date":"2022-03-06","externalUrl":null,"permalink":"/posts/2022/groovy/","section":"Posts","summary":"Groovy 是一种强大的动态语言，适用于 Java 平台，本文介绍了 Groovy 的特性、与 Java 的区别以及在 Jenkins 中的应用场景。","title":"在 Jenkins 上用了这么久的 Groovy，是时候认识一下它了","type":"posts"},{"content":"","date":"2022-03-02","externalUrl":null,"permalink":"/en/tags/blackduck/","section":"Tags","summary":"","title":"BlackDuck","type":"tags"},{"content":" Details # Failure: PIP - Pip Inspector The Pip Inspector tree parse failed to produce output. Overall Status: FAILURE_DETECTOR - Detect had one or more detector failures while extracting dependencies. For more output please click to expand.\n👉 Click to see more output 👈 [main] --- ======== Detect Issues ======== [main] --- [main] --- DETECTORS: [main] --- Detector Issue [main] --- /workdir/test [main] --- Failure: PIP - Pip Inspector [main] --- The Pip Inspector tree parse failed to produce output. [main] --- [main] --- ======== Detect Result ======== [main] --- [main] --- Black Duck Project BOM: https://org.blackducksoftware.com/api/projects/246c8952-7cb8-40e9-9987-35f7d4602ae1/versions/e1cb4204-42d0-4445-8675-978df62b150d/components [main] --- [main] --- ======== Detect Status ======== [main] --- [main] --- GIT: SUCCESS [main] --- PIP: FAILURE [main] --- [main] --- Signature scan / Snippet scan on /workdir/test: SUCCESS [main] --- Overall Status: FAILURE_DETECTOR - Detect had one or more detector failures while extracting dependencies. Check that all projects build and your environment is configured correctly. [main] --- [main] --- If you need help troubleshooting this problem, generate a diagnostic zip file by adding \u0026#39;-d\u0026#39; to the command line, and provide it to Synopsys Technical Support. See \u0026#39;Diagnostic Mode\u0026#39; in the Detect documentation for more information. [main] --- [main] --- =============================== [main] --- [main] --- Detect duration: 00h 00m 54s 951ms [main] --- Exiting with code 5 - FAILURE_DETECTOR ENVIRONMENT:\nProduct: synopsys-detect-7.11.1.jar Others: OpenJDK 11, Python 3.6 and Python 2.7.5 Root cause # More output of this run, I see it used python (which is python2) not python3, so run pip-inspector.py failed.\nDEBUG [main-Executable_Stream_Thread] --- Python 2.7.5 ... [main] --- Running executable \u0026gt;/usr/bin/python /home/****/blackduck/runs/2022-03-01-07-45-05-986/shared/pip/pip-inspector.py --projectname=test Solution # Link python to python3, it works in my case.\nFor example\n# save python to other name sudo mv /usr/bin/python /usr/bin/python.old # link python3 to python sudo ln -s /usr/bin/python3 /usr/bin/python Then try to run bash \u0026lt;(curl -s -L https://detect.synopsys.com/detect7.sh) again, my test commands:\nbash \u0026lt;(curl -s -L https://detect.synopsys.com/detect7.sh) --blackduck.url=https://org.blackducksoftware.com --blackduck.api.token=MmMwMjdlOTctMT --detect.project.name=HUB --detect.project.version.name=TEST_v1.1.1 --detect.source.path=/workdir/test --logging.level.com.synopsys.integration=DEBUG --blackduck.trust.cert=TRUE --detect.tools.excluded=POLARIS --detect.blackduck.signature.scanner.snippet.matching=SNIPPET_MATCHING If you want to use Docker to do Blackduck scan, you can create a Docker image. like this\nFROM openjdk:11 # Set DETECT version you need, if it\u0026#39;s empty download the latest version. # https://sig-repo.synopsys.com/artifactory/bds-integrations-release/com/synopsys/integration/synopsys-detect ENV DETECT_LATEST_RELEASE_VERSION=\u0026#34;\u0026#34; RUN apt-get update \\ \u0026amp;\u0026amp; apt-get upgrade -y \\ \u0026amp;\u0026amp; apt-get install -y \\ git \\ python \\ pip \\ \u0026amp;\u0026amp; apt-get autoremove \\ \u0026amp;\u0026amp; apt-get clean RUN curl -sSOL https://detect.synopsys.com/detect7.sh \u0026amp;\u0026amp; bash detect7.sh --help \\ \u0026amp;\u0026amp; rm -rf /usr/bin/python \\ \u0026amp;\u0026amp; ln -s /usr/bin/python3 /usr/bin/python WORKDIR /src Hope this help.\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-03-02","externalUrl":null,"permalink":"/en/posts/2022/blackduck-troubleshooting/","section":"Posts","summary":"This article explains how to resolve the “The Pip Inspector tree parse failed to produce output” error in Black Duck Detect, including the root cause and solution.","title":"Resolved - The Pip Inspector tree parse failed to produce output","type":"posts"},{"content":"DevOps 是 IT 界最近几年的一个热门话题，而且还会越来越热。\n最近有幸和一位做传播咨询的读者朋友交流关于 2022 年最值得关注的 DevOps 趋势以及一些问题和回答，分享给大家。\n行业趋势 # 趋势一：转向无服务器计算 # 无服务器计算是一种新兴趋势，实际上已经存在了十多年。企业购买无服务器框架需要一段时间，主要是因为对行业支持和对投资回报的担忧。\n无服务器具有许多越来越难以忽视的优势，主要的两个最大好处是效率和可靠性。没有基础设施管理的负担，企业可以将资源集中在正重要的事项上。此外，无服务器还降低了传统框架可能出现的潜在维护问题的风险。\n无服务器提供固有的可扩展性和可靠性并自动化开发人员不喜欢的日常操作任务，2022 年无服务器计算会经历下一次发展。\n趋势二：微服务架构增长 # 随着无服务器计算在 2022 年的发展，微服务也将如此。\n微服务架构是将单体应用分化为小的独立单元，或服务，从而为大型团队提供了更大的灵活性。它有以下优势：\n为企业提供比单体应用程序更好的可扩展性和敏捷性 开发人员可以使用他们熟悉的编程语言和工具，消除传统应用程序开发的局限 开发人员能够在不破坏整个代码库的情况下部署小的特性或功能 DevOps 团队可以根据业务需求来扩展每个应用部分，而不是一次性扩展整个应用 出现问题微服务可以轻松控制问题，而不会中断整个应用程序 当然也必须认识到微服务的一个弊端，如果实施不佳可能导致严重问题，包括数据丢失、可靠性差和安全风险。\n趋势三：Kubernetes 成为基础架构 # Kubernetes，也称 K8s，是容器编排开源平台，它能够与容器组交互，同时管理更多集群。除了容器管理，还提供安全、网络和存储服务，自我监控，节点和容器的健康状况检查。它可以处理从虚拟机集群管理到负载平衡等所有方方面面，提高生产力，简化 DevOps 开发、测试和部署流程。\n根据 Flexera 的 2021 年云计算状况报告，48% 的企业使用 Kubernetes，另有 25% 的企业计划使用它。另外 53% 的组织使用 Docker，21% 的组织计划使用。\n趋势四：DevSecOps 成为重要组成部分 # 安全性正在成为 DevOps 领域的另一个日益关注的问题。\n为了避免网络攻击，许多大型企业正在将安全性集成到他们的 DevOps 流程中。从 DevOps 到 DevSecOps 的转变预计在 2022 会有更多公司在软件开发生命周期的早期加入安全控制。 这使 DevOps 团队能够在开发阶段持续监控和修复安全缺陷，从而提高交付速度和质量。DevSecOps 正在成为许多公司组织结构图的重要组成部分。\n行业问答 # 问题一: DevOps 整个目前行业头部本土和国际玩家有哪些（GitLab)？ # 以我所在的外企而言通常是选择国际玩家，以最常用的代码管理和项目管理的工具为例：\n上了年头的外企大公司通常在使用 Atlassian 家的 Jira 和 Bitbucket。船大难掉头，选择 GitLab，GitHub 这样一站式的 DevOps 迁移成本很高，需要有足够的理由才可能换工具。 对于年轻的公司，GitLab 和 GitHub 都是很好的选择。GitLab 在企业内部建立私服居多；GitHub 也提供企业版私服，但对于开源项目而言 GitHub 依然是代码托管的首选。 其他用到的付费级 DevOps 工具还包括 Synopsys (Polaris, Blackduck)，Jfrog (Artifactory)，SonarQube 等。\n问题二: 行业目前有哪些重点趋势？比如安全这块，是不是目前行业关注度比较高？有哪些工具？ # 安全领域的关注度在逐年升高，尤其在外企很注重安全这块，他们愿意花钱来购买安全扫描工具来扫描代码，甚至还会要求所有的发布的产品代码中不能有高危漏洞。\n一些常用的工具包括：静态代码扫描，比如 Polaris, Veracode, Snyk, SonarQube, PVS-Studio；代码组成分析，比如 Blackduck，X-Ray 等等。\n问题三：企业在选择 DevOps 平台时主要考虑的因素有哪些？比如数据库安全，公司成熟度，海外知名度，等等 # 我认为主要考虑公司的知名度，其次产品的知名度，如果是开源产品会着重关注 GitHub 上的 Contributors 数量，它更能代表社区的活跃度，其次是 Fork 和 Star 数量。\n问题四：目前 DevOps 处于哪个阶段? 未来的发展机会是在哪里？ # DevOps 市场目前处在相对成熟的阶段，每个细分领域都有很多工具可以选择。未来基础设施会更多的向容器云方向发展。\n具有创新的 DevOps 产品依然会很有市场，像是 GitLab，HashiCorp 等公司的产品，他们在短短十年内成为世界级的软件公司。\n问题五：有哪些主流或平时重点关注的行业媒体号或自媒体公众号？ # 会经常看一些 DevOps 相关的以及 InfoQ，阿里、腾讯、美团等技术公众号。\n还会看 YouTube 上一些 DevOps 的个人及公司频道：TechWorld with Nana, CloudBeesTV，CNCF，DevOps Paradox，DevOps Toolkit 等。\n问题六：除了公众号外，你平时会上哪些行业社区？比如说 GitHub 或 CSDN？ # 最常看的是 GitHub 以及 GitHub Trending 来看最近受关注的项目。还会\n社区会定期去看 DEV Community, Medium, InfoQ 。会看知乎上一些话题下的精华，很少看 CSDN，懂得都懂。\n参考 # Top DevOps Trends to Watch in 2022 DevOps Trends To Look Out for in 2022 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-02-24","externalUrl":null,"permalink":"/posts/2022/devops-trends-2022/","section":"Posts","summary":"本文介绍了2022年DevOps领域的主要趋势，包括无服务器计算、微服务架构、Kubernetes的普及以及DevSecOps的兴起，并回答了一些关于DevOps行业的问题。","title":"2022 年最值得关注的 DevOps 趋势和问答","type":"posts"},{"content":"Unknowingly, I\u0026rsquo;ve been writing blog posts and WeChat articles for five years now. I never expected it to last this long.\nI\u0026rsquo;d like to share my career path changes over these years, and what I\u0026rsquo;ve gained from writing blogs and WeChat articles. Consider this a personal retrospective. It would be even better if it resonates with you and provides some help.\nFrom QA to DEV to DEVOPS # My earliest followers probably connected with me through my software testing posts. Yes, I worked in software testing for nearly 10 years, successively at SIMcom, DongSoft, JD.com, and a foreign company, engaging in functional, automation, and performance testing.\nStarting with functional testing, I gradually realized that programming wasn\u0026rsquo;t a unique skill for developers; it was also essential for testing engineers. Only with good coding skills can one perform automation, unit testing, and test development work.\nAs an automation testing engineer, I discovered that \u0026ldquo;solving\u0026rdquo; problems brought me more joy than \u0026ldquo;finding\u0026rdquo; them. I began to dream of becoming a developer. This would not only improve my programming skills but also offer more career possibilities with expertise in both development and testing.\nFinally, due to opportunity, my initiative, and passable coding skills, I transitioned from testing to development. The initial difficulties and pressures were unprecedented in my almost 10-year career. Days, nights, and weekends were spent looking at code\u0026hellip;every single day. After more than six months of hard work, I finally reached the shore, capable of doing C/C++ project bug fixes.\nIt was my experience in development, automation, and continuous integration that led me to the role of Build/Release Engineer. This was my ideal position, responsible for automated product building, release, infrastructure construction, CI/CD, and development efficiency improvements.\nThat\u0026rsquo;s how I went from QA to DEV to DEVOPS. The name changes of my WeChat Official Account reflect my career path:\nJuly 28, 2019: \u0026ldquo;Software Testing and Development\u0026rdquo; changed to \u0026ldquo;DevOps Engineer\u0026rdquo; December 29, 2018: \u0026ldquo;DevQA\u0026rdquo; changed to \u0026ldquo;Software Testing and Development\u0026rdquo; December 26, 2018: \u0026ldquo;Software Testing QA\u0026rdquo; changed to \u0026ldquo;DevQA\u0026rdquo; August 1, 2017: Registered \u0026ldquo;Software Testing QA\u0026rdquo; Five Years of Writing: The Harvest # Writing is something where the long-term benefits far outweigh the short-term gains.\nFor most people, there\u0026rsquo;s little immediate tangible benefit. It also takes up a lot of spare time—it\u0026rsquo;s essentially fueled by passion. From a monetary perspective, the input and output are completely disproportionate, making it hard to maintain.\nHowever, from a long-term perspective, consistent writing will definitely bring value. I\u0026rsquo;ve summarized five benefits:\nA bad memory is no match for a good pen – When we figure out a difficult technical problem, even if we understand it at the time, if we don\u0026rsquo;t record it, we might not know how to solve it when we encounter the same problem later. Only when you can make others understand, do you truly understand – Sometimes, we think we understand a problem, but when we share it with others, we find some logic doesn\u0026rsquo;t make sense. This forces us to think further and fully understand it. Building a learning flywheel – When you consistently share and people pay attention and interact with you, you\u0026rsquo;ll be motivated to continue sharing, learning new knowledge, and then sharing it again. Once the learning flywheel is built, persistence becomes easier. Indirect benefits – Consistently writing something will help with job searching, at least showing you\u0026rsquo;re a learner. If your sharing impresses your colleagues or future interviewers, it could lead to new job opportunities. Direct benefits – Direct benefits include platform traffic and advertising revenue, selling columns, and consulting. This requires a high level of skill, not only in self-media operation, but also in strong output capabilities, which requires more effort than others. In 2017, I didn\u0026rsquo;t think too much about it. I just thought I could write something, so on July 6, 2017, I built my personal blog using GitHub Pages.\nWith the content ready, copying it wasn\u0026rsquo;t difficult, and I could learn how to use WeChat Official Accounts. So, I opened a WeChat Official Account in August of the same year. Later, wanting to learn about mini-programs, I created a WeChat mini-program (DevOps Engineer) for my blog during the May Day holiday in 2020.\nHaving spent so much spare time writing, let me talk about the specific gains:\nLearned how to build and publish a blog on GitHub, making GitHub one of my most frequently visited websites. Learned how to integrate Hexo blog with Disqus, Google Analytics, Google Adsense, etc., and made many improvements. Learned how to use and integrate tools like GitHub Actions, Travis, SonarQube, etc. Learned how to operate a WeChat Official Account; learned how to create and publish a WeChat mini-program. Participated in open-source projects, learning coding, broadening my horizons, and gaining exposure to best practices in open-source projects. A colleague told me they read my articles, found and followed my WeChat Official Account, which honored me. Received consultations and even thank-you red packets from those I helped. Helping others and receiving positive feedback made me very happy. I was unexpectedly invited by a major publishing house to write a technical book, but I declined due to work commitments and other important knowledge I wanted to learn. \u0026hellip; \u0026hellip; Of these gains, I think the biggest is building a learning flywheel and developing a habit of sharing.\nThe best time was ten years ago, the second best is now. Daily progress, no effort is wasted. Keep doing the right things, and leave the rest to time.\n—— Updated at dawn on February 20, 2022.\nRelated Reading # Why did I switch from testing to development after 9 years? Five Months from Testing to Development Please indicate the author and source when reprinting this article. Do not use it for any commercial purposes. Welcome to follow the WeChat Official Account \u0026ldquo;DevOps Engineer\u0026rdquo;.\n","date":"2022-02-21","externalUrl":null,"permalink":"/en/misc/from-qa-to-dev-to-devops/","section":"Miscs","summary":"What have I gained from five years of coding, transitioning from software testing to development to DevOps? Sharing my career development and writing experience.","title":"From QA to DEV to DEVOPS — Five Years of Coding and Writing","type":"misc"},{"content":"","date":"2022-02-14","externalUrl":null,"permalink":"/en/tags/vagrant/","section":"Tags","summary":"","title":"Vagrant","type":"tags"},{"content":"For an introduction to Vagrant, please refer to the previous article: What is Vagrant? The Difference Between Vagrant and VirtualBox\nWhat is Vagrant # For an introduction to Vagrant, please refer to the previous article: What is Vagrant? The Difference Between Vagrant and VirtualBox\nVagrant vs. Docker # One of the most frequently asked questions about Vagrant is: What\u0026rsquo;s the difference between Vagrant and Docker?\nDirectly comparing Vagrant and Docker without considering the context is inappropriate. In some simple scenarios, their functions overlap, but in many more scenarios, they are not interchangeable.\nSo, when should you use Vagrant, and when should you use Docker?\nTherefore, if you only want to manage virtual machines, you should use Vagrant; if you want to quickly develop and deploy applications, you should use Docker.\nLet\u0026rsquo;s discuss why in more detail.\nVagrant is a VM management tool, or orchestration tool; Docker is a tool used to build, run, and manage containers. The question actually boils down to the difference between a virtual machine (VM) and a container (Container).\nLet\u0026rsquo;s use a set of images from the internet to illustrate the differences between a physical machine (Host), a virtual machine (VM), and a container (Container).\nPhysical Machine (Host)\nVirtual Machine (VM)\nContainer (Container)\nFrom the images, we can more easily understand the differences between a virtual machine (VM) and a container (Container):\nFeature Virtual Machine Container Isolation Level Operating System Level Process Level Isolation Strategy Hypervisor CGROUPS System Resources 5 - 15% 0 - 5% Startup Time Minutes Seconds Image Storage GB MB Summary: Differences in Use Cases for Vagrant and Docker\nVagrant is designed to manage virtual machines, while Docker is designed to manage application environments.\nVagrant is more suitable for development and testing, solving environment consistency issues; Docker is more suitable for rapid development and deployment, and CI/CD.\nFinally, both Vagrant and Docker have a large number of community-contributed \u0026ldquo;Boxes\u0026rdquo; and \u0026ldquo;Images\u0026rdquo; available.\nWelcome to follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo; - Focusing on DevOps knowledge sharing.\n","date":"2022-02-14","externalUrl":null,"permalink":"/en/posts/2022/vagrant-vs-docker/","section":"Posts","summary":"This article compares Vagrant and Docker, analyzing their respective use cases and advantages to help readers choose the right tool for managing virtual machines or containers.","title":"Vagrant vs. Docker —— Which One Should You Choose?","type":"posts"},{"content":"","date":"2022-02-14","externalUrl":null,"permalink":"/en/tags/virtualbox/","section":"Tags","summary":"","title":"VirtualBox","type":"tags"},{"content":" What is Vagrant # Vagrant is an open-source software product used to easily build and maintain virtual software development environments.\nFor example, it can build development environments based on providers such as VirtualBox, VMware, KVM, Hyper-V, AWS, and even Docker. It improves development efficiency by simplifying the configuration management of virtualized software.\nVagrant is developed in Ruby, but its ecosystem supports development in several other languages.\nSimply put, Vagrant is an encapsulation layer over traditional virtual machines, allowing you to use virtual development environments more conveniently.\nVagrant\u0026rsquo;s History # Vagrant was initially launched as a personal project by Mitchell Hashimoto in January 2010.\nThe first version of Vagrant was released in March 2010. In October 2010, Engine Yard announced that they would sponsor the Vagrant project.\nThe first stable version of Vagrant, Vagrant 1.0, was released in March 2012, exactly two years after the original release.\nIn November of the same year, Mitchell founded HashiCorp to support full-time development of Vagrant. Vagrant remains open-source software, and HashiCorp is committed to creating commercial versions and providing professional support and training for Vagrant.\nNow HashiCorp has become a world-leading open-source company. Through a series of products, including Vagrant, Packer (packaging), Nomad (deployment), Terraform (cloud environment configuration), Vault (access management), and Consul (monitoring), it has redefined the entire DevOps process from end to end.\nVagrant initially supported VirtualBox, and version 1.1 added support for other virtualization software (such as VMware and KVM), as well as support for server environments such as Amazon EC2. Starting with version 1.6, Vagrant natively supported Docker containers, which can replace fully virtualized operating systems in some cases.\nHow to Use Vagrant # Prerequisites for using Vagrant:\nInstall Vagrant. Download Vagrant Install VirtualBox Once both are ready, you can create and use your virtual machine via the command line.\nFor example, you need an Ubuntu 18.04 LTS 64-bit virtual machine. More virtual machines can be searched for on the Box website, which is similar to Docker Hub, where users can download and upload various Vagrant Boxes.\nYou only need to execute a few simple commands to complete startup, login, logout, and destruction.\nInitialize Vagrant\nvagrant init hashicorp/bionic64 Start the virtual machine. This should take a few tens of seconds (the first time will take longer to download the image, depending on your internet speed).\nvagrant up Log in to your virtual machine, and you can then use your created Ubuntu virtual machine.\nvagrant ssh When you\u0026rsquo;re done, execute logout to log out.\nDifferences Between Vagrant and Traditional Virtual Machine Software # Vagrant is much more convenient than traditional virtual machine usage. Let\u0026rsquo;s look at how to create a virtual machine using the traditional method.\nTaking VirtualBox as an example, assuming you have already installed VirtualBox, the steps to create a virtual machine using the traditional method are as follows:\nFirst, download the corresponding ISO file. Then, load the ISO using VirtualBox or VMware. Finally, configure the CPU, memory, disk, network, user, etc., and wait for the installation to complete.\nThis method is very cumbersome to configure, requiring step-by-step operations. These configuration steps often require documentation to ensure that an \u0026ldquo;identical\u0026rdquo; virtual development environment can be created later.\nBy comparison, you should now have a general understanding of how Vagrant is used and some of the differences between it and traditional virtual machine usage.\nSummary # The advantages of Vagrant over traditional virtual machine usage: it provides easy-to-configure, reproducible, and portable work environments, thereby improving productivity and flexibility.\nVagrant can be said to be the easiest and fastest way to create and manage virtualized environments!\nIt is able to be so convenient because it stands on the shoulders of giants (VirtualBox, VMware, AWS, OpenStack, or other providers), and then uses tools such as Shell scripts, Ansible, Chef, and Puppet to automatically install and configure software on the virtual machine.\nThe next article will introduce the differences between Vagrant and Docker.\n","date":"2022-02-11","externalUrl":null,"permalink":"/en/posts/2022/vagrant/","section":"Posts","summary":"This article introduces the concept and history of Vagrant, and how to use Vagrant to create and manage virtual machines, emphasizing the advantages of Vagrant over traditional virtual machines.","title":"What is Vagrant? Differences Between Vagrant and VirtualBox","type":"posts"},{"content":"","date":"2022-01-18","externalUrl":null,"permalink":"/en/tags/go/","section":"Tags","summary":"","title":"Go","type":"tags"},{"content":"Go is an open-source programming language that makes it easy to build simple, reliable, and efficient software.\nGo or Golang # Let\u0026rsquo;s start with a question most people might overlook: Is Google\u0026rsquo;s open-source programming language called Go or Golang? Or both? Three seconds to think\u0026hellip;\nGoogle says: It\u0026rsquo;s called Go. The reason some people call it Golang is that the previous Go language website was golang.org (because go.org was already taken), so some people mixed up Golang and Go.\nNow, entering golang.org will redirect you to go.dev, which is a way to officially clarify the name.\nAdvantages of Go # The official website describes Go as follows:\nGo is suitable for building large-scale, reliable, and efficient software quickly. Go is an open-source programming language backed by Google. Easy to learn and get started with. Built-in concurrency and a powerful standard library. A constantly evolving ecosystem of partners, communities, and tools. Today, Go is used in a variety of applications:\nGo is popular in cloud-based or server-side applications. Cloud infrastructure. Many of today\u0026rsquo;s most popular infrastructure tools are written in Go, such as Kubernetes, Docker, and Prometheus. Many command-line tools are written in Go. DevOps and web reliability automation often use Go. Go is also used in the fields of artificial intelligence and data science. It\u0026rsquo;s also used in microcontroller programming, robotics, and game development. This is why Go is becoming increasingly popular.\nIt was these advantages, and the need to write a CLI for work, that led me to learn Go.\nGo\u0026rsquo;s Ranking # Go\u0026rsquo;s popularity in China is quite high. Let\u0026rsquo;s take a look at Go\u0026rsquo;s current ranking.\nThis is the TIOBE January 2022 top 20 programming languages ranking. Go is ranked 13th, up one position from last year.\nCompared to Python, C, Java, C++, and C#, which are in the top five, do you think Go can catch up?\nFrom my observations of non-cloud companies and colleagues, most developers use C/C++, Java, C#, and Python. Therefore, I think this ranking is quite expected.\nShould Beginners Learn Python or Go? # Python has been around for over 30 years, but its popularity continues to grow. Python is an excellent object-oriented language, and you can also use a functional programming style to write code. Among all programming languages, you might not find one used by more non-programmers than Python.\nIts flexibility is one reason Python is so popular. It\u0026rsquo;s often used for scripting, web development, data science, teaching programming to children, creating animations, and more. So how does Go compare to Python?\nBoth Python and Go have simple syntax. Both Python and Go are easy for beginners to get started with and relatively easy to learn (Python is relatively easier). Python often dominates the data science field; Go is well-suited for systems programming. Go is much faster than Python in terms of program execution speed. As a high-level language, Python has a wider range of libraries and a larger community built around it. Go is ideal for handling large concurrent applications and supports concurrency—the ability to run multiple programs/tasks simultaneously. Python does not. Today, Python and Go are two of the most popular and convenient programming languages. For beginners, should they learn Python or Go?\nIf you\u0026rsquo;re a complete beginner, it\u0026rsquo;s recommended to learn Python first. Compared to Go, Python is easier to learn. If you\u0026rsquo;re a test engineer and want to learn a programming language, learn Python. Most automation testing positions require Python proficiency. If you\u0026rsquo;re a software developer or DevOps engineer, you should learn both. \u0026ldquo;Kids make choices, adults take both.\u0026rdquo; How to Learn Go # Read documentation or watch videos, but most importantly, get your hands dirty!!\nI first watched Go language video tutorials between 2010 and 2020, but because I didn\u0026rsquo;t do much coding, I remained in a state of knowing only a little.\nFor newcomers learning any programming language, tutorials only teach you about 30%. To truly learn, you must practice personally, otherwise, it will be: \u0026ldquo;Looks easy, but impossible to write\u0026rdquo;.\nChoose a direction and start coding immediately.\nMy direction was to write a CLI tool. Although Go\u0026rsquo;s built-in Flag package can be used to write CLI commands, after looking at many CLI projects developed using Go, I noticed that most of them didn\u0026rsquo;t use the built-in Flag package, but mostly used spf13/cobra or urfave/cli.\nThis is a list of projects using cobra here, including well-known projects like Kubernetes, Hugo, Docker, and the GitHub CLI. As for urfave/cli, I saw Jfrog CLI using it, but I didn\u0026rsquo;t see a list of other well-known projects using urfave/cli like cobra. For beginners like me, the most important thing is to start immediately, so you don\u0026rsquo;t need to spend too much time choosing a framework. Cobra has so many excellent projects backing it up, just use it. The most important thing is to start coding as soon as possible. In the coding process, choose top-tier projects that also use this framework as a reference. This can help us write better code by reading other people\u0026rsquo;s code. Don\u0026rsquo;t just Ctrl + C and Ctrl + V.\nFinally, here are some other excellent projects when developing CLIs:\ngithub.com/AlecAivazis/survey/v2 - Build interactive command lines in the terminal. github.com/enescakir/emoji - Emoji library, supports emoji output in the terminal. github.com/mgutz/ansi - Create ANSI color strings. ","date":"2022-01-18","externalUrl":null,"permalink":"/en/posts/2022/what-is-go/","section":"Posts","summary":"This article introduces the basic concepts, advantages, and ranking of the Go programming language. It also guides beginners on choosing between learning Python or Go, providing practical learning suggestions and resources.","title":"What is Go? Advantages, Current Status, and Choosing Between Python and Go for Beginners","type":"posts"},{"content":"","date":"2022-01-12","externalUrl":null,"permalink":"/en/tags/dokerfile/","section":"Tags","summary":"","title":"Dokerfile","type":"tags"},{"content":"This article shares some best practices to follow when writing Dockerfiles and using Docker. It\u0026rsquo;s a long read, so consider bookmarking it to read later—you\u0026rsquo;re guaranteed to learn a lot!\nTable of Contents # Dockerfile Best Practices\nUse Multi-Stage Builds Adjust the Order of Dockerfile Commands Use Small Base Docker Images Minimize the Number of Layers Use Unprivileged Containers Prefer COPY over ADD Cache Python Packages to the Docker Host Run Only One Process per Container Prefer Array over String Syntax Understand the Difference Between ENTRYPOINT and CMD Add Health Checks HEALTHCHECK Docker Image Best Practices\nDocker Image Versioning Don\u0026rsquo;t Store Secrets in Images Use a .dockerignore File Inspect and Scan Your Dockerfiles and Images Sign and Verify Images Set Memory and CPU Limits Dockerfile Best Practices # 1. Use Multi-Stage Builds # Leverage the power of multi-stage builds to create leaner and more secure Docker images. Multi-stage Docker builds (multi-stage builds) allow you to divide your Dockerfile into several stages.\nFor example, you can have one stage for compiling and building your application, which can then be copied to a subsequent stage. Since only the last stage is used to create the image, dependencies and tools related to building the application are discarded, leaving a lean, modular, production-ready image.\nWeb development example:\n# Temporary stage FROM python:3.9-slim as builder WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y --no-install-recommends gcc COPY requirements.txt . RUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt # Final stage FROM python:3.9-slim WORKDIR /app COPY --from=builder /app/wheels /wheels COPY --from=builder /app/requirements.txt . RUN pip install --no-cache /wheels/* In this example, the GCC compiler is required when installing certain Python packages, so we add a temporary, build-time stage to handle the build stage.\nSince the final runtime image doesn\u0026rsquo;t contain GCC, it\u0026rsquo;s leaner and more secure. Image size comparison:\nREPOSITORY TAG IMAGE ID CREATED SIZE docker-single latest 8d6b6a4d7fb6 16 seconds ago 259MB docker-multi latest 813c2fa9b114 3 minutes ago 156MB Let\u0026rsquo;s look at another example:\n# Temporary stage FROM python:3.9 as builder RUN pip wheel --no-cache-dir --no-deps --wheel-dir /wheels jupyter pandas # Final stage FROM python:3.9-slim WORKDIR /notebooks COPY --from=builder /wheels /wheels RUN pip install --no-cache /wheels/* Image size comparison:\nREPOSITORY TAG IMAGE ID CREATED SIZE ds-multi latest b4195deac742 2 minutes ago 357MB ds-single latest 7c23c43aeda6 6 minutes ago 969MB In summary, multi-stage builds can reduce the size of your production images, helping you save time and money. Additionally, this will simplify your production containers. Being smaller and simpler, they will have a relatively smaller attack surface.\n2. Adjust the Order of Dockerfile Commands # Pay close attention to the order of your Dockerfile commands to leverage layer caching.\nDocker caches each step (or layer) in a given Dockerfile to speed up subsequent builds. When a step changes, not only that step but all subsequent steps\u0026rsquo; cache will be invalidated.\nFor example:\nFROM python:3.9-slim WORKDIR /app COPY sample.py . COPY requirements.txt . RUN pip install -r /requirements.txt In this Dockerfile, we copy the application\u0026rsquo;s code before installing the requirements. Now, every time we change sample.py, the build will reinstall the packages. This is highly inefficient, especially when using Docker containers as development environments. Therefore, putting frequently changing files towards the end of the Dockerfile is key.\nYou can also prevent unnecessary cache invalidation by using a .dockerignore file to exclude unnecessary files from being added to the Docker build context and the final image. More on this later.\nTherefore, in the Dockerfile above, you should move the COPY sample.py . command to the bottom as follows:\nFROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r /requirements.txt COPY sample.py . Note:\nAlways put layers that are likely to change as low as possible in the Dockerfile. Combine multiple RUN apt-get update, RUN apt-get install, etc. commands into one. (This also helps reduce the image size, which will be mentioned shortly). If you want to disable the cache for a given Docker build, you can add the --no-cache=True flag. 3. Use Small Base Docker Images # Smaller Docker images are more modular and secure. Smaller base Docker images build, push, and pull faster. They also tend to be more secure because they only include the necessary libraries and system dependencies required to run your application.\nWhich base Docker image should you use? There\u0026rsquo;s no one-size-fits-all answer; it depends on what you are doing. Here\u0026rsquo;s a comparison of the size of various Docker base images for Python.\nREPOSITORY TAG IMAGE ID CREATED SIZE python 3.9.6-alpine3.14 f773016f760e 3 days ago 45.1MB python 3.9.6-slim 907fc13ca8e7 3 days ago 115MB python 3.9.6-slim-buster 907fc13ca8e7 3 days ago 115MB python 3.9.6 cba42c28d9b8 3 days ago 886MB python 3.9.6-buster cba42c28d9b8 3 days ago 886MB While the Alpine flavor, based on Alpine Linux, is the smallest, it often leads to increased build times if you can\u0026rsquo;t find pre-compiled binaries that work with it. Therefore, you may end up having to build binaries yourself, which might increase the image size (depending on required system-level dependencies) and build time (due to having to compile from source).\nRead Best Docker Base Images for Python Applications and Using Alpine Can Make Python Docker Builds 50x Slower to learn more about why you might best avoid Alpine-based base images.\nUltimately, it\u0026rsquo;s all about balance. When in doubt, start with the *-slim flavor, especially during development mode as you are building your application. You want to avoid having to constantly update your Dockerfile to install necessary system-level dependencies as you add new Python packages. As you harden your application and Dockerfile for production, you might want to explore using Alpine for the final image of a multi-stage build.\nAlso, don\u0026rsquo;t forget to regularly update your base images to improve security and performance. When a new version of a base image is released, e.g., 3.9.6-slim \u0026ndash;\u0026gt; 3.9.7-slim, you should pull the new image and update your running containers to get all the latest security patches.\n4. Minimize the Number of Layers # Try to combine RUN, COPY, and ADD commands because they create layers. Each layer adds to the size of the image because they are cached. Therefore, as the number of layers increases, so does the image size.\nYou can test this using the docker history command.\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE dockerfile latest 180f98132d02 51 seconds ago 259MB docker history 180f98132d02 IMAGE CREATED CREATED BY SIZE COMMENT 180f98132d02 58 seconds ago COPY . . # buildkit 6.71kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 58 seconds ago RUN /bin/sh -c pip install -r requirements.t… 35.5MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago COPY requirements.txt . # buildkit 58B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago WORKDIR /app ... Notice the size. Only RUN, COPY, and ADD commands increase the image size, and you can reduce the size of the final image by combining commands whenever possible. For example:\nRUN apt-get update RUN apt-get install -y gcc Can be combined into a single RUN command:\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y gcc Thus, creating a single layer instead of two, thereby reducing the size of the final image. While reducing the number of layers is a good idea, it\u0026rsquo;s more important that it\u0026rsquo;s not a goal in itself but rather a side effect of reducing image size and build time. In other words, instead of trying to micro-optimize every command, you should focus on the first three practices!!!\nMulti-stage builds The order of Dockerfile commands And using a small base image. Note # RUN, COPY, and ADD all create layers Each layer contains the difference from the previous layer Layers add to the final image size Tip # Combine related commands Remove unnecessary files during the RUN step in the creation process Minimize running apt-get upgrade as it upgrades all packages to the latest versions. For multi-stage builds, don\u0026rsquo;t worry too much about over-optimizing commands in temporary stages Finally, for better readability, it is recommended to sort multi-line arguments alphanumerically.\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ gcc \\ git \\ matplotlib \\ pillow \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* 5. Use Unprivileged Containers # By default, Docker runs container processes as root inside the container. However, this is bad practice because processes running as root inside the container also run as root on the Docker host.\nTherefore, if an attacker gains access to the container, they gain all root privileges and can perform several attacks against the Docker host, such as:\nCopying sensitive information from the host\u0026rsquo;s filesystem to the container Executing remote commands To prevent this, ensure your container processes run as a non-root user.\nRUN addgroup --system app \u0026amp;\u0026amp; adduser --system --group app USER app You can go a step further and remove shell privileges, ensuring there is no home directory.\nRUN addgroup --gid 1001 --system app \u0026amp;\u0026amp; \\ adduser --no-create-home --shell /bin/false --disabled-password --uid 1001 --system --group app USER app Verification\ndocker run -i sample id uid=1001(app) gid=1001(app) groups=1001(app) Here, the application inside the container is running under a non-root user. However, remember that the Docker daemon and the container itself are still running with root privileges.\nBe sure to check out running the Docker daemon as a non-root user for help running both the daemon and containers as a non-root user.\n6. Prefer COPY over ADD # Unless you are sure you need the extra functionality that ADD provides, use COPY.\nSo what\u0026rsquo;s the difference between COPY and ADD?\nFirst, both commands allow you to copy files into a Docker image from a specific location.\nADD \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; COPY \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; While they seem to do the same thing, ADD has some extra capabilities.\nCOPY is used to copy local files or directories from the Docker host to the image. ADD can be used for the same thing, but it can also be used to download external files. Additionally, if you use a compressed file (tar, gzip, bzip2, etc.) as the \u0026lt;src\u0026gt; argument, ADD will automatically extract the contents to the specified location. # Copy local file on host to destination COPY /source/path /destination/path ADD /source/path /destination/path # Download external file and copy to destination ADD http://external.file/url /destination/path # Copy and extract local compressed file ADD source.file.tar.gz /destination/path Finally, COPY is semantically clearer and easier to understand than ADD.\n7. Cache Python Packages to the Docker Host # When a requirements file is changed, the image needs to be rebuilt to install new packages. Previous steps will be cached, as mentioned in minimizing the number of layers. Downloading all packages on every image rebuild causes significant network activity and requires a considerable amount of time. It takes the same amount of time on every rebuild to download common packages across different builds.\nFor Python, you can avoid this by mapping the pip cache directory to a directory on the host. So on every rebuild, the cached versions persist, improving build speed.\nAdd a volume in your Docker run as -v $HOME/.cache/pip-docker/:/root/.cache/pip or as a mapping in your Docker Compose file.\nThe directory mentioned above is for reference only; make sure you are mapping the cache directory, not the site-packages (where built-in packages live).\nMoving the cache from the docker image to the host can save you space in your final image.\n# omit ... COPY requirements.txt . RUN --mount=type=cache,target=/root/.cache/pip \\ pip install -r requirements.txt # omit ... 8. Run Only One Process per Container # Why is it recommended to run only one process per container?\nLet\u0026rsquo;s say your application stack consists of two web servers and a database. While you could easily run all three from one container, you should run each service in a separate container for easier reusability and scaling of each individual service.\nScalability - Because each service is in a separate container, you can horizontally scale just one of your web servers to handle more traffic as needed. Reusability - Maybe you have another service that needs a containerized database; you can simply reuse the same database container without having two unnecessary services along for the ride. Logging - Coupled containers make logging much more complex. (We will discuss this in more detail later in this article) Portability and Predictability - When containers have fewer parts working, it is much easier to make security patches or debug issues. 9. Prefer Array over String Syntax # You can use CMD and ENTRYPOINT commands in your Dockerfiles in array (exec) or string (shell) format\nIn Dockerfiles, you can use the CMD and ENTRYPOINT commands in array (exec) or string (shell) format\n# Array (exec) CMD [\u0026#34;gunicorn\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;-k\u0026#34;, \u0026#34;uvicorn.workers.UvicornWorker\u0026#34;, \u0026#34;main:app\u0026#34;] # String (shell) CMD \u0026#34;gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app\u0026#34; Both are correct and achieve almost the same thing; however, you should use the exec format whenever possible.\nQuoting from the official Docker documentation:\nMake sure you use the exec form of CMD and ENTRYPOINT in your Dockerfile. For example, use [\u0026quot;program\u0026quot;, \u0026quot;arg1\u0026quot;, \u0026quot;arg2\u0026quot;] instead of \u0026quot;program arg1 arg2\u0026quot;. Using the string form causes Docker to run your process using bash, which doesn\u0026rsquo;t handle signals correctly. Compose always uses the JSON form, so don\u0026rsquo;t worry if you override the command or entrypoint in your Compose file. Therefore, since most shells don\u0026rsquo;t handle signals to child processes, if you use the shell format, CTRL-C (which generates SIGTERM) might not stop a child process.\nExample:\nFROM ubuntu:18.04 # BAD: String (shell) format ENTRYPOINT top -d # GOOD: Array (exec) format ENTRYPOINT [\u0026#34;top\u0026#34;, \u0026#34;-d\u0026#34;] Both do the same thing. But notice that in the string (shell) format case, CTRL-C won\u0026rsquo;t kill the process. Instead, you will see ^C^C^C^C^C^C^C^C^C^C.\nAnother caveat is that the string (shell) format carries the shell\u0026rsquo;s PID, not the process itself.\n# Array format root@18d8fd3fd4d2:/app# ps ax PID TTY STAT TIME COMMAND 1 ? Ss 0:00 python manage.py runserver 0.0.0.0:8000 7 ? Sl 0:02 /usr/local/bin/python manage.py runserver 0.0.0.0:8000 25 pts/0 Ss 0:00 bash 356 pts/0 R+ 0:00 ps ax # String format root@ede24a5ef536:/app# ps ax PID TTY STAT TIME COMMAND 1 ? Ss 0:00 /bin/sh -c python manage.py runserver 0.0.0.0:8000 8 ? S 0:00 python manage.py runserver 0.0.0.0:8000 9 ? Sl 0:01 /usr/local/bin/python manage.py runserver 0.0.0.0:8000 13 pts/0 Ss 0:00 bash 342 pts/0 R+ 0:00 ps ax 10. Understand the Difference Between ENTRYPOINT and CMD # Should I use ENTRYPOINT or CMD to run the container process? There are two ways to run commands inside a container.\nCMD [\u0026#34;gunicorn\u0026#34;, \u0026#34;config.wsgi\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:8000\u0026#34;] # and ENTRYPOINT [\u0026#34;gunicorn\u0026#34;, \u0026#34;config.wsgi\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:8000\u0026#34;] Both essentially do the same thing: start the application using the Gunicorn server on config.wsgi, binding it to 0.0.0.0:8000.\nCMD is easily overwritten. If you run docker run \u0026lt;image_name\u0026gt; uvicorn config.asgi, the above CMD will be replaced with the new arguments.\nFor example, uvicorn config.asgi. To override the ENTRYPOINT command, you must specify the --entrypoint option.\ndocker run --entrypoint uvicorn config.asgi \u0026lt;image_name\u0026gt; Here, it\u0026rsquo;s clear that we are overriding the entrypoint. So it\u0026rsquo;s recommended to use ENTRYPOINT instead of CMD to prevent accidentally overriding commands.\nThey can also be used together. For example:\nENTRYPOINT [\u0026#34;gunicorn\u0026#34;, \u0026#34;config.wsgi\u0026#34;, \u0026#34;-w\u0026#34;] CMD [\u0026#34;4\u0026#34;] When used together like this, the command run for launching the container becomes:\ngunicorn config.wsgi -w 4 As mentioned above, CMD is easily overwritten. Therefore, CMD can be used to pass arguments to the ENTRYPOINT command. For instance, it\u0026rsquo;s easy to change the number of workers, like so:\ndocker run \u0026lt;image_name\u0026gt; 6 This will launch the container with 6 Gunicorn workers instead of the default 4.\n11. Add Health Checks HEALTHCHECK # Use HEALTHCHECK to determine if the process running inside the container is not only up and running but also \u0026ldquo;healthy.\u0026rdquo;\nDocker exposes an API to check the status of the running processes within the container; it provides information beyond just whether a process is \u0026ldquo;running\u0026rdquo; because \u0026ldquo;running\u0026rdquo; encompasses \u0026ldquo;it\u0026rsquo;s running,\u0026rdquo; \u0026ldquo;still starting up,\u0026rdquo; and even \u0026ldquo;stuck in some infinite loop error state.\u0026rdquo; You interact with this API via the HEALTHCHECK instruction.\nFor example, if you are serving a web application, you can use the following to determine if the / endpoint is up and able to handle service requests:\nHEALTHCHECK CMD curl --fail http://localhost:8000 || exit 1 If you run docker ps, you can see the status of the HEALTHCHECK.\nHealthy example\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 09c2eb4970d4 healthcheck \u0026#34;python manage.py ru…\u0026#34; 10 seconds ago Up 8 seconds (health: starting) 0.0.0.0:8000-\u0026gt;8000/tcp, :::8000-\u0026gt;8000/tcp xenodochial_clarke Unhealthy example\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 09c2eb4970d4 healthcheck \u0026#34;python manage.py ru…\u0026#34; About a minute ago Up About a minute (unhealthy) 0.0.0.0:8000-\u0026gt;8000/tcp, :::8000-\u0026gt;8000/tcp xenodochial_clarke You can go a step further and set up a custom endpoint solely for health checks, then configure HEALTHCHECK to test against the returned data.\nFor instance, if the endpoint returns a JSON response of {\u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot;}, you can instruct HEALTHCHECK to verify the response body.\nHere\u0026rsquo;s how to view the health check status using docker inspect:\nSome output omitted here.\n❯ docker inspect --format \u0026#34;{{json .State.Health }}\u0026#34; ab94f2ac7889 { \u0026#34;Status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;FailingStreak\u0026#34;: 0, \u0026#34;Log\u0026#34;: [ { \u0026#34;Start\u0026#34;: \u0026#34;2021-09-28T15:22:57.5764644Z\u0026#34;, \u0026#34;End\u0026#34;: \u0026#34;2021-09-28T15:22:57.7825527Z\u0026#34;, \u0026#34;ExitCode\u0026#34;: 0, \u0026#34;Output\u0026#34;: \u0026#34;...\u0026#34; You can also add health checks to your Docker Compose file:\nversion: \u0026#34;3.8\u0026#34; services: web: build: . ports: - \u0026#39;8000:8000\u0026#39; healthcheck: test: curl --fail http://localhost:8000 || exit 1 interval: 10s timeout: 10s start_period: 10s retries: 3 Options:\ntest: The command to test. interval: The interval to test—that is, test every x time unit. timeout: The maximum time to wait for a response. start_period: When to start the health check. It can be used when other tasks are performed before the container is ready, such as running migrations. retries: The maximum number of retries before declaring the test as failed. If you\u0026rsquo;re using an orchestration tool other than Docker Swarm (like Kubernetes or AWS ECS), they very likely have their own internal systems for handling health checks. Refer to your specific tool\u0026rsquo;s documentation before adding the HEALTHCHECK instruction.\nDocker Image Best Practices # 1. Docker Image Versioning # Avoid using the latest tag for your images whenever possible.\nIf you rely on the latest tag (which isn\u0026rsquo;t a real \u0026ldquo;tag\u0026rdquo; as it is applied by default when an image doesn\u0026rsquo;t have an explicit tag), you can\u0026rsquo;t tell from the image tag which version of your code is running.\nRollbacks become difficult, and it\u0026rsquo;s easy to be overwritten (accidentally or maliciously). Tags, like your infrastructure and deployments, should be immutable.\nSo regardless of how you treat your internal images, you shouldn\u0026rsquo;t use latest for base images, as you might inadvertently deploy a new version with breaking changes to production.\nFor internal images, use descriptive tags to more easily tell which version of the code is running, handle rollbacks, and avoid naming conflicts. For instance, you can use the following descriptors to compose a tag.\nTimestamp Docker image ID Git commit hash Semantic version See also this answer in a Stack Overflow question \u0026ldquo;Properly Versioning Docker Images\u0026rdquo; for more options.\nFor example:\ndocker build -t web-prod-b25a262-1.0.0 . Here, we compose the tag using:\nProject name: web Environment name: prod Git commit short hash: b25a262 (obtained via the command git rev-parse --short HEAD) Semantic version: 1.0.0 Choosing a tagging scheme and sticking to it is crucial. Because commit hashes readily link image tags to code, it\u0026rsquo;s recommended to include them in your tagging scheme.\n2. Don\u0026rsquo;t Store Secrets in Images # Secrets are sensitive information such as passwords, database credentials, SSH keys, tokens, and TLS certificates. This information should not be placed in your images unencrypted because unauthorized users who gain access to the image can simply inspect the layers to extract keys.\nTherefore, don\u0026rsquo;t add plain-text secrets to your Dockerfiles, especially when you\u0026rsquo;re pushing images to a public repository like Docker Hub!!\nFROM python:3.9-slim ENV DATABASE_PASSWORD \u0026#34;SuperSecretSauce\u0026#34; Instead, they should be injected via:\nEnvironment variables (at runtime) Build-time arguments (at build time) Orchestration tools such as Docker Swarm (via Docker secrets) or Kubernetes (via Kubernetes secrets). Additionally, you can help prevent secrets from being leaked by adding common secret files and folders to your .dockerignore file.\n**/.env **/.aws **/.ssh Finally, be explicit about which files are copied into the image instead of recursively copying all files.\n# Bad practice COPY . . # Good practice COPY ./app.py . Being explicit also helps limit cache invalidation.\nEnvironment Variables # You can pass secrets via environment variables, but they are visible in all subprocesses, linked containers, and logs and via docker inspect. They are also difficult to update.\ndocker run --detach --env \u0026#34;DATABASE_PASSWORD=SuperSecretSauce\u0026#34; python：3.9-slim b25a262f870eb0fdbf03c666e7fcf18f9664314b79ad58bc7618ea3445e39239 docker inspect --format=\u0026#39;{{range .Config.Env}}{{println .}}{{end}}\u0026#39; b25a262f870eb0fdbf03c666e7fcf18f9664314b79ad58bc7618ea3445e39239 DATABASE_PASSWORD=SuperSecretSauce PATH=/usr/local/bin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin LANG=C.UTF-8 GPG_KEY=E3FF2839C048B25C084DEBE9B26995E310250568 python_version=3.9.7 python_pip_version=21.2.4 python_setuptools_version=57.5.0 python_get_pip_url=https://github.com/pypa/get-pip/raw/c20b0cfd643cd4a19246ccf204e2997af70f6b21/public/get-pip.py PYTHON_GET_PIP_SHA256=fa6f3fb93cce234cd4e8dd2beb54a51ab9c247653b52855a48dd44e6b21ff28b This is the most straightforward secret management approach. While not the most secure, it keeps honest people honest, as it provides a thin layer of protection, helping to keep secrets out of the gaze of curious wandering eyes.\nUsing shared volumes to pass secrets is a better solution, but they should be encrypted, via Vault or AWS Key Management Service (KMS), because they are saved to disk.\nBuild-Time Arguments # You can use build-time arguments to pass secrets, but these secrets are visible to those who can access the image via docker history.\nExample\nFROM python:3.9-slim ARG DATABASE_PASSWORD Build\ndocker build --build-arg \u0026#34;DATABASE_PASSWORD=SuperSecretSauce\u0026#34; . If you only need to use a secret temporarily as part of the build. For example, an SSH key for cloning a private repo or downloading a private package. You should use multi-stage builds because the builder\u0026rsquo;s history is discarded by the temporary stage.\n# Temporary stage FROM python:3.9-slim as builder # Secret argument arg ssh_private_key # Install git RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y --no-install-recommends git # Clone repo using ssh key RUN mkdir -p /root/.ssh/ \u0026amp;\u0026amp; \\ echo \u0026#34;${PRIVATE_SSH_KEY}\u0026#34; \u0026gt; /root/.ssh/id_rsa RUN touch /root/.ssh/known_hosts \u0026amp;\u0026amp; \\ ssh-keyscan bitbucket.org \u0026gt;\u0026gt; /root/.ssh/known_hosts RUN git clone git@github.com:testdrivenio/not-real.git # Final stage FROM python:3.9-slim WORKDIR /app # Copy repo from temporary image COPY --from=builder /your-repo /app/your-repo Multi-stage builds only preserve the history of the final image. You can use this for persistent secrets your application needs, such as database credentials.\nYou can also use the new --secret option in docker build to pass secrets to your Docker image, which aren\u0026rsquo;t stored in the image.\n# \u0026#34;docker_is_awesome\u0026#34; \u0026gt; secrets.txt FROM alpine # Display the secret from the default secret location. RUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret This will mount the secret from the secrets.txt file.\nBuilding the image\ndocker build --no-cache --progress=plain --secret id=mysecret,src=secrets.txt . # Output ... #4 [1/2] FROM docker.io/library/alpine #4 sha256:665ba8b2cdc0cb0200e2a42a6b3c0f8f684089f4cd1b81494fbb9805879120f7 #4 cached #5 [2/2] RUN --mount=type=secret,id=mysecret cat /run/secrets/myecret #5 sha256:75601a522ebe80ada66dedd9dd86772ca932d30d7e1b11bba94c04aa55c237de #5 0.635 docker_is_awesome#5 DONE 0.7s #6 export to image Finally, check the history to see if the secret is leaked.\n❯ docker history 49574a19241c IMAGE CREATED CREATED BY SIZE COMMENT 49574a19241c 5 minutes ago CMD [\u0026#34;/bin/sh\u0026#34;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 5 minutes ago RUN /bin/sh -c cat /run/secrets/mysecret # b… 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 4 weeks ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/sh\u0026#34;] 0B \u0026lt;missing\u0026gt; 4 weeks ago /bin/sh -c #(nop) ADD file:aad4290d27580cc1a… 5.6MB Docker Secrets # If you are using Docker Swarm, you can manage secrets using Docker secrets.\nFor example, initialize Docker Swarm mode.\ndocker swarm init Create a docker secret.\necho \u0026#34;supersecretpassword\u0026#34; | docker secret create postgres_password - qdqmbpizeef0lfhyttxqfbty0 docker secret ls ID NAME DRIVER CREATED UPDATED qdqmbpizeef0lfhyttxqfbty0 postgres_password 4 seconds ago 4 seconds ago When a container is given access to the secret above, it will be mounted at /run/secrets/postgres_password. This file will contain the actual value of the secret in plain text.\nUsing other orchestration tools?\nAWS Secrets Manager with Kubernetes Secrets: https://docs.aws.amazon.com/eks/latest/userguide/manage-secrets.html DigitalOcean Kubernetes - Recommended steps to secure a DigitalOcean Kubernetes cluster Google Kubernetes Engine - Using Secret Manager with other products Nomad - [Vault integration and retrieving dynamic secrets](https://learn.hashicorp.com/tutorials/nomad/vault-postgres?in=nomad/integrate- ","date":"2022-01-12","externalUrl":null,"permalink":"/en/posts/2022/docker-best-practice/","section":"Posts","summary":"This article shares some best practices to follow when writing Dockerfiles and using Docker, including suggestions on multi-stage builds, image optimization, and security.","title":"You Must Know These 17 Docker Best Practices!","type":"posts"},{"content":"工作十几年用过了不少显示器，从最初的 17寸，到后来的 23寸、27寸、32寸、再到现在的 34 寸，根据我自己的使用体验，来个主观推荐：\n第一名，一个34寸曲面显示器 第二名，一个27寸 + 一个23寸的双屏组合 第三名，一个32寸 + 一个23寸的双屏组合 第三名，两个 23 寸的双屏组合（并列第三名）\n以上这些屏幕推荐购买 2K 及以上的分辨率，1080p 的分辨率不推荐。\n下面我就按照时间轴说说我用过的那些显示器。\n我用过的显示器 # 双屏 23寸 1080p # 五年前公司配置的是两个 23 寸的 1080p 显示器，还有显示器支架，这在当时是非常好用的组合了，对工作效率的提升确实很有帮助。\n（真不知道那些不肯在显示器上花钱的公司是怎么想的，换个大屏显示器是花最少的钱提高程序员效率的最有效办法了）\n放在现在 1080p 的分辨率已经不推荐了，建议购买 2K 以及以上分辨率的显示屏。\n单屏 27寸 2K # 在家办公和学习苦于家里的办公桌大小有限，放置两个 23 寸显示器没那么多空间，因此我打算购买一个 27 寸显示器。 但分辨率是继续选择 1080p 还是 2K 呢？\n当时我看好的 Dell 一款 2K 分辨率的显示器，就是价格有点贵了（两千多），于是我就在网上买了另外一款 1080p 的 27 寸显示器。\n当我拿到这款 1080p 的 27 寸显示器，能明显的看到它的颗粒感，一旦注意到了就没法再忽略它了。最后退而求其次，花了不到 1500 选择了优派的一款 27 寸 2K 显示器。\n这一用就是三年，直到今年搬家了，我有了更大的办公桌，27 寸的 2K 显示器在大小上已经不太能满足我了，我就开始搜罗新的升级目标。\n双屏 27寸 2K # 我开始考虑是购买一个 34 寸显示器还是两个 27 寸 2K 显示器？\n最后处于成本的考虑，我打算在闲鱼上收一台同款的 27 寸 2K 显示器组成双屏。但因为货源确实很少，加上一本不出外地，迟迟没有买到，最后不得不考虑购买两个新的 27 寸 2K 显示器。我再次在购买的两块 27 寸的 2K 显示器，还在闲鱼上买的显示器支架全都到货了，就等快乐的组装了。\n可当我把两个显示器都摆在桌子上，我后悔了，比我想象中的要长，一米六的桌子都快盛不下它们了，另外可能是我支架没调好的原因，这两个显示器摆放在一起的时候中间有一条上宽下窄的缝 \u0026hellip; \u0026hellip;\n曲面单屏 34寸 2K # 没有尝试过真的不知道到底什么才是适合自己的。现在我终于知道了\u0026hellip; 目前最适合我的就是 34寸 显示曲面屏。\n为什么是曲面的呢？\n我目前公司的显示器组合是 32寸2K + 23寸1080p，这个32寸显示器就是直面屏幕的，在看屏幕边缘的内容时没有曲面屏幕来的舒服。\n回到34寸曲面屏，市场中种类繁多，如果不想踩坑（折腾），建议直接买小米34寸曲面显示器。双十一钱购买的好像不足 1900。\n最后 # 以上就是我用过的显示器的简单分享，比较主流但也比较有限。比如我没用过超过 2K 分辨率以上的显示器，也没用过更大尺寸和三屏以上的组合。\n最后还是希望这个分享对你选购显示器有一点点帮助。\n","date":"2021-12-21","externalUrl":null,"permalink":"/posts/2021/choose-monitor/","section":"Posts","summary":"本文分享了个人在选择显示器时的经验和建议，包括不同尺寸、分辨率和屏幕组合的优缺点，以及如何根据工作需求选择最合适的显示器。","title":"2022年序员如何选择显示器？1080p还是2K? 单屏还是多屏？","type":"posts"},{"content":"","date":"2021-12-21","externalUrl":null,"permalink":"/tags/monitor/","section":"标签","summary":"","title":"Monitor","type":"tags"},{"content":"","date":"2021-12-07","externalUrl":null,"permalink":"/tags/badge/","section":"标签","summary":"","title":"Badge","type":"tags"},{"content":" 问题 # 在一个组织内，不同的团队之间可能会有不同的维度来评估 CI/CD 的成熟度。这使得对衡量每个团队的 CI/CD 的表现变得困难。\n如何快速评估哪些项目遵循最佳实践？如何更容易地构建高质量的安全软件？组织内需要建立一个由团队成员一起讨论出来的最佳实践来帮助团队建立明确的努力方向。\n如何评估 # 这里我参考了开源项目 CII 最佳实践徽章计划，这是 Linux 基金会 (LF) 发起的一个开源项目。它提供一系列自由/开源软件 (FLOSS) 项目的最佳实践方法。参照这些最佳实践标准的项目可以进行自认证, 以获得核心基础设施促进会(CII)徽章。要做到这点无需任何费用，你的项目可以使用 Web 应用（BadgeApp) 来证明是如何符合这些实践标准的以及其详细状况。\n这些最佳实践标准可以用来：\n鼓励项目遵循最佳实践。 帮助新的项目找到那些它们要遵循的最佳实践 帮助用户了解哪些项目遵循了最佳实践（这样用户可以更倾向于选择此类项目）。 最佳实践包含以下五个标准：基本，变更控制，报告，质量，安全，分析。\n更多关于标准的细分可以参考 CII 中文文档 或 CII 英文文档。\n已经很多知名的项目比如 Kubernetes, Node.js 等在使用这个最佳实践徽章计划\n如果你的项目在 GitHub 上或是你可以按照上述的徽章计划进行评估，就可以使用它来评估你项目的最佳实践，并可以在项目主页的 README 上显示徽章结果。\n定制最佳实践标准 # 如果上述项目不能满足你的评估要求，结合我的实践，制定了如下“最佳实践标准”并分配了相应的成熟度徽章，供参考。\n计算规则 # 每个最佳实践标准都有分数，通常一般的标准是10分，重要的标准是20分 带有🔰的最佳实践标准表示“一定要有” 带有👍的最佳实践标准表示“应当有” 每个项目的最佳实践标准分数之和落在的区间获得对应的徽章 徽章分数对照表 # 徽章 分数 描述 🚩WIP \u0026lt; 100 小于100分获得 🚩Work In Progress 徽章 ✔️PASSING = 100 等于100分获得 ✔️PASSING 徽章 🥈SILVER \u0026gt; 100 \u0026amp;\u0026amp; \u0026lt;= 150 大于100，小于等于150分获得🥈银牌徽章 🥇GOLD \u0026gt; 150 大于等于150分获得🥇金牌徽章 注：这个分数区间可调整。\n最佳实践标准和分数 # 类别 最佳实践标准 分数 描述 基本 🔰构建任何分支 20 Jenkins：支持任何分支构建 🔰构建任何PR 20 Jenkins：支持对任何 Pull Request 在 Merge 之前进行构建 🔰上传制品 10 Jenkins：构建产物上传到制品仓库保存 👍容器化构建 10 推荐使用容器化技术实现Pipeline 质量 🔰自动化测试 20 Jenkins：支持触发冒烟/单元/回归测试 👍性能测试 10 Jenkins：支持触发性能测试 👍代码覆盖率收集 10 Jenkins：支持获得代码覆盖率 安全 🔰漏洞扫描 10 Jenkins：支持触发漏洞扫描 🔰License扫描 10 Jenkins：支持触发证书扫描 分析 👍Code Lint 10 Jenkins：支持对PR进行代码格式检查 👍静态代码分析 10 Jenkins：支持对PR进行静态代码分析 👍动态代码分析 10 Jenkins：支持对PR进行动态代码分析 报告 🔰Email或Slack通知 10 支持通过Email或Slack等方式通知 注：以Jenkins为例。\n最终的结果 # No Repository Name 实现的最佳实践标准 徽章 1 project-a 🔰构建任何分支🔰构建任何PR🔰上传制品🔰自动化测试🔰Email或Slack通知 🚩WIP 2 project-b 🔰构建任何分支🔰构建任何PR🔰上传制品🔰自动化测试🔰漏洞扫描🔰License扫描🔰Email或Slack通知 ✔️PASSING 3 project-c 🔰构建任何分支🔰构建任何PR🔰上传制品👍容器化构建🔰自动化测试🔰漏洞扫描🔰License扫描🔰Email或Slack通知 🥈SILVER 4 project-d 🔰构建任何分支🔰构建任何PR🔰上传制品👍容器化构建🔰自动化测试👍性能测试👍代码覆盖率收集🔰漏洞扫描🔰License扫描👍Code Lint👍静态代码分析👍动态代码分析🔰Email或Slack通知 🥇GOLD Q\u0026amp;A # Q: 为什么使用徽章而不是分数？\nA: 使用徽章能更好的帮助团队朝着目标而不是分数努力。\nQ: 建立最佳实践标准还有哪些帮助？\nA: 团队之间容易进行技术共享，更容易地构建高质量的安全软件，保持团队之间在统一的高水准。\n","date":"2021-12-07","externalUrl":null,"permalink":"/posts/2021/cicd-assessment/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 的 generic-webhook-trigger 插件来实时获取 Bitbucket 仓库的事件信息，如 Pull Request ID 等。","title":"组织内如何评估 CI/CD 成熟度","type":"posts"},{"content":"","date":"2021-11-09","externalUrl":null,"permalink":"/tags/actions/","section":"标签","summary":"","title":"Actions","type":"tags"},{"content":"最近实现了一个很有意思的 Workflow，就是通过 GitHub Actions 自动将每次最新发布的文章自动同步到我的 GitHub 首页。\n就像这样在首页显示最近发布的博客文章。\n要实现这样的工作流需要了解以下这几点：\n需要创建一个与 GitHub 同名的个人仓库，这个仓库的 README.md 信息会显示在首页 通过 GitHub Actions 自动获取博客的最新文章并更新 README.md 只有当有新的文章发布的时候才触发自动获取、更新文章 GitHub Action GitHub 同名的个人仓库是一个特殊仓库，即创建一个与你的 GitHub 账号同名的仓库，添加的 README.md 会在 GitHub 个人主页显示。\n举个例子：如果你的 GitHub 名叫 GeBiLaoWang，那么当你创建一个叫 GeBiLaoWang 的 Git 仓库，添加 README.md 后就会在主页显示。\n针对这个功能 GitHub 上有很多丰富多彩的个人介绍（如下）。更多灵感可以参看这个链接：https://awesomegithubprofile.tech/\n自动获取文章并更新 README.md # 在 GitHub 上有很多开发者为 GitHub Actions 开发新的小功能。我这里用到一个开源项目叫 blog-post-workflow，它可以通过 RSS（订阅源）来获取到博客的最新文章。\n它不但支持 RSS 还支持获取 StackOverflow 以及 Youtube 视频等资源。\n我只需要在 GitHub 同名的仓库下添加一个这样的 Workflow YML .github/workflows/blog-post-workflow.yml 即可。\nname: Latest blog post workflow on: schedule: - cron: \u0026#39;* 2 * * *\u0026#39; workflow_dispatch: jobs: update-readme-with-blog: name: Update this repo\u0026#39;s README with latest blog posts runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: gautamkrishnar/blog-post-workflow@master with: # 我的博客 RSS 链接 feed_list: \u0026#34;https://shenxianpeng.github.io/atom.xml\u0026#34; # 获取最新 10 篇文章 max_post_count: 10 刚开始我需要让这个 Workflow 能工作即可。因此用的定时触发，即就是每天早上两点就自动获取一次最新文章并更新这个特殊仓库 README.md。\n这个做法还可以，但不够节省资源也不够完美。最好的做法是：只有当有新文章发布时才触发上面的 Workflow 更新 README.md。这就需要有一个 Webhook 当检测到有文章更新时自动触发这里的 Workflow。\n触发另一个 GitHub Action # GitHub Actions 提供了一个 Webhook 事件叫做 repository_dispatch 可以来做这件事。\n它的原理：使用 GitHub API 来触发一个 Webhook 事件，这个事件叫做 repository_dispatch，这个事件里的类型是可以自定义的，并且在要被触发的 workflow 里需要使用 repository_dispatch 事件。\n即：在存放博客文章的仓库里要有一个 Workflow 通过发送 repository_dispatch 事件触发特殊仓库中的 Workflow 来更新 README.md。\n这里我定义事件类型名叫 special_repository，它只接受来自 GitHub API repository_dispatch 事件。\n再次调整上面的 .github/workflows/blog-post-workflow.yml 文件如下：\n# special_repository.yml name: Latest blog post workflow on: repository_dispatch: # 这里的类型是可以自定义的，我将它起名为：special_repository types: [special_repository] workflow_dispatch: jobs: update-readme-with-blog: name: Update this repo\u0026#39;s README with latest blog posts runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: gautamkrishnar/blog-post-workflow@master with: feed_list: \u0026#34;https://shenxianpeng.github.io/atom.xml\u0026#34; max_post_count: 10 接受事件的 Workflow 修改好了。如何发送类型为 special_repository 的 repository_dispatch 事件呢？我这里通过 curl 直接调用 API 来完成。\ncurl -XPOST -u \u0026#34;${{ secrets.PAT_USERNAME}}:${{secrets.PAT_TOKEN}}\u0026#34; \\ -H \u0026#34;Accept: application/vnd.github.everest-preview+json\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; https://api.github.com/repos/shenxianpeng/shenxianpeng/dispatches \\ --data \u0026#39;{\u0026#34;event_type\u0026#34;: \u0026#34;special_repository\u0026#34;}\u0026#39; 最后，发送事件 Workflow YML .github/workflows/send-dispatch.yml 如下:\nname: Tigger special repository on: push: # 当 master 分支有变更的时候触发 workflow branches: - master workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: Send repository dispatch event run: | curl -XPOST -u \u0026#34;${{ secrets.PAT_USERNAME}}:${{secrets.PAT_TOKEN}}\u0026#34; \\ -H \u0026#34;Accept: application/vnd.github.everest-preview+json\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; https://api.github.com/repos/shenxianpeng/shenxianpeng/dispatches \\ --data \u0026#39;{\u0026#34;event_type\u0026#34;: \u0026#34;special_repository\u0026#34;}\u0026#39; 注：PAT_USERNAME 和 PAT_TOKEN 需要在当前的仓库【设置 -\u0026gt; Secrets】里进行添加，这里就不具体介绍了，需要可以自行搜索。\n以上就是通过 GitHub Actions 实现当博客有新发布的文章后自动更新 GitHub 首页的所有内容了。\n如果还有什么有意思的玩法欢迎评论区里分享一下吧。\n","date":"2021-11-09","externalUrl":null,"permalink":"/posts/2021/github-special-repository/","section":"Posts","summary":"本文介绍了如何使用 GitHub Actions 自动将发布的博客文章更新到 GitHub 个人主页，提升个人主页的动态性和可读性。","title":"GitHub Actions 还能这么玩？自动将发布的博客文章更新到 GitHub 个人主页","type":"posts"},{"content":" Preface # The 2021-22 World Quality Report (WQR), a collaborative effort by Micro Focus, Capgemini, and Sogeti, is the only report analyzing software quality and testing trends globally.\nThis report surveyed 1750 executives and professionals. This includes everyone from top management to QA test managers and quality engineers, spanning 10 industries across 32 countries.\nThe World Quality Report (WQR) is a unique global study, and this year\u0026rsquo;s survey highlights the impact of evolving pandemic-influenced application demands in newly deployed methods, as well as the adoption of QA into Agile and DevOps practices, and the continued rise of AI.\nFollowing test-focused software quality reports like this helps us quickly understand the current state and trends in the software testing industry.\nFive Key Themes # A key message from the WQR: In the continuing COVID-19 pandemic, we\u0026rsquo;ve seen the convergence of digital transformation and the real-time adoption of Agile and DevOps practices. Furthermore, QA is emerging as a leader in adopting Agile and DevOps practices, providing teams with the tools and processes to foster quality throughout the Software Development Life Cycle (SDLC).\nThe WQR highlights five specific themes around key findings and trends:\nImpact of the COVID-19 pandemic on QA organizations and software testing Real-time convergence of digital transformation with DevOps and Agile adoption and the increasingly crucial role of QA Geographically dispersed teams focusing on business outcomes when deploying applications across environments Artificial Intelligence (AI) enhancing Agile and DevOps to cultivate a growing culture of quality accountability across all teams Using AI-driven continuous testing and quality management tools to address customer experience priorities and rapidly changing pandemic-influenced requirements Key Findings and Trends # 1. Impact of the COVID-19 Pandemic on QA Organizations and Software Testing # The COVID-19 pandemic had a direct and tangible impact on almost all aspects of business, including QA. However, many QA organizations were able to adapt to the realities of new hybrid work environments and transition to the new reality of working in distributed teams. This was likely possible because the trend towards hybrid distributed teams was already growing, and the pandemic simply accelerated this trend.\nCustomer Experience is King # The COVID-19 pandemic refocused attention on customers and their experiences. The top-rated aspects this year were:\nEnhanced customer experience, chosen by 63% of respondents Followed by enhanced security (62%) Then higher responsiveness to business needs (61%) And higher-quality software solutions (61%) From Guardian to Quality Champion # Testing and QA goals have also been reordered in the past year. Last year, guardians of business outcomes and quality were clear leaders, while this year, the support rate between these metrics has narrowed.\nQuality guardians, quality speed, and quality enablement within teams led at 62% Business assurance, digital wellbeing, and automation came in second at 61% QA teams are evolving from guardians of quality to champions of quality. QA teams are becoming vibrant leaders in organizational quality initiatives, empowering everyone on the team to achieve quality while contributing to business outcomes and growth.\n2. Real-time Convergence of Digital Transformation with DevOps and Agile Adoption and the Increasingly Crucial Role of QA # Driving Digital Transformation # This year, digital transformation initiatives aligned with pandemic requirements. Before the pandemic began, Agile and DevOps were a growing trend. During the pandemic, we began to see QA now playing a key role in organizations\u0026rsquo; adoption of Agile and DevOps, blurring the lines between development and testing, while creating a blended approach to quality.\nLet\u0026rsquo;s look at the drivers of digital transformation: Improved productivity and efficiency led at 47%; followed by improved service/product quality at 46%; then speed, better agility and flexibility; customer experience directly behind speed; closely followed by cost reduction and creating innovative opportunities; competitive differentiation was last.\nBecause competitive differentiation seems more like a side benefit of digital transformation, whereas digital transformation itself helps improve efficiency, quality, speed, and overall better customer experience.\nThe Growing Role of QA in DevOps and Agile Adoption – Guided by Business Priorities # This year, we saw a significant shift in business requirements, becoming more important than the demands of the technology stack. Compared to last year, the number of participants weighting the tech stack decreased by 16 percentage points, replaced by the largest increase in:\nBusiness priorities, now ranked number one, increased by 11 percentage points compared to last year Also significantly increased compared to last year is culture/agility, up 21 percentage points 3. Geographically Dispersed Teams Focusing on Business Outcomes When Deploying Applications Across Environments # Last year\u0026rsquo;s survey, conducted at the start of the global pandemic, showed signs of the transformations needed to meet business objectives, and the new requirements of remote work and digital transformation. With this year\u0026rsquo;s survey, we see that digital transformation continues, even while keeping pace with pandemic-influenced new work requirements. This accelerated company plans to migrate workloads to the cloud, partly due to planned digital transformation initiatives, and the rapid shift to remote work, fueling a need for increased security.\nDue to the pandemic-influenced workplace, the top-rated focus was on remote access to testing systems and test environments (using SaaS and cloud). Supporting this remote access were secondary factors based on remote, including better team collaboration tools. To support the quality of modern applications, the test environments themselves must also be modernized. This year, we saw:\nOrganizations increasingly satisfied with using cloud and containers to modernize test environments (highest satisfaction) Followed by improved booking and management of test environments (+16) Then providing visibility (+22) Lastly cost efficiency (+18) 4. Artificial Intelligence (AI) Enhancing Agile and DevOps to Cultivate a Growing Culture of Quality Accountability Across All Teams # Artificial intelligence continues to transform how test automation is built, and how testing is executed. We are seeing increasing confidence in the level of AI-based testing within organizations, with almost half of the respondents stating that they already possess the repository of test execution data needed for AI and ML, and that they are willing to act on the intelligence provided by their AI and ML platforms.\nThis year, we also asked respondents to predict their likelihood of leveraging a range of approaches to accelerate and optimize testing in Agile and DevOps environments. Compared to the same period last year:\nAutomated quality gates integrating testing into CI/CD pipelines (+5) Implementing intelligent and automated dashboards to achieve continuous quality monitoring saw the largest increase (+9) The newly added this year, using AI to optimize test cases ranked second overall, only behind shifting testing left. 5. Using AI-driven Continuous Testing and Quality Management Tools to Address Customer Experience Priorities and Rapidly Changing Pandemic-Influenced Requirements # This year, we asked respondents about the benefits of test automation:\nFirst, all items showed a year-over-year downward trend compared to last year, showcasing the challenges faced when working in hybrid and distributed teams. Such as better defect detection, shorter test cycles, lower overall security risks, better test coverage, lower testing costs, and control over the transparency of testing are all evident benefits. AI/ML is the fourth highest benefit, also proving its potential and value. Key Recommendations # QA Orchestration in Agile and DevOps # Focus on what matters most: customer experience and business objectives, meeting both with efficiency and speed. Simultaneously, adopt an engineering mindset within your teams, and embrace multi-skilling and upskilling. A new trend rapidly becoming the new normal is the SDET (Software Development Engineer in Test). Invest in insights, especially real-time insights across your entire QA and testing function. Focus on developing intelligent dashboards with real-time KPIs, from short-term tactical plans to long-term planning and strategic direction.\nIntelligent Test Automation # Standardize the use of test automation in QA by adopting an automation-first approach to software quality delivery. Expand automation across the E2E lifecycle, incorporating automation into all QA activities.\nArtificial Intelligence and Machine Learning # Drive the use of AI – don’t be driven by it. AI and ML promise exponential improvements, but use AI as a tool, not a replacement for the business decisions you’re making. For example, use AI to illuminate what to do and when to do it. It helps not just identify failures but also helps identify the reasons those failures are happening. Furthermore, focus AI on what matters most, identifying the most challenging areas of quality in software delivery. If you haven’t incorporated AI into quality yet, now is the best time to start.\nTest Environment Management (TEM) and Test Data Management (TDM) # Cloud adoption is continuing to show steady and consistent growth, but be mindful to ensure that the future doesn’t overshadow present needs. The key to successfully adopting cloud is to ensure integrity with legacy applications. Additionally, data analytics is now a key aspect of a test data management framework.\nSecurity and Smart Industries # Remote connectivity demands appropriate consideration of security and resilience for testing and QA organizations. Invest in innovation, in your labs, and in your teams. Whether you start with a POC to prove feasibility, securing management support is key to implementing change.\nSummary # The changes I learned from reading the entire WQR report:\nGuided by business priorities. Compared to last year, the weighting of the tech stack decreased by 16%, replaced by business priorities. Then culture, agility increased by 21% Highest satisfaction rating for modernizing test environments using cloud and containers Automated quality gates integrating testing into CI/CD pipelines (+5%) Implementing intelligent and automated dashboards to achieve continuous quality monitoring saw the largest increase (+9%) Using AI to optimize test cases ranked second overall, only behind shifting testing left Artificial intelligence continues to transform how automation is built and how testing is executed. Almost half of the respondents stated that they possess the repository of test execution data needed for AI/ML, and that they are willing to act on what AI/ML provides. Follow the WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo; and reply with \u0026ldquo;WQR\u0026rdquo; to download the complete version of the \u0026ldquo;2021-22 World Quality Report (WQR)\u0026rdquo;.\n","date":"2021-11-06","externalUrl":null,"permalink":"/en/posts/2021/world-quality-report/","section":"Posts","summary":"This article presents the key findings and trends from the 2021-22 World Quality Report (WQR), highlighting the impact of the COVID-19 pandemic on software quality and testing, and the crucial role of QA in Agile and DevOps.","title":"2021-22 World Quality Report (WQR)","type":"posts"},{"content":"","date":"2021-11-06","externalUrl":null,"permalink":"/en/tags/quality/","section":"Tags","summary":"","title":"Quality","type":"tags"},{"content":"","date":"2021-11-06","externalUrl":null,"permalink":"/en/tags/report/","section":"Tags","summary":"","title":"Report","type":"tags"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/en/tags/coverity/","section":"Tags","summary":"","title":"Coverity","type":"tags"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/en/tags/polaris/","section":"Tags","summary":"","title":"Polaris","type":"tags"},{"content":"This might be the most detailed article about Polaris available in Chinese.\nWhat is Polaris? # Polaris - A SaaS platform for managed Static Application Security Testing (SAST) tools. It\u0026rsquo;s a web site used to categorize and remediate vulnerabilities and run reports. SAST - A tool that analyzes source code or build artifacts to find security vulnerabilities. It\u0026rsquo;s a crucial step in ensuring security throughout the Software Development Life Cycle (SDLC). Coverity - Coverity is the original Static Application Security Testing (SAST) tool provided by Synopsys. Polaris is the SaaS version of Coverity. Synopsys - The company that develops Polaris and other software scanning tools, such as BlackDuck.\nWhich Languages Does Polaris Support? # C/C++ C# Java JavaScript TypeScript PHP Python Fortran Swift ...and more Polaris SaaS Platform # Usually, if your organization has introduced the Polaris SaaS service, you will have the following URL available: https://organization.polaris.synopsys.com\nAfter logging in, you can create projects for your Git repositories.\nRecommendation: Name the project the same as the Git repository.\nHow Does Polaris Perform Vulnerability Scanning? # Polaris Installation # Before performing a Polaris scan, you need to download and install polaris.\nIf your Polaris server URL is: POLARIS_SERVER_URL=https://organization.polaris.synopsys.com\nThe download link is: $POLARIS_SERVER_URL/api/tools/polaris_cli-linux64.zip\nThen unzip the downloaded polaris_cli-linux64.zip to your local machine and add its bin directory to your PATH.\nPolaris YAML Configuration File # Before scanning, you need to create a YAML file for your project. The default configuration file name is polaris.yml, located in the project root directory. If you want to specify a different configuration file name, you can use the -c option in the polaris command.\nRun polaris setup in the project root directory to generate a generic polaris.yml file.\nRun polaris configure to verify that your file is syntactically correct and that polaris has no problems.\nCapture - Capture # A YAML configuration file can contain three types of Capture:\nBuild - Run build commands and then analyze the results. Filesystem - For interpreted languages, provide the project type and a list of extensions to analyze. Buildless - For some languages that use dependency managers, such as Maven. Languages Build Options C, C++, ObjectiveC, Objective C++,Go, Scala, Swift Use Build capture PHP, Python, Ruby Use Buildless or Filesystem capture C#, Visual Basic. Use Build capture for more accurate results; use Buildless capture for simplicity Java Use Build capture for more accurate results; use Buildless capture for simplicity JavaScript,TypeScript Use Filesystem capture; use Buildless capture for simplicity Analyze - Analyze # If you are scanning C/C++ code, you should include this analysis section to make full use of Polaris\u0026rsquo;s scanning capabilities:\nanalyze: mode: central coverity: cov-analyze: [\u0026#34;--security\u0026#34;,\u0026#34;--concurrency\u0026#34;] Polaris YAML Example Files # Example 1: A C/C++ project\nversion: \u0026#34;1\u0026#34; project: name: test-cplus-demo branch: ${scm.git.branch} revision: name: ${scm.git.commit} date: ${scm.git.commit.date} capture: build: cleanCommands: - shell: [make, -f, GNUmakefile, clean] buildCommands: - shell: [make, -f, GNUmakefile] analyze: mode: central install: coverity: version: default serverUrl: https://organization.polaris.synopsys.com Example 2: A Java project\nversion: \u0026#34;1\u0026#34; project: name: test-java-demo branch: ${scm.git.branch} revision: name: ${scm.git.commit} date: ${scm.git.commit.date} capture: build: cleanCommands: - shell: [gradle, -b, build.gradle, --no-daemon, clean] buildCommands: - shell: [gradle, -b, build.gradle, --no-daemon, shadowJar] fileSystem: ears: extensions: [ear] files: - directory: ${project.projectDir} java: files: - directory: ${project.projectDir} javascript: files: - directory: client-vscode - excludeRegex: node_modules|bower_components|vendor python: files: - directory: ${project.projectDir} wars: extensions: [war] files: - directory: ${project.projectDir} analyze: mode: central install: coverity: version: default serverUrl: https://organization.polaris.synopsys.com Example 3: A CSharp project\nversion: \u0026#34;1\u0026#34; project: name: test-ssharp-demo branch: ${scm.git.branch} revision: name: ${scm.git.commit} date: ${scm.git.commit.date} capture: build: buildCommands: # If the build process is complex, you can write a script and then call it. - shell: [\u0026#39;script\\polaris.bat\u0026#39;] # Skip some files you don\u0026#39;t want to scan. skipFiles: - \u0026#34;*.java\u0026#34; - \u0026#34;*.text\u0026#34; - \u0026#34;*.js\u0026#34; analyze: mode: central install: coverity: version: default serverUrl: https://organization.polaris.synopsys.com For more details on writing polaris.yml, please refer to the official Polaris documentation: https://sig-docs.synopsys.com/polaris/topics/c_conf-overview.html\nExecuting Analysis # Use the following command to perform Polaris analysis:\npolaris -c polaris.yml analyze -w --coverity-ignore-capture-failure --coverity-ignore-capture-failure - Ignore Coverity capture failures. Run polaris help analyze to see more information about analysis commands.\nPolaris Analysis Results # If the Polaris analysis is successful, you will see a success message in the console as follows:\n[INFO] [1zb99xsu] Coverity job completed successfully! [INFO] [1zb99xsu] Coverity - analyze phase took 4m 36.526s. Analysis Completed. Coverity analysis { \u0026#34;JobId\u0026#34;: \u0026#34;mlkik4esb961p0dtq8i6m7pm14\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Success\u0026#34; } Job issue summary { \u0026#34;IssuesBySeverity\u0026#34;: { \u0026#34;Critical\u0026#34;: 0, \u0026#34;High\u0026#34;: 250, \u0026#34;Medium\u0026#34;: 359, \u0026#34;Low\u0026#34;: 81 }, \u0026#34;Total\u0026#34;: 690, \u0026#34;NewIssues\u0026#34;: 0, \u0026#34;ClosedIssues\u0026#34;: 0, \u0026#34;SummaryUrl\u0026#34;: \u0026#34;https://organization.polaris.synopsys.com/projects/bb079756-194e-4645-9121-5131493a0c93/branches/d567c376-4d5d-4941-8733-aa27bb2f5f5b\u0026#34; } This shows a total of 690 vulnerabilities found, and how many of each severity level. Specific vulnerability information needs to be viewed by logging into the Polaris SaaS platform.\nClicking the link in SummaryUrl will directly jump to the Polaris scan results for that project.\n","date":"2021-10-24","externalUrl":null,"permalink":"/en/posts/2021/what-is-polaris/","section":"Posts","summary":"This article introduces the basic concepts of Polaris, the supported programming languages, how to use the SaaS platform, and how to configure and run Polaris for static code analysis. It also provides example YAML configuration files and how to view the analysis results.","title":"Polaris - Static Code Analysis","type":"posts"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/en/tags/security/","section":"Tags","summary":"","title":"Security","type":"tags"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/en/tags/static/","section":"Tags","summary":"","title":"Static","type":"tags"},{"content":"不管是对于 Git 的初学者还是经常使用 Git 的码农们，在日常工作中难免会有遇到有的命令一时想不起来。不妨将下面总结的一些 Git 常用命令及技巧收藏或打印出来，以备需要的时候可以很快找到。\ngit config # # 检查 git 配置 git config -l # 设置你的 git 提交 username 和 email # 例如：对于公司里项目 git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your_email@organization.com\u0026#34; # 例如：对于个人的 GitHub 项目 git config user.name \u0026#34;Your Name\u0026#34; git config user.email \u0026#34;your_emailn@gmail.com\u0026#34; # 使用 HTTP/HTTPS 避免每次输入密码 git config --global credential.helper cache git init # # 初始化一个仓库 git init git add # # 将文件添加到暂存区 git add file_name # 将所有文件添加到暂存区 git add . # 仅将某些文件添加到暂存区, 例如:仅添加所有以 \u0026#39;test*\u0026#39; 开头的文件 git add test* git status # # 检查仓库状态 git status git commit # # 提交更改 git commit # 提交带有消息的更改 git commit -m \u0026#34;This is a commit message\u0026#34; git log # # 查看提交历史 git log # 查看提交历史和显示相应的修改 git log -p # 显示提交历史统计 git log --stat # 显示特定的提交 git show commit_id # 以图形方式显示当前分支的提交信息 git log --graph --oneline # 以图形方式显示所有分支的提交信息 git log --graph --oneline --all # 获取远程仓库的当前提交日志 git log origin/master git diff # # 在使用 diff 提交之前所做的更改 git diff git diff some_file.js git diff --staged git rm # # 删除跟踪文件 git rm file_name git mv # # 重命名文件 git mv old_file_name new_file_name git checkout # # 切换分支 git checkout branch_name # 还原未暂存的更改 git checkout file_name git reset # # 还原暂存区的更改 git reset HEAD file_name git reset HEAD -p git commit --amend # # 修改最近的提交信息 git commit --amend # 修改最近的提交信息为：New commit message git commit --amend -m \u0026#34;New commit message\u0026#34; git revert # # 回滚最后一次提交 git revert HEAD # 回滚指定一次提交 git revert commit_id git branch # # 创建分支 git branch branch_name # 创建分支并切到该分支 git checkout -b branch_name # 显示当前分支 git branch # 显示所有分支 git branch -a # 检查当前正在跟踪的远程分支 git branch -r # 删除分支 git branch -d branch_name git merge # # 将 branch_name 合并到当分支 git merge branch_name # 中止合并 git merge --abort git pull # # 从远程仓库拉取更改 git pull git fetch # # 获取远程仓库更改 git fetch git push # # 推送更改到远程仓库 git push # 推送一个新分支到远程仓库 git push -u origin branch_name # 删除远程仓库分支 git push --delete origin branch_name git remote # # 添加远程仓库 git add remote https://repository_name.com # 查看远程仓库 git remote -v # 查看远程仓库的更多信息 git remote show origin Git技巧和窍门 # 清理已合并分支 # 清理已经合并的本地分支\ngit branch --merged master | grep -v \u0026#34;master\u0026#34; | xargs -n 1 git branch -d .gitignore # 指明 Git 应该忽略的故意不跟踪的文件的文件，比如 .gitignore 如下\n# 忽略 .vscode 目录 .vscode/ # 忽略 build 目录 build/ # 忽略文件 output.log .gitattributes # 关于 .gitattributes 请参考\nhttps://www.git-scm.com/docs/gitattributes https://docs.github.com/en/get-started/getting-started-with-git/configuring-git-to-handle-line-endings ","date":"2021-10-23","externalUrl":null,"permalink":"/posts/2021/git-cheatsheet/","section":"Posts","summary":"本文总结了 Git 的常用命令和技巧，帮助开发者快速查找和使用 Git 命令，提高工作效率。","title":"Git 常用命令备忘录","type":"posts"},{"content":"","date":"2021-09-18","externalUrl":null,"permalink":"/en/tags/gradle/","section":"Tags","summary":"","title":"Gradle","type":"tags"},{"content":"After you have set up the SonarQube instance, you will need to integrate SonarQube with project.\nBecause I used the community edition version, it doesn\u0026rsquo;t support the C/C++ project, so I only demo how to integrate with Maven, Gradle, and Others.\nFor example, the demo project name and ID in SonarQube are both test-demo, and I build with Jenkins.\nBuild with Maven # Add the following to your pom.xml file:\n\u0026lt;properties\u0026gt; \u0026lt;sonar.projectKey\u0026gt;test-demo\u0026lt;/sonar.projectKey\u0026gt; \u0026lt;/properties\u0026gt; Add the following code to your Jenkinsfile:\nstage(\u0026#39;SonarQube Analysis\u0026#39;) { def mvn = tool \u0026#39;Default Maven\u0026#39;; withSonarQubeEnv() { sh \u0026#34;${mvn}/bin/mvn sonar:sonar\u0026#34; } } Build with Gradle # Add the following to your build.gradle file:\nplugins { id \u0026#34;org.sonarqube\u0026#34; version \u0026#34;3.3\u0026#34; } sonarqube { properties { property \u0026#34;sonar.projectKey\u0026#34;, \u0026#34;test-demo\u0026#34; } } Add the following code to your Jenkinsfile:\nstage(\u0026#39;SonarQube Analysis\u0026#39;) { withSonarQubeEnv() { sh \u0026#34;./gradlew sonarqube\u0026#34; } } Build with Other(for JS, TS, Python, \u0026hellip;) # Create a sonar-project.properties file in your repository and paste the following code:\nsonar.projectKey=test-demo Add the following code to your Jenkinsfile:\nstage(\u0026#39;SonarQube Analysis\u0026#39;) { def scannerHome = tool \u0026#39;SonarScanner\u0026#39;; withSonarQubeEnv() { sh \u0026#34;${scannerHome}/bin/sonar-scanner\u0026#34; } } More about how to integrate with SonarQube, please visit your SonarQube instance documentation: http://localhost:9000/documentation\n","date":"2021-09-18","externalUrl":null,"permalink":"/en/posts/2021/sonarqube-integration/","section":"Posts","summary":"This article explains how to integrate SonarQube Community Edition with Maven, Gradle, and other projects, including the necessary configurations and Jenkins pipeline setup.","title":"How does SonarQube Community Edition integrate with the project","type":"posts"},{"content":"","date":"2021-09-18","externalUrl":null,"permalink":"/en/tags/sonarqube/","section":"Tags","summary":"","title":"SonarQube","type":"tags"},{"content":"","date":"2021-09-07","externalUrl":null,"permalink":"/en/tags/lcov/","section":"Tags","summary":"","title":"Lcov","type":"tags"},{"content":"","date":"2021-09-07","externalUrl":null,"permalink":"/en/tags/perl/","section":"Tags","summary":"","title":"Perl","type":"tags"},{"content":"When execute command: lcov --capture --directory . --no-external --output-file coverage.info to generate code coverage report, I encountered the following error:\n$ lcov --capture --directory . --no-external --output-file coverage.info Capturing coverage data from . Can\u0026#39;t locate JSON/PP.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/local/bin/geninfo line 63. BEGIN failed--compilation aborted at /usr/local/bin/geninfo line 63. sh-4.2$ perl -MCPAN -e \u0026#39;install JSON\u0026#39; Can\u0026#39;t locate CPAN.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .). BEGIN failed--compilation aborted. Can\u0026rsquo;t locate CPAN.pm # fixed this problem \u0026ldquo;Can\u0026rsquo;t locate CPAN.pm\u0026rdquo; by running the command yum install perl-CPAN\nsh-4.2$ sudo perl -MCPAN -e \u0026#39;install JSON\u0026#39; [sudo] password for sxp: Can\u0026#39;t locate CPAN.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .). BEGIN failed--compilation aborted. sh-4.2$ sudo yum install perl-CPAN Then run sudo perl -MCPAN -e 'install JSON' again, it works.\nCan\u0026rsquo;t locate JSON/PP.pm # fixed this problem by copying backportPP.pm to the PP.pm file.\n$ cd /usr/local/share/perl5/JSON $ ls backportPP backportPP.pm $ cp backportPP.pm PP.pm Can\u0026rsquo;t locate Module/Load.pm # bash-4.2$ geninfo --version Can\u0026#39;t locate Module/Load.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/local/bin/geninfo line 63. BEGIN failed--compilation aborted at /usr/local/bin/geninfo line 63. bash-4.2$ Install perl-Module-Load-Conditional can resolved.\nsudo yum install perl-Module-Load-Conditional Can\u0026rsquo;t locate Capture/Tiny.pm in @INC # sh-4.2$ lcov --version Can\u0026#39;t locate Capture/Tiny.pm in @INC (@INC contains: /usr/local/bin/../lib /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/local/bin/../lib/lcovutil.pm line 13. BEGIN failed--compilation aborted at /usr/local/bin/../lib/lcovutil.pm line 13. Compilation failed in require at /usr/local/bin/lcov line 104. BEGIN failed--compilation aborted at /usr/local/bin/lcov line 104. Fixed with following command\nperl -MCPAN -e \u0026#39;install Capture::Tiny\u0026#39; Then run lcov --version back to work.\nsh-4.2$ lcov --version lcov: LCOV version v1.16-16-g038c2ca Can\u0026rsquo;t locate DateTime.pm # $ genhtml --help Can\u0026#39;t locate DateTime.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/local/bin/genhtml line 87. BEGIN failed--compilation aborted at /usr/local/bin/genhtml line 87. Need to install the perl module DateTime, On Centos7 run\nsudo yum install 'perl(DateTime)'\nBut this still doesn\u0026rsquo;t work for me.\nRun geninfo command failed # Capturing coverage data from . Compress::Raw::Zlib version 2.201 required--this is only version 2.061 at /usr/local/share/perl5/IO/Uncompress/RawInflate.pm line 8. BEGIN failed--compilation aborted at /usr/local/share/perl5/IO/Uncompress/RawInflate.pm line 8. Compilation failed in require at /usr/local/share/perl5/IO/Uncompress/Gunzip.pm line 12. BEGIN failed--compilation aborted at /usr/local/share/perl5/IO/Uncompress/Gunzip.pm line 12. Compilation failed in require at /usr/local/bin/geninfo line 62. BEGIN failed--compilation aborted at /usr/local/bin/geninfo line 62. sh-4.2$ Install package Compress::Raw::Zlib fixed.\nperl -MCPAN -e \u0026#39;install Compress::Raw::Zlib\u0026#39; ","date":"2021-09-07","externalUrl":null,"permalink":"/en/posts/2021/lcov-error/","section":"Posts","summary":"This article explains how to resolve the “Can’t locate JSON/PP.pm in @INC …” error when running lcov, including installing missing Perl modules.","title":"Run lcov failed \"Can't locate JSON/PP.pm in @INC ...\"","type":"posts"},{"content":"","date":"2021-08-05","externalUrl":null,"permalink":"/en/tags/postgresql/","section":"Tags","summary":"","title":"PostgreSQL","type":"tags"},{"content":" Backgroud # In my opinion, SonarQube is not a very easy setup DevOps tool to compare with Jenkins, Artifactory. You can\u0026rsquo;t just run some script under the bin folder to let the server boot up.\nYou must have an installed database, configuration LDAP in the config file, etc.\nSo I\u0026rsquo;d like to document some important steps for myself, like setup LDAP or PostgreSQL when I install SonarQube of v9.0.1. It would be better if it can help others.\nPrerequisite and Download # Need to be installed JRE/JDK 11 on the running machine.\nHere is the prerequisites overview: https://docs.sonarqube.org/latest/requirements/requirements/\nDownload SonarQube: https://www.sonarqube.org/downloads/\ncd sonarqube/ ls wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-9.0.1.46107.zip unzip sonarqube-9.0.1.46107.zip cd sonarqube-9.0.1.46107/bin/linux-x86-64 sh sonar.sh console Change Java version # I installed SonarQube on CentOS 7 machine, the Java version is OpenJDK 1.8.0_242 by default, but the prerequisite shows at least need JDK 11. There is also JDK 11 available on my machine, so I just need to change the Java version.\nI recommend using the alternatives command change Java version，refer as following:\n$ java -version openjdk version \u0026#34;1.8.0_242\u0026#34; OpenJDK Runtime Environment (build 1.8.0_242-b08) OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode) $ alternatives --config java There are 3 programs which provide \u0026#39;java\u0026#39;. Selection Command ----------------------------------------------- 1 java-1.7.0-openjdk.x86_64 (/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.251-2.6.21.1.el7.x86_64/jre/bin/java) *+ 2 java-1.8.0-openjdk.x86_64 (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-1.el7.x86_64/jre/bin/java) 3 java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.12.0.7-0.el7_9.x86_64/bin/java) Enter to keep the current selection[+], or type selection number: 3 $ java -version openjdk version \u0026#34;11.0.12\u0026#34; 2021-07-20 LTS OpenJDK Runtime Environment 18.9 (build 11.0.12+7-LTS) OpenJDK 64-Bit Server VM 18.9 (build 11.0.12+7-LTS, mixed mode, sharing) Install Database # SonarQube needs you to have installed a database. It supports several database engines, like Microsoft SQL Server, Oracle, and PostgreSQL. Since PostgreSQL is open source, light, and easy to install, so I choose PostgreSQL as its database.\nHow to download and install PostgreSQL please see this page: https://www.postgresql.org/download/linux/redhat/\nTroubleshooting # 1. How to establish a connection with SonarQube and PostgreSQL # Please refer to the sonar.properties file at the end of this post.\n2. How to setup LDAP for users to log in # sonar.security.realm=LDAP ldap.url=ldap://den.exmaple-org:389 ldap.bindDn=user@exmaple-org.com ldap.bindPassword=mypassword ldap.authentication=simple ldap.user.baseDn=DC=exmaple-org,DC=com ldap.user.request=(\u0026amp;(objectClass=user)(sAMAccountName={login})) ldap.user.realNameAttribute=cn ldap.user.emailAttribute=email 3. How to fix LDAP login SonarQube is very slowly # Comment out ldap.followReferrals=false in sonar.properties file would be help.\nRelated post: https://community.sonarsource.com/t/ldap-login-takes-2-minutes-the-first-time/1573/7\n4. How to fix \u0026lsquo;Could not resolve 11 file paths in lcov.info\u0026rsquo; # I want to display Javascript code coverage result in SonarQube, so I added sonar.javascript.lcov.reportPaths=coverage/lcov.info to the sonar-project.properties\nBut when I run sonar-scanner.bat in the command line, the code coverage result can not show in sonar. I noticed the following error from the output:\nINFO: Analysing [C:\\workspace\\xvm-ide\\client\\coverage\\lcov.info] WARN: Could not resolve 11 file paths in [C:\\workspace\\xvm-ide\\client\\coverage\\lcov.info] There are some posts related to this problem, for example, https://github.com/kulshekhar/ts-jest/issues/542, but no one works in my case.\n# here is an example error path in lcov.info ..\\src\\auto-group\\groupView.ts Finally, I have to use the sed command to remove ..\\ in front of the paths before running sonar-scanner.bat, then the problem was solved.\nsed -i \u0026#39;s/\\..\\\\//g\u0026#39; lcov.info Please comment if you can solve the problem with changing options in the tsconfig.json file.\n4. How to output to more logs # To output more logs, change sonar.log.level=INFO to sonar.log.level=DEBUG in below.\nNote: all above changes of sonar.properties need to restart the SonarQube instance to take effect.\nFinal sonar.properties # For the sonar.properties file, please see below or link\n# DATABASE # # IMPORTANT: # - The embedded H2 database is used by default. It is recommended for tests but not for # production use. Supported databases are Oracle, PostgreSQL and Microsoft SQLServer. # - Changes to database connection URL (sonar.jdbc.url) can affect SonarSource licensed products. # User credentials. # Permissions to create tables, indices and triggers must be granted to JDBC user. # The schema must be created first. sonar.jdbc.username=sonarqube sonar.jdbc.password=mypassword #----- PostgreSQL 9.6 or greater # By default the schema named \u0026#34;public\u0026#34; is used. It can be overridden with the parameter \u0026#34;currentSchema\u0026#34;. sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube # Binding IP address. For servers with more than one IP address, this property specifies which # address will be used for listening on the specified ports. # By default, ports will be used on all IP addresses associated with the server. sonar.web.host=10.118.245.19 # Web context. When set, it must start with forward slash (for example /sonarqube). # The default value is root context (empty value). sonar.web.context= # TCP port for incoming HTTP connections. Default value is 9000. sonar.web.port=9000 # LDAP CONFIGURATION # Follow or not referrals. See http://docs.oracle.com/javase/jndi/tutorial/ldap/referral/jndi.html (default: true) ldap.followReferrals=false # Enable the LDAP feature sonar.security.realm=LDAP # Set to true when connecting to a LDAP server using a case-insensitive setup. # sonar.authenticator.downcase=true # URL of the LDAP server. Note that if you are using ldaps, then you should install the server certificate into the Java truststore. ldap.url=ldap://den.exmaple-org:389 # Bind DN is the username of an LDAP user to connect (or bind) with. Leave this blank for anonymous access to the LDAP directory (optional) ldap.bindDn=user@exmaple-org.com # Bind Password is the password of the user to connect with. Leave this blank for anonymous access to the LDAP directory (optional) ldap.bindPassword=mypassword # Possible values: simple | CRAM-MD5 | DIGEST-MD5 | GSSAPI See http://java.sun.com/products/jndi/tutorial/ldap/security/auth.html (default: simple) ldap.authentication=simple # USER MAPPING # Distinguished Name (DN) of the root node in LDAP from which to search for users (mandatory) ldap.user.baseDn=DC=exmaple-org,DC=com # LDAP user request. (default: (\u0026amp;(objectClass=inetOrgPerson)(uid={login})) ) ldap.user.request=(\u0026amp;(objectClass=user)(sAMAccountName={login})) # Attribute in LDAP defining the user’s real name. (default: cn) ldap.user.realNameAttribute=cn # Attribute in LDAP defining the user’s email. (default: mail) ldap.user.emailAttribute=email sonar.search.javaAdditionalOpts=-Dbootstrap.system_call_filter=false # Global level of logs (applies to all 4 processes). # Supported values are INFO (default), DEBUG and TRACE sonar.log.level=INFO # Paths to persistent data files (embedded database and search index) and temporary files. # Can be absolute or relative to installation directory. # Defaults are respectively \u0026lt;installation home\u0026gt;/data and \u0026lt;installation home\u0026gt;/temp sonar.path.data=/var/sonarqube/data sonar.path.temp=/var/sonarqube/temp ","date":"2021-08-05","externalUrl":null,"permalink":"/en/posts/2021/sonarqube-setup/","section":"Posts","summary":"This article documents the steps to install SonarQube, configure LDAP, and set up PostgreSQL as the database. It also includes troubleshooting tips for common issues encountered during setup.","title":"SonarQube installation and troubleshootings","type":"posts"},{"content":"","date":"2021-07-27","externalUrl":null,"permalink":"/en/tags/coverage/","section":"Tags","summary":"","title":"Coverage","type":"tags"},{"content":"","date":"2021-07-27","externalUrl":null,"permalink":"/en/tags/gcov/","section":"Tags","summary":"","title":"Gcov","type":"tags"},{"content":" Problem # When we introduced Gocv to build my project for code coverage, I encountered the following error message:\nerror 1 # g++ -m64 -z muldefs -L/lib64 -L/usr/lib64 -lglib-2.0 -m64 -DUV_64PORT -DU2_64_BUILD -fPIC -g DU_starter.o NFA_msghandle.o NFA_svr_exit.o du_err_printf.o -L/workspace/code/myproject/src/home/x64debug/bin/ -L/workspace/code/myproject/src/home/x64debug/bin/lib/ -lundata -lutcallc_nfasvr -Wl,-rpath=/workspace/code/myproject/src/home/x64debug/bin/ -Wl,-rpath=/.dulibs28 -Wl,--enable-new-dtags -L/.dulibs28 -lodbc -lm -lncurses -lrt -lcrypt -lgdbm -ldl -lpam -lpthread -ldl -lglib-2.0 -lstdc++ -lnsl -lrt -lgcov -o /workspace/code/myproject/src/home/x64debug/objs/du/share/dutsvr /usr/bin/ld: /workspace/code/myproject/src/home/x64debug/objs/du/share/dutsvr: hidden symbol `__gcov_init\u0026#39; in /usr/lib/gcc/x86_64-redhat-linux/4.8.5/libgcov.a(_gcov.o) is referenced by DSO error 2 # It may also be such an error\n/home/p7539c/cutest/CuTest.c:379: undefined reference to `__gcov_init\u0026#39; CuTest.o:(.data+0x184): undefined reference to `__gcov_merge_add\u0026#39; Positioning problem # Let\u0026rsquo;s take the error 1.\nFrom the error message, I noticed -lundata -lutcallc_nfasvr are all the linked libraries (-llibrary)\nI checked libraries undata and utcallc_nfasvr one by one, and found it displayed U __gcov_init and U means undefined symbols.\nUse the find command to search the library and the nm command to list symbols in the library.\n-sh-4.2$ find -name *utcallc_nfasvr* ./bin/libutcallc_nfasvr.so ./objs/du/work/libutcallc_nfasvr.so -sh-4.2$ nm ./bin/libutcallc_nfasvr.so | grep __gcov_init U __gcov_init How to fix # In my case, I just added the following code LIB_1_LIBS := -lgcov to allow the utcallc_nfasvr library to call gcov.\nLIB_1 := utcallc_nfasvr # added below code to my makefile LIB_1_LIBS := -lgcov Rebuild, the error is gone, then checked library, it displayed t __gcov_init this time, it means symbol value exists not hidden.\n-sh-4.2$ nm ./bin/libutcallc_nfasvr.so | grep __gcov_init t __gcov_init Or in your case may build a shared library like so, similarly, just add the compile parameter -lgcov\ng++ -shared -o libMyLib.so src_a.o src_b.o src_c.o -lgcov Summary # I have encountered the following problems many times\nundefined reference to `__gcov_init\u0026#39; undefined reference to `__gcov_merge_add\u0026#39; `hidden symbol `__gcov_init\u0026#39; in /usr/lib/gcc/x86_64-redhat-linux/4.8.5/libgcov.a(_gcov.o) is referenced by DSO` Each time I can fix it by adding -glcov then recompile. the error has gone after rebuild. (you use the nm command to double-check whether the symbol has been added successfully.)\nHopes it can help you.\n","date":"2021-07-27","externalUrl":null,"permalink":"/en/posts/2021/how-to-fix-gcov-hidden-symbol/","section":"Posts","summary":"This article explains how to resolve the “hidden symbol `__gcov_init’ in ../libgcov.a(_gcov.o) is referenced by DSO” error when building a project with Gcov, including how to ensure symbols are not hidden.","title":"How to fix \"hidden symbol `__gcov_init' in ../libgcov.a(_gcov.o) is referenced by DSO\"","type":"posts"},{"content":" Backgorud # When you want to add build status to your Bitbucket the specific commit of a branch when you start a build from the branch\nWhen the build status is wrong, you want to update it manually. for example, update build status from FAILED to SUCCESSFUL\nYou can call Bitbucket REST API to do these.\nCode snippet # Below is the code snippet to update Bitbucket build status with REST API in the shell script.\nThe code on GitHub Gist: https://gist.github.com/shenxianpeng/bd5eddc5fb39e54110afb8e2e7a6c4fb\nClick Read More to view the code here.\n#!/bin/sh username=your-bitbucket-user password=your-bitbucket-password commit_id=\u0026#39;57587d7d4892bc4ef2c4375028c19b27921e2485\u0026#39; # build_result has 3 status: SUCCESSFUL, FAILED, INPROGRESS build_result=\u0026#39;SUCCESSFUL\u0026#39; description=\u0026#39;Manully update bitbucket status\u0026#39; build_name=\u0026#39;test #1\u0026#39; build_url=http://localhost:8080/job/test/ bitbucket_rest_api=\u0026#39;https://myorg.bitbucket.com/rest/build-status/latest/commits\u0026#39; gen_post_data() { cat \u0026lt;\u0026lt;EOF { \u0026#34;state\u0026#34;: \u0026#34;$build_result\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;$commit_id\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;$build_name\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;$build_url\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;$description\u0026#34; } EOF } echo \u0026#34;$(gen_post_data)\u0026#34; curl -u $username:$password \\ -H \u0026#34;Accept: application/json\u0026#34; \\ -H \u0026#34;Content-Type:application/json\u0026#34; \\ -X POST $bitbucket_rest_api/$commit_id --data \u0026#34;$(gen_post_data)\u0026#34; if [ $? -ne 0 ] then echo \u0026#34;$0: Update bitbucket build status failed.\u0026#34; exit 1 else echo \u0026#34;$0: Update bitbucket build status success.\u0026#34; exit 0 fi And the screenshot of the final update result\n","date":"2021-07-25","externalUrl":null,"permalink":"/en/posts/2021/bitbucket-update-build-status/","section":"Posts","summary":"How to add or update the build status of a specific commit in Bitbucket using its REST API. It includes a shell script example for updating the build status and provides context on when to use this functionality.","title":"Add or update Bitbucket build status with REST API","type":"posts"},{"content":"","date":"2021-07-25","externalUrl":null,"permalink":"/en/tags/shell/","section":"Tags","summary":"","title":"Shell","type":"tags"},{"content":"This article briefly introduces: What is code coverage? Why measure code coverage? Code coverage metrics, working principles, mainstream code coverage tools, and why not to overestimate code coverage metrics.\nWhat is Code Coverage? # Code coverage is a measure of the amount of code executed during the entire testing process. It measures which statements in the source code have been executed during testing and which have not.\nWhy Measure Code Coverage? # As we all know, testing can improve the quality and predictability of software releases. But do you know how effective your unit tests or even your functional tests are in actually testing the code? Are more tests needed?\nThese are the questions that code coverage can attempt to answer. In short, we need to measure code coverage for the following reasons:\nUnderstand the effectiveness of our test cases on the source code Understand whether we have done enough testing Maintain test quality throughout the software lifecycle Note: Code coverage is not a panacea. Coverage measurement cannot replace good code review and excellent programming practices.\nGenerally, we should adopt a reasonable coverage target and strive for uniform coverage across all modules, rather than just looking at whether the final number is high enough to be satisfactory.\nFor example: Suppose the code coverage is very high in some modules, but there are not enough test cases covering some key modules. Even though the code coverage is high, it does not mean that the product quality is high.\nTypes of Code Coverage Metrics # Code coverage tools typically use one or more standards to determine whether your code has been executed after being subjected to automated testing. Common metrics seen in coverage reports include:\nFunction Coverage: What percentage of defined functions have been called Statement Coverage: What percentage of statements in the program have been executed Branch Coverage: What percentage of branches in control structures (e.g., if statements) have been executed Condition Coverage: What percentage of Boolean sub-expressions have been tested as true and false Line Coverage: What percentage of lines of source code have been tested How Code Coverage Works # Code coverage measurement primarily uses three methods:\n1. Source Code Instrumentation # Instrumentation statements are added to the source code, and the code is compiled using a normal compiler toolchain to generate an instrumented assembly. This is what we commonly call instrumentation; Gcov belongs to this category of code coverage tools.\n2. Runtime Instrumentation # This method collects information from the runtime environment while the code is executing to determine coverage information. In my understanding, the principles of JaCoCo and Coverage tools belong to this category.\n3. Intermediate Code Instrumentation # New bytecode is added to instrument compiled class files, generating a new instrumented class. To be honest, I Googled many articles and didn\u0026rsquo;t find a definitive statement on which tools belong to this category.\nUnderstanding the basic principles of these tools, combined with existing test cases, helps in correctly selecting code coverage tools. For example:\nIf the product\u0026rsquo;s source code only has E2E (end-to-end) test cases, usually only the first type of tool can be selected, i.e., an executable file compiled through instrumentation, followed by testing and result collection. If the product\u0026rsquo;s source code has unit test cases, usually the second type of tool is selected, i.e., runtime collection. This type of tool has high execution efficiency and is easy to integrate continuously. Current Mainstream Code Coverage Tools # There are many code coverage tools. Below are code coverage tools I\u0026rsquo;ve used for different programming languages. When selecting tools, try to choose those that are open-source, popular (active), and easy to use.\nProgramming Language Code Coverage Tool C/C++ Gcov Java JaCoCo JavaScript Istanbul Python Coverage.py Golang cover Don\u0026rsquo;t Overestimate Code Coverage Metrics # Code coverage is not a panacea; it only tells us which code has not been \u0026ldquo;executed\u0026rdquo; by test cases. A high percentage of code coverage does not equal high-quality and effective testing.\nFirst, high code coverage is not enough to measure effective testing. Instead, code coverage more accurately gives a measure of the extent to which code has not been tested. This means that if our code coverage metric is low, then we can be sure that important parts of the code have not been tested; however, the converse is not necessarily true. High code coverage does not fully indicate that our code has been adequately tested.\nSecond, 100% code coverage should not be one of our explicit goals. This is because there is always a trade-off between achieving 100% code coverage and actually testing important code. While it is possible to test all code, the value of testing is also likely to diminish as you approach this limit, considering the tendency to write more meaningless tests to meet coverage requirements.\nBorrowing a quote from Martin Fowler\u0026rsquo;s article on Test Coverage:\nCode coverage is a useful tool for finding untested parts of your codebase, but it\u0026rsquo;s not much use as a number telling you how good your tests are.\nReferences # https://www.lambdatest.com/blog/code-coverage-vs-test-coverage/ https://www.atlassian.com/continuous-delivery/software-testing/code-coverage https://www.thoughtworks.com/insights/blog/are-test-coverage-metrics-overrated\n","date":"2021-07-14","externalUrl":null,"permalink":"/en/posts/2021/code-coverage/","section":"Posts","summary":"This article briefly introduces the concept, importance, common metrics, working principle, and mainstream tools of code coverage, emphasizing that code coverage metrics should not be over-relied upon.","title":"About Code Coverage","type":"posts"},{"content":"This article shares how to use Gcov and LCOV to metrics code coverage for C/C++ projects. If you want to know how Gcov works, or you need to metrics code coverage for C/C++ projects later, I hope this article is useful to you.\nProblems # The problem I\u0026rsquo;m having: A C/C++ project from decades ago has no unit tests, only regression tests, but you want to know what code is tested by regression tests? Which code is untested? What is the code coverage? Where do I need to improve automated test cases in the future?\nCan code coverage be measured without unit tests? Yes.\nCode coverage tools for C/C++ # There are some tools on the market that can measure the code coverage of black-box testing, such as Squish Coco, Bullseye, etc. Their principle is to insert instrumentation when build product.\nI\u0026rsquo;ve done some research on Squish Coco, because of some unresolved compilation issues that I didn\u0026rsquo;t buy a license for this expensive tool.\nWhen I investigated code coverage again, I found out that GCC has a built-in code coverage tool called Gcov.\nPrerequisites # For those who want to use Gcov, to illustrate how it works, I have prepared a sample program that requires GCC and LCOV to be installed before running the program.\nIf you don\u0026rsquo;t have an environment or don\u0026rsquo;t want to install it, you can check out this example repository\nNote: The source code is under the master branch master, and code coverage result html under branch coverage.\n# This is the version of GCC and lcov on my test environment. sh-4.2$ gcc --version gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39) Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. sh-4.2$ lcov -v lcov: LCOV version 1.14 How Gcov works # Gcov workflow diagram\nThere are three main steps:\nAdding special compilation options to the GCC compilation to generate the executable, and *.gcno. Running (testing) the generated executable, which generates the *.gcda data file. With *.gcno and *.gcda, generate the gcov file from the source code, and finally generate the code coverage report. Here\u0026rsquo;s how each of these steps is done exactly.\n1. Compile # The first step is to compile. The parameters and files used for compilation are already written in the makefile.\nmake build Click to see the output of the make command sh-4.2$ make build gcc -fPIC -fprofile-arcs -ftest-coverage -c -Wall -Werror main.c gcc -fPIC -fprofile-arcs -ftest-coverage -c -Wall -Werror foo.c gcc -fPIC -fprofile-arcs -ftest-coverage -o main main.o foo.o As you can see from the output, this program is compiled with two compile options -fprofile-arcs and -ftest-coverage. After successful compilation, not only the main and .o files are generated, but also two .gcno files are generated.\nThe .gcno record file is generated after adding the GCC compile option -ftest-coverage, which contains information for reconstructing the base block map and assigning source line numbers to blocks during the compilation process.\n2. Running the executable # After compilation, the executable main is generated, which is run (tested) as follows\n./main Click to see the output when running main sh-4.2$ ./main Start calling foo() ... when num is equal to 1... when num is equal to 2... When main is run, the results are recorded in the .gcda data file, and if you look in the current directory, you can see that two .gcda files have been generated.\n$ ls foo.c foo.gcda foo.gcno foo.h foo.o img main main.c main.gcda main.gcno main.o makefile README.md .gcda record data files are generated because the program is compiled with the -fprofile-arcs option introduced. It contains arc transition counts, value distribution counts, and some summary information.\n3. Generating reports # make report Click to see the output of the generated report sh-4.2$ make report gcov main.c foo.c File \u0026#39;main.c\u0026#39; Lines executed:100.00% of 5 Creating \u0026#39;main.c.gcov\u0026#39; File \u0026#39;foo.c\u0026#39; Lines executed:85.71% of 7 Creating \u0026#39;foo.c.gcov\u0026#39; Lines executed:91.67% of 12 lcov --capture --directory . --output-file coverage.info Capturing coverage data from . Found gcov version: 4.8.5 Scanning . for .gcda files ... Found 2 data files in . Processing foo.gcda geninfo: WARNING: cannot find an entry for main.c.gcov in .gcno file, skipping file! Processing main.gcda Finished .info-file creation genhtml coverage.info --output-directory out Reading data file coverage.info Found 2 entries. Found common filename prefix \u0026#34;/workspace/coco\u0026#34; Writing .css and .png files. Generating output. Processing file main.c Processing file foo.c Writing directory view page. Overall coverage rate: lines......: 91.7% (11 of 12 lines) functions..: 100.0% (2 of 2 functions) Executing make report to generate an HTML report actually performs two main steps behind this command.\nWith the .gcno and .gcda files generated at compile and run time, execute the command gcov main.c foo.c to generate the .gcov code coverage file.\nWith the code coverage .gcov file, generate a visual code coverage report via LCOV.\nThe steps to generate the HTML result report are as follows.\n# 1. Generate the coverage.info data file lcov --capture --directory . --output-file coverage.info # 2. Generate a report from this data file genhtml coverage.info --output-directory out Delete all generated files # All the generated files can be removed by executing make clean command.\nClick to see the output of the make clean command sh-4.2$ make clean rm -rf main *.o *.so *.gcno *.gcda *.gcov coverage.info out Code coverage report # The home page is displayed in a directory structure\nAfter entering the directory, the source files in that directory are displayed\nThe blue color indicates that these statements are overwritten\nRed indicates statements that are not overridden\nLCOV supports statement, function, and branch coverage metrics.\nSide notes:\nThere is another tool for generating HTML reports called gcovr, developed in Python, whose reports are displayed slightly differently from LCOV. For example, LCOV displays it in a directory structure, while gcovr displays it in a file path, which is always the same as the code structure, so I prefer to use the former.\n","date":"2021-07-11","externalUrl":null,"permalink":"/en/posts/2021/gcov-example/","section":"Posts","summary":"This article shares how to use Gcov and LCOV to metrics code coverage for C/C++ projects. It explains the steps to compile, run tests, and generate coverage reports, including example commands and expected outputs.","title":"Code coverage testing of C/C++ projects using Gcov and LCOV","type":"posts"},{"content":"I\u0026rsquo;ve run into some situations when the build fails, perhaps because some processes don\u0026rsquo;t finish, and even setting a timeout doesn\u0026rsquo;t make the Jenkins job fail.\nSo, to fix this problem, I used try .. catch and error to make my Jenkins job failed, hopes this also helps you.\nPlease see the following example:\npipeline { agent none stages { stage(\u0026#39;Hello\u0026#39;) { steps { script { try { timeout(time: 1, unit: \u0026#39;SECONDS\u0026#39;) { echo \u0026#34;timeout step\u0026#34; sleep 2 } } catch(err) { // timeout reached println err echo \u0026#39;Time out reached.\u0026#39; error \u0026#39;build timeout failed\u0026#39; } } } } } } Here is the output log\n00:00:01.326 [Pipeline] Start of Pipeline 00:00:01.475 [Pipeline] stage 00:00:01.478 [Pipeline] { (Hello) 00:00:01.516 [Pipeline] script 00:00:01.521 [Pipeline] { 00:00:01.534 [Pipeline] timeout 00:00:01.534 Timeout set to expire in 1 sec 00:00:01.537 [Pipeline] { 00:00:01.547 [Pipeline] echo 00:00:01.548 timeout step 00:00:01.555 [Pipeline] sleep 00:00:01.558 Sleeping for 2 sec 00:00:02.535 Cancelling nested steps due to timeout 00:00:02.546 [Pipeline] } 00:00:02.610 [Pipeline] // timeout 00:00:02.619 [Pipeline] echo 00:00:02.621 org.jenkinsci.plugins.workflow.steps.FlowInterruptedException 00:00:02.625 [Pipeline] echo 00:00:02.627 Time out reached. 00:00:02.630 [Pipeline] error 00:00:02.638 [Pipeline] } 00:00:02.656 [Pipeline] // script 00:00:02.668 [Pipeline] } 00:00:02.681 [Pipeline] // stage 00:00:02.696 [Pipeline] End of Pipeline 00:00:02.709 ERROR: build timeout failed 00:00:02.710 Finished: FAILURE ","date":"2021-06-24","externalUrl":null,"permalink":"/en/posts/2021/jenkins-timeout/","section":"Posts","summary":"This article explains how to handle Jenkins job timeouts effectively by using try and catch blocks to ensure the job fails correctly when a timeout occurs.","title":"How to make Jenkins job fail after timeout? (Resolved)","type":"posts"},{"content":" Preface # This article documents two Git clone failure issues encountered during continuous integration with Jenkins and AIX, and shares their solutions.\nDependent module /usr/lib/libldap.a(libldap-2.4.so.2) could not be loaded. Authentication failed during Git clone via SSH Issue 1: Dependent module /usr/lib/libldap.a(libldap-2.4.so.2) could not be loaded # When Jenkins checked out code via HTTPS, the following error occurred:\n[2021-06-20T14:50:25.166Z] ERROR: Error cloning remote repo \u0026#39;origin\u0026#39; [2021-06-20T14:50:25.166Z] hudson.plugins.git.GitException: Command \u0026#34;git fetch --tags --force --progress --depth=1 -- https://git.company.com/scm/vas/db.git +refs/heads/*:refs/remotes/origin/*\u0026#34; returned status code 128: [2021-06-20T14:50:25.166Z] stdout: [2021-06-20T14:50:25.166Z] stderr: exec(): 0509-036 Cannot load program /opt/freeware/libexec64/git-core/git-remote-https because of the following errors: [2021-06-20T14:50:25.166Z] 0509-150 Dependent module /usr/lib/libldap.a(libldap-2.4.so.2) could not be loaded. [2021-06-20T14:50:25.166Z] 0509-153 File /usr/lib/libldap.a is not an archive or [2021-06-20T14:50:25.166Z] the file could not be read properly. [2021-06-20T14:50:25.166Z] 0509-026 System error: Cannot run a file that does not have a valid format. [2021-06-20T14:50:25.166Z] [2021-06-20T14:50:25.166Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:2450) [2021-06-20T14:50:25.166Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandWithCredentials(CliGitAPIImpl.java:2051) [2021-06-20T14:50:25.166Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.access$500(CliGitAPIImpl.java:84) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$1.execute(CliGitAPIImpl.java:573) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$2.execute(CliGitAPIImpl.java:802) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:161) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:154) .......................... [2021-06-20T14:50:25.167Z] Suppressed: hudson.remoting.Channel$CallSiteStackTrace: Remote call to aix-devasbld-01 [2021-06-20T14:50:25.167Z] at hudson.remoting.Channel.attachCallSiteStackTrace(Channel.java:1800) .......................... [2021-06-20T14:50:25.168Z] at java.lang.Thread.run(Thread.java:748) [2021-06-20T15:21:20.525Z] Cloning repository https://git.company.com/scm/vas/db.git However, directly cloning the code using the command git clone https://git.company.com/scm/vas/db.git on the virtual machine was successful without any issues.\nSetting LIBPATH to LIBPATH=/usr/lib reproduces the error, indicating that Jenkins searches for libldap.a under /usr/lib/ when downloading code. Setting the LIBPATH variable to empty export LIBPATH= or unset LIBPATH allows git clone https://... to execute normally. Attempts to modify the LIBPATH variable to empty when starting the Jenkins agent failed to resolve this issue. The reason remains unclear.\nLet\u0026rsquo;s investigate the issue with /usr/lib/libldap.a.\n# ldd reveals problems with this static library $ ldd /usr/lib/libldap.a /usr/lib/libldap.a needs: /opt/IBM/ldap/V6.4/lib/libibmldapdbg.a /usr/lib/threads/libc.a(shr.o) Cannot find libpthreads.a(shr_xpg5.o) /opt/IBM/ldap/V6.4/lib/libidsldapiconv.a Cannot find libpthreads.a(shr_xpg5.o) Cannot find libc_r.a(shr.o) /unix /usr/lib/libcrypt.a(shr.o) Cannot find libpthreads.a(shr_xpg5.o) Cannot find libc_r.a(shr.o) # Shows that it links to IBM LDAP $ ls -l /usr/lib/libldap.a lrwxrwxrwx 1 root system 35 Jun 10 2020 /usr/lib/libldap.a -\u0026gt; /opt/IBM/ldap/V6.4/lib/libidsldap.a # Shows that the same libldap.a in /opt/freeware/lib/ is fine $ ldd /opt/freeware/lib/libldap.a ldd: /opt/freeware/lib/libldap.a: File is an archive. $ ls -l /opt/freeware/lib/libldap.a lrwxrwxrwx 1 root system 13 May 27 2020 /opt/freeware/lib/libldap.a -\u0026gt; libldap-2.4.a Issue 1: Solution # # Attempt a replacement # First rename libldap.a to libldap.a.old (don\u0026#39;t delete in case of needing to restore) $ sudo mv /usr/lib/libldap.a /usr/lib/libldap.a.old # Recreate the symbolic link $ sudo ln -s /opt/freeware/lib/libldap.a /usr/lib/libldap.a $ ls -l /usr/lib/libldap.a lrwxrwxrwx 1 root system 27 Oct 31 23:27 /usr/lib/libldap.a -\u0026gt; /opt/freeware/lib/libldap.a After recreating the symbolic link, reconnect the AIX agent and rerun the Jenkins job to clone the code. Success!\nIssue 2: Authentication failed during Git clone via SSH # Due to AIX 7.1-TL4-SP1 nearing its End of Service Pack Support, an upgrade was necessary. However, after upgrading to AIX 7.1-TL5-SP6, SSH code downloads failed.\n$ git clone ssh://git@git.company.com:7999/vas/db.git Cloning into \u0026#39;db\u0026#39;... Authentication failed. fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. This type of error is frequently encountered when using the Git SSH method for cloning code, typically due to a missing public key. Generating keys with ssh-keygen -t rsa -C your@email.com and adding the id_rsa.pub content to the GitHub/Bitbucket/GitLab public key usually resolves this.\nBut this time was different. The error persisted even after setting the public key. Surprisingly, it worked fine on AIX 7.1-TL4-SP1 but failed after upgrading to AIX 7.1-TL5-SP6.\nUsing the command ssh -vvv \u0026lt;git-url\u0026gt; to examine debug information during the request to the Git server revealed the following:\n# AIX 7.1-TL4-SP1 bash-4.3$ oslevel -s 7100-04-01-1543 bash-4.3$ ssh -vvv git.company.com OpenSSH_6.0p1, OpenSSL 1.0.1e 11 Feb 2013 debug1: Reading configuration data /etc/ssh/ssh_config debug1: Failed dlopen: /usr/krb5/lib/libkrb5.a(libkrb5.a.so): 0509-022 Cannot load module /usr/krb5/lib/libkrb5.a(libkrb5.a.so). 0509-026 System error: A file or directory in the path name does not exist. debug1: Error loading Kerberos, disabling Kerberos auth. ....... ....... ssh_exchange_identification: read: Connection reset by peer # New machine AIX 7.1-TL5-SP6 $ oslevel -s 7100-05-06-2015 $ ssh -vvv git.company.com OpenSSH_7.5p1, OpenSSL 1.0.2t 10 Sep 2019 debug1: Reading configuration data /etc/ssh/ssh_config debug1: Failed dlopen: /usr/krb5/lib/libkrb5.a(libkrb5.a.so): 0509-022 Cannot load module /usr/krb5/lib/libkrb5.a(libkrb5.a.so). 0509-026 System error: A file or directory in the path name does not exist. debug1: Error loading Kerberos, disabling Kerberos auth. ....... ....... ssh_exchange_identification: read: Connection reset by peer The difference is the OpenSSH version, which may be the cause. This led to quickly finding similar problems and answers (Stackoverflow link)\nIssue 2: Solution # Add the option AllowPKCS12keystoreAutoOpen no to the ~/.ssh/config file.\nHowever, this option is a custom AIX option and does not exist on Linux.\nThis results in successful Git clone via SSH for the same domain account on AIX but failure on Linux.\n# Linux does not recognize this option stderr: /home/****/.ssh/config: line 1: Bad configuration option: allowpkcs12keystoreautoopen /home/****/.ssh/config: terminating, 1 bad configuration options fatal: Could not read from remote repository. Conditional options in the config file would be ideal—adding AllowPKCS12keystoreAutoOpen no only for AIX. Unfortunately, config does not support this. Separately setting the current AIX ssh config file would be helpful. Modifying /etc/ssh/ssh_config as follows, restarting the service, and retrying the SSH clone was successful~! Host * AllowPKCS12keystoreAutoOpen no # ForwardAgent no # ForwardX11 no # RhostsRSAAuthentication no # RSAAuthentication yes # PasswordAuthentication yes # HostbasedAuthentication no # GSSAPIAuthentication no # GSSAPIDelegateCredentials no # GSSAPIKeyExchange no # GSSAPITrustDNS no # ....omitted ","date":"2021-06-20","externalUrl":null,"permalink":"/en/posts/2021/git-clone-failed-on-aix/","section":"Posts","summary":"This article documents two issues encountered when using Jenkins for Git clone on AIX and their solutions, including dependent library loading failure and SSH authentication failure.","title":"Solving Two Git Clone Failure Issues on AIX","type":"posts"},{"content":"Recently, when downloading code from Bitbucket using AIX 7.1, I encountered this error:\nfatal: write error: A file cannot be larger than the value set by ulimit.\n$ git clone -b dev https://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@git.company.com/scm/vmcc/opensrc.git --depth 1 Cloning into \u0026#39;opensrc\u0026#39;... remote: Counting objects: 2390, done. remote: Compressing objects: 100% (1546/1546), done. fatal: write error: A file cannot be larger than the value set by ulimit. fatal: index-pack failed On AIX 7.3, I encountered this error:\nfatal: fetch-pack: invalid index-pack output\n$ git clone -b dev https://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@git.company.com/scm/vmcc/opensrc.git --depth 1 Cloning into \u0026#39;opensrc\u0026#39;... remote: Counting objects: 2390, done. remote: Compressing objects: 100% (1546/1546), done. fatal: write error: File too large68), 1012.13 MiB | 15.38 MiB/s fatal: fetch-pack: invalid index-pack output This is because the files in this repository are too large, exceeding the AIX limit on user file resource usage.\nThis can be checked using ulimit -a. More information about the ulimit command can be found at ulimit Command\n$ ulimit -a time(seconds) unlimited file(blocks) 2097151 data(kbytes) unlimited stack(kbytes) 32768 memory(kbytes) 32768 coredump(blocks) 2097151 nofiles(descriptors) 2000 threads(per process) unlimited processes(per user) unlimited We can see that file has an upper limit of 2097151. Changing this to unlimited should solve the problem.\nThe limits file /etc/security/limits is accessible by the root user (ordinary users do not have access permissions).\n# The following is part of the content of this file default: fsize = 2097151 core = 2097151 cpu = -1 data = -1 rss = 65536 stack = 65536 nofiles = 2000 Changing fsize = 2097151 to fsize = -1 removes the file size limit. After making this change, log in again for the changes to take effect.\nExecuting ulimit -a again shows that the change has taken effect.\n$ ulimit -a time(seconds) unlimited file(blocks) unlimited data(kbytes) unlimited stack(kbytes) 32768 memory(kbytes) 32768 coredump(blocks) 2097151 nofiles(descriptors) 2000 threads(per process) unlimited processes(per user) unlimited Now file(blocks) is unlimited. Let\u0026rsquo;s try git clone again.\ngit clone -b dev https://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@git.company.com/scm/vmcc/opensrc.git --depth 1 Cloning into \u0026#39;opensrc\u0026#39;... remote: Counting objects: 2390, done. remote: Compressing objects: 100% (1546/1546), done. remote: Total 2390 (delta 763), reused 2369 (delta 763) Receiving objects: 100% (2390/2390), 3.80 GiB | 3.92 MiB/s, done. Resolving deltas: 100% (763/763), done. Checking out files: 100% (3065/3065), done. Success!\n","date":"2021-06-17","externalUrl":null,"permalink":"/en/posts/2021/aix-ulimit/","section":"Posts","summary":"Resolving Git large repository download failures on AIX due to file size limits by modifying ulimit settings.","title":"Resolving Git Large Repository Download Failures on AIX by Removing File Resource Limits","type":"posts"},{"content":"","date":"2021-06-17","externalUrl":null,"permalink":"/en/tags/ulimit/","section":"Tags","summary":"","title":"Ulimit","type":"tags"},{"content":"Recently, I encountered extremely slow artifact upload speeds when using Artifactory Enterprise. After collaborating with IT and the Artifactory administrator, the issue was finally resolved. I\u0026rsquo;m sharing the troubleshooting process here.\nThis might be helpful if you\u0026rsquo;ve encountered similar problems.\nProblem Description # Recently, I noticed that uploading artifacts to Artifactory via Jenkins occasionally became extremely slow. This was especially true when a Jenkins stage involved multiple uploads; the second upload often resulted in extremely slow transfer speeds (KB/s).\nTroubleshooting and Resolution # My build environment and Jenkins were unchanged, and all build jobs experienced slow uploads. To rule out the Artifactory plugin as the cause, I tested uploads using the curl command, which also exhibited slow speeds.\nThis pointed to a problem with Artifactory itself.\nWas Artifactory recently upgraded? Were any settings recently changed in Artifactory? Was there a problem with the Artifactory server? After communicating with the Artifactory administrator, possibilities 1 and 2 were ruled out. To completely eliminate Artifactory as the source of the problem, I tested using scp for copying files, and it also exhibited slow transfer speeds. This indicated a network issue.\nThis required IT\u0026rsquo;s assistance to troubleshoot the network problem. IT ultimately suggested trying a different network interface card (NIC) (due to similar past incidents). This would cause a brief network interruption, but the administrator eventually agreed.\nFortunately, after changing the NIC, the speed of Jenkins uploading artifacts to Artifactory returned to normal.\nSummary # A few takeaways from handling this incident:\nBecause this issue involved several teams, clearly defining the problem, providing well-reasoned hypotheses, and explaining the severity of the consequences (e.g., impact on releases) were crucial for getting everyone involved to prioritize the issue. Otherwise, everyone would wait, and nobody would solve the problem.\nWhen the Artifactory administrator suggested using a different data center instance, I recommended they first try changing the NIC. If the problem persisted, they could create another server in the same data center. Only if the issue remained should they consider migrating to a different data center instance. This significantly reduced the extra work involved in trial-and-error from the user\u0026rsquo;s perspective.\n","date":"2021-06-16","externalUrl":null,"permalink":"/en/posts/2021/artifactory-slow-upload/","section":"Posts","summary":"Encountered slow upload speeds and occasional failures when uploading artifacts to JFrog Artifactory. This post shares the troubleshooting process and lessons learned.","title":"Regarding Extremely Slow and Occasional Upload Failures to Artifactory — A Case Study","type":"posts"},{"content":"","date":"2021-06-07","externalUrl":null,"permalink":"/en/tags/eslint/","section":"Tags","summary":"","title":"ESlint","type":"tags"},{"content":"I\u0026rsquo;m just documenting to myself that it was solved by following.\nWhen I want to integrate the ESlint report with Jenkins. I encourage a problem\nThat is eslint-report.html display different with it on my local machine, and I also log to Jenkins server and grab the eslint-report.html to local, it works well.\nI used HTML Publisher plugin to display the HTML report, but only the ESlint HTML report has problems other report work well, so I guess this problem may be caused by Jenkins.\nFinally, I find it. (Stackoverflow URL)\nFollow the below steps for solution # Open the Jenkin home page. Go to Manage Jenkins. Now go to Script Console. And in that console paste the below statement and click on Run. System.setProperty(\u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;\u0026#34;) After that, it will load CSS and JS. According to Jenkins\u0026rsquo;s new Content Security Policy and I saw No frames allowed.\nThat is exactly the error I get on chrome by right-clicking on Elements.\n","date":"2021-06-07","externalUrl":null,"permalink":"/en/posts/2021/jenkins-eslint/","section":"Posts","summary":"This article explains how to resolve the issue of ESlint HTML report not displaying correctly in Jenkins jobs due to Content Security Policy restrictions, including the steps to configure Jenkins to allow the report to load properly.","title":"Resolved problem that ESlint HTML report is not displayed correctly in Jenkins job","type":"posts"},{"content":"在使用 Git 提交代码之前，建议做以下这些设置。\n叫指南有点夸张，因为它在有些情况下下不适用，比如你已经有了 .gitattributes 或 .editorconfig 等文件，那么有些设置就不用做了。\n因此暂且叫他指北吧，它通常情况下还是很有用的。\n废话不多说，看看都需要哪些设置吧。\n1. 配置 name 和 email # # 注意，你需要将下面示例中我的 name 和 email 换成你自己的 $ git config --global user.name \u0026#34;shenxianpeng\u0026#34; $ git config --global user.email \u0026#34;xianpeng.shen@gmail.com\u0026#34; 对于，我还推荐你设置头像，这样方便同事间的快速识别。\n当你不设置头像的时候，只有把鼠标放到头像上才知道 Pull Request 的 Reviewers 是谁（来自于Bitubkcet）。\n2. 设置 core.autocrlf=false # 为了防止 CRLF(windows) 和 LF(UNIX/Linux/Mac) 的转换问题。为了避免在使用 Git 提交代码时出现历史被掩盖的问题，强烈建议每个使用 Git 的人执行以下命令\n$ git config --global core.autocrlf false # 检查并查看是否输出 \u0026#34;core.autocrlf=false\u0026#34;，这意味着命令设置成功。 $ git config --list 如果你的项目底下已经有了 .gitattributes 或 .editorconfig 文件，通常这些文件里面都有放置 CRLF 和 LF 的转换问题的设置项。\n这时候你就不必特意执行命令 git config --global core.autocrlf false\n3. 编写有规范的提交 # 我在之前的文章里分享过关于如何设置提交信息规范，请参看《Git提交信息和分支创建规范》。\n4. 提交历史的压缩 # 比如你修改一个 bug，假设你通过 3 次提交到你的个人分支才把它改好。这时候你提 Pull Request 就会显示有三个提交。\n如果提交历史不进行压缩，这个 PR 被合并到主分支后，以后别人看到你这个 bug 的修复就是去这三个 commits 里去一一查看，进行对比，才能知道到底修改了什么。\n压缩提交历史就是将三次提交压缩成一次提交。\n可以通过 git rebase 命令进行 commit 的压缩，比如将最近三次提交压缩成一次可以执行\ngit rebase -i HEAD~3 5. 删除已经 merge 的分支 # 有些 SCM，比如 Bitbucket 不支持默认勾选 Delete source branch after merging，这个问题终于在 Bitbucket 7.3 版本修复了。详见 BSERV-9254 和 BSERV-3272 （2013年创建的）。\n注意在合并代码时勾选删除源分支这一选项，否则会造成大量的开发分支留在 Git 仓库下。\n如果还需要哪些设置这里没有提到的，欢迎补充。\n","date":"2021-05-14","externalUrl":null,"permalink":"/posts/2021/git-guidelines/","section":"Posts","summary":"本文介绍了在使用 Git 提交代码之前需要进行的一些常见设置，包括配置用户名和邮箱、处理换行符、编写规范的提交信息等，帮助开发者更好地管理代码版本。","title":"Git 常见设置指北","type":"posts"},{"content":" Why need branching naming convention # To better manage the branches on Git(I sued Bitbucket), integration with CI tool, Artifactory, and automation will be more simple and clear.\nFor example, good unified partition naming can help the team easily find and integrate without special processing. Therefore, you should unify the partition naming rules for all repositories.\nBranches naming convention # main branch naming # In general, the main\u0026rsquo;s branch names most like master or main.\nDevelopment branch naming # I would name my development branch just called develop.\nBugfix and feature branches naming # For Bitbucket, it has default types of branches for use, like bugfix/, feature/. So my bugfix, feature combine with the Jira key together, such as bugfix/ABC-1234 or feature/ABC-2345.\nHotfix and release branches naming # For hotfix and release, my naming convention always like release/1.1.0, hotfix/1.1.0.HF1.\nOther branches # Maybe your Jira task ticket you don\u0026rsquo;t want to make it in bugfix or feature, you can name it to start with task, so the branch name is task/ABC-3456.\nIf you have to provide diagnostic build to custom, you can name your branch diag/ABC-5678.\nSummary # Anyway, having a unified branch naming convention is very important for implement CI/CD and your whole team.\n","date":"2021-05-13","externalUrl":null,"permalink":"/en/posts/2019/git-branching-strategy/","section":"Posts","summary":"This article introduces the conventional branch naming specification, including the purpose of branch names, key points, and basic rules for naming branches in Git. It also provides examples of branch prefixes and their meanings.","title":"Branch Naming Convention","type":"posts"},{"content":" Problem # When you do CI with JFrog Artifactory when you want to download the entire folder artifacts, but maybe your IT doesn\u0026rsquo;t enable this function, whatever some seasons.\nYou can try the below JFrog Artifactory API to know if you\u0026rsquo;re using Artifactory whether allowed to download the entire folder artifacts.\njust visit this API URL: https://den-artifactory.company.com/artifactory/api/archive/download/team-generic-release-den/project/abc/main/?archiveType=zip\nYou will see an error message returned if the Artifactory is not allowed to download the entire folder.\n{ \u0026#34;errors\u0026#34;: [ { \u0026#34;status\u0026#34;: 403, \u0026#34;message\u0026#34;: \u0026#34;Download Folder functionality is disabled.\u0026#34; } ] } More details about the API could find here Retrieve Folder or Repository Archive\nWorkaround # So to be enabled to download entire folder artifacts, I found other JFrog Artifactory APIs provide a workaround.\nHow to download the entire folder artifacts programmatically? this post will show you how to use other Artifactory REST API to get a workaround.\n1. Get All Artifacts Created in Date Range # API URL: Artifacts Created in Date Range\nThis is the snippet code I use this API\n# download.sh USERNAME=$1 PASSWORD=$2 REPO=$3 # which day ago do you want to download N_DAY_AGO=$4 # today START_TIME=$(($(date --date=\u0026#34;$N_DAY_AGO days ago\u0026#34; +%s%N)/1000000)) END_TIME=$(($(date +%s%N)/1000000)) ARTIFACTORY=https://den-artifactory.company.com/artifactory if [ ! -x \u0026#34;`which sha1sum`\u0026#34; ]; then echo \u0026#34;You need to have the \u0026#39;sha1sum\u0026#39; command in your path.\u0026#34;; exit 1; fi RESULTS=`curl -s -X GET -u $USERNAME:$PASSWORD \u0026#34;$ARTIFACTORY/api/search/creation?from=$START_TIME\u0026amp;to=$END_TIME\u0026amp;repos=$REPO\u0026#34; | grep uri | awk \u0026#39;{print $3}\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed -r \u0026#39;s/^.{1}//\u0026#39;` echo $RESULTS for RESULT in $RESULTS ; do echo \u0026#34;fetching path from $RESULT\u0026#34; PATH_TO_FILE=`curl -s -X GET -u $USERNAME:$PASSWORD $RESULT | grep downloadUri | awk \u0026#39;{print $3}\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed -r \u0026#39;s/^.{1}//\u0026#39;` echo \u0026#34;download file path $PATH_TO_FILE\u0026#34; curl -u $USERNAME:$PASSWORD -O $PATH_TO_FILE done Then you just use this as: sh download.sh ${USERNAME} ${PASSWORD} ${REPO_PATH} ${N_DAY_AGO}\n2. Get all artifacts matching the given Ant path pattern # More about this API see: Pattern Search\nTake an example screenshot of pattern search:\nThen you can use Shell, Python language to get the file path from the response, then use curl -u $USERNAME:$PASSWORD -O $PATH_TO_FILE command to download the file one by one.\nIf you have better solutions, suggestions, or questions, you can leave a comment.\n","date":"2021-05-04","externalUrl":null,"permalink":"/en/posts/2021/artifactory-api-search/","section":"Posts","summary":"This article explains how to download the entire folder artifacts from JFrog Artifactory when the “Download Folder functionality is disabled”. It provides a workaround using Artifactory REST APIs to retrieve and download artifacts programmatically.","title":"How to download the entire folder artifacts when Artifactory \"Download Folder functionality is disabled\"?","type":"posts"},{"content":"","date":"2021-05-04","externalUrl":null,"permalink":"/en/tags/jfrog/","section":"Tags","summary":"","title":"JFrog","type":"tags"},{"content":" What\u0026rsquo;s the problem? # Today I am having a problem where the Windows installer I created is not installing, and the following Windows installer box pops up.\nBut it works well in the previous build, and I didn\u0026rsquo;t make any code changes. It is strange, actually fix this problem is very easy but not easy to find.\nHow to fix it? # In my case, I just remove the space from my build folder naming. I have made follow mistakes:\nMy previous build name is v2.2.2.3500-da121sa-Developer, but for this build, I named it to v2.2.2.3500-32jkjdk - Developer\nHow to find the solution? # This problem takes me several hours until I google this article which inspired me.\nJust like the above article, if I try to use the command line msiexec.exe other-commands ..., compare with the works installer will also quick to find the root cause.\nI realized it immediately I should try to remove the spaces from the build folder\u0026hellip; Wow, the installer back to work.\nIf this happens to you, I hope it also works for you, and leave a comment if it works for you.\n","date":"2021-04-22","externalUrl":null,"permalink":"/en/posts/2021/why-windows-installer-pop-up/","section":"Posts","summary":"This article explains a common issue with Windows installers where a pop-up appears unexpectedly, and how to resolve it by correcting the build folder naming convention.","title":"Why Windows Installer pop up? (Resolved)","type":"posts"},{"content":"","date":"2021-04-06","externalUrl":null,"permalink":"/tags/jacoco/","section":"标签","summary":"","title":"JaCoCo","type":"tags"},{"content":"本文适用的是 Gradle 来构建和适用 JaCoCo。\n分别介绍了 build.gradle 的文件配置，执行测试和生成报告，报告参数说明，以及如何忽略指定的包或类从而影响测试覆盖率的结果。\nbuild.gradle 文件配置 # 比如使用 gradle 来管理的项目可以在 build.gradle 里添加如下代码\nplugins { id \u0026#39;jacoco\u0026#39; } jacoco { toolVersion = \u0026#34;0.8.5\u0026#34; } test { useJUnitPlatform() exclude \u0026#39;**/**IgnoreTest.class\u0026#39; // 如果有 test case 不通过，如有必要可以通过这样忽略掉 finalizedBy jacocoTestReport // report is always generated after tests run } jacocoTestReport { dependsOn test // tests are required to run before generating the report reports { xml.enabled true csv.enabled false html.destination file(\u0026#34;${buildDir}/reports/jacoco\u0026#34;) } } 执行测试，生成代码覆盖率报告 # 然后执行 gradle test 就可以了。之后可以可以在 build\\reports\\jacoco 目录下找到报告了。\n重点是如何分析报告。打开 index.html，报告显示如下：\n报告参数说明 # Coverage Counters（覆盖计数器） # JaCoCo 使用一组不同的计数器来计算覆盖率指标，所有这些计数器都来自于 Java 类文件中包含的信息，这些信息基本上是 Java 字节码指令和嵌入类文件中的调试信息。这种方法可以在没有源代码的情况下，对应用程序进行有效的即时检测和分析。在大多数情况下，收集到的信息可以映射到源代码，并可视化到行级粒度。\n这种方法也有一定的局限性，就是类文件必须与调试信息一起编译，以计算行级覆盖率并提供源码高亮。但不是所有的 Java 语言结构都可以直接编译成相应的字节码。在这种情况下，Java 编译器会创建所谓的合成代码，有时会导致意外的代码覆盖率结果。\nInstructions (C0 Coverage) - 指令（C0覆盖率） # 最小的单位 JaCoCo 计数是单个 Java 字节码指令，指令覆盖率提供了关于被执行或遗漏的代码量的信息，这个指标完全独立于源码格式化，即使在类文件中没有调试信息的情况下也始终可用。\nBranches (C1 Coverage) - 分支（C1覆盖率） # JaCoCo 还计算所有 if 和 switch 语句的分支覆盖率，这个指标计算一个方法中此类分支的总数，并确定执行或遗漏的分支数量。即使在类文件中没有调试信息的情况下，分支覆盖率总是可用的。但请注意在这个计数器定义的上下文中异常处理不被认为是分支。\n如果类文件没有编译调试信息，决策点可以被映射到源行并相应地高亮显示。\n没有覆盖。行中没有分支被执行（红菱形 部分覆盖。仅执行了该线的部分分支（黄钻 全覆盖。线路中的所有分支都已执行（绿色菱形） Cyclomatic Complexity - 环形复杂度 # JaCoCo 还计算了每个非抽象方法的循环复杂度并总结了类、包和组的复杂度。循环复杂度是指在（线性）组合中，能够产生通过一个方法的所有可能路径的最小路径数。 因此复杂度值可以作为完全覆盖某个软件的单元测试用例数量的指示，即使在类文件中没有调试信息的情况下，也可以计算出复杂度数字。\n循环复杂度v(G)的正式定义是基于将方法的控制流图表示为一个有向图。\nv(G) = E - N + 2\n其中E为边数，N为节点数。JaCoCo根据分支数(B)和决策点数(D)计算方法的循环复杂度，其等价公式如下。\nv(G) = B - D + 1\n根据每个分支的覆盖状态，JaCoCo还计算每个方法的覆盖和遗漏复杂度。遗漏的复杂度再次表明了完全覆盖一个模块所缺少的测试用例数量。请注意，由于JaCoCo不考虑异常处理作为分支，尝试/捕获块也不会增加复杂性。\nLines - 行 # 对于所有已经编译过调试信息的类文件，可以计算出各个行的覆盖率信息。当至少有一条分配给该行的指令被执行时，就认为该源行已被执行。\n由于单行通常会编译成多条字节码指令，源码高亮显示每行包含源码的三种不同状态。\nNo coverage: 该行没有指令被执行（红色背景）。 部分覆盖。该行中只有部分指令被执行（黄色背景）。 全覆盖。该行的所有指令都已执行（绿色背景）。 根据源码的格式，一个源码的一行可能涉及多个方法或多个类。因此，方法的行数不能简单地相加来获得包含类的总行数。同样的道理也适用于一个源文件中多个类的行数。JaCoCo根据实际的源代码行数来计算类和源代码文件的行数。\nMethod - 方法 # 每个非抽象方法至少包含一条指令。当至少有一条指令被执行时，一个方法就被认为是被执行的。由于 JaCoCo 工作在字节代码层面，构造函数和静态初始化器也被算作方法，其中一些方法在 Java 源代码中可能没有直接的对应关系，比如隐式的，因此生成了默认的构造函数或常量的初始化器。\nClasses - 类 # 当一个类中至少有一个方法被执行时，该类被认为是被执行的。请注意，JaCoCo 认为构造函数和静态初始化器都是方法。由于 Java 接口类型可能包含静态初始化器，这种接口也被视为可执行类。\n覆盖率的计算原文\n从代码覆盖率报告中忽略指定的包或代码 # 对于有些包和代码可能不属于你的项目，但也被统计在内，可以通修改在 build.gradle 将指定的代码或是包从 JaCoCo 报告中忽略掉。如下：\n// 省略部分代码 jacocoTestReport { dependsOn test // tests are required to run before generating the report reports { xml.enabled true csv.enabled false html.destination file(\u0026#34;${buildDir}/reports/jacoco\u0026#34;) } afterEvaluate { classDirectories.setFrom(files(classDirectories.files.collect { fileTree(dir: it, exclude: [ \u0026#39;com/vmware/antlr4c3/**\u0026#39;]) \u0026#39;com/vmware/antlr4c3/**\u0026#39;, \u0026#39;com/basic/parser/BasicParser*\u0026#39; ]) })) } } ","date":"2021-04-06","externalUrl":null,"permalink":"/posts/2021/jacoco-imp/","section":"Posts","summary":"本文介绍了 JaCoCo 的使用方法，包括 Gradle 配置、执行测试生成报告、报告参数说明以及如何忽略指定的包或类影响测试覆盖率结果。","title":"JaCoCo 代码覆盖率实践分享","type":"posts"},{"content":"Python, needless to say, is one of the easiest dynamic programming languages to learn. Its cross-platform compatibility, readability, ease of writing, and rich packages make it one of the most commonly used languages among DevOps/testing/development engineers.\nMany have used it to accomplish a lot of work, but do you simply stop at functional implementation and ignore writing more concise and elegant Pythonic code?\nWhen I first started using Python, I didn\u0026rsquo;t know the word Pythonic. Years ago, a senior programmer mentioned during my training that some code in a project wasn\u0026rsquo;t Pythonic enough and needed refactoring. From the context, I understood it to mean: the Python code wasn\u0026rsquo;t written in the Pythonic way.\nWhat is Pythonic # Making full use of Python\u0026rsquo;s features to produce clear, concise, and maintainable code. Pythonic means that the code is not only syntactically correct but also follows the conventions of the Python community and uses the language as intended.\nExamples # Here\u0026rsquo;s a snippet of code from a C/C++ programmer:\nint a = 1; int b = 100; int total_sum = 0; while (b \u0026gt;= a) { total_sum += a; a++; } Without learning Python programming patterns, converting the above code to Python might look like this:\na = 1 b = 100 total_sum = 0 while b \u0026gt;= a: total_sum += a a += 1 A Pythonic way to write this would be:\ntotal_sum = sum(range(1, 101)) Here\u0026rsquo;s another common example. A for loop in Java might be written like this:\nfor(int index=0; index \u0026lt; items.length; index++) { items[index].performAction(); } In Python, a cleaner approach would be:\nfor item in items: item.perform_action() Or even a generator expression:\n(item.some_attribute for item in items) Essentially, when someone says something isn\u0026rsquo;t pythonic, they\u0026rsquo;re saying the code could be rewritten in a way that\u0026rsquo;s more aligned with Python\u0026rsquo;s coding style. Also, get familiar with Python\u0026rsquo;s Built-in Functions instead of reinventing the wheel.\nThe \u0026ldquo;Official Introduction\u0026rdquo; to Pythonic # In fact, an introduction to Pythonic is secretly \u0026ldquo;hidden\u0026rdquo; in the Python command line. Just open the Python console and type import this, and you\u0026rsquo;ll see:\nC:\\Users\\xshen\u0026gt;python Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32 Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren\u0026#39;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you\u0026#39;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it\u0026#39;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let\u0026#39;s do more of those! \u0026gt;\u0026gt;\u0026gt; A direct translation is: Tim Peters\u0026rsquo; \u0026ldquo;The Zen of Python\u0026rdquo;\nBeautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren\u0026#39;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you\u0026#39;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it\u0026#39;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let\u0026#39;s do more of those! Have you grasped the concept of Pythonic?\n","date":"2021-03-28","externalUrl":null,"permalink":"/en/posts/2021/pythonic/","section":"Posts","summary":"This article introduces the concept of Pythonic code and demonstrates through examples how to write more concise and elegant Python code, helping developers improve code quality and readability.","title":"Is Your Python Code Pythonic Enough?","type":"posts"},{"content":"","date":"2021-03-28","externalUrl":null,"permalink":"/en/tags/pythonic/","section":"Tags","summary":"","title":"Pythonic","type":"tags"},{"content":" Problem # When you use Jenkins multibranch pipeline, you may want to have different default parameters settings for defferent branches build.\nFor example:\nFor develop/hotfix/release branches, except regular build, you also want to do some code analyzes, like code scanning, etc. For other branches, like feature/bugfix or Pull Request that you just want to do a regular build.\nSo you need to have dynamic parameter settings for your multibranch pipeline job.\nSolution # So for these cases, how to deal with Jenkins multibranch pipeline. Here are some code snippet that is works well in my Jenkinsfile.\ndef polarisValue = false def blackduckValue = false if (env.BRANCH_NAME.startsWith(\u0026#34;develop\u0026#34;) || env.BRANCH_NAME.startsWith(\u0026#34;hotfix\u0026#34;) || env.BRANCH_NAME.startsWith(\u0026#34;release\u0026#34;)) { polarisValue = true blackduckValue = true } pipeline { agent { node { label \u0026#39;gradle\u0026#39; } } parameters { booleanParam defaultValue: polarisValue, name: \u0026#39;Polaris\u0026#39;, summary: \u0026#39;Uncheck to disable Polaris\u0026#39; booleanParam defaultValue: blackduckValue, name: \u0026#39;BlackDuck\u0026#39;, summary: \u0026#39;Uncheck to disable BD scan\u0026#39; } stages { // ... } } ","date":"2021-03-24","externalUrl":null,"permalink":"/en/posts/2021/jenkins-dynamic-default-parameters/","section":"Posts","summary":"This article explains how to set different default parameters for different branches in Jenkins multibranch pipelines, allowing for dynamic configuration based on the branch being built.","title":"Different branches have different default parameters in Jenkins","type":"posts"},{"content":"","date":"2021-03-20","externalUrl":null,"permalink":"/en/tags/codereview/","section":"Tags","summary":"","title":"CodeReview","type":"tags"},{"content":" Background # Code review is the process of having someone else examine your code. Its purpose is to ensure that the overall code health of the codebase continuously improves over time.\nThere\u0026rsquo;s a Chinese proverb: \u0026ldquo;Three people walking together, one is bound to be my teacher.\u0026rdquo;\nThe same is true for code review:\nOthers\u0026rsquo; reviews may offer different perspectives and suggestions; Everyone makes mistakes, and having an extra pair of eyes reduces the chance of errors. Therefore, code review is the most important check before merging your code into the main branch. It\u0026rsquo;s also an excellent and low-cost way to find bugs in software.\nGiven the importance and obvious benefits of code review, it\u0026rsquo;s often heard that code review is difficult to implement and ineffective in teams. Where does the problem lie?\nFrom my observation, there are two main reasons:\nFirst, reading other people\u0026rsquo;s code takes time, often requiring the code submitter to explain the business logic to the reviewer, thus consuming time for both parties; Second, if the code reviewer is overworked and stressed and doesn\u0026rsquo;t have time, it\u0026rsquo;s easy to lead to inadequate execution and going through the motions;\nHow can we conduct code reviews more effectively? Let\u0026rsquo;s first look at how large companies do it. Google\u0026rsquo;s article on code review provides specific principles.\nGoogle\u0026rsquo;s Code Review Principles # When conducting a code review, ensure that:\nThe code is well-designed The functionality is helpful to the code\u0026rsquo;s users Any UI changes are sensible and look good Any concurrent programming is done safely The code is no more complex than necessary The developer hasn\u0026rsquo;t implemented things they might need in the future The code has appropriate unit tests The tests are well-designed The developer has used clear names for everything The comments are clear, useful, and primarily explain why rather than what The code is properly documented The code follows our style guide Ensure you examine every line of code you are asked to review, look at the context, ensure you are improving code health, and praise the developer for good work.\nOriginal: https://google.github.io/eng-practices/review/reviewer/looking-for.html\nImplementation of Code Review Principles # To better implement code review, it\u0026rsquo;s necessary to first establish principles. You can adapt, remove, or add to the above principles based on your actual situation;\nSecond, technical leadership should actively promote the unified code review principles to developers;\nThird, the specific rules in the principles should be incorporated into the Pull Request process as much as possible through process control and automated checks.\nAlso, remember to be peaceful as a Reviewer!!! When reviewing code, avoid giving suggestions with a \u0026ldquo;teaching\u0026rdquo; tone, as this can easily backfire.\nHere are some incomplete practices for reference.\nProcess Control # Preventing any code without review from entering the main branch # Using Bitbucket as an example. GitHub and GitLab have similar settings.\nTurn on the option Prevent changes without a pull request in the branch permission settings. Of course, if necessary, you can add exceptions to this option, allowing the added person to commit code without a Pull Request.\nEnable the Minimum approvals option in Merge Check. For example, set Number of approvals = 1, so at least one Reviewer needs to click the Approve button to allow merging.\nAutomated Checks # Verify compilation and testing through the CI pipeline # Establish an automated build and test pipeline so that builds, tests, and checks can be automatically performed when creating a Pull Request. Jenkins\u0026rsquo; Multi-branch pipeline can meet this need.\nTurn on the Minimum successful builds option in Bitbucket\u0026rsquo;s Merge Check to verify build/test results, preventing any code that hasn\u0026rsquo;t passed build and tests from merging into the main branch.\nAdditionally, you can achieve this by writing your own tools or integrating other CI tools for checks, such as:\nAnalyze the commit history of the Pull Request to suggest Reviewers; Use Lint tools to check coding standards; Use REST APIs to check if commits need to be compressed to ensure a clear commit history; Use SonarQube to check the Quality Gate, etc. Implementing automated checks can help Reviewers focus their review efforts on the specific implementation of the code, leaving other aspects to the tools.\nConclusion # The effectiveness of code review is positively correlated with whether a team has a good technical atmosphere or whether there is technical leadership and influential senior engineers.\nIf most engineers in a team are high-quality, they will enjoy writing excellent code (or nitpicking). Conversely, if the team doesn\u0026rsquo;t value standards and only pursues short-term performance, it will only lead to accumulating more and more technical debt and products becoming increasingly worse. Welcome to leave comments and share your opinions or suggestions.\n","date":"2021-03-20","externalUrl":null,"permalink":"/en/posts/2021/code-review/","section":"Posts","summary":"This article introduces Google’s code review principles and shares practical experience on how to effectively implement code review in a team, including process control and automated checks.","title":"Thoughts and Practices Based on Google's Code Review Principles","type":"posts"},{"content":"Today, when I tried to upgrade my team\u0026rsquo;s Jenkins server from Jenkins 2.235.1 to Jenkins 2.263.3, I met a problem that can not launch the Windows agent.\n[2021-01-29 23:50:40] [windows-agents] Connecting to xxx.xxx.xxx.xxx Checking if Java exists java -version returned 11.0.2. [2021-01-29 23:50:40] [windows-agents] Installing the Jenkins agent service [2021-01-29 23:50:40] [windows-agents] Copying jenkins-agent.exe ERROR: Unexpected error in launching an agent. This is probably a bug in Jenkins Also: java.lang.Throwable: launched here at hudson.slaves.SlaveComputer._connect(SlaveComputer.java:286) at hudson.model.Computer.connect(Computer.java:435) at hudson.slaves.SlaveComputer.doLaunchSlaveAgent(SlaveComputer.java:790) ... ... at java.lang.Thread.run(Thread.java:748) java.lang.NullPointerException at hudson.os.windows.ManagedWindowsServiceLauncher.launch(ManagedWindowsServiceLauncher.java:298) This issue had been raised in the Jenkins Jira project: JENKINS-63198 and JENKINS-63198\nThere is also a Windows Support Updates guide here that mentioned this problem.\nFinally, I fixed this problem by the following steps:\nUpdate windows-slaves-plugin to the lastest version 1.7 (fixes for Jenkins 2.248+) Then the error should be like this\n[2021-01-30 23:53:40] [windows-agents] Connecting to xxx.xxx.xxx.xxx Checking if Java exists java -version returned 11.0.2. [2021-01-30 23:53:47] [windows-agents] Copying jenkins-agent.xml [2021-01-30 23:53:48] [windows-agents] Copying agent.jar [2021-01-30 23:53:48] [windows-agents] Starting the service ERROR: Unexpected error in launching an agent. This is probably a bug in Jenkins org.jinterop.dcom.common.JIException: Unknown Failure at org.jvnet.hudson.wmi.Win32Service$Implementation.start(Win32Service.java:149) Caused: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedMethodAccessor219.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.kohsuke.jinterop.JInteropInvocationHandler.invoke(JInteropInvocationHandler.java:140) Also: java.lang.Throwable: launched here Then change jenkins-agent.exe.config file. remove or comment out this line \u0026lt;supportedRuntime version=\u0026quot;v2.0.50727\u0026quot; /\u0026gt; as below also do this for jenkins-slave.exe.config in case it also exists.\n\u0026lt;configuration\u0026gt; \u0026lt;runtime\u0026gt; \u0026lt;!-- see http://support.microsoft.com/kb/936707 --\u0026gt; \u0026lt;generatePublisherEvidence enabled=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;/runtime\u0026gt; \u0026lt;startup\u0026gt; \u0026lt;!-- this can be hosted either on .NET 2.0 or 4.0 --\u0026gt; \u0026lt;!-- \u0026lt;supportedRuntime version=\u0026#34;v2.0.50727\u0026#34; /\u0026gt; --\u0026gt; \u0026lt;supportedRuntime version=\u0026#34;v4.0\u0026#34; /\u0026gt; \u0026lt;/startup\u0026gt; \u0026lt;/configuration\u0026gt; Then try to Launch agent. If it still does not work and has this error message \u0026ldquo;.NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service\u0026rdquo;, you need to upgrade your .NET Framework.\nHere is a link for update .NET Framework.\nHopefully, this could help you to fix connect the issue of the Windows agent. Let me know in case of any questions.\n","date":"2021-02-11","externalUrl":null,"permalink":"/en/posts/2021/jenkins-windows-agent-cannot-start/","section":"Posts","summary":"This article explains how to resolve the issue of Windows agents not starting after upgrading Jenkins, including the necessary steps to update the Windows Slaves plugin and modify configuration files.","title":"Jenkins upgrade issue \"Windows agents won't start\" workaround","type":"posts"},{"content":"DevOps 实际上是什么意思？🤔\nDevOps 是一种软件开发方法，涉及持续开发，持续测试，持续集成，部署和监视。这一系列过程跨越了传统上孤立的开发和运营团队，DevOps 试图消除它们之间的障碍。\n因此，DevOps 工程师基本上与 Development 和 Operations 团队合作。它是这两个主要部分之间的链接。\n概念与工具 # DevOps 包括诸如构建自动化、CI/CD、基础架构即代码等概念，并且有许多工具可以实现这些概念。由于这些工具数量众多，因此可能会造成混乱和压倒性的结果。\n最重要的是要了解概念，并为每个类别的学习找一种特定的工具。例如，当你已经知道什么是 CI/CD 并知道如何使用 Jenkins 时，也将很容易学习同类型的其他替代工具。\n接下来让就来看看学习 DevOps 需要掌握哪些技能。\n1）软件开发的概念 # 作为一名 DevOps 工程师，你不会直接对应用程序进行编程，但是当你与开发团队紧密合作以改善和自动化他们的任务时，你需要了解以下概念：\n开发人员的工作方式 他们正在使用哪个 git 工作流程 如何配置应用程序 自动化测试 2）操作系统 # 作为 DevOps 工程师，你负责准备在操作系统上部署应用程序的所需要的基础结构环境。并且由于大多数服务器是 Linux 服务器，因此你需要了解 Linux 操作系统，并善于使用命令行，所以你需要知道：\n基本的 Shell 命令 Linux 文件系统 管理服务器的基础知识 SSH 密钥管理 在服务器上安装不同的工具 3）网络与安全 # 你还需要了解网络和安全性的基础知识才能配置基础架构，例如：\n配置防火墙以保护应用程序 了解 IP 地址，端口和 DNS 的工作方式 负载均衡器 代理服务器 HTTP/HTTPS 但是，要在 DevOps 和 IT Operations 之间划清界线，你不是系统管理员。因此，在这里不需要高级知识，理解和了解基本知识就够了。IT 方面是这些 SysAdmins，Networking 或 Security Engineers 人的专长。\n4）容器化 # 随着容器成为新标准，你可能会将应用程序作为容器运行，这意味着你需要大致了解：\n虚拟化的概念 容器的概念 学习哪个工具？ Docker - 当今最受欢迎的容器技术 5）持续集成和部署 # 在 DevOps 中，所有代码更改（例如开发人员的新功能和错误修复）都应集成到现有应用程序中，并以自动化方式连续地部署到最终用户。因此，建立完整的 CI/CD 管道是 DevOps 工程师的主要任务和职责。\n在完成功能或错误修正后，应自动触发在 CI 服务器（例如 Jenkins ）上运行的管道，该管道：\n运行测试 打包应用程序 构建 Docker 镜像 将 Docker Image 推送到工件存储库，最后 将新版本部署到服务器（可以是开发，测试或生产服务器） 因此，你需要在此处学习技能：\n设置 CI/CD 服务器 构建工具和程序包管理器工具以执行测试并打包应用程序 配置工件存储库（例如 Nexus，Artifactory） 当然，可以集成更多的步骤，但是此流程代表 CI/CD 管道的核心，并且是 DevOps 任务和职责的核心。\n学习哪个工具？Jenkins 是最受欢迎的人之一。其他：Bamboo，Gitlab，TeamCity，CircleCI，TravisCI。\n6）云提供商 # 如今，许多公司正在使用云上的虚拟基础架构，而不是管理自己的基础架构。这些是基础架构即服务（IaaS）平台，可提供一系列服务，例如备份，安全性，负载平衡等。\n因此，你需要学习云平台的服务。例如。对于 AWS，你应该了解以下基本知识：\nIAM 服务-管理用户和权限 VPC 服务-你的专用网络 EC2 服务-虚拟服务器 AWS 提供了更多的服务，但是你只需要了解你实际需要的服务即可。例如，当 K8s 集群在 AWS 上运行时，你还需要学习 EKS 服务。 AWS 是功能最强大，使用最广泛的一种，但也是最困难的一种。\n学习哪个工具？AWS 是最受欢迎的一种。其他热门：Azure，Google Cloud，阿里云，腾讯云。\n7）容器编排 # 如前所述，容器已被广泛使用，在大公司中，成百上千个容器正在多台服务器上运行，这意味着需要以某种方式管理这些容器。\n为此目的，有一些容器编排工具，而最受欢迎的是 Kubernetes。因此，你需要学习：\nKubernetes 如何工作 管理和管理 Kubernetes 集群 并在其中部署应用程序 学习哪个工具？Kubernetes - 最受欢迎，没有之一。\n8）监视和日志管理 # 软件投入生产后，对其进行监视以跟踪性能，发现基础结构以及应用程序中的问题非常重要。因此，作为 DevOps 工程师的职责之一是：\n设置软件监控 设置基础架构监控，例如用于你的 Kubernetes 集群和底层服务器。 学习哪个工具？Prometheus, Grafana\u0026hellip;\n9）基础设施即代码 # 手动创建和维护基础架构非常耗时且容易出错，尤其是当你需要复制基础架构时，例如用于开发，测试和生产环境。\n在 DevOps 中，希望尽可能地自动化，那就是将“基础结构即代码（Infrastructure as Configuration）”引入其中。因此使用 IaC ，我们将使用代码来创建和配置基础结构，你需要了解两种 IaC 方式：\n基础设施配置 配置管理 使用这些工具，可以轻松地复制和恢复基础结构。因此，你应该在每个类别中都知道一种工具，以使自己的工作更有效率，并改善与同事的协作。\n学习哪个工具？\n基础架构设置：Terraform 是最受欢迎的一种。 配置管理：Ansible，Puppet，Chef。\n10）脚本语言 # 作为 DevOps 工程师就常见的工作就是编写脚本和小型的应用程序以自动化任务。为了能够做到这一点，你需要了解一种脚本或编程语言。\n这可能是特定于操作系统的脚本语言，例如 bash 或 Powershell。\n还需要掌握一种独立于操作系统的语言，例如 Python 或 Go。这些语言功能更强大，更灵活。如果你善于使用其中之一，它将使你在就业市场上更具价值。\n学习哪个工具？Python：目前是最需要的一个，它易于学习，易于阅读并且具有许多可用的库。其他：Go，NodeJS，Ruby。\n11）版本控制 # 上述所有这些自动化逻辑都作为代码编写，使用版本控制工具（例如Git）来管理这些代码和配置文件。\n学习哪个工具？Git - 最受欢迎和广泛使用，没有之一。\nDevOps Roadmap [2021] - How to become a DevOps Engineer\n","date":"2021-01-21","externalUrl":null,"permalink":"/posts/2021/devops-roadmap-2021/","section":"Posts","summary":"本文介绍了成为DevOps工程师所需的技能和工具，涵盖软件开发、操作系统、网络安全、容器化、持续集成与部署等方面的知识。","title":"2021年DevOps工程师的学习路线","type":"posts"},{"content":"DevOps 已经走了很长一段路，毫无疑问，它将在今年继续发光。由于许多公司都在寻求有关数字化转型的最佳实践，因此重要的是要了解领导者认为行业发展的方向。从这个意义上讲，以下文章是 DevOps 领导者对 DevOps 趋势的回应的集合，需要在 2021 年关注。\n让我们看看他们每个人对来年的 DevOps 有何评价。\n迁移到微服务将成为必须 —— Wipro Limited 首席 DevOps 工程师 “从整体迁移到微服务和容器化架构对于所有公司的数字化转型之旅都是必不可少的，它不再是一个选择或选项。这就是Kubernetes 的采用率将上升的地方，当组织迁移到多重云时，Terraform 将成为实现基础架构自动化的最终选择。”\nHybrid将成为部署规范 —— JFrog 开发人员关系 VP “2020年将加速远程工作，加快向云的迁移，并将 DevOps 从最佳实践转变为每个业务的重要组成部分。随着我们进入2021年，该行业将在多个方面拥抱Hybrid。首先，企业将完全采用混合型劳动力，将远程工作和现场团队协作的优势相结合。 其次，商业模式将变得混合，例如将虚拟规模与本地网络合并的会议。最终，随着公司对堆栈进行现代化以利用云原生技术的优势，混合将成为部署规范，但要意识到并非所有事物都可以迁移到外部。2021年的赢家将是拥抱业务，模型和产品混合的公司。”\nDataOps将蓬勃发展 - 乐天高级 DevOps 工程师 “DataOps 肯定会在 2021 年蓬勃发展，COVID 可能会在其中发挥作用。由于 COVID 和 WFH 的情况，数字内容的消费量猛增，这要求自动缩放和自我修复系统的自动化达到新水平，以满足增长和需求。\n到目前为止，DevOps 设置的系统仅用于日志记录，监视和警报（ELK/EFK 堆栈，Prometheus/Grafana/Alertmanager等）。现在，DevOps 现在应该加强并使用可用数据和指标来 产生有价值的见解，学习并应用机器学习模型来预测事件或中断，开发从数据中学习自身并预测能力的自动化以改进预算计划。许多人已经开始对此部分调用 MLOps/AIOps。”\n弹性测试将成为主流 —— Neotys 产品负责人 从我的角度来看，可观察性，性能测试和弹性测试之间的交叉点将成为主流。 随着 AWS 和 Google 等 WW 领导者最近发布的 Ops 问题，以及各个领域的数字化转型都在加速发展，市场将逐渐意识到，公共或私有云形式提供的无限可扩展性是不够的。” - Neotys 产品负责人 Patrick Wolf\nGitOps 将成为常态 —— 梅西百货的首席架构师 “一个“构建，拥有，拥有”的开发过程需要开发人员知道和理解的工具。GitOps 是 DevOps 如何使用开发人员工具来驱动操作的名称。\nGitOps 是一种进行持续交付的方法。 更具体地说，它是用于构建统一部署，监视和管理的 Cloud Native 应用程序的操作模型。 它通过将 Git 用作声明性基础结构和应用程序的真实来源来工作。 当在 Git 中推送和批准提交时，自动化的 CI/CD 管道将对你的基础架构进行更改。它还利用差异工具将实际生产状态与源代码控制下的生产状态进行比较，并在出现差异时提醒你。GitOps 的最终目标是加快开发速度，以便你的团队可以安全可靠地对 Kubernetes 中运行的复杂应用程序进行更改和更新。” -梅西百货（Macy\u0026rsquo;s）首席建筑师 Soumen Sarkar\n将会有更多的迁移到无服务器 —— ADP Lifion 的站点 SRE 经理 “2021 年将是注视更多迁移到无服务器的一年。如果容器和业务流程是 Z 代.. 那么无服务器的实时负载将是 Gen+ .. 仅在你使用时使用和付款可能看起来是一样的.. 但请考虑运行基于 k8s pod 的微服务，以便在需要时在无服务器上运行相同的服务。” - ADP Lifion 网站可靠性工程经理 Shivaramakrishnan G\nNoOps 出现 —— ClickIT Smart Technologies 的 CEO “我希望出现更多托管服务，并减少我们的 DevOps 运营并减少客户的运营支出。更多无服务器应用程序，更多无服务器服务，例如 Aurora 无服务器，Fargate，Amazon S3 和无服务器静态网站。数据中心中的 Amazon ECS/EKS（新版本 re：invent 2020）以及云管理服务，可让你减少数据中心的维护和开发。同样，将更多云原生的原理和功能移植到数据中心，例如。亲戚。” - ClickIT Smart Technologies 首席执行官 Alfonso Valdes\nBizDevOps 将大放异彩 —— Petco 的 DevOps 经理 “在架构和公司层次结构方面朝着成本优化的方向发展-随着业务的发展，DevOps 的价值不断提高。\n专注于灵活的，云原生的架构和工具，这些功能一旦具备了“大佬”的能力，就可以打包成小型企业使用的包装（Snowflake 或 Hazelcast 与 Oracle/Teradata）\nFaaS 才刚刚起步（无服务器，Lambda 等）- 运营问题正在得到解决，人们正在意识到潜力。”\n基础设施即代码（IaC）的地位将更高 —— 沃尔沃高级解决方案架构师 “基础架构即代码（IaC）：云中 DevOps 的核心原则。你的基础架构本地或云中的服务器，网络和存储设备（定义为代码）。这使公司可以自动化并简化其基础架构。 IaC 还提供了一个简单的基础结构版本控制系统，该系统可让团队在发生灾难性故障时回退到“有效的最后配置”。这意味着可以快速恢复并减少停机时间。”\n自动化和混乱工程变得非常重要 —— 直布罗陀印度开发中心的集团开发经理 “一切都是自动化的-构建，部署，测试，基础架构和发布。\n具有所需质量门的生产线。更快，可重复，可自定义和可靠的自动化是任何项目成功的关键。混沌工程-在当今混合基础设施世界中非常关键的方面。系统行为和客户体验紧密结合在一起，你越早对其进行测试，就越能为客户提供更好的体验。”\n云原生方法将被标准化 —— Ben Sapp “由于云空间已经真正地发展起来（最近十年左右），并且容器化已成为规范，所以一切都非常标准化，几乎就像大型机时代一样。\n当然，会有趋势和赚钱的机会。但是我不知道下一个大破坏者是什么。现在的一切基本上都与五年前的最佳做法相同，但更加可靠。我想越来越多的人将继续从宠物转移到牛身上，剩下诸如 Ansible 和 puppet 之类的工具仅用于打包程序和云 init 来构建容器主机。\nimo 是软件开发的黄金时代。 DevOps 和云原生方法已经实现了许多目标。管道，托管，存储，负载平衡……这些都在 5 分钟之内解决了。”\n安全将成为重中之重 —— CloudSkiff “从 DevSecOps 角度绝对跟踪基础设施中不受控制的变化。作为代码的基础架构很棒，但是有太多可移动的部分：代码库，状态文件，实际云状态。事情倾向于漂移。这些更改可能有多种原因：从开发人员通过 Web 控制台创建或更新基础架构而不告知任何人，到云提供商方面不受控制的更新。处理基础架构漂移与代码库之间的挑战可能会充满挑战。” - CloudSkiff\nChaos Engineering 将变得越来越重要 —— International Technology Ventures 的 CTO “在更多组织中的 DevOps 规划讨论中，混沌工程将变得越来越重要（且更常见）。大多数组织通常不执行混沌工程学（Chaos Engineering），即在生产中对软件系统进行实验以建立对系统抵御动荡和意外情况能力的信心。\n如果我们在传统的五个成熟度模型框架内考虑 DevOps，那么 Chaos Engineering 将是第 4 或第 5 级学科，将包含在 DevOps 实践的保护范围内。正如将单独的测试/质量保证小组的传统角色纳入 DevOops 的学科一样，Chaos Engineering 也应如此。”\n更关注即时日志以快速验证成功或失败 —— ADESA 平台稳定性总监 “在后期部署中使用日志来验证发布是否成功，或存在严重错误。人们需要建立的最大联系是定义手动流程，然后实现自动化的巨大飞跃。一键部署，即时日志可快速验证成功或失败，然后触发回滚。随之而来的是复杂性以及跨服务依赖性，是否可以回滚某些内容，或者是否需要对其他服务进行进一步测试。想象一下 100 种微服务（又称管道，甚至还有 100 个容器）。\n作为一项，我总是庆祝成功的回滚，因为它不会对服务产生影响，而且是成功的。” -ADESA平台稳定性总监Craig Schultz\nDevSecOps 将成为 DevOps 的默认部分 —— JFrog 的 DevOps 架构师 DevSecOps 的 “Sec” 部分将越来越成为软件开发生命周期中不可或缺的一部分，真正的安全性 “向左移动” 方法将成为新的规范，CI/CD 管道中的专用安全性步骤将更少从开发人员的 IDE 到依赖关系和静态代码分析，安全和自动识别和采取措施将是所有流程步骤的一部分。在没有适当（自动？）解决这些问题的情况下，不会发布软件组件。真正的安全问题是免费软件。”\n希望你喜欢我们对 DevOps 趋势的专家综述，并在 2021 年关注。如果你认为这里缺少应考虑的内容，请在评论中分享你的观点。\n原文 15 DevOps Trends to Expect in 2021 的翻译。\n","date":"2021-01-21","externalUrl":null,"permalink":"/posts/2021/devops-trends-2021/","section":"Posts","summary":"本文介绍了 2021 年 DevOps 领域的主要趋势，包括微服务架构、无服务器计算、Kubernetes 的普及以及 DevSecOps 的兴起。","title":"预测 2021 年的 DevOps 趋势","type":"posts"},{"content":"This is just a note to myself about the difference between Jenkins result and currentResult.\nDeclarative Pipeline # Here is a test code from this ticket JENKINS-46325\npipeline { agent any stages { stage (\u0026#39;Init\u0026#39;) { steps { echo \u0026#34;Init result: ${currentBuild.result}\u0026#34; echo \u0026#34;Init currentResult: ${currentBuild.currentResult}\u0026#34; } post { always { echo \u0026#34;Post-Init result: ${currentBuild.result}\u0026#34; echo \u0026#34;Post-Init currentResult: ${currentBuild.currentResult}\u0026#34; } } } stage (\u0026#39;Build\u0026#39;) { steps { echo \u0026#34;During Build result: ${currentBuild.result}\u0026#34; echo \u0026#34;During Build currentResult: ${currentBuild.currentResult}\u0026#34; sh \u0026#39;exit 1\u0026#39; } post { always { echo \u0026#34;Post-Build result: ${currentBuild.result}\u0026#34; echo \u0026#34;Post-Build currentResult: ${currentBuild.currentResult}\u0026#34; } } } } post { always { echo \u0026#34;Pipeline result: ${currentBuild.result}\u0026#34; echo \u0026#34;Pipeline currentResult: ${currentBuild.currentResult}\u0026#34; } } } Output\nInit result: null Init currentResult: SUCCESS Post-Init result: null Post-Init currentResult: SUCCESS During Build result: null During Build currentResult: SUCCESS [test-pipeline] Running shell script + exit 1 Post-Build result: FAILURE Post-Build currentResult: FAILURE Pipeline result: FAILURE Pipeline currentResult: FAILURE ERROR: script returned exit code 1 Finished: FAILURE Scripted Pipeline # Here is a test code from cloudbees support case\nExample that forces a failure then prints result:\nnode { try { // do something that fails sh \u0026#34;exit 1\u0026#34; currentBuild.result = \u0026#39;SUCCESS\u0026#39; } catch (Exception err) { currentBuild.result = \u0026#39;FAILURE\u0026#39; } echo \u0026#34;RESULT: ${currentBuild.result}\u0026#34; } Output:\nStarted by user anonymous [Pipeline] Allocate node : Start Running on master in /tmp/example/workspace/test [Pipeline] node { [Pipeline] sh [test] Running shell script + exit 1 [Pipeline] echo RESULT: FAILURE [Pipeline] } //node [Pipeline] Allocate node : End [Pipeline] End of Pipeline Finished: FAILURE Example that doesn’t fail then prints result:\nnode { try { // do something that does not fail echo \u0026#34;Im not going to fail\u0026#34; currentBuild.result = \u0026#39;SUCCESS\u0026#39; } catch (Exception err) { currentBuild.result = \u0026#39;FAILURE\u0026#39; } echo \u0026#34;RESULT: ${currentBuild.result}\u0026#34; } Output:\nStarted by user anonymous [Pipeline] Allocate node : Start Running on master in /tmp/example/workspace/test [Pipeline] node { [Pipeline] echo Im not going to fail [Pipeline] echo RESULT: SUCCESS [Pipeline] } //node [Pipeline] Allocate node : End [Pipeline] End of Pipeline Finished: SUCCESS ","date":"2021-01-14","externalUrl":null,"permalink":"/en/posts/2021/jenkinsresult-vs-currentresult/","section":"Posts","summary":"This article explains the difference between result and currentResult in Jenkins pipelines, including examples of how they behave in both declarative and scripted pipelines.","title":"What's the difference between result and currentResult in Jenkins?","type":"posts"},{"content":"Recently our Bamboo server has an error when connecting to the Windows build system.\nSome errors like: Can not connect to the host XXXX 22 port.\nI log into the build machine find port 22 with the below command\nnetstat -aon|findstr \u0026#34;22\u0026#34; but not found port 22 is inbound.\nInbound port 22 # There\u0026rsquo;s a lot of articles will tell you how to open ports on Windows. see this one\nhttps://www.windowscentral.com/how-open-port-windows-firewall\nBut still not works for me # My problem is when I inbound port 22, execute the above command still can\u0026rsquo;t see the port 22 is listening.\nSo I install the Win32-OpenSSH, this will lanch two services, then port 22 is listening.\nHere are the wiki page about download and installation https://github.com/PowerShell/Win32-OpenSSH/wiki/Install-Win32-OpenSSH\n","date":"2021-01-12","externalUrl":null,"permalink":"/en/posts/2021/how-to-open-port-22/","section":"Posts","summary":"This article explains how to open port 22 on Windows and ensure it is listening, which is necessary for SSH connections. It includes steps to install OpenSSH and configure the firewall.","title":"How to open port 22 and make it listening on Windows","type":"posts"},{"content":"","date":"2021-01-12","externalUrl":null,"permalink":"/en/tags/openssh/","section":"Tags","summary":"","title":"OpenSSH","type":"tags"},{"content":"As I have management our team\u0026rsquo;s git repositories for more than two years, and as my daily work using Bitbucket, so I\u0026rsquo;ll take it as an example.\nHere are some settings recommend you to enable.\nSet Reject Force Push Set Branch Prevent deletion Set tags for each hotfix/GA releases Merge Check -\u0026gt; Minimum approvals (1) Yet Another Commit Checker Reject Force Push # This is the first setting I highly recommend you/your team to open it. if not, when someone using the git push -f command to the git repository, you may lost commits if his local code is old then remote repository.\nyou have to recover the lost commits manually, I have heard 3 times around me. so enable the hook: Reject Force Push ASAP!\nSet Branch Prevent deletion # If some branch is very important, you don\u0026rsquo;t want to lost it, set Branch Prevent deletion ASAP!\nSet tags for each hotfix/GA releases # For each hotfix/GA release, highly recommend create tags after release.\nMerge Check # Pull Request is a very good workflow process for each team. in case of commits were merged directly without review, we enable Minimum approvals (1).\nSo, every branch wants to be merged to the main branch MUST add a reviewer (not allow yourself) and the review must click the Approve button otherwise the merge button is disabled.\nYet Another Commit Checker # This is a very powerful feature. it helps to standardize commit messages and create a branch.\nMore details about this tool you can refer to this introduction\nI have a Chinese article to describe how to use Yet Another Commit Checker implement. if you interest it, you can see the post here\n","date":"2021-01-12","externalUrl":null,"permalink":"/en/posts/2021/git-repository-settings/","section":"Posts","summary":"Provides a list of recommended settings for Bitbucket and GitHub repositories, including enabling force push rejection, branch protection, tag management, merge checks, and commit message standards.","title":"These settings in Bitbucket/GitHub recommends enable","type":"posts"},{"content":"This post just a note to myself and it works on my environment. it has not been widely tested.\nEnable git sparse-checkout # Just in my case, I cloned a git repo exists problems on the Windows platform with some folder, in order to work on the Windows platform we get a work around solution as following:\nCase 1: when you not already cloned a repository\nmkdir git-src cd git-src git init git config core.sparseCheckout true echo \u0026#34;/assets/\u0026#34; \u0026gt;\u0026gt; .git/info/sparse-checkout git remote add origin git@github.com:shenxianpeng/shenxianpeng.git git fetch git checkout master Case 2: when you already cloned a repository\ncd git-src git config core.sparseCheckout true echo \u0026#34;/assets/\u0026#34; \u0026gt;\u0026gt; .git/info/sparse-checkout rm -rf \u0026lt;other-file/folder-you-dont-need\u0026gt; git checkout Disable git sparse-checkout # git config core.sparseCheckout false git read-tree --empty git reset --hard ","date":"2021-01-11","externalUrl":null,"permalink":"/en/posts/2021/git-sparse-checkout/","section":"Posts","summary":"This article explains how to enable and disable git sparse-checkout, including examples of how to configure sparse-checkout for specific directories and how to reset it.","title":"git sparse-checkout enable and disable","type":"posts"},{"content":"","date":"2021-01-06","externalUrl":null,"permalink":"/en/tags/codesign/","section":"Tags","summary":"","title":"CodeSign","type":"tags"},{"content":"Many programmers may have encountered issues with the Verisign Timestamp server, http://timestamp.verisign.com/scripts/timstamp.dll, becoming unavailable when performing code signing, especially at the start of the new year. The following error may have appeared:\nThe reason is that the default timestamp server for code signing is no longer accessible.\nA response on Stack Overflow post provided an answer from Verisign Support:\nTheir authentication services were sold to Symantec, and the current service provider is Digicert. This server has been deprecated.\nThey suggest contacting Digicert or finding free timestamp servers online.\nThis was a user\u0026rsquo;s response; I didn\u0026rsquo;t find an official response online, so I decided to send an email to formally confirm. I received a reply shortly after:\nSimilar to the previous reply: Years ago, Verisign\u0026rsquo;s authentication and certificate business was sold to Symantec and has now transitioned to Digicert. You will need to work with your current provider for support or an updated timestamp URL. Please visit http://www.digicert.com for more information.\nNow that\u0026rsquo;s confirmed, let\u0026rsquo;s confidently proceed with changing to a new timestamp server.\nI found that Digicert\u0026rsquo;s timestamp server is http://timestamp.digicert.com. After switching to the new timestamp server, the digital signature process resumed normally.\nIn addition to the Digicert URL above, the following URLs can be used as replacements:\nhttp://timestamp.comodoca.com/authenticode http://timestamp.globalsign.com/scripts/timestamp.dll http://tsa.starfieldtech.com However, I didn\u0026rsquo;t choose any of these; I opted for the official timestamp service and kept it as a backup. What if the \u0026ldquo;official\u0026rdquo; provider gets sold again someday?\n","date":"2021-01-06","externalUrl":null,"permalink":"/en/posts/2021/verisign-server-not-working/","section":"Posts","summary":"This article describes how to resolve issues with the unavailable Verisign timestamp server, providing alternative timestamp server addresses to help developers successfully complete code signing.","title":"Resolving the Unavailable Code Sign Default Timestamp Server http://timestamp.verisign.com/scripts/timstamp.dll","type":"posts"},{"content":" 前言 # 自 2020 年因疫情开始，越来越多的 IT 公司都因不得不在家办公从而彻底转为 WFH（Work From Home） 公司，因此对于 IT 从业者来说，工作机会今后将会是全球性的。\n如果你有意想进入一个跨国公司工作，想与世界各地的人在一起工作，那么就不能仅仅的关注国内的这些大厂，要将眼光放眼到全世界，看看这些耳熟能详的公司对于工程师的职位要求有哪些。\n今天就先来看看 DevOps 岗位的需求是什么样的，了解这些，一来可以帮助我们在2021 年树立学习方向，而来如果你有意向去这些公司，了解并提早做准备才能有机会获取你想要的岗位。\n由于这些职位的介绍和要求会很长，因此我就先说结论。\n主要技能 # 国外很多公司他们使用的云服务商主要是 AWS，因此熟悉和使用 AWS 熟练使用 DevOps 工具，如 Jenkins, Ansible（Chef）, Git 等 Docker 和 Kubernetes 是每个想从事 DevOps 需要掌握的 熟悉操作系统，至少要 Linux 大多数都是要求 Python 很熟练，高级一些的岗位会要求熟悉 Go, Java 语言 最后，乐于学习，积极主动，具有创造性思维是每个 DevOps 最重要的特质，因此新的技术和工具层出不穷，我们需要保持和新进同行 具体的职位要求细节，请看后面的职位介绍吧 \u0026hellip;\nZOOM：DevOps Engineer # 工作地点：San Jose, CA\n岗位链接 https://www.linkedin.com/jobs/view/2337488435\nZOOM 不用过多介绍了，2020 年因为疫情，业务极具增长的一家视频会议的公司。也多次被推荐为最佳雇主，以及最佳的工作场所。\n岗位职责\n设计、部署、监控 ZOOM 平台服务 提升 ZOOM 平台服务从规划到上线的全生命周期策略 与跨职能干系人紧密合作，分析和解决复杂的生产问题 构建弹性和可扩展的服务基础设施，以适应基于区域的数据中心 优化当前CI/CD流程，简化服务器配置和部署的自动化工作 支持测试自动化和部署策略，以优化服务性能，确保产品质量 职位要求\n本科/硕士(CS或相关专业优先) 至少 5 年 DevOps 或 SRE 经验 对 AWS 基础架构(如DynamoDB, S3, Nginx, CloudWatch)，Linux 批处理命令，ELK 堆栈和容器编排(如 K8s, Docker)有深入的了解 熟练使用 Jenkins, Ansible 和 Git 仓库 能够监控，调试和自动化日常任务 熟悉云基础设施技术和基于云的测试自动化解决方案 乐于学习，积极主动，具有创造性思维 苹果：DevOps Engineer (CI/CD) # 地点：Cupertino, CA\n岗位链接：https://www.linkedin.com/jobs/view/2346233023/?eBP=JOB_SEARCH_ORGANIC\u0026amp;recommendedFlavor=COMPANY_RECRUIT\u0026amp;refId=RA%2BGKQ6QNrbwK1PHj3opBQ%3D%3D\u0026amp;trackingId=%2FCHeWxW%2FHm%2BULb2XD5Yyyg%3D%3D\u0026amp;trk=flagship3_search_srp_jobs\n我们的团队为苹果的应用程序运行 CI/C D管道，支持全球成千上万的开发者。我们对不断改进软件开发生命周期的方式充满热情，并为大规模工程问题重新发明前沿解决方案开辟边界。作为团队的一员，你将开发应用程序和微服务来构建和改进我们的下一代 CI/CD 管道。\n职位要求\n精通 Python 编程 有 Unix/Linux 平台工作经验 熟练使用 DevOps 工具，如 Chef, Docker, Kubernetes 具有软件开发过程的经验，如构建、单元测试、代码分析、发布过程和代码覆盖 有 CI/CD 流程和平台经验，如 Jenkins 较强的分析和解决问题的能力 优秀的书面和口头沟通能力，能够与大型开发团队合作 工作内容\n开发和维护应用开发团队的 CI/CD 流程 跨团队合作，改进产品的构建、集成和发布流程 开发和维护应用服务 CI/CD 管道的服务和集成 维护和管理包含 Linux/Unix/macOS 系统的动态构建场 能够参与下班后的轮班工作 教育和经验\n计算机科学或同等学历 其他要求\n有使用 Django/Flask 开发基于 Python 的微服务的经验 熟悉GitHub开发流程 有 Jenkins 管理和扩展的经验 具备扩展 CI/CD 系统和微服务的经验 有 Xcode 和开发 iOS, macOS 和其他苹果平台应用的经验 Oracle：Software Developer 4 for DevOps # 工作地点：Pleasanton, CA 职位链接：https://www.linkedin.com/jobs/view/2351849053/?eBP=JOB_SEARCH_ORGANIC\u0026amp;recommendedFlavor=COMPANY_RECRUIT\u0026amp;refId=RA%2BGKQ6QNrbwK1PHj3opBQ%3D%3D\u0026amp;trackingId=XQXcRFZiVeqSGJApqI4Grw%3D%3D\u0026amp;trk=flagship3_search_srp_jobs\n作为 Oracle Analytics Cloud 团队的开发人员，你将有机会在广泛分布的多租户云环境中构建和运行一套大规模集成云服务。你将需要在分布式系统方面有丰富的经验，熟练地解决大型数据系统中的实际挑战，对如何构建具有弹性，高可用性，可扩展性的解决方案有深刻的了解，并且需要具有可靠的设计，开发和交付回溯能力，终端系统。\n职位要求\n计算机科学，计算机工程学士学位或更高学位，或具有8年以上应用经验的同等学历 关于 Oracle Cloud Infrastructure 的动手经验–用户/策略管理，创建资源，配置和部署软件，自动化端到端到端供应 精通 Python 编程技巧 精通关系数据库，擅长 SQL 有使用容器技术的经验，尤其是 Docker 在 Linux 环境中的经验 有配置 Git 等源代码系统的经验 成为 CI/CD 和开发最佳实践的拥护者，尤其是在自动化和测试方面 具有构建高性能，弹性，可扩展性和精心设计的系统的经验 良好的沟通能力，能够清楚地阐明工程设计 敏捷软件开发经验 所需技能：\n熟悉 Kubernetes，Mesos 等（编排） 身份管理，安全性，网络等 NVIDIA DevOps Engineer # 工作地点：Santa Clara, CA 职位链接：https://www.linkedin.com/jobs/view/2249840303/?alternateChannel=search\u0026amp;refId=Y17t502dpy%2FAU43SIQPRIA%3D%3D\n工作内容\n为多个内部服务开发和支持暂存和生产环境 与全球各地的各个团队进行协调，以促进和改进 CI/CD 的实践 使用内部 Kubernetes 和商业云帮助构建服务 你将在团队中灵活地做出技术决策 岗位要求\n计算机工程，计算机科学或相关技术学科的理学学士学位或同等工作经验 6年以上经验 出色的脚本语言编程和调试技能 成熟的 Linux 系统管理经验（强烈建议使用 CentOS 和 Ubuntu） 对 CI/CD管道和工具有很好的了解（GitLab 或类似的工具） 数据库管理和性能调优经验 具有容器（Docker，Kubernetes）Web服务（SOAP/REST）和可扩展存储（HDFS/Ceph）的经验 从人群中脱颖而出的方法\n你具有 Python 编程方面的专业知识 熟悉 Windows 系统管理是一个巨大的优势 你曾经使用过云服务（AWS，Azure等） Cisco CX Cloud - Senior DevOps Release Engineer # 工作地点：San Jose, CA 职位链接：https://www.linkedin.com/jobs/view/2310027987/?alternateChannel=search\u0026amp;refId=COCpYBYNZm9w483i3SERXg%3D%3D\u0026amp;trackingId=FpZJedsrgIhNxlm961X75Q%3D%3D\n工作范围\n你可以跨 CX E\u0026amp;PI 和更广泛的客户体验（CX）组织进行协作，以启用我们的DevOps Release转换功能 你将以思想和实践的开发人员的身份开展工作，他不仅会认识并建立我们的组织实力，还将为团队带来新的观点和想法 与多元化的包容性软件工程师团队合作，在发布过程中实现端到端的DevOps，以确保加速的吞吐量和系统可靠性 与安全，应用程序和基础架构团队合作，从开发，测试，阶段，预生产和生产环境中检测简化的变更生命周期 运用你的经验将基础架构实现为代码，转换发布管道，并在高性能 DevOps 管道上以 NoOps 的心态部署到生产中 促进纪律严明的方法以确保部署的可预测性和质量 基准化和优化关键运营指标，确保我们符合运营SLA 测试并控制安全性，可靠性，可伸缩性和性能标准 积极寻求持续改进和学习的机会 岗位要求\n至少 10 年以上的设计，架构，启用和执行 DevOps 管道 以诚信，信任和透明的态度进行道德领导 具有在面向敏捷 DevOps 的团队和文化中担任高级主管的经验，并使用现代框架，技术，将基础架构用作代码工具的 DevOps 实践 将自动进行配置管理（使用Ansible等工具） 精通 DevOps，Blue/Green 和 Canary 部署以及云工程中的最佳实践 已经证明了微服务，作为代码的基础架构，监视和日志记录方面的专业知识 将编写代码，列出在 AWS 上运行的框架和架构 与跨职能团队合作时，你在复杂产品体系结构的持续集成和连续部署方面拥有深厚的专业知识 具有出色的能力，可以为支持全球客户群的云原生 SaaS 应用程序实现高可用性，灾难恢复，监视和警报，自动化以及持续的高性能 具有 Terraform 和 CloudFormation 模板的专业知识 在代码管道解决方案方面拥有深厚的专业知识 使用 Docker 和 Kubernetes 管理容器 使用 Jenkins 等平台管理构建管道 具有出色的组织和人际关系技巧，可以促进协作；在多功能矩阵管理环境中可以很好地工作 应该能够为 AWS 编写 Terraform 模块 已经展示了领域的专业知识，可以使用 SaaS 或消费者云软件公司的 DevOps 团队使用AWS之类的技术执行发布管道转换 在整合复杂的，跨公司的流程和信息策略方面拥有丰富的经验，包括技术规划和执行以及策略制定和维护 对临时性工作负载具有丰富的经验 体验 Kubernetes 有 Atlantis 知识者优先+ 在 DevOps 执行或工程职位上有10年以上的经验 具有丰富的经验，可在云体系结构上实现发布管道，并符合代码和临时工作负载 将基础架构作为代码和将配置作为代码技术的丰富经验 使用 Terraform，Kubernetes 和 Docker 等工具构建，扩展和保护应用程序云基础架构的丰富经验 在 GitHub 中管理多个代码库的丰富经验 在大型公共云中构建 Cloud Native 应用程序的丰富经验 具有实施可观察性，应用程序监视和日志聚合解决方案的经验 与跨职能团队合作并指导跨部门团队提供激情和经验，以提供受 DevOps 启发的解决方案 为大型数据管道开发高度可扩展的云原生架构 将异构构建，测试，部署和发布活动转换为在AWS上运行的同类企业级 DevOps 实施 交付和创作持续集成构建和部署自动化方面的丰富经验，例如 CI/CD 管道对 CloudFormation 模板，Ansible 和类似框架等解决方案的丰富经验 与应用程序和基础架构部署相关的 AWS 平台的丰富经验 能够在 Python，Go和 Java 中流畅开发 ","date":"2021-01-03","externalUrl":null,"permalink":"/posts/2021/devops-job-requirement/","section":"Posts","summary":"了解国外 IT 公司对 DevOps 工程师的技能要求，帮助你在 2021 年树立学习方向，获取理想岗位。","title":"2021 年国外 IT 公司对于 DevOps 工程师的要求有哪些？","type":"posts"},{"content":"Time flies, and 2021 is just around the corner. It\u0026rsquo;s time to summarize my experiences and achievements this year.\n2020 was an extraordinary year. The COVID-19 pandemic affected everyone\u0026rsquo;s life and work.\nBut life must go on. Scrolling through Weibo, Douyin, and other social media takes up a day; working and studying takes up another. I\u0026rsquo;m the kind of person who feels particularly anxious if I feel I haven\u0026rsquo;t made progress for a while. Completing the latter always makes me feel more at ease.\nReviewing 2020 # Regarding Work # For 2020, I\u0026rsquo;ve always strived to do my best, and looking back at the end of the year, I\u0026rsquo;m quite satisfied.\nWhen writing my work summary at the end of the year, I found that I had indeed done a lot of work. Besides doing my job of building and releasing, I did a lot of work improving processes and increasing efficiency. Here are some important ones:\nAchieved unattended automated builds using Jenkins Shared Libraries and Multi-branch Pipeline, saving a lot of time for the team and myself; also improved the stability and quality of code check-in through validating PR builds and tests.\nRelated article: Three Best Practices Every Jenkins User Should Know\nPromoted the team\u0026rsquo;s migration to enterprise-level Artifactory for downloading and storing builds, thereby improving CI/CD efficiency and unattended capabilities. This facilitates better automated downloading, installation, and testing.\nRelated article: For Those Who Want to Use JFrog Artifactory to Manage Artifacts\nUsed Python to do a lot of integrations with Jira and BitBucket, improving efficiency through automation. I also released a Python project UOPY.\nRelated article: What to Note When Releasing a Python Project on GitHub\nImplemented Git commit message and branch creation specifications, unifying Git commit messages and branch creation standards.\nRelated article: Programmer Self-Cultivation: Git Commit Message and Branch Creation Specifications\nLearned and used some new tools, such as Ansible playbook, ELK, and JaCoCo, and applied them to projects.\nRelated articles: Getting to Know Ansible, JaCoCo Practice\nAs a DevOps/Software Engineer, I also need good communication skills. Otherwise, no matter how good your work is, if you can\u0026rsquo;t clearly share it with your colleagues and leaders, it will be counterproductive. Especially in a multinational company like mine, I also need to share in English.\nAlso, I want to improve my technical skills through consistent reading and practice. I maintained this well for a while, completing the reading of several technical books, but good habits are easily broken, and once broken, they are hard to maintain. Previously, I usually read technical books for more than half an hour during my lunch break. Later, because I often had to supervise the renovation of my new house at noon, plus the many things I had to do every day (work, learning English, reading, renovation), many times things were done, but the results were not as expected.\nSharing # In 2020, I updated 41 articles on my blog and 26 articles on my WeChat official account \u0026ldquo;DevOps攻城狮\u0026rdquo;.\nThis quantity and quality are really incomparable to some other technical WeChat official accounts, but for me: a developer who doesn\u0026rsquo;t make a living from writing and only writes original articles, as long as I can output continuously, I\u0026rsquo;m quite satisfied with this number.\nAlso, at the beginning of the year, an editor from Tsinghua University contacted me about writing a book, which really surprised me. I know my own limitations, but to receive an invitation and the contract was the biggest encouragement for my writing.\nI planned to write a book called \u0026ldquo;Jenkins 2 Practical Guide\u0026rdquo;. If I signed the contract, I would need to invest at least one year of my spare time to possibly complete it.\nFinally, considering my situation at the time: prioritizing work; secondly, there were more important technologies than Jenkins that I needed to learn; and also weekend renovations\u0026hellip;I ultimately gave up this opportunity.\nI know I\u0026rsquo;m still in a stage where input is more urgent than output. I think as long as the little flame in my heart is still there, I will just do good deeds and not ask about the future.\nWork Goals # A personal year-end summary not only reviews what I\u0026rsquo;ve done and not done well in the past year and what can be improved, but also sets goals for the new year.\nIn 2021, I hope to be more focused, make better use of my time, and work and learn more efficiently.\nBesides doing a good job, the most important thing is to improve my spoken English and deepen my technical learning, and then output.\nJust having goals is not enough; they need to be translated into concrete actions.\nImprove Spoken English # 🚩 In 2021, I will continue to practice spoken English for at least half an hour every day; and have a 50-minute oral communication with a foreign teacher once a week. I hope to be able to give presentations in English without notes and pass the TOEIC exam.\nDeepen Technical Learning # My current methods for learning technology include: directly reading English documentation from the official website; if I still don\u0026rsquo;t understand after reading, I will search for the corresponding technology on LinkedIn Learning or Udemy, not only learning the technology but also practicing my English listening; if I need to systematically learn a programming language and some underlying technologies, I will read some classic technical books.\n🚩 In 2021, I will also spend at least half an hour reading technology every day and finish at least one technical book a month with output.\nLife Goals # In life, I hope my family and friends are all healthy and happy.\n🚩 Financial stability, preferably growth; maintain a healthy lifestyle and lose 10 kilograms.\nFinally, I will regularly review my 2021 goals and see if I have deviated from my initial direction. 2021, here I come!\nPast Year-End Summaries # 2019 Year-End Summary 2018: Five Months From Tester to Developer\n","date":"2020-12-31","externalUrl":null,"permalink":"/en/misc/2020-summary/","section":"Miscs","summary":"Time flies, and 2021 is just around the corner. It’s time to summarize my experiences and achievements this year.\n2020 was an extraordinary year. The COVID-19 pandemic affected everyone’s life and work.\n","title":"2020 Year-End Summary","type":"misc"},{"content":"","date":"2020-11-24","externalUrl":null,"permalink":"/en/tags/backup/","section":"Tags","summary":"","title":"Backup","type":"tags"},{"content":"I believe most of you have used the Jenkins configuration as code principle, so you build/test/release process will be described in code.\nIt looks like all is good, but not all of the configuration is in code, same of the Jenkins system configuration is store in the Jenkins service, so we also need to backup Jenkins in case of any disaster.\nThere are two ways to backup Jenkins, one used Jenkins plugin, the other is create shell scripts.\nUsing plug-in backup # I used the ThinBackup plugin, here is my thinBackup configuration.\nbackup to a folder which user jenkins has permission to write. this is important.\nPreviously I backup Jenkins to a mount folder, but it doesn\u0026rsquo;t work recently. so I use user jenkins to log in on the Jenkins server and found my jenkins can\u0026rsquo;t access the directory, but I personal user can.\nI will daily backup my Jenkins server, from Monday to Saturday.\nFo me max number of backup sets is 3, because every backup archive more than 400 MB.\nOthers check boxs\nBackup build results Backup \u0026lsquo;userContent\u0026rsquo; folder Backup next build number file Backup plugins archives Move old backups to ZIP files Using shell script backup # Here is a popular repository and code for your reference.\nrepository: https://github.com/sue445/jenkins-backup-script gist: https://gist.github.com/abayer/527063a4519f205efc74 ","date":"2020-11-24","externalUrl":null,"permalink":"/en/posts/2020/jenkins-backup/","section":"Posts","summary":"This article explains how to backup Jenkins using the ThinBackup plugin and shell scripts, ensuring that your Jenkins configuration and build data are safely stored.","title":"How to backup Jenkins","type":"posts"},{"content":"想要对 Java 项目进行代码覆盖率的测试，很容易就找到 JaCoCo 这个开源代码覆盖率分析工具是众多工具中最后欢迎的哪一个。\n本篇仅仅是在学习 JaCoCo 时对其实现设计文档 https://www.jacoco.org/trunk/doc/implementation.html 的粗略翻译。\n实现设计(Implementation Design) # 这是实现设计决策的一个无序列表，每个主题都试图遵循这样的结构:\n问题陈述 建议的解决方案 选择和讨论 覆盖率分析机制(Coverage Analysis Mechanism) # 覆盖率信息必须在运行时收集。为此，JaCoCo 创建原始类定义的插装版本，插装过程发生在加载类期间使用一个叫做 Java agents 动态地完成。\n有几种收集覆盖率信息的不同方法。每种方法都有不同的实现技术。下面的图表给出了 JaCoCo 使用的技术的概述:\n字节码插装非常快，可以用纯 Java 实现，并且可以与每个 Java VM 一起工作。可以将带有 Java 代理钩子的动态插装添加到 JVM 中，而无需对目标应用程序进行任何修改。\nJava 代理钩子至少需要 1.5 个 JVMs。用调试信息(行号)编译的类文件允许突出显示源代码。不幸的是，一些 Java 语言结构被编译成字节代码，从而产生意外的突出显示结果，特别是在使用隐式生成的代码时（如缺省构造函数或 finally 语句的控制结构）。\n覆盖 Agent 隔离(Coverage Agent Isolation) # Java 代理由应用程序类装入器装入，因此代理的类与应用程序类生活在相同的名称空间中，这可能会导致冲突，特别是与第三方库 ASM。因此，JoCoCo 构建将所有代理类移动到一个唯一的包中。\nJaCoCo 构建将包含在 jacocoagent.jar 中的所有类重命名为具有 org.jacoco.agent.rt_\u0026lt;randomid\u0026gt; 前缀，包括所需的 ASM 库类。标识符是从一个随机数创建的，由于代理不提供任何 API，因此没有人会受到此重命名的影响，这个技巧还允许使用 JaCoCo 验证 JaCoCo 测试。\n最低的Java版本(Minimal Java Version) # JaCoCo 需要 Java 1.5 及以上版本。\nJava 1.5 VMs 提供了用于动态插装的 Java 代理机制。使用 Java 1.5 语言级别进行编码和测试比使用旧版本更有效、更少出错——而且更有趣。JaCoCo 仍然允许运行针对这些编译的 Java 代码。\n字节码操纵(Byte Code Manipulation) # 插装需要修改和生成 Java 字节码的机制。JaCoCo 在内部使用 ASM 库来实现这个目的。\n实现 Java 字节码规范将是一项广泛且容易出错的任务。因此，应该使用现有的库。ASM库是轻量级的，易于使用，在内存和 CPU 使用方面非常高效，它被积极地维护并包含为一个巨大的回归测试套件，它的简化 BSD 许可证得到了 Eclipse 基金会的批准，可以与 EPL 产品一起使用。\nJava类的身份(Java Class Identity) # 在运行时加载的每个类都需要一个唯一的标识来关联覆盖率数据，JaCoCo 通过原始类定义的 CRC64 哈希代码创建这样的标识。\n在多类加载器环境中，类的纯名称不能明确地标识类。例如，OSGi 允许在相同的虚拟机中加载相同类的不同版本。在复杂的部署场景中，测试目标的实际版本可能与当前开发版本不同。代码覆盖率报告应该保证所呈现的数字是从有效的测试目标中提取出来的。类定义的散列代码允许区分类和类的版本。CRC64 哈希计算简单而快速，结果得到一个小的64位标识符。\n类加载器可能加载相同的类定义，这将导致 Java 运行时系统产生不同的类。对于覆盖率分析来说，这种区别应该是不相关的。类定义可能会被其他基于插装的技术(例如 AspectJ)改变。在这种情况下，哈希码将改变，标识将丢失。另一方面，基于被改变的类的代码覆盖率分析将会产生意想不到的结果。CRC64 代码可能会产生所谓的冲突，即为两个不同的类创建相同的哈希代码。尽管 CRC64 在密码学上并不强，而且很容易计算碰撞示例，但对于常规类文件，碰撞概率非常低。\n覆盖运行时依赖(Coverage Runtime Dependency) # 插装代码通常依赖于负责收集和存储执行数据的覆盖运行时。JaCoCo 只在生成的插装代码中使用 JRE 类型。\n在使用自己的类加载机制的框架中，使运行时库对所有插装类可用可能是一项痛苦或不可能完成的任务。自 Java 1.6 java.lang.instrument.Instrumentation。插装有一个扩展引导带加载器的API。因为我们的最低目标是 Java 1.5，所以 JaCoCo 只通过官方的 JRE API 类型来解耦插装类和覆盖运行时。插装的类通过 Object.equals(Object) 方法与运行时通信。插装类可以使用以下代码检索其探测数组实例。注意，只使用 JRE APIs:\nObject access = ... // Retrieve instance Object[] args = new Object[3]; args[0] = Long.valueOf(8060044182221863588); // class id args[1] = \u0026#34;com/example/MyClass\u0026#34;; // class name args[2] = Integer.valueOf(24); // probe count access.equals(args); boolean[] probes = (boolean[]) args[0]; 最棘手的部分发生在第 1 行，上面的代码片段中没有显示必须获得通过 equals() 方法提供对覆盖运行时访问的对象实例。到目前为止，已经实施和测试了不同的方法:\nSystemPropertiesRuntime: 这种方法将对象实例存储在系统属性下。这个解决方案打破了系统属性必须只包含 java.lang.String 的约定。字符串值，因此会在依赖于此定义的应用程序(如Ant)中造成麻烦。 LoggerRuntime: 这里我们使用共享的 java.util.logging.Logger。并通过日志参数数组而不是 equals() 方法进行通信。覆盖运行时注册一个自定义处理程序来接收参数数组。这种方法可能会破坏安装自己日志管理器的环境(例如Glassfish)。 ModifiedSystemClassRuntime: 这种方法通过插装将公共静态字段添加到现有的 JRE 类中。与上面的其他方法不同，此方法仅适用于活动 Java 代理的环境。 InjectedClassRuntime：这个方法使用 Java 9 中引入的 java.lang.invoke.MethodHandles.Lookup.defineClass 定义了一个新类。 从 0.8.3 版本开始，在 JRE 9 或更高版本上运行时，JaCoCo Java 代理实现使用 InjectedClassRuntime 在引导类装入器中定义新类，否则使用ModifiedSystemClassRuntime 向现有 JRE 类添加字段。从版本 0.8.0 开始，字段被添加到类 java.lang.UnknownError 中。version 0.5.0 - 0.7.9 向类 java.util.UUID 中添加了字段，与其他代理发生冲突的可能性较大。\n内存使用(Memory Usage) # 对于具有数千类或数十万行代码的大型项目，覆盖率分析应该是可能的。为了允许合理的内存使用，覆盖率分析是基于流模式和“深度优先”遍历的。\n一个庞大的覆盖率报告的完整数据树太大了，无法适合合理的堆内存配置。因此，覆盖率分析和报告生成被实现为“深度优先”遍历。也就是说，在任何时间点，工作记忆中只需要保存以下数据:\n当前正在处理的单个类。 这个类的所有父类(包、组)的汇总信息。 Java元素标识符(Java Element Identifiers) # Java 语言和 Java VM 对Java 元素使用不同的字符串表示格式。例如，Java 中的类型引用读起来像 java.lang.Object。对象，VM 引用的类型与 Ljava/lang/Object 相同。JaCoCo API 仅基于VM标识符。\n直接使用 VM 标识符不会在运行时造成任何转换开销。有几种基于 Java VM 的编程语言可能使用不同的符号。因此，特定的转换应该只在用户界面级别发生，例如在报表生成期间。\nJaCoCo实现的模块化(Modularization of the JaCoCo implementation) # JaCoCo 是在提供不同功能的几个模块中实现的。这些模块是作为带有适当清单文件的 OSGi 包提供的。但是它不依赖于 OSGi 本身。\n使用 OSGi bundle 允许在开发时和运行时在 OSGi 容器中定义良好的依赖关系。由于对 OSGi 没有依赖关系，捆绑包也可以像普通的 JAR 文件一样使用。\n","date":"2020-11-17","externalUrl":null,"permalink":"/posts/2020/jacoco/","section":"Posts","summary":"介绍 JaCoCo 的实现设计，包括覆盖率分析机制、Java 版本要求、字节码操纵、内存使用等方面的内容。","title":"JaCoCo 实现原理 (JaCoCo Implementation Design)","type":"posts"},{"content":"最近在思考如何将团队里的所有的虚拟机都很好的管理并监控起来，但是由于我们的虚拟机的操作系统繁多，包括 Windows, Linux, AIX, HP-UX, Solaris SPARC 和 Solaris x86. 到底选择哪种方式来管理比较好呢？这需要结合具体场景来考虑。\nAnsible 和其他工具的对比 # 这里有一个关于 Chef，Puppet，Ansible 和 Saltstack 的对比文章\nhttps://www.edureka.co/blog/chef-vs-puppet-vs-ansible-vs-saltstack/\n选择合适的工具 # 仅管理 Windows 和 Linux # 如果你的虚拟机没有这么多平台，只是 Windows 和 Linux，假如你已经有了 VMware vSphere 来管理了，那么可以通过 VMware vSphere API 来查看这些机器的状态。\n这里是 VMware 官方的 API Library 供使用：\nVMware vSphere API Python Bindings Go library for the VMware vSphere API 管理多个操作系统 # 如果你和我的情况一下，想监控很多个操作操作系统，那么就只能通过 ssh 来登录到每一台机器上去查看，比如执行 uptime 等命令。可以写 shell 脚本来完成这些登录、检测等操作。\n另外就是使用 Ansible 的 Playbook。Playbook 里描述了你要做的操作，这是一个权衡，学习 Ansible 的 Playbook 需要花些时间的。\n如果想了解下 Ansible 那么可以试试 Ansible Playbook。以下是我使用 Ansible 做了一些练习。\nPlaybook结构 # +- vars | +- vars.yml | +- ... +- hosts # save all hosts you want to monitor +- run.yml # ansible executable file Playbook具体代码 # vars/vars.yml\n--- # system ip: \u0026#34;{{ ansible_default_ipv4[\u0026#39;address\u0026#39;] }}\u0026#34; host_name: \u0026#34;{{ ansible_hostname }}\u0026#34; os: \u0026#34;{{ ansible_distribution }}\u0026#34; version: \u0026#34;{{ ansible_distribution_version }}\u0026#34; total_mb: \u0026#34;{{ ansible_memtotal_mb }}\u0026#34; vcpus: \u0026#34;{{ ansible_processor_vcpus }}\u0026#34; hosts\n[unix-vm] aix ansible_host=walbld01.dev.company.com ansible_user=test ansible_become_pass=test hp-ux ansible_host=walbld04.dev.company.com ansible_user=test ansible_become_pass=test linux ansible_host=walbld05.dev.company.com ansible_user=test ansible_become_pass=test [win-vm] win-bld02 ansible_host=walbld02.dev.company.com ansible_user=Administrator ansible_password=admin ansible_port=5985 ansible_connection=winrm ansible_winrm_server_cert_validation=ignore [other-vm] solaris ansible_host=walbld07.dev.company.com ansible_user=test ansible_become_pass=test win-udb03 ansible_host=walbld03.dev.company.com ansible_user=administrator ansible_become_pass=admin run.yml\n--- # this playbook is simple test - name: \u0026#34;get unix build machine info\u0026#34; hosts: unix-vm gather_facts: True tasks: - name: get uname, hostname and uptime shell: \u0026#34;uname \u0026amp;\u0026amp; hostname \u0026amp;\u0026amp; uptime\u0026#34; register: output - debug: var=output[\u0026#39;stdout_lines\u0026#39;] - name: \u0026#34;get windows build machine os info\u0026#34; hosts: win-vm gather_facts: True tasks: - debug: var=hostvars[\u0026#39;win-bld02\u0026#39;].ansible_facts.hostname - debug: var=hostvars[\u0026#39;win-bld02\u0026#39;].ansible_distribution 如何执行 # 首先需要安装了 ansible，然后执行\n# run with playbook ansible-playbook -i hosts run.yml 注：上面的代码是脱敏过的，需要根据你的环境进行调整才能执行成功。\nAnsible TroubleShotting # \u0026quot;msg\u0026quot;: \u0026quot;winrm or requests is not installed: No module named winrm\u0026quot;\nNeed install pywinrm on your master server.\n\u0026ldquo;msg\u0026rdquo;: \u0026ldquo;plaintext: auth method plaintext requires a password\u0026rdquo;\nwhen I run ansible mywin -i hosts -m win_ping -vvv, I notice the output used Python2.7, so I install pywinrm with command sudo pip2 install pywinrm, then my problem was resolved.\nmywin | UNREACHABLE! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;plaintext: auth method plaintext requires a password\u0026#34;, \u0026#34;unreachable\u0026#34;: true } Result: You should be using ansible_password and not ansible_pass. link\n","date":"2020-10-28","externalUrl":null,"permalink":"/posts/2020/ansible-practice/","section":"Posts","summary":"通过 Ansible 实践，探索如何高效管理和监控多种操作系统的虚拟机。","title":"Ansible 实践","type":"posts"},{"content":"I have been using Artifactory for continuous integration for some time now, and I have gained some experience and insights into enterprise-level Artifactory. I hope this sharing will help those who are just getting started with this tool to understand what Artifactory is, what it can do, why you should choose it, and what to pay attention to during its use.\nWhat is Artifactory # In short: Artifactory is a tool for storing artifacts. Currently, Artifactory is a very influential and powerful tool.\nAdvantages of Artifactory # Your team may already have its own way of managing artifacts, such as FTP. What can Artifactory bring? Let\u0026rsquo;s take a look at its advantages.\nNote: The following advantages are introduced for the JFrog Artifactory Enterprise Edition. The open-source edition, the OSS version, does not have the following rich features.\nAdvantage 1: It\u0026rsquo;s a Universal Repository Manager # JFrog Artifactory Enterprise Edition fully supports repository managers for all major package formats. It can not only manage binary files but also manage dependencies of packages in almost all languages on the market, as shown in the figure below:\nTherefore, using Artifactory allows you to store all binary files and packages in one place.\nAdvantage 2: Better Integration with CI Tools # It supports all mainstream CI tools (as shown in the figure below) and captures detailed build environment information during deployment to achieve fully reproducible builds.\nIn addition, it provides rich REST APIs, so any operation on the GUI page can be programmatically completed through code, facilitating CI/CD implementation.\nAdvantage 3: Powerful Search Functionality # If your builds are stored on FTP and you want to find a specific artifact from a large number of artifacts without knowing its name, it can be very difficult.\nArtifactory provides powerful search capabilities. You can search by name with regular expressions; you can also search by file checksum; and you can quickly search by properties, as shown in the following examples:\nExample 1: Search by Name # If you want to find a build artifact from a specific commit, such as the commit hash a422912, you can directly enter *a422912* and press Enter to quickly find it from numerous artifacts, such as Demo_Linux_bin_a422912.zip\nExample 2: Search by Property # For example, to find all builds with the property release.status set to released, you can search like this.\nExample 3: Search by Checksum # If you only know the file\u0026rsquo;s checksum, you can also search by it. For example, calculate the file\u0026rsquo;s checksum using sha1sum:\n$ sha1sum test.zip ad62c72fb097fc4aa7723e1fc72b08a6ebcacfd1 *test.zip Advantage 4: Artifact Lifecycle Management # By defining repositories with different maturity levels and using the Artifactory Promote function, you can move artifacts to different maturity repositories and better manage and maintain the artifact lifecycle through metadata properties.\nIn addition to these advantages, Artifactory has many more features that I won\u0026rsquo;t go into detail here.\nMore features can be found on the JFrog Artifactory official website: https://jfrog.com/artifactory/features/\nNext, through a demo, we will introduce how to use Artifactory and some best practices to avoid detours.\nArtifactory Homepage Introduction # Top of the Page # You can see that this Artifactory has served over 5000 artifacts. You can also see the current version number of Artifactory and the latest version.\nMiddle of the Page, From Left to Right # The far left is the search function, which allows you to easily find artifacts through a variety of search criteria. Then there is some user manual, video, REST API documentation, etc.\nIn the middle is Set Me Up, which allows you to select and filter the repositories you want to operate on. Clicking on a specific repository will pop up detailed instructions on how to use it.\nOn the far right, it displays the recently deployed builds and the most downloaded artifacts (95 represents the number of downloads).\nBottom of the Page # At the bottom are some integrated tools and technical user documentation related to Artifactory, making it easy to quickly find the most authoritative technical documentation during integration.\nPractices and Workflow # Setting Up Watched Repositories # In the Set Me Up section on the homepage, you can see that we have many repositories. However, among the many repositories, most members are only interested in some of them. You can then only focus on some repositories. Add a like, and then click the like button to list only your favorite Artifact Repositories.\nRepository Permissions and Retention Policies # Repository (maturity) Retention Policy Personal Account Service Account Admin dev Usually not cleaned read/write read/write all int One week or a few days read read/write all stage Never cleaned read read/write all release Never cleaned read read/write all The table makes it easy to understand the permission settings and retention policies. This is suitable for most situations, but not necessarily for all enterprise situations.\nArtifactory Repository Naming Convention # In this list of repositories, you can see that certain naming conventions are followed. Here, the JFrog Artifactory officially recommended naming convention is followed. It is strongly recommended that you do the same. It consists of four parts:\n\u0026lt;team\u0026gt;-\u0026lt;technology\u0026gt;-\u0026lt;maturity\u0026gt;-\u0026lt;locator\u0026gt;\nIn the figure, the team has been anonymized; let\u0026rsquo;s call it team1. Then comes the technology; there are many options, such as generic, Docker, Maven, NPM, etc. I used generic because our product is a binary file compiled from C/C++, which belongs to the generic category. Next is maturity. A repository usually consists of four levels of maturity, from low to high, which are dev, int, stage, and release. Finally, it indicates the location of the artifact. For example, a multinational company may have Artifactory instances in different regions to ensure upload/download speed and other needs. The den in the figure is an abbreviation for the location of the current Artifactory. Understanding the Workflow from Build Generation to Release # dev stands for development. This repository has read/write permissions for all product members, who can upload libraries or other binary files.\nint stands for integration. For example, artifacts successfully built from Jenkins will first be placed in this repository. If the build fails, it will not be uploaded to Artifactory.\nstage represents the pre-release repository. Artifacts that have passed Unit Test/Smoke Test will be Promoted to this repository for further testing, such as manual testing.\nrelease Artifacts that have passed testing will be Promoted to this repository.\nTo better manage the Artifactory directory and artifact lifecycle, I recommend standardizing branch naming and adding properties to artifacts at different stages.\n1. Standardized Branch Naming for Clear Artifactory Directories # For example, a product is called ART, its Git repository is also called ART, and it has a branch called feature/ART-1234.\nThe environment variables in the Jenkins Pipeline are set as follows:\nenvironment { INT_REPO_PATH = \u0026#34;team1-generic-int-den/ART/${BRANCH_NAME}/${BUILD_NUMBER}/\u0026#34; } Let\u0026rsquo;s see how this branch build flows.\nAfter the first successful build of this branch through Jenkins, it will first be placed under the directory team1-generic-int-den/ART/feature/ART-1234/1/. If a second build is successful, the artifact directory will be: team1-generic-int-den/ART/feature/ART-1234/2/, and so on.\nTo better manage the directories under the repository, it is recommended that the team agree on branch naming conventions in advance so that all builds of the same type of branch will appear under the same directory.\nFor branch naming conventions, see this article: Programmer\u0026rsquo;s Self-Cultivation: Git Commit Message and Branch Creation Specification\nIf you also want to put Pull Request Builds on Artifactory, it is recommended to set it up as follows:\nenvironment { PR_INT_REPO_PATH = \u0026#34;team1-generic-int-den/ART/PRs/${BRANCH_NAME}/${BUILD_NUMBER}/\u0026#34; } In this way, all successful Pull Request Builds will be placed under the PRs directory, which is convenient for searching and management.\n2. Adding Different Properties at Different Stages # If the above builds pass some quality checkpoints, such as unit tests, automated tests, and SonaQube scans, it is recommended to add different properties, such as:\nunit.test.status=passed automated.test.status=passed sonaqube.scan.status=passed\nThen, based on the above status, promote the artifacts that meet the conditions from the int repository to the stage repository. Test engineers go to the stage repository to get the build and perform testing. After passing the test, add the corresponding property status to the artifact, such as adding manual.test.status=passed in the Property.\nThen, the release pipeline goes to the stage repository to find the build that meets all the conditions for release.\nunit.test.status=passed automated.test.status=passed sonaqube.scan.status=passed manual.test.status=passed\nAfter successful release, promote the build from the stage repository to the release repository and add the property release.status=released. This completes the release.\nConclusion # In software delivery, quality trustworthiness and security trustworthiness are two important criteria for evaluating the reliability of a version. In this process, like using a funnel, the build is screened layer by layer, from the int repository to the stage repository, and finally to the release repository, completing the artifact release. By using Artifactory to create a single source of truth for artifact management, it paves the way for continuous software delivery.\nPrevious Related Articles # Introduction to JFrog Artifactory Artifactory and Jenkins Integration Solving the Problem of Jenkins Artifactory Plugin Failing to Upload Artifacts to HTTPS Artifactory Only on AIX ","date":"2020-10-04","externalUrl":null,"permalink":"/en/posts/2020/what-is-artifactory/","section":"Posts","summary":"This article introduces the concepts, advantages, working principles, and best practices of JFrog Artifactory, helping readers understand how to use Artifactory to manage software artifacts.","title":"For Those Who Want to Use JFrog Artifactory to Manage Artifacts","type":"posts"},{"content":" Why Establish Conventions # The old saying goes, \u0026ldquo;No rules, no circle.\u0026rdquo; In team collaborative development, everyone writes commit messages when submitting code, but without conventions, everyone will have their own writing style. Therefore, when reviewing the git log, you often see a variety of styles, which is very unfavorable for reading and maintenance.\nLet\u0026rsquo;s look at the comparison between no conventions and conventions through the following two examples, and what benefits conventions can bring.\nCommit Messages—Unconventional vs. Conventional # From this commit message, you don\u0026rsquo;t know what he modified or the intention of the modification.\nThis is the Angular commit message, which follows Conventional Commits.\nThis is the most widely used Git commit message convention in the industry, and many projects are already using it. If your project has not yet established a Git commit message convention, it is recommended to copy or refer to this convention.\nFor a team, when many people work together on a project, establishing a commit message convention in advance is very helpful for the long-term development of the project and the subsequent addition and maintenance of personnel.\nThe benefits are summarized as follows:\nHelps others better understand your change intentions, making it easier to contribute/modify code. Structured commit messages facilitate the identification of automation scripts and CI/CD. Provides the ability to automatically generate CHANGELOGs. Finally, this also reflects a programmer\u0026rsquo;s self-cultivation. Branch Creation—Unconventional vs. Conventional # If there is no convention for creating branches and no restrictions are imposed, many branches will look like this: ABC-1234-Test, ABC-2345-demo, Hotfix-ABC-3456, Release-1.0, or even worse. When there are many branches, it will appear chaotic and inconvenient to search.\nIf branch creation conventions are established, for example, the branches above must start with a type through a Hook during creation, then the newly created branches will look like this: bugfix/ABC-1234, feature/ABC-2345, hotfix/ABC-3456, release/1.0. This not only helps with retrieval but also allows others to understand the purpose of the branch through the type and facilitates the development of subsequent CI/CD pipelines.\nHow to Solve Convention Issues # We should proceed from two aspects:\nFirst, establish commit message and branch creation conventions for the team, letting team members understand and follow the conventions. Then, when submitting code or creating branches, use Git Hooks to prevent unconventional submissions to the remote repository. Establishing Git Commit Message Conventions # The most effective way to establish reasonable conventions is to refer to whether there are common conventions in the software industry. Currently, the most widely used convention in the industry is Conventional Commits, which is used by many projects, including Angular.\nYou can formulate conventions suitable for your team based on the above conventions, for example:\nJIRA-1234 feat: support for async execution ^-------^ ^--^: ^-------------------------^ | | | | | +--\u0026gt; Summary in present tense. | | | +--\u0026gt; Type: feat, fix, docs, style, refactor, perf, test or chore. | +--\u0026gt; Jira ticket number Type Must be one of the following: feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing or correcting existing tests chore: Changes to the build process, .gitignore or auxiliary tools and libraries such as documentation generation, etc. Setting Up Git Hooks # Here, Bitbucket is used as an example. Other Git tools such as GitHub and Gitlab have similar functions.\nBitbucket uses the Yet Another Commit Checker free plugin.\nFirst, enable Yet Another Commit Checker.\nThen, let\u0026rsquo;s introduce some commonly used settings of Yet Another Commit Checker one by one.\n1. Enable Require Valid JIRA Issue(s) # Enabling this function automatically verifies the existence of a Jira issue number during commit message submission through a Hook. If not, the submission fails. This forces the association of the commit message with the Jira issue number when submitting code.\n2. Commit Message Regex # For example, setting a simple regular expression like [A-Z\\-0-9]+ .* requires that the Jira issue number must start with this format ABCD-1234, and there must be a space between the description and the Jira issue number.\nWith the above settings, the commit message will be limited to the following format:\nABCD-1234 Balabala...... For example, this more complex regular expression:\n^[A-Z-0-9]+ .*(?\u0026lt;type\u0026gt;chore|ci|docs|feat|fix|perf|refactor|revert|style|test|Bld|¯\\\\_\\(ツ\\)_\\/¯)(?\u0026lt;scope\u0026gt;\\(\\w+\\)?((?=:\\s)|(?=!:\\s)))?(?\u0026lt;breaking\u0026gt;!)?(?\u0026lt;subject\u0026gt;:\\s.*)?|^(?\u0026lt;merge\u0026gt;Merge.* \\w+)|^(?\u0026lt;revert\u0026gt;Revert.* \\w+) This regular expression not only limits the beginning to a JIRA issue number followed by a space, but also requires the type to be filled in the description information, and finally the description information. It also supports different descriptions for Merge or Revert.\nUse the following test cases to specifically understand what kind of commit message convention restrictions the above regular expression will produce.\n# Test cases that pass NV-1234 chore: change build progress DT-123456 docs: update xdemo usage QA-123 ci: update jenkins automatic backup CC-1234 feat: new fucntional about sync Merge branch master into develop Reverted: Revert support feature \u0026amp; bugfix branches build Merge pull request from develop to master # Test cases that fail NV-1234 build: update NV-1234 Chore: change progress DT-123456 Docs: update xdemo QA-123ci: update jenkins automatic backup CC-1234 Feat: new fucntional about sync DT-17734: 8.2.2 merge from CF1/2- Enhance PORT.STATUS DT-17636 fix AIX cord dump issue DT-18183 Fix the UDTHOME problem for secure telnet DT-18183 Add new condition to get UDTHOME DT-15567 code merge by Xianpeng Shen. Test results can also be found here: https://regex101.com/r/5m0SIJ/10.\nSuggestion: If you also want to set such a strict and complex regular expression in your Git repository, it is recommended to consider and test it thoroughly before officially putting it into your Git repository\u0026rsquo;s Hook settings.\n3. Commit Regex Error # This setting is used to prompt error messages. When a team member submits a message that does not conform to the specifications and the submission fails, a reasonable prompt message will be given, which helps to find the problem. For example, if the submission fails, the following information will be seen on the command line:\nCommit Message Specifications: \u0026lt;Jira-ticket-number\u0026gt; \u0026lt;type\u0026gt;: \u0026lt;Description\u0026gt; Example: ABC-1234 feat: Support for async execution 1. Between Jira ticket number and type MUST has one space. 2. Between type and description MUST has a colon and a space. Type MUST be one of the following and lowercase feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing or correcting existing tests chore: Changes to the build process, .gitignore or auxiliary tools and libraries such as documentation generation, etc. Based on this description information, the submitter can easily know what the correct convention looks like, and then modify their last commit message using the git commit --amend command.\n4. Branch Name Regex # This is a convention restriction for creating branches. After setting the corresponding regular expression, developers can only push branches that meet the conditions of the regular expression to the remote repository.\nFor example, this branch creation regular expression: ^(bugfix|feature|release|hotfix).*|(master)|(.*-dev)\nThis restricts all branches to start with bugfix, feature, release, hotfix, or types like v1.0-dev.\nYou can design a branch regular expression for your project based on the above regular expression.\n5. Branch Name Regex Error # This setting is to prompt for errors when pushing unconventional branches. Pre-setting the corresponding error prompt information helps users quickly find the reason for the push failure. For example, the following error message:\nBranches must begin with these types: bugfix/ feature/ release/ hotfix/ Tells the user that the branch must start with bugfix/, feature/, release/, hotfix/.\n6. Other Settings # There are also some other settings, such as the status of the associated Jira issue. This prevents developers from secretly submitting code to already closed Jira issues, which may cause untested code to enter the repository.\nThere are also Require Matching Committer Email and Require Matching Committer Name to limit developers to configure usernames and emails that match their login usernames and emails to standardize the usernames and emails displayed in commit information, and to facilitate the collection of subsequent data such as Git information statistics.\nReferences # Conventional Commits https://www.conventionalcommits.org/en/v1.0.0/ Angular Commit Guidelines: https://github.com/angular/angular.js/blob/master/DEVELOPERS.md#commits Projects Using Conventional Commits: https://www.conventionalcommits.org/en/v1.0.0/#projects-using-conventional-commits Yet Another Commit Checker: https://mohamicorp.atlassian.net/wiki/spaces/DOC/pages/1442119700/Yet+Another+Commit+Checker+YACC+for+Bitbucket\n","date":"2020-09-24","externalUrl":null,"permalink":"/en/posts/2020/commit-messages-specification/","section":"Posts","summary":"This article introduces how to formulate and implement Git commit message and branch creation conventions to improve code quality and team collaboration efficiency.","title":"Programmer's Self-Cultivation — Git Commit Message and Branch Creation Conventions","type":"posts"},{"content":"This article introduces the important aspects to consider when publishing a Python project on GitHub for individuals or enterprises.\nConfiguring setup.py Publishing to PyPI Generating pydoc Choosing a Version Number Choosing a License Configuring setup.py # Packaging and publishing are accomplished by preparing a setup.py file. Assume your project directory structure is as follows:\ndemo ├── LICENSE ├── README.md ├── MANIFEST.in # Used to customize the contents of `dist/*.tar.gz` during packaging ├── demo │ └── __init__.py ├── setup.py ├── tests │ └── __init__.py │ └── __pycache__/ └── docs Using the packaging command python setup.py sdist bdist_wheel will generate two files, demo-1.0.0-py3-none-any.whl and demo-1.0.0.tar.gz, in the dist directory.\nThe .whl file is used for installation via pip install dist/demo-1.0.0-py3-none-any.whl, installing it to the ...\\Python38\\Lib\\site-packages\\demo directory.\n.tar.gz is an archive of the packaged source code. MANIFEST.in controls the contents of this file.\nThe following example shows how to use MANIFEST.in to customize the contents of dist/*.tar.gz. The MANIFEST.in file contents are as follows:\ninclude LICENSE include README.md include MANIFEST.in graft demo graft tests graft docs global-exclude __pycache__ global-exclude *.log global-exclude *.pyc Based on the above file contents, when using the command python setup.py sdist bdist_wheel to generate the demo-1.0.0.tar.gz file, it will include the LICENSE, README.md, and MANIFEST.in files, as well as all files in the demo, tests, and docs directories. Finally, it excludes all __pycache__, *.log, and *.pyc files.\nFor more information on the MANIFEST.in file syntax, see https://packaging.python.org/guides/using-manifest-in/\nOfficial examples and documentation are available at https://packaging.python.org/tutorials/packaging-projects/\nA Python sample project is available for reference at https://github.com/pypa/sampleproject\nCarefully reviewing the links above will fully satisfy the publishing requirements for most projects.\nPublishing to PyPI # As Python users know, external libraries can be downloaded using the following command. Python has a large number of third-party libraries; publishing open-source projects to PyPI makes them easily accessible to users.\npip install xxxx What is PyPI? # PyPI is short for The Python Package Index, a repository for finding, installing, and publishing Python packages.\nPyPI has two environments:\nTest environment TestPyPI Production environment PyPI Preparation # To familiarize yourself with the PyPI publishing tools and process, use the test environment TestPyPI. If you are already familiar with the PyPI publishing tools and process, you can directly use the production environment PyPI. TestPyPI and PyPI require separate registrations. That is, if you are registered in the production environment, you will also need to register to use the test environment. Note: The same account cannot be registered on both PyPI and TestPyPI simultaneously. Assuming your project is complete and ready to be published to PyPI, execute the following commands to publish your project:\nrm dist/* # Generate the source archive .tar.gz file and build file .whl file python setup.py sdist bdist_wheel # Use the following command to publish to TestPyPI twine upload --repository testpypi dist/* # Use the following command to publish to PyPI twine upload dist/* About pydoc # Python has a built-in doc feature called pydoc. Running python -m pydoc shows its options and functionality.\ncd docs python -m pydoc -w ..\\ # Generate all documentation Running python -m pydoc -b starts a local web page to access the documentation for all libraries in your ...\\Python38\\Lib\\site-packages\\ directory.\nHow can this local web documentation be accessed externally? GitHub\u0026rsquo;s built-in GitHub Pages feature makes it easy to provide an online URL.\nOpen your GitHub Python project settings -\u0026gt; find GitHub Pages -\u0026gt; select your branch and path for the Source, and save to immediately get a URL. For example:\nhttps://xxxxx.github.io/demo/ is your project homepage, displaying the README.md information https://xxxxx.github.io/demo/docs/demo.html is your project\u0026rsquo;s pydoc documentation About Version Number # For official releases, pay attention to version number selection.\nFor simple projects with low completeness, it is recommended to start with version 0.0.1. For projects with complete functionality and high completeness, you can start with version 1.0.0. For example, a project has four stages from preparation to official release: Alpha, Beta, Release Candidate, and Official Release. If the official release version number is 1.1.0, according to the following versioning specification:\nX.YaN # Alpha release X.YbN # Beta release X.YrcN # Release Candidate X.Y # Final release The Alpha, Beta, Release Candidate, and Final Release versions are as follows:\nAlpha release version number is 1.1.0a1, 1.1.0a1, 1.1.0aN... Beta release version number is 1.1.0b1, 1.1.0b1, 1.1.0bN... Release Candidate version number is 1.1.0rc1, 1.1.0rc2, 1.1.0rcN... Final release version number 1.1.0, 1.1.1, 1.1.N...\nPython\u0026rsquo;s official versioning and dependency specification document\nChoosing a License # For enterprise projects, the license is generally provided by the company\u0026rsquo;s legal team; publishers only need to format the license file (e.g., formatting the license.txt file to 70-80 characters per line).\nFor personal projects or to learn about open-source licenses, common software open-source licenses (listed in order of condition count):\nGNU AGPLv3 GNU GPLv3 GNU LGPLv3 Mozilla Public License 2.0 Apache License 2.0 MIT License Boost Software License 1.0 The Unlicense This article provides a reference: 《How to Choose an Open Source License for Your Github Repository》\nChoosing a License: https://choosealicense.com/licenses Choosing a License for GitHub repositories: https://github.com/github/choosealicense.com Choosing a License Appendix: https://choosealicense.com/appendix\n","date":"2020-09-13","externalUrl":null,"permalink":"/en/posts/2020/how-to-release-python-project/","section":"Posts","summary":"This article introduces the important aspects to consider when publishing a Python project on GitHub, including project structure, dependency management, and version control.","title":"Publishing a Python Project on GitHub — Things to Note","type":"posts"},{"content":"","date":"2020-09-13","externalUrl":null,"permalink":"/en/tags/pypi/","section":"Tags","summary":"","title":"PyPI","type":"tags"},{"content":"","date":"2020-09-13","externalUrl":null,"permalink":"/en/tags/release/","section":"Tags","summary":"","title":"Release","type":"tags"},{"content":" Backgroup # If you want to release python project on PyPI, you must need to know about PyPI usage characteristics, then I did some test about pip install command.\nFor example: I have a Python project called demo-pip. and beta release would like 1.1.0.xxxx, offical release version is 1.1.0 to see if could success upgrade when using pip command.\nBase on the below test results, I summarized as follows:\nInstall a specific version of demo-pip from PyPI, with --upgrade option or not, they\u0026rsquo;ll all both success. Install the latest package version of demo-pip from PyPI that version is large than the locally installed package version, with --upgrade option installs successfully. without --upgrade option install failed. Install the latest package version of demo-pip from PyPI that version is less than the locally installed package version, with --upgrade option or not, install failed. 1.1.0.xxxx version naming is OK, but when the beta version is larger than 1.1.0, for example, the beta version is 1.1.0.1000, pip install with --upgrade not work when our official release version is 1.1.0. a.\tOne option is the official release version start from 1.1.0.1000, beta version starts from 1.1.0.0001, 1.1.0.0002… Or the beta version should be less than 1.1.0, maybe 1.0.0.xxxx b.\tAnother option is follow up python official versioning that is the best practice, then the beta release version will be 1.1.b1, 1.1.b2, 1.1.bN… (it passed No.5 test below) My Test Case # \u0026lt;No.\u0026gt; \u0026lt;Test Case Steps\u0026gt; \u0026lt;Test Output\u0026gt; \u0026lt;Test Results\u0026gt; 1 1.build and install demo-pip-1.0.5\n2.install from PyPI. on PyPI, the latest version is 1.0.4 C:\\workspace\\demo-pip\u0026gt;pip install dist\\demo-pip-1.0.5-py3-none-any.whl\nProcessing c:\\workspace\\demo-pip\\dist\\demo-pip-1.0.5-py3-none-any.whl\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.4\nUninstalling demo-pip-1.0.4:\nSuccessfully uninstalled demo-pip-1.0.4\nSuccessfully installed demo-pip-1.0.5\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nRequirement already up-to-date: demo-pip in c:\\program files\\python38\\lib\\site-packages (1.0.5) install with --upgrade option failed when the installed version number is less than the current version number 2 1.rebuild and install demo-pip-1.0.3\n2.install from PyPI again with --upgrade option C:\\workspace\\demo-pip\u0026gt;pip install dist\\demo-pip-1.0.3-py3-none-any.whl\nProcessing c:\\workspace\\demo-pip\\dist\\demo-pip-1.0.3-py3-none-any.whl\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.5\nUninstalling demo-pip-1.0.5:\nSuccessfully uninstalled demo-pip-1.0.5\nSuccessfully installed demo-pip-1.0.3\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nCollecting demo-pip\nDownloading https://test-files.pythonhosted.org/packages/41/c5/fe16fdc482927b2831c36f96d6e5a1c5b7a2a676ddc4c00c67a9ccf644e9/demo-pip-1.0.4-py3-none-any.whl (51 kB)\n|████████████████████████████████| 51 kB 362 kB/s\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.3\nUninstalling demo-pip-1.0.3:\nSuccessfully uninstalled demo-pip-1.0.3\nSuccessfully installed demo-pip-1.0.4 install with --upgrade option success from PyPI when install version number is larger than the current version number 3 1. create a new build demo-pip-1.0.3.1000\n2. install demo-pip-1.0.3.1000 with --upgrade option\n3. install demo-pip-1.0.3.1000, without --upgrade option C:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nCollecting demo-pip\nDownloading https://test-files.pythonhosted.org/packages/41/c5/fe16fdc482927b2831c36f96d6e5a1c5b7a2a676ddc4c00c67a9ccf644e9/demo-pip-1.0.4-py3-none-any.whl (51 kB)\n|████████████████████████████████| 51 kB 83 kB/s\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.3.1000\nUninstalling demo-pip-1.0.3.1000:\nSuccessfully uninstalled demo-pip-1.0.3.1000\nSuccessfully installed demo-pip-1.0.4\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nRequirement already satisfied: demo-pip in c:\\program files\\python38\\lib\\site-packages (1.0.3.1000) 1. install with --upgrade option success\n2. install without --upgrade option failed 4 1. create a new build demo-pip-1.0.4.1000\n2. install demo-pip-1.0.4.1000 with --upgrade option.\n3. install specific version of demo-pip C:\\workspace\\demo-pip\u0026gt;pip install dist\\demo-pip-1.0.4.1000-py3-none-any.whl\nProcessing c:\\workspace\\demo-pip\\dist\\demo-pip-1.0.4.1000-py3-none-any.whl\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.4\nUninstalling demo-pip-1.0.4:\nSuccessfully uninstalled demo-pip-1.0.4\nSuccessfully installed demo-pip-1.0.4.1000\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nRequirement already up-to-date: demo-pip in c:\\program files\\python38\\lib\\site-packages (1.0.4.1000)\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ demo-pip==1.0.4\nLooking in indexes: https://test.pypi.org/simple/\nCollecting demo-pip==1.0.4\nDownloading https://test-files.pythonhosted.org/packages/41/c5/fe16fdc482927b2831c36f96d6e5a1c5b7a2a676ddc4c00c67a9ccf644e9/demo-pip-1.0.4-py3-none-any.whl (51 kB)\n|████████████████████████████████| 51 kB 362 kB/s\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0.4.1000\nUninstalling demo-pip-1.0.4.1000:\nSuccessfully uninstalled demo-pip-1.0.4.1000\nSuccessfully installed demo-pip-1.0 Install failed when the install version number is less than the currently installed version number\nif install a specific version of demo-pip with --upgrade option or not. will both works. 5 1. Follow up python official version naming for beta-version, create a new build demo-pip-1.0.b1\n2. install from PyPi without --upgrade option\n3. install from PyPi with --upgrade option C:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nRequirement already satisfied: demo-pip in c:\\program files\\python38\\lib\\site-packages (1.0b1)\nC:\\workspace\\demo-pip\u0026gt;pip install -i https://test.pypi.org/simple/ \u0026ndash;upgrade demo-pip\nLooking in indexes: https://test.pypi.org/simple/\nCollecting demo-pip\nDownloading https://test-files.pythonhosted.org/packages/41/c5/fe16fdc482927b2831c36f96d6e5a1c5b7a2a676ddc4c00c67a9ccf644e9/demo-pip-1.0.4-py3-none-any.whl (51 kB)\n|████████████████████████████████| 51 kB 362 kB/s\nInstalling collected packages: demo-pip\nAttempting uninstall: demo-pip\nFound existing installation: demo-pip 1.0b1\nUninstalling demo-pip-1.0b1:\nSuccessfully uninstalled demo-pip-1.0b1\nSuccessfully installed demo-pip-1.0.4 install successful with --upgrade option\nso it means 1.0.b1 version number is less than 1.0.4 version ","date":"2020-08-30","externalUrl":null,"permalink":"/en/posts/2020/pip-install-and-versioning/","section":"Posts","summary":"Explains the behavior of pip install commands with different versioning schemes, including how to handle beta versions and the implications of using --upgrade with specific version numbers.","title":"About Python pip install and versioning","type":"posts"},{"content":"","date":"2020-08-17","externalUrl":null,"permalink":"/en/tags/jira/","section":"Tags","summary":"","title":"Jira","type":"tags"},{"content":" Backgroud # When you are using a server account for CI/CD, if you want to make the server account avatar to looks professional on Jira update but the server account may not allowed to log to Jira, so you can not update the avatar though GUI, you could use Jira REST API to do this.\nI assume you have an account called robot, here are the examples of how to update though REST API.\nExample in Python # import http.client conn = http.client.HTTPSConnection(\u0026#34;jira.your-company.com\u0026#34;) payload = \u0026#34;{\\r\\n\\t\\\u0026#34;id\\\u0026#34;: \\\u0026#34;24880\\\u0026#34;,\\r\\n\\t\\\u0026#34;isSelected\\\u0026#34;: false,\\r\\n\\t\\\u0026#34;isSystemAvatar\\\u0026#34;: true,\\r\\n\\t\\\u0026#34;urls\\\u0026#34;: {\\r\\n\\t\\t\\\u0026#34;16x16\\\u0026#34;: \\\u0026#34;https://jira.your-company.com/secure/useravatar?size=xsmall\u0026amp;avatarId=24880\\\u0026#34;,\\r\\n\\t\\t\\\u0026#34;24x24\\\u0026#34;: \\\u0026#34;https://jira.your-company.com/secure/useravatar?size=small\u0026amp;avatarId=24880\\\u0026#34;,\\r\\n\\t\\t\\\u0026#34;32x32\\\u0026#34;: \\\u0026#34;https://jira.your-company.com/secure/useravatar?size=medium\u0026amp;avatarId=24880\\\u0026#34;,\\r\\n\\t\\t\\\u0026#34;48x48\\\u0026#34;: \\\u0026#34;https://jira.your-company.com/secure/useravatar?avatarId=24880\\\u0026#34;}\\r\\n}\u0026#34; headers = { \u0026#39;content-type\u0026#39;: \u0026#34;application/json\u0026#34;, \u0026#39;authorization\u0026#39;: \u0026#34;Basic Ymx3bXY6SzhNcnk5ZGI=\u0026#34;, \u0026#39;cache-control\u0026#39;: \u0026#34;no-cache\u0026#34;, \u0026#39;postman-token\u0026#39;: \u0026#34;ecfc3260-9c9f-e80c-e3e8-d413f48dfbf4\u0026#34; } conn.request(\u0026#34;PUT\u0026#34;, \u0026#34;/rest/api/latest/user/avatar?username=robot\u0026#34;, payload, headers) res = conn.getresponse() data = res.read() print(data.decode(\u0026#34;utf-8\u0026#34;)) Example in Postman # # URL and Method is PUT https://jira.your-company.com/rest/api/latest/user/avatar?username=robot # Authorization # Type: Basic Auth # Username: server-account-username # Password: server-accoutn-password # Body { \u0026#34;id\u0026#34;: \u0026#34;24880\u0026#34;, \u0026#34;isSelected\u0026#34;: false, \u0026#34;isSystemAvatar\u0026#34;: true, \u0026#34;urls\u0026#34;: { \u0026#34;16x16\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=xsmall\u0026amp;avatarId=24880\u0026#34;, \u0026#34;24x24\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=small\u0026amp;avatarId=24880\u0026#34;, \u0026#34;32x32\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=medium\u0026amp;avatarId=24880\u0026#34;, \u0026#34;48x48\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?avatarId=24880\u0026#34;} } How to find the avatar id # You replace other avatar ids you like. Here is how to find you avatar id you want ","date":"2020-08-17","externalUrl":null,"permalink":"/en/posts/2020/jira-update-account-avatar-with-rest-api/","section":"Posts","summary":"How to update the avatar of a Jira server account using the REST API, including examples in Python and Postman.","title":"Update Jira server account avatar with rest API","type":"posts"},{"content":" Problem # Sometimes my Windows server 2012 R2 has RDP connect problem below:\nThe remote session was disconnected because there are no Remote Desktop client access licenses available for this computer. Please contact the server administrator. How to Fix # You could log in to the vSphere Web Client if you have via console or have some other way to log in to the machine.\nOpen regedit.exe and navigate to\nSearch and Delete LicensingGracePeriod and LicensingGracePeriodExpirationWarningDays\nIf deletion failed, this failure message appears unable to delete all specified values, you need change permission. Refer to the related videos on YouTuBe.\nReboot the system if it is still doesn\u0026rsquo;t work.\nIn my case, every 90 to 120 days, the RDP end of grace period shows up, this is not the final solution. please let me know if you have a better solution.\nFinally, thanks to Bill K. shared with me the above solution.\n","date":"2020-08-10","externalUrl":null,"permalink":"/en/posts/2020/rdp-problem/","section":"Posts","summary":"Fix the RDP connection issue on Windows Server 2012 R2, where the error message indicates that there are no Remote Desktop client access licenses available.","title":"Fixed \"Remote session was disconnected because there are no Remote Desktop client access licenses available\"","type":"posts"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/en/tags/daily/","section":"Tags","summary":"","title":"Daily","type":"tags"},{"content":"Starting August 7, 2020, I would like to start recording some daily/important things in English.\n3/15/2025 - I appreciate people who say great things about me behind my back # Today, one of my colleagues shared with me:\n\u0026ldquo;For example, Xianpeng does a lot of work, but he’s very modest, so some people might take advantage of that. You have to stand-up for yourself a bit.\u0026rdquo;\nI can’t thank him directly, as he’ll know who passed the message to me, but I truly appreciate him for saying these words.\n1/25/2025 - I have been Vilnius for 6 months # I have been living and working in Vilnius for six months now. I am satisfied with my decision to move here, the work and life are all great.\nThe only drawback is the weather during winter, as it can be quite gloomy. However, the summer season has been wonderful so far.\nI am looking forward to my upcoming trip to China, where I will be able to see my family and enjoy some sunshine. I will return soon.\n7/26/2024 # I have arrived LT last week and this week is my first week working in our Vilnius office.\nI feel very good to work here, onboarding was very smooth, the colleagues are nice, and I like the work environment in and out the office. Hope everything goes well for my family.\n6/25/2024 # Wow, I\u0026rsquo;m in the AUTHORS.txt of pip, see. I just made some commits of pip.\nOh, by the way, my family was approved last week. I\u0026rsquo;m waiting to pick it up in Japan.\n6/13/2024 - Blue Card Approved # It\u0026rsquo;s been 20 days since my visa interview and I finally know that my blue card is approved, but my familly still has to wait.\nI still have a lot of things to do such as normal work, KT, release, looking at appartment, confirming things, and so on.\nI hope my family will get approved soon so I can go to Japan and pick up our cards by the end of this month. Hope everthing goes well.\n5/18/2024 - Ready for Lithuania visa interview # Start May, I\u0026rsquo;m very busy, not only I need to work but also need to prepare for the visa application.\nFor work, I need to do KT, Q\u0026amp;A and our regular work. For visa, I need to apply Japan visa first and submit documents to Lithuania Embassy in Japan for some reason.\nHope everything goes well on Tuesday (5/21) 🙏.\n4/3/2024 - I accepted an offer position in Lithuania # After two months back and force, I finally get the offer in Lithuania.\nIt\u0026rsquo;s a big change for me and my family. I hope it\u0026rsquo;s a better choice without regrets.\n1/31/2024 - Grateful your recognitions # I want to thank my boss and leader. Thank them for for their recognition and wanting me to move to the next level. ❤️\nIt was very nice to talk with you. I have an honor to work alongside such talented people like yourself! Your presentation was awesome! \u0026ndash; from Pavel\n2023 is the busy year. In life, I\u0026rsquo;m the \u0026ldquo;daddy\u0026rdquo;; In work, I \u0026rsquo;m stickler for \u0026ldquo;best practices\u0026rdquo;, and after work, I\u0026rsquo;m a fan of \u0026ldquo;open source and writing\u0026rdquo;.\nOnward and Upward.\n12/02/2022 - I don\u0026rsquo;t have time # I don\u0026rsquo;t have time to contribute to open source I don\u0026rsquo;t have time to learn from Udemy or YouTuBe I don\u0026rsquo;t have time to update my blog and WeChat Account I don\u0026rsquo;t have time to play football after work\nMost of my time is at work, taking care of children, resting or sleeping.\nIf I want to do some of the above part-time things, just rest and sleep less :)\n06/13/2022 - Be thankful # It\u0026rsquo;s been over a month since I\u0026rsquo;ve written anything because my daughter was born and I\u0026rsquo;ve been busy all May.\nFirst, I worked remotely in the confinement center for nearly a month. After leaving the club, I started to take paternity leave.\nThe two of us are taking care of the children, which is still very busy for new parents.\nAside from being busy, thank goodness it all went smoothly.\n04/20/2022 - Bless my wife and baby # It\u0026rsquo;s been a tough April, wish my wife well and my baby well. Bless us ❤️\n12/25/2021 - I was surprised # Several days ago, A female financial colleague told me I did a great job sharing about CI/CD practices. (I posted the recording to slack channel of the China lab for all the engineers who had attended the meeting.)\nI was surprised that as a non-tech colleague she was also watched, and this was our first conversation. Thanks so much.\n11/24/2021 - Brother Ma told me that I am very good at CI/CD # Yesterday I gave a presentation to other teams in Dalian, it\u0026rsquo;s all about my practices in the DevOps. To be honest I’m not a good speaker.\nBrother Ma is a very very senior software engineer and I think he is also a full-stack engineer. He told me I am very good at CI/CD and he wants to learn from me in the next year.\nI\u0026rsquo;m very surprised that he gave me such high praise. I will keep on making more efforts in DevOps.\nThank you very much, Brother Ma.\n11/10/2021 - Record every time I was praised # I would record every time I was praised, so that when I failed on something, I will look back to see these praised to gain confidence.\nToday, I have a meeting with foreign colleagues, I say more fluent than ever, and my manage mentioned my English improved a lot. this is the second time he praised my English improved.\nNext week, I need to prepare to share related to CI/CD/DevOps best practice I did before with other colleagues, this time just need to use Chinese :)\n08/30/2021 - Bless my wife and me # Hopes our dream could come true this time. Bless us ❤️\n06/26/2021 - Mentor and mentee program # Since this month(June), I begin to have a mentor :) (WIP)\n05/26/2021 - Congrats to myself # This week I had two things that were worth remembering for me:\nThe first, when I had a meeting with our product family India team to share my CI/CD practice with Jenkins Yesterday. After the meeting, my boss told me that he was surprised with my English improvement. It gives more motivation to learn English.\nThe second, this year, my Build project Quality Gate goes into the next round. The first time was in 2019 that before the pandemic, we could go to our company\u0026rsquo;s HQ to attend five days Build event in the USA. For now, everyone can not trail overseas. I hope that the pandemic can go away ASAP, then maybe I can attend a face-to-face Build event.\n04/11/2021 - Just an update # I have moved to my new home for almost one month. It\u0026rsquo;s very happy to go to work only needs 15 minutes. I\u0026rsquo;m reading several very thick books, such as Code Complete and C++ Primer Plus books. Btw, I got promoted this month that I have worked here for six years. 02/18/2021 - The coach (Qiao Q) left Rocket and started his new career # I called Qiao a coach because he swims very well and tells me how to freestyle. He also does DevOps/Build-related works in Rocket, then we have a lot of things to talk about, so we have a good relationship.\nI a little upset when I heard that he told me he would leave Rocket. As we talk more, I understand him and support him in this decision.\nSince age and experience are no longer our advantages in Dev, the new challenges will help him get more opportunities in DevOps than he did in his job here, so he made a wise choice, and I hope he will have better development in the future.\n11/1/2020 - English and Decoration # Sunday, sunny.\nEnglish\nToday I clean English from App and have a test of the TOEIC, and the test score is 64, and my test results have never been more than 70. I hope I can keep testing every weekend so that to improve my English and hope next time the test will achieve about 70 :)\nThe House Decoration\nFrom October, my house had finished ceramic tile seam beauty, suspended ceiling, the laying off part of the marble, and now is whitewashing.\nLast Friday I paid the money for the custom kitchen and furniture, it\u0026rsquo;s more than 40,000 RMB, it\u0026rsquo;s over my budget in total. IKEA will ship furniture on Friday and install them next Saturday. Before that, I must prepare the kitchen appliances, so from Last Night (11.1) start to buy, the price is very well.\nSo in the next two weeks, the decoration\u0026rsquo;s progress very well, which was well worth the wait.\nCheers!\n8/30/2020 - About work and life # Sunday, sunny.\nI review my daily report so that I could remember what I\u0026rsquo;ve been doing for the past two weeks at work. I don\u0026rsquo;t want to talk about my regular work like build, release products, etc. I want to write down what I\u0026rsquo;ve learned in the past two weeks.\nAbout Work\nLearn how to show python project doc files online. you could create a GitHub Pages that is a GitHub build-in feature. then will have a website like https://\u0026lt;github-username\u0026gt;.github.io/\u0026lt;repo-name\u0026gt;.html\nLive demo - our team build project which analyzes Bitbucket git history and provides below features\nRecommend reviewer for Pull Request Recommend test cases for regression testing Recommend test scope for QA engineer Visualization based on data Enhancement currently our Artifactory usage, like provide move artifacts from INT(integration) repo to STAGE repo when build passed the smoke test.\nAbout Life\nFrom the last two weeks, my colleagues and I start swimming in the indoor pool every Friday night. we want to lose weight in these activities, but we always eat after swimming, this is not good for us to lose weight :(\nWe finally decide to use IKEA to decorate our house and completed the final design last Saturday. We finally started on Saturday, and they set up a simple start ceremony for me.\nThe first task was to look at tiles, and after all kinds of research and comparison, we finally ordered a big brand of tiles. Although it is more expensive, it\u0026rsquo;s still worth it.\n8/16/2020 - What is the core knowledge I need to learn # Sunday, cloudy. IKEA measures house then I go to the company to study.\nThis week I always thinking about what is my most important knowledge? Learn more about DevOps tools? or others?\nFinally, I come out with the two points I need to enhance, the ability to communicate and knowledge of the primordial.\nespecially is English communicate. I should set up some goals to archive, like the TOEIC exam. I need to use more time on reading such as Code Completed, C Primer books. OK, Keep doing both. 💪\n","date":"2020-08-07","externalUrl":null,"permalink":"/en/misc/daily/","section":"Miscs","summary":"Starting August 7, 2020, I would like to start recording some daily/important things in English.","title":"Daily Notes","type":"misc"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/en/tags/notes/","section":"Tags","summary":"","title":"Notes","type":"tags"},{"content":" Background # This article discusses how to obtain Git repository events (Events) using the Jenkins generic-webhook-trigger plugin. For example, obtaining the repository\u0026rsquo;s Pull Request ID.\nUsers familiar with Jenkins Multi-branch pipeline Jobs know that the environment variables in this Job type can provide the following Pull Request information:\nTo obtain this variable, you need to create this type of Job, and may need to clone the repository\u0026rsquo;s code, which seems a bit overkill.\nHow can we obtain Bitbucket repository and Pull Request events in real time by simply creating a regular Jenkins Job? This can be achieved using the following features and plugins:\nConfigure Bitbucket\u0026rsquo;s Webhook Receive Webhook Event events using the Jenkins generic-webhook-trigger plugin Implementation Steps # Setting up the Bitbucket Webhook # Create a webhook in the Bitbucket repository you want to monitor, as follows:\nName: test-demo URL: http://JENKINS_URL/generic-webhook-trigger/invoke?token=test-demo Note: Bitbucket has another setting. According to my tests, both this setting Post Webhooks and the above Webhooks can achieve the function described in this article.\n2. Configuring the Jenkins Job # To obtain other Event information, such as PR title, commit, etc., please refer to this link bitbucket-server-pull-request.feature, and configure it according to the above settings.\nThe token value test-demo can be named arbitrarily, but it must be consistent with the token in the Bitbucket event URL.\nTesting # Add the following code snippet echo pr_id is ${pr_id} to the Jenkins Job pipeline to check if the output Pull Request ID is as expected.\nThen create a Pull Request under the configured Bitbucket repository.\nThe Jenkins Job is automatically triggered and executed by the Pull Request Open event.\nThe successfully obtained Pull Request ID value can be seen in the Jenkins output log.\nAdvanced Usage # Suppose you have a program that can receive the Pull Request ID and use the Bitbucket REST API to obtain and analyze the content of the specified Pull Request. For example, obtain the history of relevant files to determine who modified these files the most and which Jira issues were involved in this modification, thereby making recommendations for reviews or regression testing.\nWith this PR ID, you can automatically trigger your program to execute via Jenkins.\nThis method is suitable for those who do not want to or do not know how to monitor Git server (Bitbucket, GitHub, or GitLab, etc.) events and need to create a separate service. If you have any good practices, please share them in the comments.\n","date":"2020-08-07","externalUrl":null,"permalink":"/en/posts/2020/bitbucket-pull-request-event/","section":"Posts","summary":"This article describes how to use Jenkins’ generic-webhook-trigger plugin to obtain real-time event information from a Bitbucket repository, such as the Pull Request ID.","title":"Obtaining Bitbucket Repository Events in Real Time via the generic-webhook-trigger Plugin","type":"posts"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/en/tags/webhook/","section":"Tags","summary":"","title":"Webhook","type":"tags"},{"content":"Jenkins Tip 3—Each installment describes a Jenkins tip using brief text and images.\nProblem # When using a Jenkins pipeline, if a Shell command returns a non-zero value (meaning the Shell command encountered an error), the Jenkins Job defaults to marking the current stage as failed. Therefore, the entire Job also fails.\nIn some cases, we want the Jenkins Job to show a successful status even if the Shell command fails and returns a non-zero value.\nFor example: A Shell command lists files starting with fail-list-. If such files exist, the user is notified; otherwise, no notification is sent.\nls -a fail-list-* By default, executing the above command causes the entire Job to fail.\nSolution # After some investigation, the following code snippet solved the problem.\nstage(\u0026#34;Send notification\u0026#34;) { steps { script { def fileExist = sh script: \u0026#34;ls -a fail-list-* \u0026gt; /dev/null 2\u0026gt;\u0026amp;1\u0026#34;, returnStatus: true if ( fileExist == 0 ) { // send email to user }else { // if not found fail-list-* file, make build status success. currentBuild.result = \u0026#39;SUCCESS\u0026#39; } } } } Analysis # When executing the Shell command, returnStatus: true is added. This returns and saves the status code, which is then compared to 0.\nIf it\u0026rsquo;s not equal to 0, and currentBuild.result = 'SUCCESS' is not added, the entire Jenkins Job will still be marked as failed. Adding it artificially ignores the error and sets the Job status to success.\n","date":"2020-07-22","externalUrl":null,"permalink":"/en/posts/2020/jenkins-tips-3/","section":"Posts","summary":"How to handle non-zero return codes from Shell commands in a Jenkins Pipeline to ensure the job shows a successful status even if a command fails.","title":"Jenkins—Executing Shell Scripts—Handling Non-Zero Return Codes","type":"posts"},{"content":"When I upgrade Jenkins 2.176.3 to Jenkins 2.235.1, my Windows agent can not connect with master successfully and outcome this warning message \u0026ldquo;.NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service\u0026rdquo;.\nIt means I need to upgrade my Windows agent .NET Framework. Here are steps about how to upgrade .NET Framework to version 3.5.\nInstall .NET Framework 3.5 # Open Programs and Features\nSelect .NET Framework 3.5 Features (In my screenshot, it had already installed)\nThen try to reconnect the Jenkins agent, then it should back to work.\nInstall Jenkins agent service # If you can not found a Jenkins agent like me,\nYou can try these steps to install Jenkins agent\n# install Jenkins agent service cd c:\\\\jenkins .\\jenkins-agent.exe install net start jenkinsslave-C__agent # unstall Jenkins agent service sc delete jenkinsslave-C__agent Manual install .NET Framework 3.5 # Btw, if you could not install .NET Framework 3.5 successfully. you can try to install manually by this step\nManually download microsoft-windows-netfx3-ondemand-package.cab\nSpecify the path like below to install(note: the path is the directory where the file located)\nIn my case no need to reboot the Windows agent.\nHopefully, this also works for you. Let me know in case you have any comments.\n","date":"2020-07-16","externalUrl":null,"permalink":"/en/posts/2020/jenkins-windows-agent-connect-problem/","section":"Posts","summary":"Resolve the issue of Jenkins Windows agents not connecting due to missing .NET Framework, including steps to install .NET Framework 3.5 and set up the Jenkins agent service.","title":"How to fix \".NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service\"","type":"posts"},{"content":"I am Xianpeng, a build engineer. Today, I am going to share with you three Jenkins Practice.\nI will talk about Configuration as code, followed up with shared libraries, and then Multi-Branch Pipeline in the end.\nConfiguration as Code # What is Configuration as Code?\nConfiguration as code (CAC) is an approach that managing configuration resources in a bitbucket repository\nWhat are the benefits? # First, Jenkins Job Transparency # To those who have experience with Bamboo jobs, you know how hard it is to grasp the logic from the GUI, this is especially true to people who don’t know the tools very well. So, when we migrated Bamboo jobs to Jenkins, we decided to use Configuration as code, because the code is more readable and very easy for engineers to understand the logic and flow.\nSecondly, Traceability # Another drawback of configuring Jenkins Jobs through GUI is that it cannot trace the history and see who did what. The ability to see who made changes is very very important for some critical Jenkins jobs, such as build jobs. With Configuration as code, we treat the Jenkins job code the same way as other application code, the benefits are not only on traceability-wise, but also the ability to roll-back to a specific version if needed.\nThirdly, Quick Recovery # Using Configuration as code has another benefit, which is the ability to quickly recover Jenkins\u0026rsquo;s job upon hardware issues. However, if Jenkins Job is configured through GUI, when the server that host the Jenkins corrupted, you might at risk of losing everything relates to Jenkins. So, from the business continuity perspective, it is also suggesting us to use configuration as code.\nJenkins Shared Libraries # Just like writing any application code, that we need to create functions, subroutines for reuse and sharing purpose. The same logic applies to the Jenkins configuration code. Functionalities such as sending emails, printing logs, deploying the build to FTP/Artifactory can be put into Jenkins Shared Libraries. Jenkins Shared Libraries is managed in Bitbucket.\nSo, let’s take a look, …\nxshen@localhost MINGW64 /c/workspace/cicd/src/org/devops (develop) $ ls -l total 28 -rw-r--r-- 1 xshen 1049089 5658 Jun 18 09:23 email.groovy -rw-r--r-- 1 xshen 1049089 898 Jun 13 20:05 git.groovy -rw-r--r-- 1 xshen 1049089 1184 Jun 8 12:10 opensrc.groovy -rw-r--r-- 1 xshen 1049089 1430 Jul 3 10:33 polaris.groovy -rw-r--r-- 1 xshen 1049089 2936 Jul 3 10:32 trigger.groovy drwxr-xr-x 1 xshen 1049089 0 Jun 8 12:10 utils/ -rw-r--r-- 1 xshen 1049089 787 May 12 13:24 utils.groovy As you can see, these groovy files are so-called shared libraries that complete works such as sending emails, git operations, updating opensource and so on. So, it is becoming very clear why we want to use shared libraries because it can reduce duplicate code.\nIt is also easier to maintain because instead of updating several places, we just need to update the shared libraries if any changes required. The last but not least, it encourages reuse and sharing cross teams. For example, the shared libraries I created are also used other team.\nMulti-branch pipeline # Next, Multi-branch pipeline. Some of you may have seen the same diagram like this.\nIn this picture, the pull request will trigger an automatic build, which is very helpful to engineers because their changes will not be merged to the main branch unless it passes the build test and smoke test.\nSo, I will share more detailed information here with you.\nThe thing works behind the scene is called Jenkins Multi-branch Pipeline. Before getting into the details, let’s first see what it looks like.\nNote: If your branch or Pull Request has been deleted, the branch or Pull Request will either be removed from the multi-branch Job or show a crossed-out status as shown above, this depending on your Jenkins setting.\nSo, as you can see from this page, there are multi Jenkins jobs. That is because for each bugfix or feature branch in Bitbucket, this multi-branch pipeline will automatically create a Jenkins job for them.\nSo, when developers complete their works, they can use these Jenkins jobs to create official build by themselves without the need of involving a build engineer. However, this was not the case in the past. At the time that we did not have these self-service jobs, developers always ask help from me, the build engineer to create a build for them. We have around twenty U2 developers in the team, you can image the efforts needed to satisfy these requirements.\nSo, I just covered the first benefit of this multi-branch pipeline, which creates a self-service for the team, save their time, save my time.\nAnother benefit of this self-service build and install is that our main branch will be more stable and save us from the time spent on investigating whose commit was problematic because only changes passed build, install and smoke test will be merged into the main branch.\nNow, you may wonder the value of this work, like how many issues have been discovered by this auto build and install test.\nTaking our current development as an example, there were about 30 pull requests merged last month, and six of them were found has built problems on some platforms.\nAs you all know, the cost of quality will be very low if we can find issues during the development phase, rather than being found by QA, Support or even customer.\nPlease comments in case you have any questions.\n","date":"2020-07-03","externalUrl":null,"permalink":"/en/posts/2020/jenkins-best-practice/","section":"Posts","summary":"Discusses three best practices for Jenkins: Configuration as Code, Shared Libraries, and Multi-Branch Pipeline, highlighting their benefits in terms of transparency, traceability, and self-service builds.","title":"Jenkins Top 3 best practice","type":"posts"},{"content":"《Jenkins Tips 2》 —— 每期用简短的图文描述一个 Jenkins 小技巧。\n问题 # 想要把 Linux 上不同的文本数据通过 Jenkins 发送邮件给不同的人。\n思路 # 想通过 Shell 先对数据进行处理，然后返回到 Jenkins pipeline 里，但只能得到 Shell 返回的字符串，因此需要在 Jenkinsfile 里把字符串处理成数组，然后通过一个 for 循环对数组中的值进行处理。\n以下是要处理的文本数据：\n# Example $ ls fail-list-user1.txt fail-list-user2.txt fail-list-user3.txt 要将以上文件通过 Jenkins 分别进行处理，得到用户 user1，user2，user3 然后发送邮件。\n解决 # 字符串截取 # 通过 Shell 表达式只过滤出 user1 user2 user3\n# list 所有以 fail-list 开头的文件，并赋给一个数组 l l=$(ls -a fail-list-*) for f in $l; do f=${f#fail-list-} # 使用#号截取左边字符 f=${f%.txt} # 使用%号截取右边字符 echo $f # 最终输出仅包含 user 的字符串 done 测试结果如下：\n$ ls fail-list-user1.txt fail-list-user2.txt fail-list-user3.txt $ l=$(ls -a fail-list-*) \u0026amp;\u0026amp; for f in $l; do f=${f#fail-list-}; f=${f%.txt}; echo $f ; done; user1 user2 user3 处理字符串为数组 # 以下在 Jenkinsfile 使用 groovy 将 Shell 返回的字符串处理成字符数组。\n// Jenkinsfile // 忽略 stage, steps 等其他无关步骤 ... scripts { // 将 Shell 返回字符串赋给 owners 这个变量。注意在 $ 前面需要加上 \\ 进行转义。 def owners = sh(script: \u0026#34;l=\\$(ls -a fail-list-*) \u0026amp;\u0026amp; for f in \\$l; do f=\\${f#fail-list-}; f=\\${f%.txt}; echo \\$f ; done;\u0026#34;, returnStdout:true).trim() // 查看 owners 数组是否为空，isEmpty() 是 groovy 内置方法。 if ( ! owners.isEmpty() ) { // 通过 .split() 对 owners string 进行分解，返回字符串数组。然后通过 .each() 对返回的字符串数组进行循环。 owners.split().each { owner -\u0026gt; // 打印最终的用户返回 println \u0026#34;owner is ${owner}\u0026#34; // 发送邮件，例子 email.SendEx([ \u0026#39;buildStatus\u0026#39; : currentBuild.currentResult, \u0026#39;buildExecutor\u0026#39;: \u0026#34;${owner}\u0026#34;, \u0026#39;attachment\u0026#39; : \u0026#34;fail-list-${owner}.txt\u0026#34; ]) } } } 最终完成了通过 Groovy 将 Shell 返回的字符串处理成字符数组，实现上述例子中对不同人进行邮件通知的需求。\n希望以上例子对你做其他类似需求的时候有所启示和帮助。\n","date":"2020-06-22","externalUrl":null,"permalink":"/posts/2020/jenkins-tips-2/","section":"Posts","summary":"如何在 Jenkins Pipeline 中将 Shell 返回的字符串处理为字符数组，以便在后续步骤中进行处理和使用。","title":"将 Jenkins Shell 返回的字符串处理为字符数组","type":"posts"},{"content":"《Jenkins Tips 1》 —— 每期用简短的图文描述一个 Jenkins 小技巧。\n问题 # 不希望 Shell 脚本因失败而中止 想一直运行 Shell 脚本并报告失败 解决 # 方法一 # 运行 Shell 时，你可以通过使用内置的 +e 选项来控制执行你的脚本错误。这可以禁用“非 0 退出”的默认行为。\n请参考如下四个示例中的测试 Shell 和测试结果 Console Output。\n示例一 # 执行的时候如果出现了返回值为非零（即命令执行失败）将会忽略错误，继续执行下面的脚本。\nset +e ls no-exit-file whoami 示例二 # 执行的时候如果出现了返回值为非零，整个脚本就会立即退出。\nset -e ls no-exit-file whoami 方法二 # 示例三 # 还有一种方式，如果不想停止失败的另一种方法是添加 || true 到你的命令结尾。\n# 做可能会失败，但并不关注失败的命令时 ls no-exit-file || true 示例四 # 如果要在失败时执行某些操作则添加 || \u0026lt;doSomethingOnFailure\u0026gt; 。\n# 做可能会失败的事情，并关注失败的命令 # 如果存在错误，则会创建变量 error 并将其设置为 true ls no-exit-file || error=true # 然后去判断 error 变量的值。如果为真，则退出 Shell if [ $error ] then exit -1 fi ","date":"2020-06-21","externalUrl":null,"permalink":"/posts/2020/jenkins-tips-1/","section":"Posts","summary":"如何在 Jenkins 中使用 set +e 和 set -e 来控制 Shell 脚本的执行行为，以便在出现错误时不终止整个构建流程。","title":"忽略 Jenkins Shell 步骤中的故障","type":"posts"},{"content":" 背景 # 实现定期批量登录远程虚拟机然后进行一些指定的操作，还支持用户添加新的 hostname。\n需求分解 # 通过一个简单的 shell 脚本可实现定期进行 ssh 登录操作，但如何实现的更优雅一些就需要花点时间了，比如：\n定期自动执行 输出比较直观的登录测试结果 支持用户添加新的虚拟机 hostname 到检查列表中 执行完成后，通知用户等等 希望在不引入其他 Web 页面的情况下通过现有的工具 Jenkins 使用 Shell 脚本如何实现呢？\n写一个脚本去循环一个 list 里所有的 hostname，经考虑这个 list 最好是一个 file，这样方便后续处理。 这样当用户通过执行 Jenkins job 传入新的 hostname 时，使用新的 hostname 到 file 里进行 grep，查看是否已存在。 如果 grep 到，不添加；如果没有 grep 到，将这个 hostname 添加到 file 里。 将修改后的 file 添加到 git 仓库里，这样下次 Jenkins 的定时任务就会执行最近添加的 hostname 了。 实现重点 # 使用 expect。在使用 ssh 连接远程虚拟机的时候需要实现与远程连接时实现交互，例如：可以期待屏幕上的输出，然后进而进行相应的输入。在使用 expect 之前需要先安装，以 Redhat 的安装命令为例： sudo yum install expect 来进行安装。\n更多有关 expect 使用的可以参看这个连接：http://xstarcd.github.io/wiki/shell/expect.html\n使用了 Shell 数组。使用 Shell 读取文件数据，进行登录操作，将操作失败的记录到一个数组里，然后打印出来。\n在通过 Jenkins 提交新的 hostname 到 Git 仓库时，origin 的 URL 需要是 https://${USERNAME}:${PASSWORD}@git.company.com/scm/vmm.git 或 git@company.com:scm/vmm.git（需要提前在执行机器上生成了 id_rsa.pub）\n代码已经上传 GitHub 请参看 https://github.com/shenxianpeng/vmm.git\n最终效果 # 开始执行，提供输入新的 hostname # 执行完成，将执行结果归档以便查看 # 打开归档结果如下 # ##################################################### ######### VM login check via SSH results ############ ##################################################### # # # Compelted (success) 14/16 (total) login vm check. # # # # Below 2 host(s) login faied, need to check. # # # abc.company.com xyz.company.com # # ##################################################### 最后 # 现在技术的更新非常快，尤其作为 DevOps 工程师，各种工具层出不穷，想要每一样工具都掌握几乎是不可能的。\n只会工具不了解其背后的原理，等到新工具出现替换掉旧的工具，其实这些年是没有进步的。\n只有认真的把在工作中遇到的每个问题背后来龙去脉去搞懂，才能地基打的稳，这样不论工具怎么变，学习起来都会很快。\n掌握操作系统，Shell，以及一门擅长的编程语言之后再去学习那些工具，要不永远都是漂浮在空中。\n","date":"2020-06-13","externalUrl":null,"permalink":"/posts/2020/vm-status-check-via-jenkins/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 实现定期批量登录远程虚拟机，并支持用户添加新的主机名称，提供了完整的实现代码和步骤。","title":"这也能用Jenkins？快速实现一个定期批量登录远程虚拟机并支持添加新的主机名称的Job","type":"posts"},{"content":"本文对于同样在 AIX 遇到这个问题的人会非常有帮助。另外，不要被标题无聊到，解决问题的过程值得参考。\n分享一个花了两天时间才解决的一个问题：使用 Jenkins Artifactory 插件上传制品到 https 协议的企业级的 Artifactory 失败。该问题只在 AIX 平台上出现的，其他 Windows，Linux, Unix 均正常。\n前言 # 最近计划将之前使用的 Artifactory OSS（开源版）迁移到 Artifactory Enterprise（企业版）上。为什么要做迁移？这里有一个 Artifactory 对比的矩阵图 https://www.jfrog.com/confluence/display/JFROG/Artifactory+Comparison+Matrix\n简单来说，开源版缺少与 CI 工具集成时常用的 REST API 功能，比如以下常用功能\n设置保留策略(Retention)。设置上传的制品保留几天等，达到定期清理的目的。 提升(Promote)。通过自动化测试的制品会被提升到 stage（待测试）仓库，通过手工测试的提升到 release（发布）仓库。 设置属性(set properties)。对于通过不同阶段的制品通过 CI 集成进行属性的设置。 正好公司已经有企业版了，那就开始迁移吧。本以为会很顺利的完成，没想到唯独在 IBM 的 AIX 出现上传制品失败的问题。\n环境信息\nJenkins ver. 2.176.3 Artifactory Plugin 3.6.2 Enterprise Artifactory 6.9.060900900 AIX 7.1 \u0026amp;\u0026amp; java version 1.8.0 以下是去掉了无相关的信息的错误日志。\n[consumer_0] Deploying artifact: https://artifactory.company.com/artifactory/generic-int-den/database/develop/10/database2_cdrom_opt_AIX_24ec6f9.tar.Z Error occurred for request GET /artifactory/api/system/version HTTP/1.1: A system call received a parameter that is not valid. (Read failed). Error occurred for request PUT /artifactory/generic-int-den/database/develop/10/database2_cdrom_opt_AIX_24ec6f9.tar.Z;build.timestamp=1591170116591;build.name=develop;build.number=10 HTTP/1.1: A system call received a parameter that is not valid. (Read failed). Error occurred for request PUT /artifactory/generic-int-den/database/develop/10/database2_cdrom_opt_AIX_24ec6f9.tar.Z;build.timestamp=1591170116591;build.name=develop;build.number=10 HTTP/1.1: A system call received a parameter that is not valid. (Read failed). [consumer_0] An exception occurred during execution: java.lang.RuntimeException: java.net.SocketException: A system call received a parameter that is not valid. (Read failed) at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:44) at org.jfrog.build.extractor.producerConsumer.ConsumerRunnableBase.run(ConsumerRunnableBase.java:11) at java.lang.Thread.run(Thread.java:785) Caused by: java.net.SocketException: A system call received a parameter that is not valid. (Read failed) at java.net.SocketInputStream.socketRead(SocketInputStream.java:127) at java.net.SocketInputStream.read(SocketInputStream.java:182) at java.net.SocketInputStream.read(SocketInputStream.java:152) at com.ibm.jsse2.a.a(a.java:227) at com.ibm.jsse2.a.a(a.java:168) at com.ibm.jsse2.as.a(as.java:702) at com.ibm.jsse2.as.i(as.java:338) at com.ibm.jsse2.as.a(as.java:711) at com.ibm.jsse2.as.startHandshake(as.java:454) at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:436) at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384) at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:374) at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.ServiceUnavailableRetryExec.execute(ServiceUnavailableRetryExec.java:85) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) at org.jfrog.build.client.PreemptiveHttpClient.execute(PreemptiveHttpClient.java:89) at org.jfrog.build.client.ArtifactoryHttpClient.execute(ArtifactoryHttpClient.java:253) at org.jfrog.build.client.ArtifactoryHttpClient.upload(ArtifactoryHttpClient.java:249) at org.jfrog.build.extractor.clientConfiguration.client.ArtifactoryBuildInfoClient.uploadFile(ArtifactoryBuildInfoClient.java:692) at org.jfrog.build.extractor.clientConfiguration.client.ArtifactoryBuildInfoClient.doDeployArtifact(ArtifactoryBuildInfoClient.java:379) at org.jfrog.build.extractor.clientConfiguration.client.ArtifactoryBuildInfoClient.deployArtifact(ArtifactoryBuildInfoClient.java:367) at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:39) ... 2 more Failed uploading artifacts by spec 很奇怪会出现上述问题，从开源版的 Artifactory 迁移到企业版的 Artifactory，它们之间最直接的区别是使用了不同的传输协议，前者是 http 后者是 https。\nHTTPS 其实是有两部分组成：HTTP + SSL/TLS，也就是在 HTTP 上又加了一层处理加密信息的模块，因此更安全。\n本以为 Google 一下就能找到此类问题的解决办法，可惜这个问题在其他平台都没有，只有 AIX 上才有，肯定这个 AIX 有什么“过人之处”和其他 Linux/Unix 不一样。\n使用 curl 来替代 # 由于上述问题重现在需要重新构建，比较花时间，就先试试直接用 curl 命令来调用 Artifactory REST API 看看结果。\n做了以下测试，查看 Artifactory 的版本\ncurl https://artifactory.company.com/artifactory/api/system/version curl: (35) Unknown SSL protocol error in connection to artifactory.company.com:443 # 打开 -v 模式，输出更多信息 bash-4.3$ curl -v https://artifactory.company.com/artifactory/api/system/version * Trying 10.18.12.95... * Connected to artifactory.company.com (10.18.12.95) port 443 (#0) * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH * TLSv1.2 (OUT), TLS handshake, Client hello (1): * TLSv1.2 (IN), TLS handshake, Server hello (2): * NPN, negotiated HTTP1.1 * TLSv1.2 (IN), TLS handshake, Certificate (11): * TLSv1.2 (OUT), TLS alert, Server hello (2): * Unknown SSL protocol error in connection to artifactory.company.com:443 * Closing connection 0 curl: (35) Unknown SSL protocol error in connection to artifactory.company.com:443 果然也出错了，curl 也不行，可能就是执行 curl 命令的时候没有找到指定证书，查了 curl 的 help，有 --cacert 参数可以指定 cacert.pem 文件。\nbash-4.3$ curl --cacert /var/ssl/cacert.pem https://artifactory.company.com/artifactory/api/system/version { \u0026#34;version\u0026#34; : \u0026#34;6.9.0\u0026#34;, \u0026#34;revision\u0026#34; : \u0026#34;60900900\u0026#34;, \u0026#34;addons\u0026#34; : [ \u0026#34;build\u0026#34;, \u0026#34;docker\u0026#34;, \u0026#34;vagrant\u0026#34;, \u0026#34;replication\u0026#34;, \u0026#34;filestore\u0026#34;, \u0026#34;plugins\u0026#34;, \u0026#34;gems\u0026#34;, \u0026#34;composer\u0026#34;, \u0026#34;npm\u0026#34;, \u0026#34;bower\u0026#34;, \u0026#34;git-lfs\u0026#34;, \u0026#34;nuget\u0026#34;, \u0026#34;debian\u0026#34;, \u0026#34;opkg\u0026#34;, \u0026#34;rpm\u0026#34;, \u0026#34;cocoapods\u0026#34;, \u0026#34;conan\u0026#34;, \u0026#34;vcs\u0026#34;, \u0026#34;pypi\u0026#34;, \u0026#34;release-bundle\u0026#34;, \u0026#34;replicator\u0026#34;, \u0026#34;keys\u0026#34;, \u0026#34;chef\u0026#34;, \u0026#34;cran\u0026#34;, \u0026#34;go\u0026#34;, \u0026#34;helm\u0026#34;, \u0026#34;rest\u0026#34;, \u0026#34;conda\u0026#34;, \u0026#34;license\u0026#34;, \u0026#34;puppet\u0026#34;, \u0026#34;ldap\u0026#34;, \u0026#34;sso\u0026#34;, \u0026#34;layouts\u0026#34;, \u0026#34;properties\u0026#34;, \u0026#34;search\u0026#34;, \u0026#34;filtered-resources\u0026#34;, \u0026#34;p2\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;webstart\u0026#34;, \u0026#34;support\u0026#34;, \u0026#34;xray\u0026#34; ], \u0026#34;license\u0026#34; : \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; } 试了下成功了。\n到这里问题已经解决了，只要使用 curl 调用 Artifactory REST API 就能完成上传操作了。但我用的 Jenkins Artifactory Plugin，如果使用 curl 我需要把之前的代码重新再实现一遍，然后再测试，就为了 AIX 一个平台的问题，实在是“懒”的重新开始。本着这样懒惰的性格，还得继续解决 Jenkins 调用 agent 去执行上传失败的问题。\n最终解决办法 # 尝试设置 SSL_CERT_FILE 环境变量 # 想试试用上述的办法来解决 Jenkins 的问题。如果能有一个环境变量能设置指定 cacert.pem 文件的路径，那样在 Jenkins 调用 agent 执行上传时候就能找到证书，可能就能解决这个问题了。果然是有这样的环境变量的 SSL_CERT_FILE，设置如下\nset SSL_CERT_FILE=/var/ssl/cacert.pem 设置好环境变量之后，通过 curl 调用，再不需要使用 --cacert 参数了。这下看起来有戏了，带着喜悦的心情把这个环境变量加到 agent 机器上，设置如下：\n或者可以修改 agent 机器上的 /etc/environment 文件。\n结果经测试错误信息依旧，看来 Jenkins 执行的 remote.jar 进行上传时跟本地配置环境没有关联，看来需要从执行 remote.jar 着手，把相应的设置或是环境变量在启动 remote.jar 时传进去。\nJenkins 管理 agent 的原理是通过在 agent 上启动一个 remote.jar 实现的\n在启动 remote.jar 时设置环境变量 # java 的 -D 参数可以完成这一点。\n进行了大量的搜索和尝试，最终在 IBM 的官方找到了这篇文档 https://www.ibm.com/support/knowledgecenter/SSYKE2_8.0.0/com.ibm.java.security.component.80.doc/security-component/jsse2Docs/matchsslcontext_tls.html\n文档大意是，IBM SDK 系统属性 com.ibm.jsse2.overrideDefaultTLS=[true|false] 有 true 和 false 两个值，如果想要与 Oracle SSLContext.getInstance(\u0026quot;TLS\u0026quot;) 的行为相匹配，请将此属性设置为 true，默认值为 false。\n下表显示了系统属性对 SSLContext.getInstance(\u0026ldquo;TLS\u0026rdquo;) 的影响\nProperty value setting Protocol enabled false TLS V1.0 true TLS V1.0, V1.1, and V1.2 绝大多数的 Java 应用都是使用 Oracle 的 JDK 来开发的，这里要与 Oracle 的行为保持一致；另外 IBM 的 SDK 默认协议只有 TLS V1.0，而上面的 log 可以看到使用的 TLSv1.2 协议，因此需要将属性设置为 true。\n最终在 Jenkins 的 agent 配置里将 JVM Options 区域加上这句 -Dcom.ibm.jsse2.overrideDefaultTLS=true，断开连接，重新启动 agent，再次执行 Pipeline，成功的把 AIX 上的制品上传到 Artifactory 上了，问题解决了。\n总结 # 遇到问题并解决问题是一件非常爽的事，从中也学到了很多之前不曾了解过的知识，解决问题的过程比 Google 随便查查更让人印象深刻，再遇到此类问题可能就会举一反三了。\n另外，凡事如果觉得自己在短时间内没有头绪、自己搞不定的时候尽快寻求有经验的同事的帮助。感谢帮助我的同事们，没有他们的帮助和指导就不能这么快的解决问题。\n","date":"2020-06-03","externalUrl":null,"permalink":"/posts/2020/java-net-socketexception-on-aix/","section":"Posts","summary":"本文介绍了在 AIX 上使用 Jenkins Artifactory 插件上传制品到 https 协议的 Artifactory 失败的问题及其解决方法，包括设置环境变量和调整 Java 系统属性。","title":"解决 Jenkins Artifactory Plugin 仅在 AIX 上传制品到 https 协议的 Artifactory 失败的问题","type":"posts"},{"content":"A popular saying in the programmer community is, \u0026ldquo;Programmers who can code can\u0026rsquo;t beat those who can make PPTs.\u0026rdquo; I remember a lyric from the song \u0026ldquo;Let Yourself Go\u0026rdquo; at New Oriental\u0026rsquo;s 2019 annual meeting that resonated with most programmers:\n\u0026ldquo;Working hard and tired, what\u0026rsquo;s the point of achievements? In the end, we can\u0026rsquo;t beat those who make PPTs.\u0026rdquo;\nFor a while, everyone seemed to agree with this statement, expressing their dissatisfaction and helplessness.\n(I) # Having worked in the field for over 10 years and witnessed various talented individuals online and at work, my understanding of \u0026ldquo;true ability\u0026rdquo; has evolved.\nInitially, I believed that those who were quiet, reserved, and technically proficient were true masters, while those who showed off their limited technical skills were merely showing off. However, when sharing technical insights with the team, I discovered that clearly explaining something to most people who don\u0026rsquo;t understand it is also a skill in itself.\nFirst, you need to fully understand the subject matter before explaining it to others. Second, you need to anticipate the questions others might ask and be able to answer them. Finally, you need to establish a clear narrative and present it logically. (II) # Recently, in Wu Jun\u0026rsquo;s \u0026ldquo;Reading and Writing\u0026rdquo; course on the \u0026ldquo;Get\u0026rdquo; app, he gave an example:\nA project team has three members. The first is highly skilled in professional work, doing the most important tasks and guiding others. The second excels at team organization, boosting morale and leading the team to overcome challenges. The third can clearly communicate the team\u0026rsquo;s work.\nIf the boss needs to promote one of them, who has the highest chance? Many would think the highly skilled professional. However, in reality, it\u0026rsquo;s often the third person. Why?\nLet\u0026rsquo;s imagine the three presenting a report:\nThe first person presents a lot of technical details, boring the leadership. The team\u0026rsquo;s work isn\u0026rsquo;t recognized, and no resources are secured. They might not be chosen for future presentations, or the project might be scrapped.\nIf the organizer presents, they come across as an administrator, peripheral to the project, giving directions without understanding the details. The presentation will be ineffective.\nIf the communicator presents, the team\u0026rsquo;s work is most likely to be recognized, and resources secured. Everyone benefits, leading to future presentations. Over time, they are perceived as the most impactful contributor.\nThis isn\u0026rsquo;t fiction. Look around your workplace; you\u0026rsquo;ll find similar situations. It\u0026rsquo;s a natural selection process within an organization. Those with good communication skills often give presentations and reports, gaining visibility and advancement opportunities.\n(III) # Many working in foreign companies notice that many executives are Indian. I\u0026rsquo;ve heard Indian colleagues present to leadership, and their presentations are far superior. Beyond English proficiency, their content and preparation stand out:\nExcellent PPTs using structured diagrams. Clear core message and focus. A coherent narrative that clearly explains the subject matter. You might find that you have everything they do, and even more, but they present better, leaving a strong impression on the audience, making them seem brilliant, while you just chuckle.\nAs programmers, improving technical skills is essential, but developing writing and communication skills will help build your brand and advance your career.\nIn Conclusion # I recently prepared an English PPT for a presentation to foreign leadership. My manager\u0026rsquo;s feedback was incredibly helpful, revealing significant differences between my presentation and theirs.\nFor example, consider explaining the use of Jenkins Shared Libraries:\nBefore: In the early stage of doing this work one year ago, I wrote many duplicate code such as sending emails, printing logs. Shared Libraries could help to solve this problem \u0026hellip;\nAfter: Just like writing any application code, that we need to create functions, subroutines for reuse and sharing purpose. The same logic applies to Jenkins configuration code \u0026hellip;\nThere\u0026rsquo;s no need to mention initial setbacks and mistakes. While honest, it doesn\u0026rsquo;t benefit you. Feedback included improvements to the opening, transitions between slides, and closing remarks.\nMy prejudice against PPTs has vanished, and I now consider creating effective PPTs a valuable skill.\nTime to get back to revising my PPT and practicing my delivery\u0026hellip;\n","date":"2020-05-30","externalUrl":null,"permalink":"/en/posts/2020/programmers-read-and-write/","section":"Posts","summary":"This article explores the importance of writing skills for programmers, highlighting the role of communication and expression in career advancement, and sharing personal experiences and insights on writing.","title":"Beyond Coding - The Importance of Writing for Programmers","type":"posts"},{"content":"","date":"2020-05-30","externalUrl":null,"permalink":"/en/tags/others/","section":"Tags","summary":"","title":"Others","type":"tags"},{"content":"A common problem I encounter when developing Jenkins Declarative Pipelines is this: the modified Pipeline looks fine, but when it\u0026rsquo;s submitted to the code repository and a Jenkins build is performed, a syntax error is discovered. Then I have to modify, submit, and build again, potentially finding other unnoticed syntax issues.\nTo reduce the frequency of submitting to the code repository due to syntax errors, it would be helpful to perform basic syntax validation before submission to check for syntax errors in the current Pipeline.\nAfter investigation, I found that Jenkins itself provides a syntax check REST API that can be directly used to validate Declarative Pipelines. This method requires executing a long curl command, which seems cumbersome. It would be much better if this could be run directly within the IDE.\nVS Code, being the most popular IDE currently, indeed has relevant plugins.\nBelow are two methods for checking for syntax errors in Jenkinsfile files for Jenkins Declarative Pipelines. Both methods use the Jenkins REST API.\nNote:\nCurrently, only Declarative Pipelines support syntax validation; Scripted Pipelines do not.\nIf you use the Jenkins replay feature or develop Pipelines using the Jenkins web page, this problem does not exist.\nREST API # If your project uses Jenkins Shared Libraries, for easier use of the REST API, consider creating a linter.sh file in that repository and adding it to your .gitignore. This allows you to configure your username and password in the file without accidentally committing them to the Git repository.\nThe following is the linter.sh script content for reference.\n# How to use # sh linter.sh your-jenkinsfile-path # Replace with your Jenkins username username=admin # Replace with your Jenkins password password=admin # Replace with your Jenkins URL JENKINS_URL=http://localhost:8080/ PWD=`pwd` JENKINS_FILE=$1 curl --user $username:$password -X POST -F \u0026#34;jenkinsfile=\u0026lt;$PWD/$JENKINS_FILE\u0026#34; $JENKINS_URL/pipeline-model-converter/validate Let\u0026rsquo;s test the effect: sh linter.sh your-jenkinsfile-path\nExample 1\n$ sh linter.sh Jenkinsfile Errors encountered validating Jenkinsfile: WorkflowScript: 161: Expected a stage @ line 161, column 9. stages { ^ Example 2\nsh linter.sh Jenkinsfile Errors encountered validating Jenkinsfile: WorkflowScript: 60: Invalid condition \u0026#34;failed\u0026#34; - valid conditions are [always, changed, fixed, regression, aborted, success, unsuccessful, unstable, failure, notBuilt, cleanup] @ line 60, column 9. failed{ ^ # Change \u0026#34;failed\u0026#34; to \u0026#34;failure\u0026#34;, and execute again; it succeeds. sh linter.sh Jenkinsfile Jenkinsfile successfully validated. When the Pipeline is very long, it\u0026rsquo;s always difficult to find unmatched brackets or missing parentheses. This script allows you to check for problems before submitting.\nJenkinsfile successfully validated. Jenkins Pipeline Linter Connector Plugin # The second method is more universal; any Declarative Pipeline can use this plugin to check for syntax errors.\nInstalling the Plugin # Search for Jenkins Pipeline Linter Connector in the VSCode plugin marketplace.\nConfiguring the Plugin # Open File -\u0026gt; Preferences -\u0026gt; Settings -\u0026gt; Extensions, find Jenkins Pipeline Linter Connector, and configure it as follows.\nRunning the Plugin # Right-click -\u0026gt; Command Palette -\u0026gt; Validate Jenkinsfile\nOr\nUse the shortcut Shift + Alt + V\nExecution Result # Summary # If you use VSCode as your development tool, the Jenkins Pipeline Linter Connector plugin is recommended.\nFor Jenkins Shared Libraries repositories, consider creating a shell script to perform validation by executing the script.\nOf course, if you only use a simple Jenkinsfile, you can also write it on the Jenkins web Pipeline page, which has built-in syntax checking.\nIf you have other methods, please feel free to leave a comment and let me know.\n","date":"2020-05-23","externalUrl":null,"permalink":"/en/posts/2020/jenkins-pipeline-linter-connector/","section":"Posts","summary":"This article introduces two methods to ensure there are no syntax errors before submitting a Jenkins Pipeline using the REST API for syntax validation and using the VSCode plugin for syntax checking.","title":"How to Ensure No Syntax Errors Before Submitting a Jenkins Pipeline","type":"posts"},{"content":" Record JMeter Scripts # use JMeter\u0026rsquo;s HTTP(S) Test Script Recorder, please refer to this official document https://jmeter.apache.org/usermanual/jmeter_proxy_step_by_step.html\nRunning JMeter Scripts # Debug scripts on JMeter in GUI Mode\nYou can debug your record scripts in GUI Mode until there are no errors\nrun test scripts in Non-GUI Mode(Command Line mode) recommend\njmeter -n -t ..\\extras\\Test.jmx -l Test.jtl Running JMeter Scripts on Jenkins # Need Tools # Jmeter - Web Request Load Testing Jmeter-plugins ServerAgent-2.2.1 - PerfMon Agent to use with Standard Set Test server # Two virtual machines\nSystem under test Jmeter execution machine, this server is also Jenkins server Implement # Develop test script # Record Scripts - use JMeter\u0026rsquo;s HTTP(S) Test Script Recorder, please refer to this official document https://jmeter.apache.org/usermanual/jmeter_proxy_step_by_step.html\nCreate Jenkins job for running JMeter scripts # Create a new item-\u0026gt;select Freestyle project\nAdd build step-\u0026gt;Execute Windows batch command\n//access to jenkins jobs workspace, empty the last test results cmd cd C:\\Users\\peter\\.jenkins\\jobs\\TEST-122 Upload large data\\workspace del /Q \u0026#34;jtl\u0026#34;\\* del /Q \u0026#34;PerfMon Metrics Collector\u0026#34;\\* Add build step-\u0026gt;Execute Windows batch command\n//add first run jmeter script command, if you want run others script you can continue to add \u0026#34;Execute Windows batch command\u0026#34; jmeter -n -t script/UploadLargeData-1.jmx -l jtl/UploadLargeData-1.jtl Configure build email - Configure System\n//Configure System, Extended E-mail Notification SMTP server: smtp.gmail.com //Job Configure, Enable \u0026#34;Editable Email Notification\u0026#34; Project Recipient List: xianpeng.shen@gmail.com Project Reply-To List: $DEFAULT_REPLYTO Content Type: HTML (text/html) Default Subject:$DEFAULT_SUBJECT Default Content: ${SCRIPT, template=\u0026#34;groovy-html.template\u0026#34;} //Advance setting Triggers: Always Send to Recipient List Generate test report # JMeter-\u0026gt;Add listener-\u0026gt;add jp@gc - PerfMon Metrics Collector, browse Test.jtl, click right key on graph Export to CSV\nAnalyze test results # Introduction test scenarios\nUsing 1, 5, 10, 20, 30, (50) users loading test, record every group user test results\nGlossary\nSample(label) - This indicates the number of virtual users per request. Average - It is the average time taken by all the samples to execute specific label Median - is a number which divides the samples into two equal halves. %_line - is the value below which 90, 95, 99% of the samples fall. Min - The shortest time taken by a sample for specific label. Max - The longest time taken by a sample for specific label. Error% - percentage of failed tests. Throughput - how many requests per second does your server handle. Larger is better. KB/Sec - it is the Throughput measured in Kilobytes per second. Example: Test results of each scenario shown in the following table\nUser # Samples Average Median 90% Line 95% LIne Min Max Error % Throughput Received Send KB/sec 1 31 348 345 452 517 773 5 773 0.00% 2.85215 2.5 5 155 1166 1164 1414 1602 1639 9 1821 0.00% 4.26445 3.73 10 310 2275 2299 2687 2954 3671 20 4104 0.00% 4.38547 3.84 20 620 4479 4620 5113 6152 6435 39 6571 0.00% 4.42826 3.88 30 930 6652 6899 7488 9552 10051 4 10060 0.00% 4.46776 3.91 Test results analysis chart\n","date":"2020-05-09","externalUrl":null,"permalink":"/en/posts/2020/jmeter-performance-testing/","section":"Posts","summary":"A guide on using JMeter for performance testing, including recording scripts, running tests in GUI and non-GUI modes, and integrating with Jenkins for automated testing.","title":"How to use JMeter to do Performance Testing","type":"posts"},{"content":"","date":"2020-05-09","externalUrl":null,"permalink":"/en/tags/jmeter/","section":"Tags","summary":"","title":"JMeter","type":"tags"},{"content":" 背景 # 最近我们团队需要将一些示例和例子从内部的 Bitbucket 同步到 GitHub。我了解 GitHub 可以创建公共的或是私人的仓库，但我们需要保持以下两点\n只分享我们想给客户分享的内容 不改变当前的工作流程，即继续使用 Bitbucket 因此我们需要在 GitHub 上创建相应的仓库，然后将内部 Bitbucket 仓库中对应的 master 分支定期的通过 CI job 同步到 BitHub 上去。\n分支策略 # 首先，需要对 Bitbucket 进行分支权限设置\nmaster 分支只允许通过 Pull Request 来进行修改 Pull Request 默认的 reviewer 至少需要一人，并且只有同意状态才允许合并 其次，为了方便产品、售后等人员使用，简化分支策略如下\n从 master 分支上创建 feature 或是 bugfix 分支（取决于你的修改目的） 然后将你的更改提交到自己的 feature 或 bugfix 分支 在你自己的分支通过测试后，提交 Pull Request 到 master 分支 当 reviewer 同意状态，才能进行合并进入到 master 分支 Jenkins Pipeline # 基于这样的工作不是特别的频繁，也为了方便维护 Jenkins Pipeline 的简单和易于维护，我没有在需要同步的每个仓库里添加 Jenkinsfile 或在 Bitbucket 里添加 webhooks。有以下几点好处：\n只创建一个 Jenkins Job，用一个 Jenkinsfile 满足所有仓库的同步 减少了冗余的 Jenkinsfile 的代码，修改时只需更维护一个文件 不需要在每个仓库里添加一个 Jenkinsfile，更纯粹的展示示例，避免给非 IT 人员造成困扰 不足之处，不能通过 SCM 来触发构建，如果想通过 webhooks 来触发，有的公司需要申请权限来添加 webhooks 比较麻烦；另外可能无法区分从哪个仓库发来的请求，实现指定仓库的同步。\n因此如果不是特别频繁的需要同步，提供手动或是定时同步即可。\n// 这个 Jenkinsfile 是用来将 Bitbucket 仓库的 master 分支同步到 GitHub 仓库的 master 分支 // This Jenkinsfile is used to synchronize Bitbucket repositories master branches to GitHub repositories master branches. @Library(\u0026#39;jenkins-shared-library@develop\u0026#39;) _ def email = new org.cicd.email() pipeline { agent { label \u0026#34;main-slave\u0026#34; } parameters { booleanParam(defaultValue: false, name: \u0026#39;git-repo-win\u0026#39;, summary: \u0026#39;Sync internal git-repo-win master branch with external git-repo-win on GitHub\u0026#39;) booleanParam(defaultValue: true, name: \u0026#39;git-repo-lin\u0026#39;, summary: \u0026#39;Sync internal git-repo-lin master branch with external git-repo-lin on GitHub\u0026#39;) booleanParam(defaultValue: false, name: \u0026#39;git-repo-aix\u0026#39;, summary: \u0026#39;Sync internal git-repo-aix master branch with external git-repo-aix on GitHub\u0026#39;) booleanParam(defaultValue: false, name: \u0026#39;git-repo-sol\u0026#39;, summary: \u0026#39;Sync internal git-repo-sol master branch with external git-repo-sol on GitHub\u0026#39;) } options { timestamps() buildDiscarder(logRotator(numToKeepStr:\u0026#39;50\u0026#39;)) } stages { stage(\u0026#34;Synchronous master branch\u0026#34;){ steps{ script { try { params.each { key, value -\u0026gt; def repoName = \u0026#34;$key\u0026#34; if ( value == true) { echo \u0026#34;Start synchronizing $key Bitbucket repository.\u0026#34; sh \u0026#34;\u0026#34;\u0026#34; rm -rf ${repoName} return_status=0 git clone -b master ssh://git@git.your-company.com:7999/~xshen/${repoName}.git cd ${repoName} git config user.name \u0026#34;Sync Bot\u0026#34; git config user.email \u0026#34;bot@your-company.com\u0026#34; git remote add github git@github.com:shenxianpeng/${repoName}.git git push -u github master return_status=\u0026#34;\\$?\u0026#34; if [ \\$return_status -eq 0 ] ; then echo \u0026#34;Synchronize ${repoName} from Bitbucket to GitHub success.\u0026#34; cd .. rm -rf ${repoName} exit 0 else echo \u0026#34;Synchronize ${repoName} from Bitbucket to GitHub failed.\u0026#34; exit 1 fi\u0026#34;\u0026#34;\u0026#34; } else { echo \u0026#34;${repoName} parameter value is $value, skip it.\u0026#34; } } cleanWs() } catch (error) { echo \u0026#34;Some error occurs during synchronizing $key process.\u0026#34; } finally { email.Send(currentBuild.currentResult, env.CHANGE_AUTHOR_EMAIL) } } } } } } 以上的 Jenkinsfile 的主要关键点是这句 params.each { key, value -\u0026gt; }，可以通过对构建时选择参数的进行判断，如果构建时参数已勾选，则会执行同步脚本；否则跳过同步脚本，循环到下一个参数进行判断，这样就实现了可以对指定仓库进行同步。\nBackground # Recently our team need to share code from internal Bitbucket to external GitHub. I know GitHub can create private and public repository, but we have these points want to keep.\nonly share the code what we want to share not change current work process, continue use Bitbucket. So we have created corresponding repositories in the internal Bitbucket, and the master branches of these repositories will periodically synchronize with the master branches of corresponding repositories on GitHub via Jenkins job.\nBranch Strategy # Then the work process will be like\nCreate a feature or bugfix branch (it depends on the purpose of your modification).\nCommit changes to your feature/bugfix branch.\nPlease pass your feature/bugfix branch test first then create a Pull Request from your branch to master branch, at least one reviewer is required by default.\nAfter the reviewer approved, you or reviewer could merge the Pull Request, then the changes will be added to the master branch.\nTiming trigger CI job will sync code from internal repositories master branch to GitHub master branch by default. also support manual trigger.\nJenkins Job # Base on this work is not very frequency, so I want make the Jenkins job simple and easy to maintain, so I don\u0026rsquo;t create every Jenkinsfile for every Bitbucket repositories.\nPros\nOnly one Jenkinsfile for all Bitbucket repositories. Less duplicate code, less need to change when maintenance. Don\u0026rsquo;t need to add Jenkinsfile into very Bitbucket repositories. Cons\nCan not support SCM trigger, in my view this need add Jenkinsfile into repository. The main part for this Jenkinsfile is below, use this function params.each { key, value -\u0026gt; } can by passing in parameters when start Jenkins build.\n","date":"2020-05-05","externalUrl":null,"permalink":"/posts/2020/sync-from-bitbucket-to-github/","section":"Posts","summary":"介绍如何通过 Jenkins 将 Bitbucket 仓库的 master 分支同步到 GitHub。","title":"如何将 Bitbucket 仓库同步到 GitHub","type":"posts"},{"content":" Background # I have set several multi-branch pipeline and it can support Bitbucket Pull Request build. So, when developer create a Pull Request on Bitbucket, Jenkins can auto-trigger PR build. but this jenkins-plugin may not very stable, it had not work two times and I actually don\u0026rsquo;t know why it does that. But I know the use Git webhook is a direct and hard approach could solve this problem. After my test, the answer is yes. it works as expect.\nPrinciple # By setting Webhook events, you can listen for git push, create Pull requests and other events, and automatically trigger Jenkins scan when these events occur, so that Jenkins can get the latest branch (or Pull Request) created (or deleted), and automatically build Jenkins Job.\nSetting # Webhook name: test-multibranch Webhook URL: http://localhost:8080/multibranch-webhook-trigger/invoke?token=test-multibranch Test connection: 200(green), it passed. Events: Repository: N/A Pull Request: Opened, Merged, Declined, Deleted. Active: enable Here is setting screenshots.\nAt first, I also enable Modified event, but I found when there is new merged commits into our develop branch(this is our PR target branch), the holding Pull Request will be triggered and merge develop branch back to source branch then re-build.\nThen I notice the Modified summary: A pull request\u0026rsquo;s description, title, or target branch is changed.\nThis is a nice feature to make sure the source code integrate with target branch and build passed, but this is too frequent for our product builds, because our product pull request build on some Unix platform need almost 3 hours, if has 5 Pull Requests waiting to review, when new commits into develop branch, these 5 PR need to rebuild again, this takes up all the build machines, resulting in those that need to be built not getting the resources.\nAfter enable above Pull Request event and have these functions.\nwhen open a new Pull Request on Bitbucket, auto create Pull Request branch and build in Jenkins. when merge the Pull Request Bitbucket, auto delete Pull Request branch in Jenkins. when decline the Pull Request Bitbucket, auto delete Pull Request branch in Jenkins. when delete the Pull Request Bitbucket, auto delete Pull Request branch in Jenkins. For other specific branches build my Jenkins job support manually build and timing trigger, so this event settings currently good to me.\nIf there is new settings need to add, I will keep update this article in the future.\n","date":"2020-04-24","externalUrl":null,"permalink":"/en/posts/2020/bitbucket-webhooks/","section":"Posts","summary":"How to configure Bitbucket webhooks to trigger Jenkins builds for multi-branch pipelines, ensuring that Jenkins can automatically respond to events like pull requests and branch updates.","title":"Bitbucket Webhooks Configuration","type":"posts"},{"content":"","date":"2020-04-20","externalUrl":null,"permalink":"/en/tags/pipeline/","section":"Tags","summary":"","title":"Pipeline","type":"tags"},{"content":"This is the second time I\u0026rsquo;ve encountered this problem while using Jenkins declarative pipelines. The first time I encountered this problem was in a Pipeline with about 600 lines of code, and I encountered the following error:\norg.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed: General error during class generation: Method code too large! java.lang.RuntimeException: Method code too large! at groovyjarjarasm.asm.MethodWriter.a(Unknown Source) [...] At that time, I also used Jenkins Shared Libraries, but the code organization was not very good, and many steps had not yet been separated as individual methods. To solve this problem, after a refactoring, I changed the original 600+ lines of Pipeline to the current 300+ lines. Unfortunately, as I continued to add features, I encountered this problem again recently.\nThe reason for this problem is that Jenkins puts the entire declarative pipeline into a single method, and at a certain size, the JVM fails due to java.lang.RuntimeException: Method code too large!. It seems that I have a method exceeding 64k.\nThere is already a ticket for this issue on Jenkins JIRA, but it has not been resolved yet. There are currently three solutions to this problem, but they each have their own advantages and disadvantages.\nMethod 1: Move Steps into Methods Outside the Pipeline # Since mid-2017, you can declare a method at the end of the pipeline and then call it in the declarative pipeline. This allows us to achieve the same effect as shared libraries, but avoids the maintenance overhead.\npipeline { agent any stages { stage(\u0026#39;Test\u0026#39;) { steps { whateverFunction() } } } } void whateverFunction() { sh \u0026#39;ls /\u0026#39; } Advantages Disadvantages No extra maintenance cost It\u0026rsquo;s unknown if this solution will always be effective All functionality is reflected in the Jenkinsfile When a method is used in multiple Jenkinsfiles, this method will still write a lot of repetitive code Method 2: Migrate from Declarative to Scripted Pipeline # Finally, we can migrate to a scripted pipeline. With it, we have complete freedom. But this will also lose the reason why we initially decided to use declarative pipelines. With a dedicated DSL, it\u0026rsquo;s easy to understand how the pipeline works.\nAdvantages Disadvantages No limitations Requires significant refactoring More prone to errors May require more code to achieve the same functionality Method 3: Use Shared Libraries # I currently use Jenkins Shared Libraries, with a shared library to execute some complex steps. Shared libraries seem to be widely used, especially in maintaining large and complex projects.\nMy final solution was to further reduce the code in the Pipeline. I also used the solution in Method 1, moving some steps outside the Pipeline {} block, especially those steps that are called repeatedly.\nAdvantages Disadvantages Reduces a lot of duplicate code Any modification will affect all references; it must be thoroughly tested before merging changes into referenced branches Can be used in chunks Difficult to understand what a step does if unfamiliar The generated Jenkinsfile will be easier to read Conclusion # Method 1: For single-repository integration, it can be implemented quickly, and most people can get started quickly. Method 2: Scripted pipelines offer few limitations and are suitable for advanced users familiar with Java and Groovy, and those with more complex needs. Method 3: For enterprise-level projects with many repositories that require extensive integration and a desire to learn about shared libraries, this method is recommended.\n","date":"2020-04-20","externalUrl":null,"permalink":"/en/posts/2020/jenkins-troubleshooting/","section":"Posts","summary":"This article introduces three methods to solve the “Method code too large” exception in Jenkins declarative pipelines, including moving steps outside the pipeline, migrating from declarative to scripted pipelines, and using Shared Libraries.","title":"Three Ways to Solve the Jenkins Declarative Pipeline \"Method code too large\" Exception","type":"posts"},{"content":"\u0026ldquo;Quality at Speed\u0026rdquo; 是软件开发中的新规范。\n企业正在朝着 DevOps 方法论和敏捷文化迈进，以加快交付速度并确保产品质量。在 DevOps 中，连续和自动化的交付周期使快速可靠的交付成为可能的基础。\n这导致我们需要适当的持续集成和持续交付（CI/CD）工具。 一个好的 CI/CD 工具可以利用团队当前的工作流程，以最佳利用自动化功能并创建可靠的 CI/CD 管道为团队发展提供所需的动力。\n随着市场上大量 CI/CD 工具的出现，团队可能难以做出艰难的决定来挑选合适的工具。该列表包含市场上最好的 14 种 CI/CD 工具及其主要特性，使你和团队在选择过程中更加轻松。\n以下罗列出了目前市场上最流行的 14 种最佳 CI/CD 工具，希望该列表为你在选择 CI/CD 前提供了足够的信息，更多详细信息你也可以查看官网做更深入的了解。最终结合你的需求以及现有基础架构以及未来潜力和改进的空间是将影响你最终选择的因素，帮助你选择到最适合你的规格的 CI/CD 软件。\nJenkins # Jenkins 是一个开源自动化服务器，在其中进行集中构建和持续集成。它是一个独立的基于 Java 的程序，带有 Windows，macOS，Unix 的操作系统的软件包。Jenkins 支持软件开发项目的构建，部署和自动化，以及成百上千的插件来满足你的需求。它是市场上最具影响力的 CI/CD 工具之一。\nJenkins 主要特性：\n易于在各种操作系统上安装和升级 简单易用的界面 可通过社区提供的巨大插件资源进行扩展 在用户界面中轻松配置环境 支持主从架构的分布式构建 根据表达式构建时间表 在预构建步骤中支持 Shell 和 Windows 命令执行 支持有关构建状态的通知 许可：免费。Jenkins 是一个拥有活跃社区的开源工具。\n主页：https://jenkins.io/\nCircleCI # CircleCI 是一种 CI/CD 工具，支持快速的软件开发和发布。CircleCI 允许从代码构建，测试到部署的整个用户管道自动化。\n你可以将 CircleCI 与 GitHub，GitHub Enterprise 和 Bitbucket 集成，以在提交新代码行时创建内部版本。CircleCI 还可以通过云托管选项托管持续集成，或在私有基础架构的防火墙后面运行。\nCircleCI 主要特性:\n与 Bitbucket，GitHub 和 GitHub Enterprise 集成 使用容器或虚拟机运行构建 简易调试 自动并行化 快速测试 个性化的电子邮件和IM通知 连续和特定于分支机构的部署 高度可定制 自动合并和自定义命令以上传软件包 快速设置和无限构建 许可：Linux 计划从选择不运行任何并行操作开始。开源项目获得了另外三个免费容器。在注册期间，将看到价格以决定所需的计划。\n主页： https://circleci.com/\nTeamCity # TeamCity 是 JetBrains 的构建管理和持续集成服务器。\nTeamCity 是一个持续集成工具，可帮助构建和部署不同类型的项目。 TeamCity 在 Java 环境中运行，并与 Visual Studio 和 IDE 集成。该工具可以安装在 Windows 和 Linux 服务器上，支持 .NET 和开放堆栈项目。\nTeamCity 2019.1 提供了新的UI和本机 GitLab 集成。它还支持 GitLab 和 Bitbucket 服务器拉取请求。该版本包括基于令牌的身份验证，检测，Go测试报告以及 AWS Spot Fleet 请求。\nTeamCity主要特性:\n提供多种方式将父项目的设置和配置重用到子项目 在不同环境下同时运行并行构建 启用运行历史记录构建，查看测试历史记录报告，固定，标记以及将构建添加到收藏夹 易于定制，交互和扩展服务器 保持CI服务器正常运行 灵活的用户管理，用户角色分配，将用户分组，不同的用户身份验证方式以及带有所有用户操作的日志，以透明化服务器上所有活动 许可：TeamCity 是具有免费和专有许可证的商业工具。\n主页： https://www.jetbrains.com/teamcity/\nBamboo # Bamboo 是一个持续集成服务器，可自动执行软件应用程序版本的管理，从而创建了持续交付管道。Bamboo 涵盖了构建和功能测试，分配版本，标记发行版，在生产中部署和激活新版本。\nBamboo主要特性:\n支持多达 100 个远程构建代理 并行运行批次测试并快速获得反馈 创建图像并推送到注册表 每个环境的权限，使开发人员和测试人员可以在生产保持锁定状态的情况下按需部署到他们的环境中 在 Git，Mercurial，SVN Repos 中检测新分支，并将主线的CI方案自动应用于它们 触发器基于在存储库中检测到的更改构建。 推送来自 Bitbucket 的通知，已设置的时间表，另一个构建的完成或其任何组合。 许可：Bamboo 定价层基于代理（Slave）而不是用户，代理越多，花费越多。\n主页：https://www.atlassian.com/software/bamboo\nGitLab # GitLab 是一套用于管理软件开发生命周期各个方面的工具。 核心产品是基于 Web 的 Git 存储库管理器，具有问题跟踪，分析和 Wiki 等功能。\nGitLab 允许你在每次提交或推送时触发构建，运行测试和部署代码。你可以在虚拟机，Docker 容器或另一台服务器上构建作业。\nGitLab主要特性:\n通过分支工具查看，创建和管理代码以及项目数据 通过单个分布式版本控制系统设计，开发和管理代码和项目数据，从而实现业务价值的快速迭代和交付 提供真实性和可伸缩性的单一来源，以便在项目和代码上进行协作 通过自动化源代码的构建，集成和验证，帮助交付团队完全接受CI。 提供容器扫描，静态应用程序安全测试（SAST），动态应用程序安全测试（DAST）和依赖项扫描，以提供安全的应用程序以及许可证合规性 帮助自动化并缩短发布和交付应用程序的时间 许可：GitLab 是一个商业工具和免费软件包。它提供了在 GitLab 或你的本地实例和/或公共云上托管 SaaS 的功能。\n主页：https://about.gitlab.com/\nBuddy # Buddy 是一个 CI/CD 软件，它使用 GitHub，Bitbucket 和 GitLab 的代码构建，测试，部署网站和应用程序。它使用具有预安装语言和框架的 Docker 容器以及 DevOps 来监视和通知操作，并以此为基础进行构建。\nBuddy主要特性:\n易于将基于 Docker 的映像自定义为测试环境 智能变更检测，最新的缓存，并行性和全面的优化 创建，定制和重用构建和测试环境 普通和加密，固定和可设置范围：工作空间，项目，管道，操作 与 Elastic，MariaDB，Memcached，Mongo，PostgreSQL，RabbitMQ，Redis，Selenium Chrome 和 Firefox 关联的服务 实时监控进度和日志，无限历史记录 使用模板进行工作流管理，以克隆，导出和导入管道 一流的Git支持和集成 许可：Buddy 是免费的商业工具。\n主页：https://buddy.works/\nTravis CI # Travis CI 是用于构建和测试项目的CI服务。Travis CI 自动检测新提交并推送到 GitHub 存储库的提交。每次提交新代码后，Travis CI 都会构建项目并相应地运行测试。\n该工具支持许多构建配置和语言，例如 Node，PHP，Python，Java，Perl 等。\nTravis 主要特性:\n快速设置 GitHub项目监控的实时构建视图 拉取请求支持 部署到多个云服务 预装的数据库服务 通过构建时自动部署 为每个版本清理虚拟机 支持 macOS，Linux 和 iOS 支持多种语言，例如 Android，C，C＃，C ++，Java，JavaScript（带有Node.js），Perl，PHP，Python，R，Ruby 等。 许可：Travis CI 是一项托管的 CI/CD 服务。私人项目可以在 travis-ci.com 上进行收费测试。可以在 travis-ci.org 上免费应用开源项目。\n主页：https://travis-ci.com\nCodeship # Codeship 是一个托管平台，可多次支持早期和自动发布软件。通过优化测试和发布流程，它可以帮助软件公司更快地开发更好的产品。\nCodeship 主要特性:\n与所选的任何工具，服务和云环境集成 易于使用。提供快速而全面的开发人员支持。 借助CodeShip的交钥匙环境和简单的UI，使构建和部署工作更快 选择AWS实例大小，CPU和内存的选项 通过通知中心为组织和团队成员设置团队和权限 无缝的第三方集成，智能通知管理和项目仪表板，可提供有关项目及其运行状况的高级概述。 许可：每月最多免费使用100个版本，无限版本从$49/月开始。你可以为更大的实例大小购买更多的并发构建或更多的并行管道。\n主页： https://codeship.com/\nGoCD # GoCD 来自 ThoughtWorks，是一个开放源代码工具，用于构建和发布支持 CI/CD 上的现代基础结构的软件。\n轻松配置相关性以实现快速反馈和按需部署 促进可信构件：每个管道实例都锚定到特定的变更集 提供对端到端工作流程的控制，一目了然地跟踪从提交到部署的更改 容易看到上游和下游 随时部署任何版本 允许将任何已知的良好版本的应用程序部署到你喜欢的任何位置 通过比较内部版本功能获得用于任何部署的简单物料清单 通过 GoCD 模板系统重用管道配置，使配置保持整洁 已经有许多插件 许可：免费和开源\n主页：https://www.gocd.org/\nWercker # 对于正在使用或正在考虑基于 Docker 启动新项目的开发人员，Wercker 可能是一个合适的选择。Wercker 支持组织及其开发团队使用 CI/CD，微服务和 Docker。\n2017 年 4 月 17 日，甲骨文宣布已签署最终协议收购 Wercker。\nWercker 主要特性:\nGit 集成，包括 GitHub，Bitbucket，GitLab 和版本控制 使用 Wercker CLI 在本地复制 SaaS 环境，这有助于在部署之前调试和测试管道 支持 Wercker 的 Docker 集成以构建最少的容器并使尺寸可管理 Walterbot – Wercker 中的聊天机器人 – 允许你与通知交互以更新构建状态 环境变量有助于使敏感信息远离存储库 Wercker 利用关键安全功能（包括源代码保护）来关闭测试日志，受保护的环境变量以及用户和项目的可自定义权限 许可：甲骨文在收购后未提供 Wercker 的价格信息。\n主页：https://app.wercker.com\nSemaphore # Semaphore 是一项托管的 CI/CD 服务，用于测试和部署软件项目。 Semaphore 通过基于拉取请求的开发过程来建立 CI/CD 标准。\nSemaphore 主要特性:\n与 GitHub 集成 自动执行任何连续交付流程 在最快的 CI/CD 平台上运行 自动缩放你的项目，以便你仅需支付使用费用 本机 Docker 支持。测试和部署基于 Docker 的应用程序 提供 Booster –一种功能，用于通过自动并行化Ruby项目的构建来减少测试套件的运行时间 许可：灵活。使用传统的CI服务，你会受到计划容量的限制。同时，Semaphore 2.0 将根据你团队的实际需求进行扩展，因此你无需使用该工具就不必付费\n主页：https://semaphoreci.com/\nNevercode # Nevercode 支持移动应用程序的 CI/CD。它有助于更​​快地构建，测试和发布本机和跨平台应用程序。\nNevercode 主要特性:\n自动配置和设置 测试自动化：单元和UI测试，代码分析，真实设备测试，测试并行化 自动发布：iTunes Connect，Google Play，Crashlytics，T​​estFairy，HockeyApp 你的构建和测试状态的详细概述 许可：灵活。针对不同需求进行持续集成的不同计划。你可以从标准计划中选择，也可以请求根据自己的需求量身定制的计划。\n主页：https://nevercode.io/\nSpinnaker # Spinnaker 是一个多云连续交付平台，支持在不同的云提供商之间发布和部署软件更改，包括 AWS EC2，Kubernetes，Google Compute Engine，Google Kubernetes Engine，Google App Engine 等。\nSpinnaker主要特性:\n创建部署管道，以运行集成和系统测试，旋转服务器组和降低服务器组以及监视部署。通过 Git 事件，Jenkins，Travis CI，Docker，cron 或其他 Spinnaker 管道触发管道 创建和部署不可变映像，以实现更快的部署，更轻松的回滚以及消除难以调试的配置漂移问题 使用它们的指标进行金丝雀分析，将你的发行版与诸如 Datadog，Prometheus，Stackdriver 或 SignalFx 的监视服务相关联 使用Halyard – Spinnaker的CLI管理工具安装，配置和更新你的 Spinnaker 实例 设置电子邮件，Slack，HipChat 或 SMS 的事件通知（通过 Twilio） 许可：开源\n主页：https://www.spinnaker.io/\nBuildbot # Buildbot 是一个基于 Python 的 CI 框架，可自动执行编译和测试周期以验证代码更改，然后在每次更改后自动重建并测试树。因此，可以快速查明构建问题。\nBuildbot 主要特性:\n自动化构建系统，应用程序部署以及复杂软件发布过程的管理 支持跨多个平台的分布式并行执行，与版本控制系统的灵活集成，广泛的状态报告 在各种从属平台上运行构建 任意构建过程并使用 C 和 Python 处理项目 最低主机要求：Python 和 Twisted 注意：Buildbot 将停止支持 Python 2.7，并需要迁移到 Python 3。 许可：开源\n主页：https://buildbot.net/\n英文原文\n","date":"2020-03-29","externalUrl":null,"permalink":"/posts/2020/ci-cd-tools/","section":"Posts","summary":"本文列出了市场上最流行的 14 种 CI/CD 工具，包括 Jenkins、CircleCI、TeamCity 等，并介绍了它们的主要特性和使用场景。","title":"2021 年务必知道的最好用的 14 款 CI/CD 工具","type":"posts"},{"content":" DevOps术语和定义 # 什么是DevOps\n用最简单的术语来说，DevOps是产品开发过程中开发（Dev）和运营（Ops）团队之间的灰色区域。 DevOps是一种在产品开发周期中强调沟通，集成和协作的文化。因此，它消除了软件开发团队和运营团队之间的孤岛，使他们能够快速，连续地集成和部署产品。\n什么是持续集成\n持续集成（Continuous integration，缩写为 CI）是一种软件开发实践，团队开发成员经常集成他们的工作。利用自动测试来验证并断言其代码不会与现有代码库产生冲突。理想情况下，代码更改应该每天在CI工具的帮助下，在每次提交时进行自动化构建（包括编译，发布，自动化测试），从而尽早地发现集成错误，以确保合并的代码没有破坏主分支。\n什么是持续交付\n持续交付（Continuous delivery，缩写为 CD）以及持续集成为交付代码包提供了完整的流程。在此阶段，将使用自动构建工具来编译工件，并使其准备好交付给最终用户。它的目标在于让软件的构建、测试与发布变得更快以及更频繁。这种方式可以减少软件开发的成本与时间，减少风险。\n什么是持续部署\n持续部署（Continuous deployment）通过集成新的代码更改并将其自动交付到发布分支，从而将持续交付提升到一个新的水平。 更具体地说，一旦更新通过了生产流程的所有阶段，便将它们直接部署到最终用户，而无需人工干预。因此，要成功利用连续部署，软件工件必须先经过严格建立的自动化测试和工具，然后才能部署到生产环境中。\n什么是持续测试及其好处\n连续测试是一种在软件交付管道中尽早、逐步和适当地应用自动化测试的实践。在典型的CI/CD工作流程中，将小批量发布构建。因此，为每个交付手动执行测试用例是不切实际的。自动化的连续测试消除了手动步骤，并将其转变为自动化例程，从而减少了人工。因此，对于DevOps文化而言，自动连续测试至关重要。\n持续测试的好处\n确保构建的质量和速度。 支持更快的软件交付和持续的反馈机制。 一旦系统中出现错误，请立即检测。 降低业务风险。 在潜在问题变成实际问题之前进行评估。 什么是版本控制及其用途？\n版本控制（或源代码控制）是一个存储库，源代码中的所有更改都始终存储在这个代码仓库中。版本控件提供了代码开发的操作历史记录，追踪文件的变更内容、时间、人等信息忠实地了记录下来。版本控制是持续集成和持续构建的源头。\n什么是Git？\nGit是一个分布式版本控制系统，可跟踪代码存储库中的更改。利用GitHub流，Git围绕着一个基于分支的工作流，该工作流随着团队项目的不断发展而简化了团队协作。\n实施DevOps的原因 # DevOps为什么重要？DevOps如何使团队在软件交付方面受益？\n在当今的数字化世界中，组织必须重塑其产品部署系统，使其更强大，更灵活，以跟上竞争的步伐。\n这就是DevOps概念出现的地方。DevOps在为整个软件开发管道（从构思到部署，再到最终用户）产生移动性和敏捷性方面发挥着至关重要的作用。DevOps是将不断更新和改进产品的更简化，更高效的流程整合在一起的解决方案。\n解释DevOps对开发人员有何帮助\n在没有DevOps的世界中，开发人员的工作流程将首先建立新代码，交付并集成它们，然后，操作团队有责任打包和部署代码。之后，他们将不得不等待反馈。而且如果出现问题，由于错误，他们将不得不重新执行一次。沿线是项目中涉及的不同团队之间的无数手动沟通。\n由于CI/CD实践已经合并并自动化了其余任务，因此应用DevOps可以将开发人员的任务简化为仅构建代码。随着流程变得更加透明并且所有团队成员都可以访问，将工程团队和运营团队相结合有助于建立更好的沟通和协作。\n为什么DevOps最近在软件交付方面变得越来越流行？\nDevOps在过去几年中受到关注，主要是因为它能够简化组织运营的开发，测试和部署流程，并将其转化为业务价值。\n技术发展迅速。因此，组织必须采用一种新的工作流程-DevOps和Agile方法-来简化和刺激其运营，而不能落后于其他公司。DevOps的功能通过Facebook和Netflix的持续部署方法所取得的成功得到了清晰体现，该方法成功地促进了其增长，而没有中断正在进行的运营。\nCI/CD有什么好处？\nCI和CD的结合将所有代码更改统一到一个单一的存储库中，并通过自动化测试运行它们，从而在所有阶段全面开发产品，并随时准备部署。\nCI/CD使组织能够按照客户期望的那样快速，高效和自动地推出产品更新。\n简而言之，精心规划和执行良好的CI/CD管道可加快发布速度和可靠性，同时减轻产品的代码更改和缺陷。这最终将导致更高的客户满意度。\n持续交付有什么好处？\n通过手动发布代码更改，团队可以完全控制产品。 在某些情况下，该产品的新版本将更有希望：具有明确业务目的的促销策略。\n通过自动执行重复性和平凡的任务，IT专业人员可以拥有更多的思考能力来专注于改进产品，而不必担心集成进度。\n持续部署有哪些好处？\n通过持续部署，开发人员可以完全专注于产品，因为他们在管道中的最后任务是审查拉取请求并将其合并到分支。通过在自动测试后立即发布新功能和修复，此方法可实现快速部署并缩短部署持续时间。\n客户将是评估每个版本质量的人。新版本的错误修复更易于处理，因为现在每个版本都以小批量交付。\n如何有效实施DevOps # 定义典型的DevOps工作流程\n典型的DevOps工作流程可以简化为4个阶段：\n版本控制：这是存储和管理源代码的阶段。 版本控件包含代码的不同版本。 持续集成：在这一步中，开发人员开始构建组件，并对其进行编译，验证，然后通过代码审查，单元测试和集成测试进行测试。 持续交付：这是持续集成的下一个层次，其中发布和测试过程是完全自动化的。 CD确保将新版本快速，可持续地交付给最终用户。 持续部署：应用程序成功通过所有测试要求后，将自动部署到生产服务器上以进行发布，而无需任何人工干预。 DevOps的核心操作是什么？\nDevOps在开发和基础架构方面的核心运营是：\nSoftware development:\nCode building Code coverage Unit testing Packaging Deployment Infrastructure:\nProvisioning Configuration Orchestration Deployment 在实施DevOps之前，团队需要考虑哪些预防措施？\n当组织尝试应用这种新方法时，对DevOps做法存在一些误解，有可能导致悲惨的失败：\nDevOps不仅仅是简单地应用新工具和/或组建新的“部门”并期望它能正常工作。实际上，DevOps被认为是一种文化，开发团队和运营团队遵循共同的框架。 企业没有为其DevOps实践定义清晰的愿景。对开发团队和运营团队而言，应用DevOps计划是一项显着的变化。因此，拥有明确的路线图，将DevOps集成到您的组织中的目标和期望将消除任何混乱，并从早期就提供清晰的指导方针。 在整个组织中应用DevOps做法之后，管理团队需要建立持续的学习和改进文化。系统中的故障和问题应被视为团队从错误中学习并防止这些错误再次发生的宝贵媒介。 SCM团队在DevOps中扮演什么角色？\n软件配置管理（SCM）是跟踪和保留开发环境记录的实践，包括在操作系统中进行的所有更改和调整。\n在DevOps中，将SCM作为代码构建在基础架构即代码实践的保护下。\nSCM为开发人员简化了任务，因为他们不再需要手动管理配置过程。 现在，此过程以机器可读的形式构建，并且会自动复制和标准化。\n质量保证（QA）团队在DevOps中扮演什么角色？\n随着DevOps实践在创新组织中变得越来越受欢迎，QA团队的职责和相关性在当今的自动化世界中已显示出下降的迹象。\n但是，这可以被认为是神话。 DevOps的增加并不等于QA角色的结束。 这仅意味着他们的工作环境和所需的专业知识正在发生变化。 因此，他们的主要重点是专业发展以跟上这种不断变化的趋势。\n在DevOps中，质量保证团队在确保连续交付实践的稳定性以及执行自动重复性测试无法完成的探索性测试任务方面发挥战略作用。 他们在评估测试和检测最有价值的测试方面的见识仍然在缓解发布的最后步骤中的错误方面起着至关重要的作用。\nDevOps使用哪些工具？ 描述您使用任何这些工具的经验\n在典型的DevOps生命周期中，有不同的工具来支持产品开发的不同阶段。 因此，用于DevOps的最常用工具可以分为6个关键阶段：\n持续开发：Git, SVN, Mercurial, CVS, Jira 持续整合：Jenkins, Bamboo, CircleCI 持续交付：Nexus, Archiva, Tomcat 持续部署：Puppet, Chef, Docker 持续监控：Splunk, ELK Stack, Nagios 连续测试：Selenium，Katalon Studio\n如何在DevOps实践中进行变更管理\n典型的变更管理方法需要与DevOps的现代实践适当集成。 第一步是将变更集中到一个平台中，以简化变更，问题和事件管理流程。\n接下来，企业应建立高透明度标准，以确保每个人都在同一页面上，并确保内部信息和沟通的准确性。\n对即将到来的变更进行分层并建立可靠的策略，将有助于最大程度地降低风险并缩短变更周期。 最后，组织应将自动化应用到其流程中，并与DevOps软件集成。\n如何有效实施CI/CD # CI/CD的一些核心组件是什么？\n稳定的CI/CD管道需要用作版本控制系统的存储库管理工具。 这样开发人员就可以跟踪软件版本中的更改。\n在版本控制系统中，开发人员还可以在项目上进行协作，在版本之间进行比较并消除他们犯的任何错误，从而减轻对所有团队成员的干扰。\n连续测试和自动化测试是成功建立无缝CI / CD管道的两个最关键的关键。 自动化测试必须集成到所有产品开发阶段（包括单元测试，集成测试和系统测试），以涵盖所有功能，例如性能，可用性，性能，负载，压力和安全性。\nCI/CD的一些常见做法是什么？\n以下是建立有效的CI / CD管道的一些最佳实践：\n发展DevOps文化 实施和利用持续集成 以相同的方式部署到每个环境 失败并重新启动管道 应用版本控制 将数据库包含在管道中 监控您的持续交付流程 使您的CD流水线流畅 什么时候是实施CI/CD的最佳时间？\n向DevOps的过渡需要彻底重塑其软件开发文化，包括工作流，组织结构和基础架构。 因此，组织必须为实施DevOps的重大变化做好准备。\n有哪些常见的CI/CD服务器\nVisual Studio Visual Studio支持具有敏捷计划，源代码控制，包管理，测试和发布自动化以及持续监视的完整开发的DevOps系统。\nTeamCity TeamCity是一款智能CI服务器，可提供框架支持和代码覆盖，而无需安装任何额外的插件，也无需模块来构建脚本。\nJenkins 它是一个独立的CI服务器，通过共享管道和错误跟踪功能支持开发和运营团队之间的协作。 它也可以与数百个仪表板插件结合使用。\nGitLab GitLab的用户可以自定义平台，以进行有效的持续集成和部署。 GitLab帮助CI / CD团队加快代码交付，错误识别和恢复程序的速度。\nBamboo Bamboo是用于产品发布管理自动化的连续集成服务器。 Bamboo跟踪所有工具上的所有部署，并实时传达错误。\n描述持续集成的有效工作流程\n实施持续集成的成功工作流程包括以下实践：\n实施和维护项目源代码的存储库 自动化构建和集成 使构建自检 每天将更改提交到基准 构建所有添加到基准的提交 保持快速构建 在生产环境的克隆中运行测试 轻松获取最新交付物 使构建结果易于所有人监视 自动化部署 每种术语之间的差异 # 敏捷和DevOps之间有哪些主要区别？\n基本上，DevOps和敏捷是相互补充的。敏捷更加关注开发新软件和以更有效的方式管理复杂过程的价值和原则。同时，DevOps旨在增强由开发人员和运营团队组成的不同团队之间的沟通，集成和协作。\n它需要采用敏捷方法和DevOps方法来形成无缝工作的产品开发生命周期：敏捷原理有助于塑造和引导正确的开发方向，而DevOps利用这些工具来确保将产品完全交付给客户。\n持续集成，持续交付和持续部署之间有什么区别？\n持续集成（CI）是一种将代码版本连续集成到共享存储库中的实践。这种做法可确保自动测试新代码，并能快速检测和修复错误。\n持续交付使CI进一步迈出了一步，确保集成后，随时可以在一个按钮内就可以释放代码库。因此，CI可以视为持续交付的先决条件，这是CI / CD管道的另一个重要组成部分。\n对于连续部署，不需要任何手动步骤。这些代码通过测试后，便会自动推送到生产环境。\n所有这三个组件：持续集成，持续交付和持续部署是实施DevOps的重要阶段。\n一方面，连续交付更适合于活跃用户已经存在的应用程序，这样事情就可以变慢一些并进行更好的调整。另一方面，如果您打算发布一个全新的软件并且将整个过程指定为完全自动化的，则连续部署是您产品的更合适选择。\n连续交付和连续部署之间有哪些根本区别？\n在连续交付的情况下，主分支中的代码始终可以手动部署。 通过这种做法，开发团队可以决定何时发布新的更改或功能，以最大程度地使组织受益。\n同时，连续部署将在测试阶段之后立即将代码中的所有更新和修补程序自动部署到生产环境中，而无需任何人工干预。\n持续集成和持续交付之间的区别是什么？\n持续集成有助于确保软件组件紧密协作。 整合应该经常进行； 最好每小时或每天一次。 持续集成有助于提高代码提交的频率，并降低连接多个开发人员的代码的复杂性。 最终，此过程减少了不兼容代码和冗余工作的机会。\n持续交付是CI / CD流程中的下一步。 由于代码不断集成到共享存储库中，因此可以持续测试该代码。 在等待代码完成之前，没有间隙可以进行测试。 这样可确保找到尽可能多的错误，然后将其连续交付给生产。\nDevOps和持续交付之间有什么区别？\nDevOps更像是一种组织和文化方法，可促进工程团队和运营团队之间的协作和沟通。\n同时，持续交付是成功将DevOps实施到产品开发工作流程中的重要因素。 持续交付实践有助于使新发行的版本更加乏味和可靠，并建立更加无缝和短的流程。\nDevOps的主要目的是有效地结合Dev和Ops角色，消除所有孤岛，并实现独立于持续交付实践的业务目标。\n另一方面，如果已经有DevOps流程，则连续交付效果最佳。 因此，它扩大了协作并简化了组织的统一产品开发周期。\n敏捷，精益IT和DevOps之间有什么区别？\n敏捷是仅专注于软件开发的方法。 敏捷旨在迭代开发，建立持续交付，缩短反馈循环以及在整个软件开发生命周期（SDLC）中改善团队协作。\n精益IT是一种旨在简化产品开发周期价值流的方法。 精益专注于消除不必要的过程，这些过程不会增加价值，并创建流程来优化价值流。\nDevOps专注于开发和部署-产品开发过程的Dev和Ops。 其目标是有效整合自动化工具和IT专业人员之间的角色，以实现更简化和自动化的流程。\n准备好在下一次DevOps面试中取得成功吗？ # 目前有无数的DevOps面试问题，我们目前还不能完全解决。但是，我们希望这些问题和建议的答案能使您掌握DevOps和CI/CD的大量知识，并成功地帮助您完成面试。\n将来，我们将在此列表中添加更多内容。 因此，如果您对此主题有任何建议，请随时与我们联系。最后，我们祝您在测试事业中一切顺利！\n","date":"2020-03-29","externalUrl":null,"permalink":"/posts/2020/top-30-devops-interview-questions/","section":"Posts","summary":"本文列出了 DevOps 领域的 30 多个常见面试问题，涵盖 DevOps 基础知识、CI/CD、DevOps 工具和实践等方面，帮助求职者准备 DevOps 面试。","title":"DevOps Top 30+ 面试问题","type":"posts"},{"content":"","date":"2020-03-29","externalUrl":null,"permalink":"/tags/interview/","section":"标签","summary":"","title":"Interview","type":"tags"},{"content":"Maintaining a Git repository often involves reducing its size. If you\u0026rsquo;ve imported a repository from another version control system, you may need to clean up unnecessary files after the import. This article mainly discusses how to remove unwanted files from a Git repository.\nPlease exercise extreme caution\u0026hellip;\nThe steps and tools in this article employ advanced techniques involving destructive operations. Ensure you carefully read and back up your repository before you begin. The easiest way to create a backup is to clone your repository using the --mirror flag, and then package and compress the entire cloned directory. With this backup, you can restore from the backup repository if key elements of your repository are accidentally corrupted during maintenance.\nRemember, repository maintenance can be disruptive to repository users. Communicating with your team or repository followers is essential. Ensure everyone has checked out their code and agrees to halt development during repository maintenance.\nUnderstanding File Removal from Git History # Recall that cloning a repository clones the entire history—including all versions of every source code file. If a user commits a large file, such as a JAR, then every subsequent clone includes that file. Even if the user eventually deletes the file in a later commit, the file remains in the repository\u0026rsquo;s history. To completely remove the file from your repository, you must:\nDelete the file from your project\u0026rsquo;s current file tree; Remove the file from the repository\u0026rsquo;s history—rewriting Git history to remove the file from all commits containing it; Delete all reflog history pointing to the old commit history; Repack the repository using git gc to garbage collect now-unused data. Git\u0026rsquo;s gc (garbage collection) will remove all actual unused data or data not referenced in any of your branches or tags in the repository. For it to work, we need to rewrite all Git repository history containing the unwanted files, the repository will no longer reference it, and git gc will discard all unused data.\nRewriting repository history is a tricky business because each commit depends on its parent commits, so even a small change alters every subsequent commit\u0026rsquo;s hash. There are two automated tools that can help you do this:\nBFG Repo Cleaner—Fast, simple, and easy to use; requires Java 6 or later runtime environment. git filter-branch—Powerful, more cumbersome to configure, slower for larger repositories, and is part of the core Git suite. Remember, after rewriting history, whether you use BFG or filter-branch, you must delete reflog entries pointing to the old history and finally run the garbage collector to remove the old data.\nRewriting History with BFG # BFG is specifically designed to remove unwanted data like large files or passwords from a Git repository, so it has a simple flag to remove large historical files (not in the current commit): --strip-blobs-bigger-than\nBFG Download\njava -jar bfg.jar --strip-blobs-bigger-than 100M Any files larger than 100MB (excluding files in your recent commits—as BFG protects your latest commits by default) will be removed from your Git repository\u0026rsquo;s history. You can also specify files by name:\njava -jar bfg.jar --delete-files *.mp4 BFG is 10–1000 times faster than git filter-branch and generally easier to use—see the full usage instructions and examples for more details.\nAlternatively, Rewriting History with git filter-branch # The filter-branch command can rewrite a Git repository\u0026rsquo;s history, like BFG, but the process is slower and more manual. If you don\u0026rsquo;t know where these large files are, your first step will be to find them:\nManually Finding Large Files in Your Git Repository # Antony Stubbs wrote a BASH script that does this well. The script inspects the contents of your pack files and lists large files. Before you start deleting files, do the following to obtain and install this script:\nDownload the script to your local system.\nPlace it in an easily accessible location relative to your Git repository.\nMake the script executable:\nchmod 777 git_find_big.sh Clone the repository to your local system.\nChange the current directory to your repository\u0026rsquo;s root.\nManually run the Git garbage collector:\ngit gc --auto Find the size of the .git folder:\n# Note the file size for later reference du -hs .git/objects 45M .git/objects Run git_find_big.sh to list large files in your repository:\ngit_find_big.sh All sizes are in kB\u0026#39;s. The pack column is the size of the object, compressed, inside the pack file. size pack SHA location 592 580 e3117f48bc305dd1f5ae0df3419a0ce2d9617336 media/img/emojis.jar 550 169 b594a7f59ba7ba9daebb20447a87ea4357874f43 media/js/aui/aui-dependencies.jar 518 514 22f7f9a84905aaec019dae9ea1279a9450277130 media/images/screenshots/issue-tracker-wiki.jar 337 92 1fd8ac97c9fecf74ba6246eacef8288e89b4bff5 media/js/lib/bundle.js 240 239 e0c26d9959bd583e5ef32b6206fc8abe5fea8624 media/img/featuretour/heroshot.png The large files are all JAR files. The pack size column is the most relevant. aui-dependencies.jar compresses to 169kb, but emojis.jar only compresses to 500kb. emojis.jar is a candidate for deletion.\nRunning filter-branch # You pass this command a filter that modifies the Git index. For example, a filter can delete each retrieved commit. This usage is:\ngit filter-branch --index-filter \u0026#39;git rm --cached --ignore-unmatch\u0026amp;nbsp; _pathname_ \u0026#39; commitHASH The --index-filter option modifies the repository\u0026rsquo;s index, and the --cached option deletes files from the index, not the disk. This is faster because you don\u0026rsquo;t need to check each revision before running the filter. The ignore-unmatch option in git rm prevents the command from failing when trying to remove a non-existent file pathname. By specifying a commit HASH, you delete pathname from every commit starting at that HASH. To delete from the beginning, omit this parameter or specify HEAD.\nIf your large files are in different branches, you will need to delete each file by name. If the large files are all on a single branch, you can delete the branch itself.\nOption 1: Deleting Files by Filename # Use the following steps to delete large files:\nUse the following command to delete the first large file you found:\ngit filter-branch --index-filter \u0026#39;git rm --cached --ignore-unmatch filename\u0026#39; HEAD Repeat step 1 for each remaining large file.\nUpdate references in your repository. filter-branch creates backups of your original references under refs/original/. Once you are sure you have deleted the correct files, run the following to remove the backups and allow the garbage collector to reclaim large objects:\ngit for-each-ref --format=\u0026#34;%(refname)\u0026#34; refs/original/ | xargs -n 1 git update-ref -d Option 2: Deleting a Branch Directly # If all your large files are on a single branch, you can delete the branch directly. Deleting the branch automatically removes all references.\nDelete the branch:\ngit branch -D PROJ567bugfix Remove all reflog references to the branch:\ngit reflog expire --expire=now PROJ567bugfix Garbage Collecting Unused Data # Delete all reflog references from now to the past (unless you explicitly operated only on one branch):\ngit reflog expire --expire=now --all Repack the repository by running the garbage collector and removing old objects:\ngit gc --prune=now Push all your changes back to the repository:\ngit push --all --force Ensure that all your tags are also up-to-date:\ngit push --tags --force Original Article\n","date":"2020-03-21","externalUrl":null,"permalink":"/en/posts/2020/maintaining-git-repository/","section":"Posts","summary":"How to remove unnecessary files and history from a Git repository to reduce its size, providing two methods using BFG Repo Cleaner or git filter-branch.","title":"How to Slim Down Your Git Repository","type":"posts"},{"content":"Due to historical reasons, our current product\u0026rsquo;s code repository has many warnings that cannot be resolved immediately. Only by continuously enriching automated test cases to ensure the final quality control can we proceed with the orderly repair of warnings. Before that, how to effectively prevent the introduction of more warnings is what we should be doing now.\nTherefore, I want to integrate C/C++ static code scanning into the Pull Request phase. However, many tools related to C/C++ are often commercial, such as SonarQube, which is our first choice. The Community version does not support C/C++ code scanning; only the Developer and Enterprise paid versions support it. Before static code scanning brings any return on investment, blindly paying will only increase the cost for the product. Therefore, I decided to look for other open-source tools as a replacement.\nFinally, I chose CPPCheck, mainly for the following reasons:\nIt\u0026rsquo;s one of the few open-source static code scanning tools for C/C++. It can be integrated with Jenkins, allowing you to view the report in Jenkins. It supports Jenkins Pipeline. This article documents my investigation and usage experience. If you have similar needs, this might provide some reference.\nInstalling Cppcheck # Installation on Linux\nsudo yum install cppcheck.x86_64 For installation on other platforms, please refer to the cppcheck official website.\nIf you can\u0026rsquo;t install it with a single command on Linux, you can also build cppcheck from source code. The following steps show how to manually build a cppcheck executable file from the source code:\ncd opt \u0026amp;\u0026amp; mkdir cppcheck \u0026amp;\u0026amp; cd cppcheck # Download the code wget https://github.com/danmar/cppcheck/archive/1.90.tar.gz # Extract tar -xvf 1.90.tar.gz # Make build cd cppcheck-1.90 mkdir build cd build cmake .. cmake --build . # link sudo ln -s /opt/cppcheck/cppcheck-1.90/build/bin/cppcheck /usr/bin/cppcheck # Check if the installation was successful which cppcheck /usr/bin/cppcheck cppcheck --version Cppcheck 1.90 Using cppcheck for Static Code Scanning # Before integrating with Jenkins, let\u0026rsquo;s see how to use this tool. By consulting the Cppcheck official documentation, the general usage is as follows:\n# For example, scan the code under the src/public and src/themes directories and output the results to cppcheck.xml cppcheck src/public src/themes --xml 2\u0026gt; cppcheck.xml Integrating Cppcheck with Jenkins # First, download the Cppcheck Jenkins plugin. The following code publishCppcheck pattern: 'cppcheck.xml' was generated through Pipeline Syntax.\nHowever, when reading the XML file to display the report, I encountered two problems:\nProblem 1: The analysis of cppcheck.xml succeeded on some Linux machines but failed on others. I suspect this is due to different JDK versions. I also found this issue on Jenkins JIRA JENKINS-60077, but it hasn\u0026rsquo;t been resolved yet.\nThe main reason I didn\u0026rsquo;t continue trying to solve Problem 1 is that it has a more fatal flaw, as described below.\nProblem 2: I cannot directly view the code through the Cppcheck Results report. Even if problems are scanned, I still need to check the specific problems in Git or the local IDE, which greatly reduces efficiency.\n# An error occurs when viewing the code file Can\u0026#39;t read file: Can\u0026#39;t access the file: file:/disk1/agent/workspace/cppcheck-ud113/src/public/dummy/err_printf.c The official ticket also records this issue JENKINS-42613 and JENKINS-54209. JENKINS-42613 is waiting to be merged, and as of this writing, it is still unresolved.\nFinally, I found that Warnings Next Generation plugin will replace the entire Jenkins static analysis suite, including plugins like Android Lint, CheckStyle, Dry, FindBugs, PMD, Warnings, Static Analysis Utilities, Static Analysis Collector. Finally, the Warnings Next Generation plugin solved the problem of report display.\nHere, you can generate the code for reading the report through Pipeline Syntax: recordIssues(tools: [cppCheck(pattern: 'cppcheck.xml')])\nFor more information on using the Warnings Next Generation plugin, please refer to the documentation.\nFinal Pipeline Example # pipeline{ agent { node { label \u0026#39;cppcheck\u0026#39; customWorkspace \u0026#34;/agent/workspace/cppcheck\u0026#34; } } parameters { string(name: \u0026#39;Branch\u0026#39;, defaultValue: \u0026#39;develop\u0026#39;, description: \u0026#39;Which branch do you want to do cppcheck?\u0026#39;) } options { timestamps () buildDiscarder(logRotator(numToKeepStr:\u0026#39;50\u0026#39;)) } stage(\u0026#34;Checkout\u0026#34;){ steps{ checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/${Branch}\u0026#39;]], browser: [$class: \u0026#39;BitbucketWeb\u0026#39;, repoUrl: \u0026#39;https://git.yourcompany.com/projects/repos/cppcheck-example/browse\u0026#39;], doGenerateSubmoduleConfigurations: false, extensions: [ [$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#39;**\u0026#39;], [$class: \u0026#39;CheckoutOption\u0026#39;, timeout: 30], [$class: \u0026#39;CloneOption\u0026#39;, depth: 1, noTags: false, reference: \u0026#39;\u0026#39;, shallow: true, timeout: 30]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;CREDENTIALS_ID\u0026#39;, url: \u0026#39;https://git.yourcompany.com/scm/cppcheck-example.git\u0026#39;]]]) } } stage(\u0026#34;Cppcheck\u0026#34;){ steps{ script { sh \u0026#39;cppcheck src/public src/themes --xml 2\u0026gt; cppcheck.xml\u0026#39; } } } stage(\u0026#39;Publish results\u0026#39;){ steps { recordIssues(tools: [cppCheck(pattern: \u0026#39;cppcheck.xml\u0026#39;)]) } } } Report Display # I applied CPPCheck to each Pull Request. When a developer submits new code, CPPCheck scans the code and compares it with the previous history. When CPPCheck executes successfully and generates a report, a button appears, which you can click to access it.\nAfter opening it, you will see the scan results for the current branch code.\nCPPCheck uses three dimensions to display static code scan results:\nSeverity Distribution: This is divided into three levels: High, Normal, and Low.\nReference Comparison: This compares the data with previous data. New issues are displayed as New, existing issues as Outstanding, and resolved issues as Fixed.\nHistory: This displays a historical trend as the code is added and modified.\nNote: The cppcheck-related XML files are stored on the Jenkins master. Only when the current Jenkins Job is manually deleted will the cppcheck XML files be deleted.\n-sh-4.2$ ls -l cppcheck* -rw-r--r-- 1 jenkins jenkins 418591 Feb 27 05:54 cppcheck-blames.xml -rw-r--r-- 1 jenkins jenkins 219 Feb 27 05:54 cppcheck-fixed-issues.xml -rw-r--r-- 1 jenkins jenkins 142298 Feb 27 05:54 cppcheck-forensics.xml -rw-r--r-- 1 jenkins jenkins 219 Feb 27 05:54 cppcheck-new-issues.xml -rw-r--r-- 1 jenkins jenkins 488636 Feb 27 05:54 cppcheck-outstanding-issues.xml Clicking the corresponding link will directly jump to the specific code warning location.\nIsn\u0026rsquo;t it pretty good?\n","date":"2020-02-16","externalUrl":null,"permalink":"/en/posts/2020/cppcheck/","section":"Posts","summary":"This article introduces the installation, usage, and integration of Cppcheck with Jenkins to improve C/C++ code quality and static analysis capabilities.","title":"A Free C/C++ Static Code Analysis Tool—Cppcheck—Integrated with Jenkins","type":"posts"},{"content":"","date":"2020-02-16","externalUrl":null,"permalink":"/en/tags/cppcheck/","section":"Tags","summary":"","title":"Cppcheck","type":"tags"},{"content":"","date":"2020-02-05","externalUrl":null,"permalink":"/tags/hp-ux/","section":"标签","summary":"","title":"HP-UX","type":"tags"},{"content":" 安装 Java8 # 安装包下载链接是 https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=JDKJRE8018\n需要先注册，然后登陆后才能下载，我下载的是 Itanium_JDK_8.0.18_June_2019_Z7550-96733_java8_18018_ia.depot\n在线安装文档 https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-c04481894\nswinstall -s /tmp/Itanium_JDK_8.0.18_June_2019_Z7550-96733_java8_18018_ia.depot # if swinstall not found /usr/sbin/swinstall -s /tmp/Itanium_JDK_8.0.18_June_2019_Z7550-96733_java8_18018_ia.depot 安装成功后，会在根目录 opt 下多了一个 java8 目录，检查下 java 版本：\nbash-5.0$ pwd /opt/java8 bash-5.0$ cd bin bash-5.0$ java -version java version \u0026#34;1.8.0.18-hp-ux\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0.18-hp-ux-b1) Java HotSpot(TM) Server VM (build 25.18-b1, mixed mode) 创建软连接\nsudo ln -s /opt/java8/bin/java /bin/java 安装 gzip 和 gunzip # # 安装 gzip /usr/bin/sudo /usr/local/bin/depothelper gzip 如果你机器上已经有 zip 和 gunzip 了，只需要软连接一下即可，防止出现命令找不到的问题\n/usr/bin/sudo ln -s /usr/contrib/bin/gzip /usr/bin/gzip /usr/bin/sudo ln -s /usr/contrib/bin/gunzip /usr/bin/gunzip Can not use bash in HP-UX # For example, when you run bash command, you have the following error:\n$ bash /usr/lib/hpux64/dld.so: Unable to find library \u0026#39;libtermcap.so\u0026#39;. Here is the solution：https://community.hpe.com/t5/HP-UX-General/Unable-to-use-bash-for-ia-machine-11-23/m-p/3980789#M128592\nIt bcasue the LIBTERMCAP is not installed, you can go here to see bash\u0026rsquo;s dependencies include gettext libiconv termcap, etc.\nHere are two very useful commands of install and uninstall.\nDownload bash to command depothelper\ndepothelper bash If you wang to remove the package on your HP-UX system, you can run the command\nsudo /usr/sbin/swremove [package-name]\n","date":"2020-02-05","externalUrl":null,"permalink":"/posts/2020/hpxu-tips/","section":"Posts","summary":"本文介绍了在 HP-UX 系统上安装 Java 8、gzip 和 gunzip 的方法，以及如何解决 HP-UX 上使用 bash 时遇到的库依赖问题。","title":"HP-UX 安装工具以及一些使用总结","type":"posts"},{"content":" ls 命令 # 列出当前目录的文件和文件夹。参数:\n-l 列出时显示详细信息\n-a 显示所有文件，包括隐藏的和不隐藏的\n可以组合使用，像这样\nls -la cp 命令 # 将源文件复制到目标。参数：\n-i 交互模式意味着等待确认，如果目标上有文件将被覆盖。\n-r 递归复制，意味着包含子目录（如果有的话）。\ncp –ir source_dir target_dir /tmp 空间不够怎么办 # 在 /etc/fstab 文件里增加一行\nsudo vi /etc/fstab # 添加如下一行 tmpfs /tmp tmpfs defaults,size=4G 0 0 重启之后，df -h 查看，/tmp 目录已经就变成 4G 了。\nMore, Refer to these links\nhttps://likegeeks.com/main-linux-commands-easy-guide/ https://dzone.com/articles/most-useful-linux-command-line-tricks ","date":"2020-02-05","externalUrl":null,"permalink":"/posts/2020/linux-tips/","section":"Posts","summary":"本文介绍了一些最有用的 Linux 命令行技巧，以提高开发和运维的效率。","title":"最有用的 Linux 命令行技巧","type":"posts"},{"content":"上一篇（GitStats - Git 历史统计信息工具），我已经给老板提供了他想看的所有仓库的 Git 提交历史分析报告了，并且把报告都部署到了一台虚拟机的 tomcat 上了，老板可以通过网址访问直接查看每个仓库的分析报告了，看看谁的贡献大，谁活跃，谁偷懒了，谁周末写代码了（这里不鼓励 996）。\n最近老板提需求了。\n老板：你弄个这个网址的数据咋不更新呢？报告上咋没见你们提交代码呢？ 小开：老板儿，您看到这些一个个仓库的数据都是小开我人肉手动生成的，要不您给我点时间，我来做个自动化任务吧。\n我这么积极主动，不是我奉承老板，我心里也知道老板如果觉得 Git Stats 这个工具好用，肯定希望看到的分析报告是最新的。既然老板先提了，那我就别磨蹭了，赶紧干吧。\n不过用啥实现呢？肯定是 Jenkins 了。一来我已经在 Jenkins 上做了很多的自动化任务了，轻车熟路；二来使用同一套系统不但可以减少繁多的系统入口，降低学习成本，也提高 Jenkins 服务器的利用率。\n设身处地的考虑了下老板的使用需求，他肯定不希望自己去 Jenkins 服务器上去运行 Job 来生成这个Git 仓库的多维度代码分析报告，那么，如果我是老板，我希望：\n这个 Jenkins 任务要定期执行，不需要太频繁，一周更新一次就行； 另外还要支持对单独仓库的单独执行，一旦老板要立即马上查看某个仓库的的分析报告呢。 最后实现的效果如下：\n手动执行 # 老板可以勾选他最关心的代码仓库进行更新\n每周末定时执行 # 老板在周一上班就能看到最新的分析数据了，可以看到这个任务 Started by timer\n最终的 Jenkinsfile 是这样的 # pipeline{ agent{ node { label \u0026#39;main-slave\u0026#39; customWorkspace \u0026#34;/workspace/gitstats\u0026#34; } } environment { USER_CRE = credentials(\u0026#34;d1cbab74-823d-41aa-abb7\u0026#34;) webapproot = \u0026#34;/workspace/apache-tomcat-7.0.99/webapps/gitstats\u0026#34; } parameters { booleanParam(defaultValue: true, name: \u0026#39;repo1\u0026#39;, summary: \u0026#39;uncheck to disable [repo1]\u0026#39;) booleanParam(defaultValue: true, name: \u0026#39;repo2\u0026#39;, summary: \u0026#39;uncheck to disable [repo2]\u0026#39;) } triggers { cron \u0026#39;0 3 * * 7\u0026#39; # 每周日早上进行定时运行，因此此时机器是空闲的。 } options { buildDiscarder(logRotator(numToKeepStr:\u0026#39;10\u0026#39;)) } stages{ stage(\u0026#34;Checkout gitstats\u0026#34;){ steps{ # 准备存放 html 报告目录 sh \u0026#34;mkdir -p html\u0026#34; # 下载 gitstats 代码 sh \u0026#34;rm -rf gitstats \u0026amp;\u0026amp; git clone https://github.com/hoxu/gitstats.git\u0026#34; } } stage(\u0026#34;Under statistics\u0026#34;) { parallel { stage(\u0026#34;reop1\u0026#34;) { when { expression { return params.repo1 } # 判断是否勾选了 } steps { # 下载要进行分析的仓库 repo1 sh \u0026#39;git clone -b master https://$USER_CRE_USR:\u0026#34;$USER_CRE_PSW\u0026#34;@git.software.com/scm/repo1.git\u0026#39; # 进行仓库 repo1 的历史记录分析 sh \u0026#34;cd gitstats \u0026amp;\u0026amp; ./gitstats ../repo1 ../html/repo1\u0026#34; } post { success { # 如果分析成功，则将分析结果放到 apache-tomcat-7.0.99/webapps/gitstats 目录下 sh \u0026#39;rm -rf ${webapproot}/repo1 \u0026amp;\u0026amp; mv html/repo1 ${webapproot}\u0026#39; # 然后删掉 repo1 的代码和 html 报告，以免不及时清理造成磁盘空间的过度占用 sh \u0026#34;rm -rf repo1\u0026#34; sh \u0026#34;rm -rf html/repo1\u0026#34; } } } } stage(\u0026#34;repo2\u0026#34;) { when { expression { return params.repo2 } } steps { sh \u0026#39;git clone -b master https://$USER_CRE_USR:\u0026#34;$USER_CRE_PSW\u0026#34;@git.software.com/scm/repo2.git\u0026#39; sh \u0026#34;cd gitstats \u0026amp;\u0026amp; ./gitstats ../repo2 ../html/repo2\u0026#34; } post { success { sh \u0026#39;rm -rf ${webapproot}/repo2 \u0026amp;\u0026amp; mv html/repo2 ${webapproot}\u0026#39; sh \u0026#34;rm -rf repo2\u0026#34; sh \u0026#34;rm -rf html/repo2\u0026#34; } } } } } } post{ always{ # 总是给执行者分送邮件通知，不论是否成功都会对工作空间进行清理 script { def email = load \u0026#34;vars/email.groovy\u0026#34; email.build(currentBuild.result, \u0026#39;\u0026#39;) } cleanWs() } } } 最后 # 如果你是测试、DevOps或是从事研发效能方面的工作，那么利用好开源工具，比如 Jenkins 和 Git Stats 就可以快速帮助老板或是你自己提供一个 Git 仓库的多维度代码分析报告，有助于更加了解产品的代码情况。\n","date":"2020-01-21","externalUrl":null,"permalink":"/posts/2020/git-stats-jenkins/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 定期自动生成 Git 仓库的多维度代码分析报告，包括手动执行和定时执行的实现方式，帮助开发者和团队更好地了解代码质量和贡献情况。","title":"通过 Jenkins 定期自动给老板提供 Git 仓库的多维度代码分析报告","type":"posts"},{"content":"记录在使用 AIX 时所遇到的问题和解决办法，以备以后遇到同样问题不要再因为这些再浪费时间，希望也能帮助到你。\n在 AIX 上无法解压超一个大约 600 MB 文件 # bash-4.3$ ls data_cdrom_debug_AIX_05949fb.tar.Z bash-4.3$ gzip -d data_cdrom_debug_AIX_05949fb.tar.Z # 错误信息 gzip: data_cdrom_debug_AIX_05949fb.tar: File too large # 解决办法 bash-4.3$ sudo vi /etc/security/limits default: fsize = -1 # 修改为 -1 core = 2097151 cpu = -1 data = -1 rss = 65536 stack = 65536 nofiles = 2000 # 需要重启 bash-4.3$ sudo reboot Rebooting . . . 修改之后，重启，再解压就没有问题了。\n安装 Java Standard Edition on AIX # 下载地址 https://developer.ibm.com/javasdk/support/aix-download-service/\ndownload Java8_64.sdk.8.0.0.600.tar.gz Java8_64.jre.8.0.0.600.tar.gz gzip -d Java8_64.sdk.8.0.0.600.tar.gz and Java8_64.jre.8.0.0.600.tar.gz tar -xvf Java8_64.sdk.8.0.0.600.tar and Java8_64.jre.8.0.0.600.tar installp -agXYd . Java8_64.jre Java8_64.sdk 2\u0026gt;\u0026amp;1 | tee installp.log # install output Installation Summary -------------------- Name Level Part Event Result ------------------------------------------------------------------------------- Java8_64.sdk 8.0.0.600 USR APPLY SUCCESS Java8_64.jre 8.0.0.600 USR APPLY SUCCESS Java8_64.jre 8.0.0.600 ROOT APPLY SUCCESS smitty install_all Input: Type \u0026ldquo;./\u0026rdquo; in the field Acceptance Install the Agreement, then start install. Troubleshooting\nbash-4.4# ./java -version Error: Port Library failed to initialize: -70 Error: Could not create the Java Virtual Machine. Error: A fatal exception has occurred. Program will exit. ","date":"2020-01-09","externalUrl":null,"permalink":"/posts/2020/aix-tips/","section":"Posts","summary":"记录在使用 AIX 时所遇到的问题和解决办法，以备以后遇到同样问题不要再因为这些再浪费时间，希望也能帮助到你。","title":"AIX 上安装工具以及一些使用总结","type":"posts"},{"content":"","date":"2020-01-07","externalUrl":null,"permalink":"/tags/solaris/","section":"标签","summary":"","title":"Solaris","type":"tags"},{"content":"记录在使用 Solaris 时所遇到的问题和解决办法，以备以后遇到同样问题不要再因为这些再浪费时间，希望也能帮助到你。\ninstall packages on solaris # https://www.opencsw.org/get-it/packages/\nInstall Git # https://www.opencsw.org/packages/git/\n","date":"2020-01-07","externalUrl":null,"permalink":"/posts/2020/solaris-tips/","section":"Posts","summary":"本文记录了在使用 Solaris 时遇到的问题和解决办法，包括安装工具、配置网络、安装软件包等，帮助用户更高效地使用 Solaris 系统。","title":"Solaris 安装工具以及一些使用总结","type":"posts"},{"content":"Time flies, and it\u0026rsquo;s already the third day of 2020. Looking back on 2019, my keyword for the year is: \u0026ldquo;tried my best\u0026rdquo;.\nThis is my second year as a software engineer. Although my title is SE (Software Engineer), my main work involves product building and release, and the implementation of CI/CD/DevOps (self-proclaimed jack-of-all-trades). Let me briefly record my work \u0026ldquo;achievements\u0026rdquo; in 2019.\nIn 2019, in addition to completing daily product builds, releases, Git management, and VM management, I attempted major adjustments to build and release automation, migrating manual and partially automated builds from Bamboo to Jenkins. I used Jenkins\u0026rsquo; multi-branch pipeline, Shared Libraries, and Artifactory for continuous integration.\nIn 2019, I submitted an innovative project internally and was fortunate enough to win first place and the Chief Product Award.\nThis gave me the opportunity to go to the United States and attend the company\u0026rsquo;s developer conference. I\u0026rsquo;m very happy that this project has finally been included in the product roadmap.\nAt the end of December 2019, I went to Beijing to attend a two-day training camp on \u0026ldquo;JFrog Jenkins, Artifactory \u0026amp; Kubernetes,\u0026rdquo; learning best practices from engineers at one of the most influential companies (among others) in the DevOps industry.\nThis year, I gained a lot of new knowledge about continuous integration and continuous delivery, but only updated 11 original articles on my public account and 33 blog posts. The difference is that blogs are more like notes; I can record and modify them at any time without fear of mistakes. My public account is more like a newspaper; the content cannot be modified or supplemented after publication. Each update requires repeated revision and review, word by word, and ultimately takes several times longer to produce a complete original piece of content than writing a blog post.\nI hope to produce more valuable content in 2020. If possible, I want to use 8 to 10 months of my spare time to complete something I\u0026rsquo;ve never dared to dream of before (if I complete it, I\u0026rsquo;ll write it down; if not, it will remain a secret).\nSharing an email I received on the last day of 2019:\nRecently, everyone has done their year-end summaries. I wonder, how many people are satisfied with their performance this year? Did you demonstrate your value at work? How many times did you submit code? How was the quality of the submitted code? How many customer problems did you solve? How many bugs did you find?\nDid your technical and business knowledge improve? Compared to your colleagues, are you satisfied with your rate of progress? How many people spend their time on WeChat and chatting? Are there people who are just going through the motions or just doing enough to get by?\nWe are all about the same age, mostly in our thirties and forties. This age is considered the best working age. I hope you don\u0026rsquo;t waste your best years here.\nSome people may say that I have achieved or am close to financial freedom, and I don\u0026rsquo;t have high demands on my work. First, I congratulate you on achieving financial freedom, but at the same time, I want to say that work is far more than just a source of income. Can you gain the recognition of your colleagues and superiors in your work, and can you demonstrate your value?\nI like to work with people who are curious, responsible, and ambitious. At the same time, I dislike people who are constantly on their phones during work hours and lack ambition. If you have something to deal with, you can step away to make a phone call and resolve it quickly. Idle, aimless phone use is not acceptable.\nThere are also people who leave work exactly on time, as if working even a few extra minutes is a loss. This mentality is absolutely unacceptable. I wonder, if your ability is inferior to your colleagues, and you don\u0026rsquo;t work harder than others every day, how can you catch up with or surpass them? Do you always want to be behind? Go the extra mile every day, and do a little more.\nI read it several times. Thank you for the above words. From a personal perspective, I strongly agree with the above points, especially the statement that work is far more than just a source of income; it is a reflection of one\u0026rsquo;s value. If you like the job, try your best to do it well; if you don\u0026rsquo;t like it, find your passion as soon as possible.\n","date":"2019-12-28","externalUrl":null,"permalink":"/en/misc/2019-summary/","section":"Miscs","summary":"Time flies, and it’s already the third day of 2020. Looking back on 2019, my keyword for the year is: “tried my best”.\nThis is my second year as a software engineer. Although my title is SE (Software Engineer), my main work involves product building and release, and the implementation of CI/CD/DevOps (self-proclaimed jack-of-all-trades). Let me briefly record my work “achievements” in 2019.\n","title":"2019 Year-End Summary","type":"misc"},{"content":" Jenkins Warnings Next Generation 插件 # Jenkins Warnings Next Generation 插件可收集编译器警告或静态分析工具报告的问题并可视化结果，它内置了对众多静态分析工具（包括多个编译器）的支持，更多支持的报告格式。\n支持的项目类型 # Warnings Next Generation 插件支持以下 Jenkins 项目类型：\n自由式项目 Maven 项目 矩阵项目 脚本化管道（顺序和并行步骤） 声明式管道（顺序步骤和并行步骤） 多分支管道 功能概述 # 当作为后续构建任务操作（或步骤）添加时，Warnings Next Generation 插件提供以下功能：\n该插件会扫描 Jenkins 版本的控制台日志或你工作区中的文件中是否存在任何问题。支持一百多种报告格式，它可以检测到的问题包括： 来自编译器的错误（C，C＃，Java等） 来自静态分析工具（CheckStyle，StyleCop，SpotBugs 等）的警告 来自复制粘贴检测器（CPD, Simian 等）的重复 漏洞 在源文件的注释中打开任务 该插件会发布有关在构建中发现的问题的报告，因此可以从以下位置导航到摘要报告，主构建页面。你还可以从那里深入了解细节： 发行新的，固定的和未解决的问题 按严重性，类别，类型，模块或程序包分发问题 所有问题的列表，包括来自报告工具的有用评论 受影响文件的带注释的源代码 问题趋势图 该插件不会运行静态分析，它只是可视化此类工具报告的结果。你仍然需要在构建文件或 Jenkinsfile 中启用和配置静态分析工具。\n配置 # 你可以在 Jenkins 作业配置用户界面中配置插件的每个选项（在自由式，maven 或矩阵作业中）。在这里你需要在工作中添加并启用生成后操作“记录编译器警告和静态分析结果”。\n在管道中，将通过添加 recordIssues 激活插件。也可以使用相同的用户界面来配置此步骤（通过使用 Snippet 编辑器）。请注意，对于脚本化管道，一些其他功能可用于汇总和分组问题，有关详细信息，请参阅“高级管道配置”部分。\n在以下各节中，将同时显示图形配置和管道配置。\n工具选择 # 下图显示了插件的基本配置：\n首先，你需要指定用于创建问题的工具，根据所选工具，你可能还会配置一些其他参数。\n对于所有读取报告文件的解析器，你需要指定应分析和扫描问题的报告文件的模式。如果未指定模式，则将扫描构建的控制台日志。对于几种流行的工具，提供了默认模式，在这种情况下，如果模式为空，则将使用默认模式。\n为了让扫描程序正确解析你的报告，需要设置文件的编码，否则将使用平台编码，这可能不正确。\n每个工具都由一个 ID 标识，该 ID 用作分析结果的 URL。对于每个工具，都提供了一个默认 URL（和名称），可以根据需要进行更改。例如，如果你打算多次使用解析器，则需要为每个调用指定不同的 ID。\n你可以指定将用于同一配置的多个工具（和模式），由于 Jenkins 的技术（或市场）限制，无法通过使用多个后期构建操作来选择不同的配置。\n通过使用“汇总结果”复选框，可以使用一项新功能。如果选中此选项，则将创建一个结果，其中包含所选工具的所有问题的汇总。这是之前静态分析收集器插件提供的。激活此选项后，你将获得所有问题的唯一入口点。以下屏幕截图显示了此新行为：\n如果未启用此选项，则将为每个工具创建单独的结果。此结果具有唯一的 URL 和图标，因此你可以快速查看创建的报告之间的区别：\n在基本配置部分中，你还可以选择是否针对失败的构建也运行该步骤。默认情况下禁用此选项，因为如果构建失败，分析结果可能会不准确。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues( enabledForFailure: true, aggregatingResults: true, tools: [java(), checkStyle(pattern: \u0026#39;checkstyle-result.xml\u0026#39;, reportEncoding: \u0026#39;UTF-8\u0026#39;)] ) 如果使用单个工具，则可以使用属性工具代替工具：\nrecordIssues enabledForFailure: true, aggregatingResults: true, tool: checkStyle(pattern: \u0026#39;checkstyle-result.xml\u0026#39;) 创建对自定义工具的支持 # 如果你的项目中没有内置工具，则可以通过多种方式添加其他工具。\n将问题导出为受支持的格式 # 将工具的分析结果获取到 Warnings 插件中的一种简单方法是将信息导出为一种已经支持的格式。例如，几种工具将其问题导出为 CheckStyle 或 PMD 格式。如果要使用警告插件的所有功能，则最好将信息导出为本机 XML 或 JSON 格式（此解析器使用 ID 问题）。 这些格式已经在用户界面中注册，你可以直接使用它们。你甚至可以在包含单行 JSON 问题的简单日志文件中提供问题，请参见示例。\n这是一个示例步骤，可用于解析本机 JSON（或 XML）格式：\nrecordIssues(tool: issues()) 使用自定义插件部署新工具 # 最灵活的方法是通过编写将在你自己的小型 Jenkins 插件中部署的 Java 类来定义新工具，有关详细信息，请参见文档“为自定义静态分析工具提供支持”。\n使用Groovy解析器创建新工具 # 如果日志消息的格式非常简单，则可以通过在 Jenkins 的用户界面中创建简单的工具配置来定义对工具的支持。 出于安全原因（Groovy 脚本可能会危害你的主服务器），此配置仅在系统配置中可用。 新解析器的配置采用正则表达式，该正则表达式将用于匹配报告格式。 如果表达式匹配，则将调用 Groovy 脚本，该脚本将匹配的文本转换为问题实例。 这是基于 Groovy 的解析器的示例：\n以编程方式创建 Groovy 解析器 # 还可以使用 Groovy 脚本从管道，Jenkins 启动脚本或脚本控制台中创建基于 Groovy 的解析器，请参见以下示例：\ndef config = io.jenkins.plugins.analysis.warnings.groovy.ParserConfiguration.getInstance() if(!config.contains(\u0026#39;pep8-groovy\u0026#39;)){ def newParser = new io.jenkins.plugins.analysis.warnings.groovy.GroovyParser( \u0026#39;pep8-groovy\u0026#39;, \u0026#39;Pep8 Groovy Parser\u0026#39;, \u0026#39;(.*):(\\\\d+):(\\\\d+): (\\\\D\\\\d*) (.*)\u0026#39;, \u0026#39;return builder.setFileName(matcher.group(1)).setCategory(matcher.group(4)).setMessage(matcher.group(5)).buildOptional()\u0026#39;, \u0026#34;optparse.py:69:11: E401 multiple imports on one line\u0026#34; ) config.setParsers(config.getParsers().plus(newParser)) } 使用配置作为代码导入解析器（JCasC） # 还可以使用 JCasC yaml 文件中的部分来指定基于 Groovy 的解析器。这是一个小示例，展示了如何添加这样的解析器：\nunclassified: warningsParsers: parsers: - name: \u0026#34;Example parser\u0026#34; id: example-id regexp: \u0026#34;^\\\\s*(.*):(\\\\d+):(.*):\\\\s*(.*)$\u0026#34; script: | import edu.hm.hafner.analysis.Severity builder.setFileName(matcher.group(1)) .setLineStart(Integer.parseInt(matcher.group(2))) .setSeverity(Severity.WARNING_NORMAL) .setCategory(matcher.group(3)) .setMessage(matcher.group(4)) return builder.buildOptional(); example: \u0026#34;somefile.txt:2:SeriousWarnings:SomethingWentWrong\u0026#34; 使用定义的工具 # 一旦注册了 Groovy 解析器，就可以在作业的工具配置部分中使用它：\n首先，你需要选择工具 “Groovy Parser” 以获取 Groovy 解析器的配置屏幕。 然后，你可以从可用解析器列表中选择解析器。 该列表是根据 Jenkins 的“系统配置”部分中定义的解析器动态创建的。可以使用与其他工具相同的方式来设置自定义 ID 和名称属性。\n为了在管道中使用 Groovy 解析器，你需要使用以下形式的脚本语句：\nrecordIssues sourceCodeEncoding: \u0026#39;UTF-8\u0026#39;, tool: groovyScript(parserId: \u0026#39;groovy-id-in-system-config\u0026#39;, pattern:\u0026#39;**/*report.log\u0026#39;, reportEncoding:\u0026#39;UTF-8\u0026#39;) 处理受影响的源代码文件的属性 # 为了让插件解析并显示你的源代码文件，需要为这些文件设置正确的编码。 此外，如果你的源代码不在工作区中（例如，它已签出到共享代理文件夹中），则该插件将不会自动找到你的源文件。 为了让插件显示这些文件，你可以添加一个附加的源目录：\n以下代码段显示了带有这些选项的示例管道，请注意，如果需要，可以不同地设置报告文件的编码：\nrecordIssues sourceCodeEncoding: \u0026#39;ISO-8859-1\u0026#39;, sourceDirectory: \u0026#39;/path/to/sources\u0026#39;, tool: java(reportEncoding: \u0026#39;UTF-8\u0026#39;) 请注意，工作区外部的文件内容可能很敏感。 为了防止意外显示此类文件，你需要在 Jenkins 系统配置屏幕中提供允许的源代码目录的白名单：\n另外，此配置设置可以由 JCasC yaml 文件中的以下子节提供\nunclassified: warningsPlugin: sourceDirectories: - path: \u0026#34;C:\\\\Temp\u0026#34; - path: \u0026#34;/mnt/sources\u0026#34; 控制参考构建的选择（基准） # 警告下一代插件的一项重要功能是将问题分类为新问题，未解决问题和已解决问题：\n新增：所有问题，属于当前报告的一部分，但未在参考报告中显示 已修复：所有问题，属于参考报告的一部分，但不再存在于当前报告中 未解决：所有问题，是当前报告和参考报告的一部分 为了计算此分类，插件需要参考构建（基准）。 然后，通过比较当前版本和基准中的问题来计算新的，已修复的和未解决的问题。 有三个选项可控制参考构建的选择。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(), ignoreQualityGate: false, ignoreFailedBuilds: true, referenceJobName: \u0026#39;my-project/master\u0026#39; 筛选问题 # 创建的问题报告可以随后进行过滤。 你可以指定任意数量的包含或排除过滤器。 当前，支持按模块名称，程序包或名称空间名称，文件名，类别或类型过滤问题。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(pattern: \u0026#39;*.log\u0026#39;), filters: [includeFile(\u0026#39;MyFile.*.java\u0026#39;), excludeCategory(\u0026#39;WHITESPACE\u0026#39;)] Quality gate 配置 # 你可以定义几个 Quality gate (质量门)，在报告问题后将对其进行检查。这些质量门使你可以修改詹金斯的生产状态，以便立即查看是否满足所需的产品质量。对于这些质量门中的每一个，都可以将构建设置为不稳定或失败。所有质量门都使用一个简单的度量标准：给定质量门将失败的问题数量。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(pattern: \u0026#39;*.log\u0026#39;), qualityGates: [[threshold: 1, type: \u0026#39;TOTAL\u0026#39;, unstable: true]] 类型确定将用来评估质量门的属性。请参阅枚举 QualityGateType 以查看支持哪些不同类型。\n健康报告配置 # 该插件可以参与你项目的运行状况报告。你可以更改将运行状况更改为 0％ 和 100％ 的问题数。此外，可以选择在创建运行状况报告时应考虑的严重性。\n健康报告配置!\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(pattern: \u0026#39;*.log\u0026#39;), healthy: 10, unhealthy: 100, minimumSeverity: \u0026#39;HIGH\u0026#39; 该作业根据严重性为 HIGH 和错误的所有警告来调整构建运行状况。如果内部版本包含 10 条或更少的警告，则运行状况为 100％。如果内部版本有 100 个以上的警告，则运行状况为 0％。\n管道配置 # 在 Jenkins Pipeline 中使用 Warnings 插件的要求可能很复杂，有时会引起争议。为了尽可能灵活，我决定将主要步骤分为两个独立的部分，然后可以彼此独立使用。\n简单的管道配置 # 步骤 recordIssues 提供了简单的管道配置，它提供了与构建后操作相同的属性（请参见上文）。此步骤扫描给定文件集（或控制台日志）中的问题，并在构建中报告这些问题。你可以使用代码片段生成器来创建一个有效的代码片段，以调用此步骤。以下示例显示了此步骤的典型示例：\nrecordIssues( enabledForFailure: true, tool: java(pattern: \u0026#39;*.log\u0026#39;), filters: [includeFile(\u0026#39;MyFile.*.java\u0026#39;), excludeCategory(\u0026#39;WHITESPACE\u0026#39;)] ) 在此示例中，将扫描文件 * .log 中的 Java 问题。仅包括文件名与模式 MyFile.*.java 匹配的问题。类别 WHITESPACE 的问题将被排除，即使构建失败，也会执行该步骤。\n为了查看所有配置选项，你可以研究步骤实现。\n声明式管道配置 # 声明性管道作业中的插件配置与脚本管道中的配置相同，请参见以下示例，该示例在 Jenkins 上构建分析模型库：\npipeline { agent \u0026#39;any\u0026#39; tools { maven \u0026#39;mvn-default\u0026#39; jdk \u0026#39;jdk-default\u0026#39; } stages { stage (\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;${M2_HOME}/bin/mvn --batch-mode -V -U -e clean verify -Dsurefire.useFile=false -Dmaven.test.failure.ignore\u0026#39; } } stage (\u0026#39;Analysis\u0026#39;) { steps { sh \u0026#39;${M2_HOME}/bin/mvn --batch-mode -V -U -e checkstyle:checkstyle pmd:pmd pmd:cpd findbugs:findbugs spotbugs:spotbugs\u0026#39; } } } post { always { junit testResults: \u0026#39;**/target/surefire-reports/TEST-*.xml\u0026#39; recordIssues enabledForFailure: true, tools: [mavenConsole(), java(), javaDoc()] recordIssues enabledForFailure: true, tool: checkStyle() recordIssues enabledForFailure: true, tool: spotBugs() recordIssues enabledForFailure: true, tool: cpd(pattern: \u0026#39;**/target/cpd.xml\u0026#39;) recordIssues enabledForFailure: true, tool: pmdParser(pattern: \u0026#39;**/target/pmd.xml\u0026#39;) } } } 高级管道配置 # 有时仅使用一个步骤发布和报告问题是不够的。例如，如果你使用多个并行步骤来构建产品，并且想要将所有这些步骤中的问题合并为一个结果。然后，你需要拆分扫描和聚合。该插件提供以下两个步骤：\nscanForIssues 此步骤使用特定的解析器扫描报告文件或控制台日志，并创建一个包含报告的中 间 AnnotatedReport 对象。有关详细信息，请参见步骤实现。 publishIssues：此步骤在你的构建中发布一个新报告，其中包含几个 scanForIssues 步骤的汇总结果。有关详细信息，请参见步骤实现。 node { stage (\u0026#39;Checkout\u0026#39;) { git branch:\u0026#39;5.0\u0026#39;, url: \u0026#39;git@github.com:jenkinsci/warnings-plugin.git\u0026#39; } stage (\u0026#39;Build\u0026#39;) { def mvnHome = tool \u0026#39;mvn-default\u0026#39; sh \u0026#34;${mvnHome}/bin/mvn --batch-mode -V -U -e clean verify -Dsurefire.useFile=false\u0026#34; junit testResults: \u0026#39;**/target/*-reports/TEST-*.xml\u0026#39; def java = scanForIssues tool: java() def javadoc = scanForIssues tool: javaDoc() publishIssues issues: [java, javadoc], filters: [includePackage(\u0026#39;io.jenkins.plugins.analysis.*\u0026#39;)] } stage (\u0026#39;Analysis\u0026#39;) { def mvnHome = tool \u0026#39;mvn-default\u0026#39; sh \u0026#34;${mvnHome}/bin/mvn --batch-mode -V -U -e checkstyle:checkstyle pmd:pmd pmd:cpd findbugs:findbugs\u0026#34; def checkstyle = scanForIssues tool: checkStyle(pattern: \u0026#39;**/target/checkstyle-result.xml\u0026#39;) publishIssues issues: [checkstyle] def pmd = scanForIssues tool: pmdParser(pattern: \u0026#39;**/target/pmd.xml\u0026#39;) publishIssues issues: [pmd] def cpd = scanForIssues tool: cpd(pattern: \u0026#39;**/target/cpd.xml\u0026#39;) publishIssues issues: [cpd] def spotbugs = scanForIssues tool: spotBugs(pattern: \u0026#39;**/target/findbugsXml.xml\u0026#39;) publishIssues issues: [spotbugs] def maven = scanForIssues tool: mavenConsole() publishIssues issues: [maven] publishIssues id: \u0026#39;analysis\u0026#39;, name: \u0026#39;All Issues\u0026#39;, issues: [checkstyle, pmd, spotbugs], filters: [includePackage(\u0026#39;io.jenkins.plugins.analysis.*\u0026#39;)] } } 新功能 # 以下各节介绍了最重要的新功能。\n发行记录：New, Fixed, Outstanding 问题 # 该插件的一大亮点是能够将后续版本的问题分类为 New, Fixed, Outstanding。\n使用此功能可以更轻松地控制项目的质量：你只能专注于最近引入的警告。\n注意：新警告的检测基于复杂的算法，该算法试图在源代码的两个不同版本中跟踪同一警告。根据源代码的修改程度，它可能会产生一些误报，即，即使应该没有警告也可能会收到一些新的固定警告。该算法的准确性仍在研究中，并将在接下来的几个月中进行完善。\nSeverities 严重程度 # 该插件在图表中显示问题严重性的分布，它定义了以下默认严重级别，但是扩展警告插件的插件可能会添加其他默认级别。\nError：表示通常会导致构建失败的错误 Warning (High, Normal, Low)：指示给定优先级的警告。映射到优先级取决于各个解析器。 请注意，并非每个解析器都能产生不同严重性的警告。某些解析器仅对所有问题使用相同的严重性。\nBuild trend 构建趋势 # 为了查看分析结果的趋势，几个图表显示了每个构建的问题数量。这些图表用于详细信息页面和作业概述中。当前提供以下不同的趋势图类型：\n问题的严重程度分布 # 默认趋势图显示问题总数，按严重性堆叠。使用此图表，你可以查看哪种严重程度对问题总数贡献最大。\n每种静态分析类型的问题 # 如果你要汇总几个静态分析结果，则类型图将使用单独的一行显示每个工具的问题数量。你可以通过单击相应的图例符号暂时隐藏工具。\n新问题与已修复问题 # 如果你对积压的问题感兴趣，可以打开新的与固定的图表。它映射了引入的问题与通过一系列构建解决的问题。这可以帮助你了解整个待办事项列表是在增加还是在减少。\n项目健康 # 仅当启用了运行状况报告后，运行状况图表才可用。在这种情况下，趋势图将显示健康和不健康区域中的警告数量。你的项目目标应该是使警告数量不逃避图表的绿色部分。\n缩放 # 细节视图中的所有趋势图都支持使用图表底部的范围滑块缩放构建轴。\n构建与日期轴 # 详细信息视图中的所有趋势图都可以显示每个构建或每天的警告数量。你可以通过选择右上角的相应图标在X轴变体之间切换，每天显示平均警告数。\n问题概述 # 你可以在几个聚合视图中快速，高效地查看报告的问题集。根据问题的数量或类型，你将看到问题的分布\nStatic Analysis Tool（静态分析工具） Module（模组） Package or Namespace（包或命名空间） Severity（严重程度） Category（类别） Type（类型） 这些详细信息视图中的每一个都是交互式的，即，你可以导航到已分类问题的子集。\n问题详情 # 一组已报告的问题显示在一个现代化的响应表中。该表使用 Ajax 调用按需加载，它提供以下功能：\nPagination（分页）：问题的数量分为几个页面，可以使用提供的页面链接进行选择。请注意，目前分页是在客户端进行的，即从服务器获取整个问题表可能要花费一些时间。 Sorting（排序）：可以通过单击表列中的仅一个来对表内容进行排序。 Filtering, Searching（过滤，搜索）：你可以通过在搜索框中输入一些文本来过滤显示的问题。 Content Aware（内容感知）：仅当有必要显示的内容时才显示列。也就是说，如果工具未报告问题类别，则该类别将被自动隐藏。 Responsive（响应式）：布局应适应实际的屏幕阈值。 Details（详细信息）：问题的详细信息消息（如果由相应的静态分析工具提供）在表中显示为子行。 源代码 Blame（归咎于） # 这个功能需要安装其他插件：Git Forensics 插件\n如果未在作业配置中禁用，则插件将执行 git blame 以确定谁是问题的负责 author。在相应的 SCM Blames 视图中，所有问题将与 auther name, email, 和 commit ID 一起列出。\n为了禁用 git blame 功能，请将属性 blameDisabled 设置为 true，请参见以下示例：\nrecordIssues blameDisabled: true, tool: java(pattern: \u0026#39;*.log\u0026#39;) Git 仓库取证 # 此功能需要安装其他插件：Git Forensics 插件\n如果未在作业配置中禁用，则该插件将以“犯罪现场代码”的样式（Adam Tornhill，2013年11月）挖掘源代码存储库，以确定受影响文件的统计信息。在相应的 “SCM 取证” 视图中，将列出所有问题以及受影响文件的以下属性：\n提交总数 不同作者总数 创作时间 最后修改时间 源代码控制概述 为了禁用 Git 取证功能，请将属性 forensicsDisabled设置为 true，请参见以下示例：\nrecordIssues forensicsDisabled: true, tool: java(pattern: \u0026#39;*.log\u0026#39;) 源代码视图 # 现在，源代码视图使用 JS 库 Prism 在受影响的文件中显示警告。该库为最流行的语言提供语法高亮显示，并在客户端呈现所有内容。\n发行总数栏 # 你可以在 Jenkins 作业表的单独列中显示作业的总数。 默认情况下，Jenkins 主列表视图将显示一个新列，该列计算所有工具的发行总数。 你可以添加可以配置的其他列\n列名 应考虑的实际工具 要显示的总计类型（总体警告，新警告，特定严重性等），请参阅 “token宏支持” 部分。 仪表板视图支持 # 还提供对 Jenkins 仪表板视图的支持。当前，以下 portlet 可用：\n每个工具和作业表的问题 # 问题表显示了作业的问题总数（由每个工具分开）。\n问题趋势 # 可以将趋势图添加为 portlet，该趋势图显示所有作业的发行总数。\n远程API # 该插件提供以下 REST API 端点。\n所有分析结果的汇总摘要 # 可以使用 URL [build-url]/warnings-ng/api/json（或 [build-url]/warnings-ng/api/xml）查询构建中已配置的所有静态分析工具。此汇总显示每个工具的 ID，名称，URL 和问题总数。\n{ \u0026#34;_class\u0026#34;: \u0026#34;io.jenkins.plugins.analysis.core.restapi.AggregationApi\u0026#34;, \u0026#34;tools\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;maven\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/maven\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Maven Warnings\u0026#34;, \u0026#34;size\u0026#34;: 9 }, { \u0026#34;id\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/java\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Java Warnings\u0026#34;, \u0026#34;size\u0026#34;: 1 }, { \u0026#34;id\u0026#34;: \u0026#34;javadoc\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/javadoc\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;JavaDoc Warnings\u0026#34;, \u0026#34;size\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;checkstyle\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/checkstyle\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;CheckStyle Warnings\u0026#34;, \u0026#34;size\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;pmd\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/pmd\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;PMD Warnings\u0026#34;, \u0026#34;size\u0026#34;: 671 }, { \u0026#34;id\u0026#34;: \u0026#34;spotbugs\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/spotbugs\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SpotBugs Warnings\u0026#34;, \u0026#34;size\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;cpd\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/cpd\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;CPD Warnings\u0026#34;, \u0026#34;size\u0026#34;: 123 }, { \u0026#34;id\u0026#34;: \u0026#34;open-tasks\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/open-tasks\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Open Tasks Scanner Warnings\u0026#34;, \u0026#34;size\u0026#34;: 11 } ] } 分析结果汇总 # 你可以使用 URL [build-url]/[tool-id]/api/xml（或 [build-url]/[tool-id]/api/json）获得特定分析报告的摘要。摘要包含问题数量，质量门状态以及所有信息和错误消息。\n这是一个示例 XML 报告：\n\u0026lt;analysisResultApi _class=\u0026#39;io.jenkins.plugins.analysis.core.restapi.AnalysisResultApi\u0026#39;\u0026gt; \u0026lt;totalSize\u0026gt;3\u0026lt;/totalSize\u0026gt; \u0026lt;fixedSize\u0026gt;0\u0026lt;/fixedSize\u0026gt; \u0026lt;newSize\u0026gt;0\u0026lt;/newSize\u0026gt; \u0026lt;noIssuesSinceBuild\u0026gt;-1\u0026lt;/noIssuesSinceBuild\u0026gt; \u0026lt;successfulSinceBuild\u0026gt;-1\u0026lt;/successfulSinceBuild\u0026gt; \u0026lt;qualityGateStatus\u0026gt;WARNING\u0026lt;/qualityGateStatus\u0026gt; \u0026lt;owner _class=\u0026#39;org.jenkinsci.plugins.workflow.job.WorkflowRun\u0026#39;\u0026gt; \u0026lt;number\u0026gt;46\u0026lt;/number\u0026gt; \u0026lt;url\u0026gt;http://localhost:8080/view/White%20Mountains/job/Full%20Analysis%20-%20Model/46/\u0026lt;/url\u0026gt; \u0026lt;/owner\u0026gt; \u0026lt;infoMessage\u0026gt;Searching for all files in \u0026#39;/tmp/node1/workspace/Full Analysis - Model\u0026#39; that match the pattern \u0026#39;**/target/spotbugsXml.xml\u0026#39; \u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; found 1 file\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Successfully parsed file /tmp/node1/workspace/Full Analysis - Model/target/spotbugsXml.xml\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; found 3 issues (skipped 0 duplicates)\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Post processing issues on \u0026#39;node1\u0026#39; with encoding \u0026#39;UTF-8\u0026#39;\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Resolving absolute file names for all issues\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; affected files for all issues already have absolute paths\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Copying affected files to Jenkins\u0026#39; build folder /Users/hafner/Development/jenkins/jobs/Full Analysis - Model/builds/46 \u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; 2 copied, 0 not in workspace, 0 not-found, 0 with I/O error\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Resolving module names from module definitions (build.xml, pom.xml, or Manifest.mf files)\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; all issues already have a valid module name\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Resolving package names (or namespaces) by parsing the affected files\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; all affected files already have a valid package name\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Creating fingerprints for all affected code blocks to track issues over different builds\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;No filter has been set, publishing all 3 issues\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;No valid reference build found - all reported issues will be considered outstanding\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Evaluating quality qualityGates\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; WARNING - Total number of issues: 3 - Quality Gate: 1\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; Some quality qualityGates have been missed: overall result is WARNING\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Health report is disabled - skipping\u0026lt;/infoMessage\u0026gt; \u0026lt;/analysisResultApi\u0026gt; Token 宏支持 # Warnings 插件提供了 token ANALYSIS_ISSUES_COUNT，可用于其他后期构建处理步骤，例如在邮件中。为了使用此 token，你需要安装 token 宏插件。token 具有以下可选参数：\ntool：选择特定的分析结果，如果未定义，则将所有结果相加 type：选择要使用的计数器的类型，请选择以下之一 Total（任何严重性） Total（仅错误） Total（仅严重度高） Total（仅严重级别正常） Total（仅限严重性低） New （任何严重程度） New （仅限错误） New （仅限严重性高） New （仅严重性为正常） New （仅限严重性低） Delta（任何严重程度） Delta（仅错误） Delta（仅严重度高） Delta（仅严重等级正常） Delta（仅严重度低） Fixed（任何严重性） 例子：\n${ANALYSIS_ISSUES_COUNT}：扩展到所有分析工具的合计数量\n${ANALYSIS_ISSUES_COUNT, tool=\u0026quot;checkstyle\u0026quot;}：扩展到CheckStyle问题的总数\n${ANALYSIS_ISSUES_COUNT, tool=\u0026quot;checkstyle\u0026quot;, type: \u0026quot;NEW\u0026quot;}：扩展到新的 CheckStyle 问题数\n从静态分析套件过渡 # 以前，静态分析套件的插件提供了相同的功能集（CheckStyle，PMD，FindBugs，静态分析实用工具，Analysis Collector，任务扫描器，Warnings 等）。为了简化用户体验和开发过程，这些插件和核心功能已合并到Warnings Next Generation 插件中。这些旧的静态分析插件不再需要，现在已经停产。如果当前使用这些旧插件之一，则应尽快迁移到新的记录器和步骤。我仍然会保留旧代码一段时间，但是主要的开发工作将花在新的代码库中。\n迁移 Pipelines 调用旧的静态分析步骤（例如，findbug，checkstyle 等）的管道需要立即调用新的 recordIssues 步骤。所有静态分析工具都使用相同的步骤，使用 step 属性工具选择实际的解析器。有关可用参数集的更多详细信息，请参见“配置”部分。\n迁移其他所有工作 使用旧版 API 的 Freestyle，Matrix 或 Maven Jobs 使用了由每个插件提供的所谓的 Post Build Action。例如，FindBugs 插件确实提供了构建后操作“发布 FindBugs 分析结果”。这些旧的插件特定操作不再受支持，它们现在在用户界面中标记为 [Deprecated]。现在，你需要添加一个新的后期构建步骤-对于所有静态分析工具，此步骤现在称为“记录编译器警告和静态分析结果”。工具的选择是此后期构建步骤配置的一部分。注意：新的后期制作操作无法读取使用旧 API 的后期制作步骤所产生的警告。也就是说，你看不到新旧结果的合并历史记录-你仅看到两个不相关的结果。也不会自动转换以旧格式存储的结果。\n插件的迁移取决于分析核心 以下插件已集成到此警告插件的新版本中：\nAndroid-Lint 插件 Analysis Collector 插件 CheckStyle 插件 CCM 插件 Dry 插件 PMD 插件 FindBugs 插件 Tasks Scanner 插件 Warnings 插件 所有其他插件仍需要集成或需要重构以使用新的 API\n","date":"2019-12-28","externalUrl":null,"permalink":"/posts/2019/jenkins-warnings-next-generation-plugin/","section":"Posts","summary":"本文介绍了 Jenkins Warnings Next Generation 插件的功能和配置方法，包括如何收集编译器警告和静态分析工具报告的问题，并可视化结果。","title":"Jenkins Warnings Next Generation 插件","type":"posts"},{"content":"If you are a member of a research and development efficiency team or are engaged in CI/CD or DevOps, in addition to providing infrastructure, metrics and data are also a very important aspect. For example, you need to analyze the code submission status of a certain Git repository:\nWho submitted the most code to this repository? What is the activity level of this repository? Submission analysis data for different periods Contribution ranking for each version Weekly/monthly/yearly contribution ranking, etc. A few days ago, I discovered a Git history statistics generation tool called GitStats (https://github.com/shenxianpeng/gitstats)\nThis is a tool written in Python. It has a small codebase but very powerful analytical capabilities. It\u0026rsquo;s one of the few open-source projects I\u0026rsquo;ve found that can generate beautiful reports and is easy to use.\nThe gitstats report is also powerful (https://shenxianpeng.github.io/gitstats/previews/main/index.html). Those interested can try it out.\nHow to Use # Dependencies required: Git, Python3, Gnuplot.\nIf you have Linux, it\u0026rsquo;s recommended to download and install on Linux. I tried setting up the environment on Windows, but I had to configure Cygwin and manually configure Gnuplot (Gnuplot is a portable command-line driven graphics tool), which was quite troublesome. The following are the installation and usage steps on Linux.\n# Install Gnuplot sudo yum -y install gnuplot # Install gitstats pip install gitstats # Clone the code repository you want to analyze git clone https://github.com/alibaba/fastjson.git # Execute the command to generate the report gitstats ../fastjson ../html/fastjson # After 15 seconds of execution, the report is generated Generating report... [0.00393] \u0026gt;\u0026gt; git --git-dir=/workspace/gitstats/.git --work-tree=/workspace/gitstats rev-parse --short HEAD [0.00236] \u0026gt;\u0026gt; git --version [0.00716] \u0026gt;\u0026gt; gnuplot --version Generating graphs... [0.01676] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/day_of_week.plot\u0026#34; [0.01571] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/files_by_date.plot\u0026#34; [0.01281] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/month_of_year.plot\u0026#34; [0.09293] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/lines_of_code_by_author.plot\u0026#34; [0.01340] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/commits_by_year.plot\u0026#34; [0.01799] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/hour_of_day.plot\u0026#34; [0.01627] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/domains.plot\u0026#34; [0.01268] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/commits_by_year_month.plot\u0026#34; [0.09435] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/commits_by_author.plot\u0026#34; [0.01522] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/lines_of_code.plot\u0026#34; Execution time 15.16208 secs, 4.58247 secs (30.22 %) in external commands) You may now run: sensible-browser \u0026#39;/workspace/html/fastjson/index.html\u0026#39; Report Analysis # General Statistics # You can see that this project is called fastjson. The first code submission was on July 31, 2011, and it has been 3060 days since then, with 893 active days. There are a total of 2960 files, with a total of 250,000 lines of code.\nActivity # The number of commits per hour of the day, per day of the week, per hour of the week, per month of the year, and per year.\nIn the past 32 weeks, the period from week 12 to week 17 was very active, with many submissions. Also, you can see that there are very few submissions between 12 pm and 8 pm. Most programmers are dealing with company work during this time and cannot contribute to open-source projects.\nSubmissions start to increase after 8 pm, indicating a very dedicated developer. You can also see that there are submissions from Monday to Sunday, with relatively fewer submissions on Saturday, likely due to rest and relaxation. Sunday submissions are significantly more than Saturday, showing that considerable personal weekend time was spent.\nYou can also see that the main completion time of this project was from 2016 to 2017, and the completion time zone was UTC+8. This dimension allows for analysis of the contribution number of developers in different regions.\nContributors # Lists all authors, number of submissions, first submission, and last submission.\nThis chart also shows who the creator of the project is, and who contributed the most each year over the years. You can also see the email accounts used by the contributors.\nFiles and Lines of Code # The total number of files is 2960, and the number of lines of code is 250,000.\nYou can also see the yearly trend chart of file increases and the ranking of the number of these file types. You can see that Java files account for 96.08%, followed by JSON.\nTags # Tags are an important analytical indicator for the team (provided that the repository to be analyzed has created a tag after the release). You can see the contribution ranking of each version.\nConclusion # If you\u0026rsquo;re interested, you can analyze your own projects or find an interesting and influential project on GitHub to analyze, such as 996.ICU and vue, and have fun.\n","date":"2019-12-17","externalUrl":null,"permalink":"/en/posts/2019/git-stats/","section":"Posts","summary":"GitStats, a Git history statistics generation tool written in Python, can generate detailed code submission statistical reports to help developers analyze project activity and contributor information.","title":"Git History Statistics Generator","type":"posts"},{"content":"","date":"2019-12-17","externalUrl":null,"permalink":"/en/tags/stats/","section":"Tags","summary":"","title":"Stats","type":"tags"},{"content":"最近做了 Black Duck 与 Jenkins 的集成，目标是给测试和开发提供定制化、定时的对各个开发代码仓库的进行源码扫描。\n为什么要做源码扫描 # 在产品开发中经常需要引入一些开源组件，但这些开源的代码会给产品风险。因此我们在发布自己产品的时候需要对这些开源组件的漏洞和许可信息进行评估。 Black Duck（黑鸭）是一款对源代码进行扫描、审计和代码管理的软件工具（同类型的工具还有 JFrog Xray）。能够搜索安全的开源代码，检测产品的开源代码使用情况，以检查外来代码的开源代码使用情况和风险情况。\n如果不能及时的进行代码扫描，在产品发布快要发布才进行扫描，如果发现问题这时候再去解决就会变得非常被动，因此团队需要尽早发现并解决问题，将 CI 工具进行集成，进行每日、每周、每月扫描就变得十分重要。\nBlack Duck 手动执行一般步骤 # 手动下载指定 Git 仓库及分支代码 去掉不相关的代码（也可以通过 Black Duck 参数去指定要扫描的特定文件或文件夹） 手动执行 Black Duck 扫描命令​ 扫描成功后，结果传到内部 Black Duck 网站供相关人员进行审查 Black Duck 与 Jenkins 的集成目标 # 一个流水线支持定制化仓库的代码下载 给开发和测试提供简单的、可随时可以执行源码扫描的界面 支持定期自动扫描，以及与其他 Jenkins 任务联动执行​ Black Duck 参数介绍 # --blackduck.url # 你的 Black Duck 网址 --blackduck.username # 你的登录用户 --blackduck.api.token # 你的登录用户 Token --detect.project.name # Black Duck 下面的项目 --detect.project.version.name # 项目版本号 --detect.source.path # 要扫描的代码目录 --logging.level.com.synopsys.integration # 扫描日志级别 --blackduck.trust.cert=TRUE # 是否信任 socket (SSL) --detect.blackduck.signature.scanner.snippet.matching # 扫描片段模式 更多其他参数可以参照官方的 CI 集成文档 Synopsys Detect for Jenkins\nBlack Duck 配置 # 首先，安装 Black Duck 插件 Synopsys Detect 到 Jenkins\n然后，配置 Synopsys Detect 插件\nJenkins -\u0026gt; Confiruration（系统配置） Black Duck URL： 公司内部的 Black Duck 网址，例如 https://yourcompany.blackducksoftware.com Black Duck credentials： 注意要选择 credentials 类型为 Secret text, Secret 填写你用户的 Token 配置完成后点击 Test connections to Black Duck，显示 Connection successful 表示配置成功。 Black Duck 流水线任务效果 # Black Duck 流水线代码 # pipeline{ agent { node { label \u0026#39;black-duck\u0026#39; customWorkspace \u0026#34;/agent/workspace/blackduck\u0026#34; } } parameters { choice( name: \u0026#39;VERSION\u0026#39;, choices: [\u0026#39;MVSURE_v1.1\u0026#39;, \u0026#39;MVSURE_v1.2\u0026#39;, \u0026#39;MVSURE_v2.2\u0026#39;], summary: \u0026#39;Which version do you want scan on black duck? MVSURE_v1.1, MVSURE_v1.2 or others?\u0026#39;) choice( name: \u0026#39;REPO\u0026#39;, choices: [\u0026#39;blog-server\u0026#39;, \u0026#39;blog-client\u0026#39;, \u0026#39;blog-docker\u0026#39;], summary: \u0026#39;Which repository code does above VERSION belong to?\u0026#39;) string( name: \u0026#39;BRANCH\u0026#39;, defaultValue: \u0026#39;develop\u0026#39;, summary: \u0026#39;Which branch does above VERSION belong to?\u0026#39;) choice( name: \u0026#39;SNIPPET-MODES\u0026#39;, choices: [\u0026#39;SNIPPET_MATCHING\u0026#39;, \u0026#39;SNIPPET_MATCHING_ONLY\u0026#39;, \u0026#39;FULL_SNIPPET_MATCHING\u0026#39;, \u0026#39;FULL_SNIPPET_MATCHING_ONLY\u0026#39;, \u0026#39;NONE\u0026#39;], summary: \u0026#39;What snippet scan mode do you want to choose?\u0026#39;) } environment { ROBOT = credentials(\u0026#34;d1cbab74-823d-41aa-abb7-858485121212\u0026#34;) hub_detect = \u0026#39;https://blackducksoftware.github.io/hub-detect/hub-detect.sh\u0026#39; blackduck_url = \u0026#39;https://yourcompany.blackducksoftware.com\u0026#39; blackduck_user = \u0026#39;robot@yourcompany.com\u0026#39; detect_project = \u0026#39;GITHUB\u0026#39; detect_project_version = \u0026#39;${VERSION}\u0026#39; detect_source_path = \u0026#39;${WORKSPACE}/${REPO}/src\u0026#39; } # 只保留最近十次 Jenkins 执行结果 options {buildDiscarder(logRotator(numToKeepStr:\u0026#39;10\u0026#39;))} # 定时触发可以在这里添加 stages { stage(\u0026#34;git clone\u0026#34;){ # 参数化 git clone 代码过程 steps{ sh \u0026#39;\u0026#39;\u0026#39; if [ -d ${REPO} ]; then rm -rf ${REPO} fi git clone -b ${BRANCH} --depth 1 https://$ROBOT_USR:\u0026#34;$ROBOT_PSW\u0026#34;@git.yourcompany.com/scm/github/${REPO}.git \u0026#39;\u0026#39;\u0026#39; } } stage(\u0026#34;black duck scan\u0026#34;){ # 参数化 Black Duck 所用到的参数值 steps { withCredentials([string(credentialsId: \u0026#39;robot-black-duck-scan\u0026#39;, variable: \u0026#39;TOKEN\u0026#39;)]) { # 用 withCredentials 来获得 Token synopsys_detect \u0026#39;bash \u0026lt;(curl -s ${hub_detect}) --blackduck.url=${blackduck_url} --blackduck.username=${blackduck_user} --blackduck.api.token=${TOKEN} --detect.project.name=${detect_project} --detect.project.version.name=${detect_project_version} --detect.source.path=${detect_source_path} --logging.level.com.synopsys.integration=debug --blackduck.trust.cert=TRUE --detect.blackduck.signature.scanner.snippet.matching=${SNIPPET-MODES}\u0026#39; } } } } post { # 不论结果任何都给执行者发送邮件通知 always { script { def email = load \u0026#34;vars/email.groovy\u0026#34; wrap([$class: \u0026#39;BuildUser\u0026#39;]) { def user = env.BUILD_USER_ID email.build(currentBuild.result, \u0026#34;${user}\u0026#34;) } } } success { echo \u0026#34;success, cleanup blackduck workspace\u0026#34; cleanWs() } } } ","date":"2019-12-08","externalUrl":null,"permalink":"/posts/2019/blackduck-interate-with-jenkins/","section":"Posts","summary":"本文介绍如何将 Black Duck 与 Jenkins 集成，实现对代码仓库的自动化安全扫描和漏洞检测。","title":"Black Duck 与 Jenkins 集成","type":"posts"},{"content":"Docker 常用命令小纸条\nDocker start|stop|restart # # 查看 Docker 版本 docker -v # or docker --version # 重启 docker sudo systemctl restart docker.service # 停止 docker sudo systemctl stop docker.service # 启动 docker sudo systemctl start docker.service Docker run # 我们通过 docker 的两个参数 -i -t，让 docker 运行的容器实现\u0026quot;对话\u0026quot;的能力：\ndocker run -i -t ubuntu:15.10 /bin/bash Login Artifactory # 注意：Open Source 版本 Artifactory 不支持 Docker，需要下载 JFrog Container Registry 或是 Artifactory 企业版。\ndocker login -u \u0026lt;USER_NAME\u0026gt; -p \u0026lt;USER_PASSWORD\u0026gt; devasvm.dev.org.com:\u0026lt;REPOSITORY_PORT\u0026gt; -sh-4.2$ sudo docker login devasvm.dev.org.com:8040 Username: admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 把 Docker image 推送到远程仓库\n// docker tag SOURCE_IMAGE[:TAG] devasvm.dev.org.com:8040/docker-local/IMAGE[:TAG] -sh-4.2$ sudo docker tag ubuntu:15.10 devasvm.dev.org.com:8040/docker-local/ubuntu:15.10 // docker push devasvm.dev.org.com:8040/docker-local/IMAGE[:TAG] -sh-4.2$ sudo docker push devasvm.dev.org.com:8040/docker-local/ubuntu:15.10 The push refers to repository [devasvm.dev.org.com:8040/docker-local/ubuntu] 98d59071f692: Pushed af288f00b8a7: Pushed 4b955941a4d0: Pushed f121afdbbd5d: Pushed 15.10: digest: sha256:a3f5e428c0cfbfd55cffb32d30b1d78fedb8a9faaf08efdd9c5208c94dc66614 size: 1150 TODO # 更多 Docker 常用命令记录到这里。\n","date":"2019-12-02","externalUrl":null,"permalink":"/posts/2019/docker-commands/","section":"Posts","summary":"一个 Docker 常用命令小纸条，记录一些常用的 Docker 命令和操作，方便日常使用和参考。","title":"Docker 常用命令","type":"posts"},{"content":"Instead of working overtime on Tuesday evening as usual (I usually leave work after 6 pm to avoid the evening rush hour), I rushed to the airport by subway to catch an 8:30 flight to Beijing. I was attending the JFrog China Jenkins, Artifactory \u0026amp; Kubernetes hands-on training camp in Wangjing, Beijing, the next day.\nFirstly, my company provides each employee with two days of paid training leave per year. If there\u0026rsquo;s a suitable training opportunity, I\u0026rsquo;ll cover the additional costs myself. Secondly, the training content was closely related to my current work, and I wanted to learn about industry best practices and exchange ideas with my peers.\nBefore my trip, I asked my manager for leave and inquired about the budget for similar training. My manager asked me to submit a cost estimate. I calculated the cost of airfare and hotel, and my manager applied for it with the company, also applying for meal expenses. The company agreed, with the only condition being that I would give a presentation upon my return. So I went to the JFrog DevOps training camp with this task, and I am especially grateful to my manager and company.\nRevisiting Beijing # It\u0026rsquo;s been over five years since I left Beijing. This was the longest 48 hours I\u0026rsquo;ve spent in Beijing, my previous visits were just layovers. Although I worked in Beijing before, I didn\u0026rsquo;t pay much attention to life then. This time, I intentionally observed more, constantly pondering: if given the chance to return to Beijing, would I still only plan to work there for a few years as I did before?\nIt was already 10:30 pm when I landed. The traffic on the airport expressway to Wangjing was still heavy. Beijing\u0026rsquo;s nightlife felt the same as before—two or three hours later than in smaller cities. Five years ago in Beijing, I usually left work after 9 pm, sometimes even later if there was a deployment, only getting home in the middle of the night and arriving at the office the next afternoon. My daily schedule was five or six hours later than it is now in smaller cities.\nMy current life is a 9-to-6 job, I don\u0026rsquo;t advocate overtime, and while I\u0026rsquo;m busy during work hours, most people don\u0026rsquo;t work overtime, it\u0026rsquo;s voluntary. During a business trip to the US, I found many US colleagues worked from 7-8 am. They usually had simple and quick lunches. If they were busy, they immediately returned to work, leaving the office around 3-4 pm. Very few stayed until 5-6 pm, but their work efficiency didn\u0026rsquo;t seem any worse. This reminded me of the intense 996 working hour debate this year. Perhaps it\u0026rsquo;s ultimately due to our stage of social development. Young people are under immense pressure to earn money to buy houses. Those with children have their parents to help take care of them, and older generations don\u0026rsquo;t think about their retirement, focusing more on helping their children. This allows young people to work 996. But Americans can’t do that. They are realistic, if they don\u0026rsquo;t leave work by 3 or 4 pm, there\u0026rsquo;s no one to pick up their children. They need family time. Therefore, what should be shouldered by companies or even society, is instead borne by the whole society, making 996 unavoidable.\nDuring rush hour, elderly people were directing traffic at some intersections in Wangjing. When the light turned green, a large number of electric bicycles and bicycles mixed with pedestrians, and I was always worried about getting hit. In the evening, after dinner, on the way back, there was no one directing traffic. The situation of pedestrians, bicycles, and electric bicycles crossing the road at a red light was quite common, resulting in low traffic efficiency when the light turned green. Also, while walking, bicycles and electric bicycles frequently passed by, making me constantly worry about being hit. This feeling of anxiety while walking is uncomfortable.\nI had lunch with a friend and listened to him talk about his work over the past five years. He had changed several companies, experiencing office politics and even a failed P2P company (with unpaid wages and ongoing arbitration). Discussing the recent layoffs in major Beijing tech companies and the recent layoff wave at NetEase, I felt that it\u0026rsquo;s difficult no matter where you are. As he\u0026rsquo;s about to become a father, he, who had never considered leaving Beijing, now wants to leave. If he can\u0026rsquo;t buy a small apartment in Beijing in the future, he might return to his hometown and live in the house he already owns.\nWith cost-cutting measures, many companies are moving to second- and third-tier cities, where rent is four or five times cheaper than in Beijing. If they can recruit people, they can establish themselves in these smaller cities. When I was in Beijing, my roommate worked at China Mobile Research Institute. Later, the institute moved to Suzhou, and he and several colleagues moved there as well. They\u0026rsquo;ve already bought houses there, and the environment is good. They have their own homes, decent salaries, and a comfortable life. Especially in the software industry, some companies allow remote work, so as long as you meet the job requirements, everything else is fine. I know some friends whose company closed its Beijing office, and they now work from home, occasionally traveling to offices in smaller cities for work and team interaction.\nThe question kept lingering in my mind: given another chance, would I still choose to live in Beijing? My decision remains the same as before—to return to my current city.\nI like the work-life balance here. Overtime is voluntary and for self-improvement, not forced for show. I like the sea here; after lunch, I can walk to the beach with colleagues. I like being close to my parents; a half-hour drive allows frequent visits and care. I like living in my own house; I don\u0026rsquo;t have to worry about moving and can continuously improve my living environment. I like that it\u0026rsquo;s not too crowded here, there are all the necessary shops and malls, and the subway makes getting around easy.\nThe training ended at 5 pm on Thursday. After dinner, I rushed to the airport and returned home overnight, returning to work the next day. These two short days, traveling between two cities for work and training, a 40-minute flight, felt like I never left, highlighting the convenience of modern transportation.\nWishing everyone a fulfilling life.\nDecember 1, 2019 23:55:00\n","date":"2019-12-01","externalUrl":null,"permalink":"/en/misc/48h-in-beijing/","section":"Miscs","summary":"A record of my experience attending the JFrog DevOps training camp in Beijing, sharing training content, personal feelings, and reflections on future work.","title":"Beijing 48 Hours — A DevOps Training Camp Experience","type":"misc"},{"content":" Docker 可分为三个版本 # Docker Engine - Community\nDocker Engine - Enterprise\nDocker Enterprise\nDocker Engine - Community 是希望开始使用 Docker 并尝试基于容器的应用程序的个人开发人员和小型团队的理想选择。\nDocker Engine - Enterprise 专为企业开发容器运行时而设计，同时考虑了安全性和企业级SLA。\nDocker Enterprise 专为企业开发和IT团队而设计，他们可以大规模构建，交付和运行关键业务应用程序。\n能力 Docker Engine - Community Docker Engine - Enterprise Docker Enterprise 容器引擎和内建的编配，网络，安全 √ √ √ 认证的基础设施，插件和ISV容器 √ √ 镜像管理 √ 容器应用程序管理 √ 镜像安全扫描 √ 安装 Docker 社区版本 # 以 CentOS 安装为例： https://docs.docker.com/install/linux/docker-ce/centos/ 其他 Docker 版本安装 # 参考 Docker 官网：https://docs.docker.com/install/overview/ ","date":"2019-12-01","externalUrl":null,"permalink":"/posts/2019/overview-of-docker-editions/","section":"Posts","summary":"概述 Docker 的不同版本，包括社区版、企业版和企业级解决方案，适用于不同规模和需求的用户。","title":"Docker 版本概述","type":"posts"},{"content":"对于如何备份 Jenkins 除了用 Jenkins 插件来定期备份之外，如果把 Jenkins 安装到 Docker 里，定期备份一个 Docker Image 最后传到 Artifactory 中，也是一个不错的方案。\n安装 Docker 版 Jenkins # 在 CentOS 上安装 Docker 版 Jenkins，这里推荐用 Long-term Support (LTS) 版本，可以从 Jenkins 官网下载。\n# 下载指定 lts 版本 2.130 sudo docker pull jenkins/jenkins:2.130 # 运行指定 docker Jenkins sudo docker run -p 8080:8080 -p 50000:50000 jenkins/jenkins:2.130 # 如果想下载最新的 lts 版 sudo docker pull jenkins/jenkins:lts # 运行最新的 lts 版 docker Jenkins sudo docker run -p 8080:8080 -p 50000:50000 jenkins/jenkins:lts 启动成功后即可打开 http://hostname:8080/ 网址\n修改登录密码 # 显示所有的 image 以及正在运行的 container\n# 列出来所有 image sudo docker image list # 列出当前运行的 container sudo docker ps # 进入容器，使用 -it 参数 sudo docker exec -it 39bc7a8307d9 /bin/bash # 查看默认 admin 密码 jenkins@a6195912b579:/$ cat /var/jenkins_home/secrets/initialAdminPassword 5193d06c813d46d3b18babeda836363a 建议登录之后，修改 admin 密码，方便下次登录\nsudo docker commit 39bc7a8307d9 myjenkins:v0.1 将宿主机目录映射到 Jenkins Docker 中 # 如果想让 Docker 里的 Jenkins 可以访问宿主机的目录，在运行 docker 时使用 -v 参数进行 mount volume\nsudo docker run -p 8080:8080 -p 50000:50000 --name mydata -v /data/backup:/home/backup jenkins/jenkins:2.130 # 映射成功，可以看到宿主机上的备份文件了 jenkins@c85db3f88115:/home/backup$ ls FULL-2019-09-14_02-00 FULL-2019-09-28_02-00 FULL-2019-10-19_02-00 FULL-2019-11-02_02-00 FULL-2019-11-23_02-00 FULL-2019-09-21_02-00 FULL-2019-10-05_02-00 FULL-2019-10-26_02-00 FULL-2019-11-09_02-00 FULL-2019-11-30_02-00 将 Jenkins Docker Image 保存在 Artifactory # 下载并安装 Artifactory 企业版或是 JFrog Container Registry，注意 Artifactory Open Source 版本不支持 Docker Registry。\n例如我的 JFrog Container Registry 是：dln.dev.mycompany.com:8040，并创建了一个 docker repository 叫 docker-local。\n上传 Docker Image 一共分为三步：\ndocker login\n# 在登录前需要添加如下配置到 /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34; : [\u0026#34;dln.dev.mycompany.com:8040\u0026#34;] } # docker login \u0026lt;DOCKER_SERVER\u0026gt;, example below: sudo docker login dln.dev.mycompany.com:8040 docker tag\n# docker tag \u0026lt;IMAGE_ID\u0026gt; artprod.mycompany/\u0026lt;DOCKER_REPOSITORY\u0026gt;:\u0026lt;DOCKER_TAG\u0026gt;, example below: sudo docker tag myjenkins:v0.1 dln.dev.mycompany.com:8040/docker-local/myjenkins:v0.1 docker push\n# docker push artprod.mycompany/\u0026lt;DOCKER_REPOSITORY\u0026gt;:\u0026lt;DOCKER_TAG\u0026gt;, example below: $ sudo docker push dln.dev.mycompany.com:8040/docker-local/myjenkins::v0.1 The push refers to repository [dln.dev.mycompany.com:8040/docker-local/myjenkins] 98d59071f692: Pushed af288f00b8a7: Pushed 4b955941a4d0: Pushed f121afdbbd5d: Pushed 15.10: digest: sha256:a3f5e428c0cfbfd55cffb32d30b1d78fedb8a9faaf08efdd9c5208c94dc66614 size: 1150 登录 JFrog Container Registry 刷新就可以到已经上次的 Image 了。说明：截图是我上传的另外一个镜像 ubuntu:15.10\n","date":"2019-12-01","externalUrl":null,"permalink":"/posts/2019/install-docker-jenkins/","section":"Posts","summary":"如何定制一个 Docker 版 Jenkins 镜像，并将其备份到 Artifactory，便于在需要时快速恢复 Jenkins 环境。","title":"定制一个 Docker 版 Jenkins 镜像","type":"posts"},{"content":"上一篇 初识 JFrog Artifactory，介绍了什么是 Artifactory，以及如何安装、启动和升级。\n本篇介绍 Artifactory 与 Jenkins 的集成，因为没有与 CI 工具集成的 Artifactory 是没有灵魂的。\n通过集成，可以让 Jenkins 在完成构建之后，可以直接将制品（比如 build）推送到 Artifactory，供测试下载、部署或是后续的 Jenkins 任务去继续进行持续集成。\nJenkins 里配置 Artifactory # 打开 Manage Jenkins-\u0026gt;Configure System，找到 Artifactory，点击 Add Artifactory Server， 输入 Server ID 和 URL\nServer ID 是给你的 Artifactory 起个别名，这样使用 Jenkins pipeline 的时候会用到 URL 是你的 Artifactory 服务器的地址，例如 http://art.company.com:8040/artifactory 配置完成后，点击Test Connection，返回 Found Artifactory 6.14.0 表示配置成功。 如图所示: 使用 Pipeline 调用 Artifactory # 这里演示了两种方式，我在项目中用的是 Jenkins Shared Library；当然你也可以仅仅使用 Jenkinsfile，把如下两个 groovy 文件组合成一个 Jenkinsfile。\n方式1：Jenkins Shared Library # build.groovy\ndef call() { pipeline { # 省略其他代码 post { # 这里只有在 Jenkins Job 成功的时候才将 build post 到 artifactory success { script { if (env.BRANCH_NAME == \u0026#39;develop\u0026#39;) { # 如果当前是 develop 分支，则将 release 和 debug build 都 post 到 artifactory artifactory(\u0026#34;${PATTERN_RELEASE_PATH}\u0026#34;, \u0026#34;${TARGET_PATH}\u0026#34;, \u0026#34;${BUILD_NAME}\u0026#34;, \u0026#34;${BUILD_NUMBER}\u0026#34;) artifactory(\u0026#34;${PATTERN_DEBUG_PATH}\u0026#34;, \u0026#34;${TARGET_PATH}\u0026#34;, \u0026#34;${BUILD_NAME}\u0026#34;, \u0026#34;${BUILD_NUMBER}\u0026#34;) } else if (env.BRANCH_NAME.startsWith(\u0026#39;PR\u0026#39;)) { # 如果当前是 pull request 分支，则只将 release build 都 post 到 artifactory artifactory(\u0026#34;${PATTERN_RELEASE_PATH}\u0026#34;, \u0026#34;${TARGET_PATH}\u0026#34;, \u0026#34;${BUILD_NAME}\u0026#34;, \u0026#34;${BUILD_NUMBER}\u0026#34;) } } } } } } artifactory.groovy\nimport groovy.transform.Field @Field artifactoryServerId = \u0026#34;art-1\u0026#34; @Field artifactoryURL = \u0026#34;http://art.company.com:8040/artifactory\u0026#34; @Field artifactoryCredential = \u0026#34;d1cbab74-823d-41aa-abb7\u0026#34; def call(String patternPath, String targetPath, String buildName, String buildNumber) { rtServer ( id: \u0026#34;${artifactoryServerId}\u0026#34;, url: \u0026#34;${artifactoryURL}\u0026#34;, credentialsId: \u0026#34;${artifactoryCredential}\u0026#34; ) rtPublishBuildInfo ( serverId: \u0026#34;${artifactoryServerId}\u0026#34; ) rtUpload ( serverId: \u0026#34;${artifactoryServerId}\u0026#34;, spec: \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;files\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;${patternPath}\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;${targetPath}\u0026#34; } ] }\u0026#34;\u0026#34;\u0026#34;, buildNumber: \u0026#34;${buildNumber}\u0026#34;, buildName: \u0026#34;${buildName}\u0026#34;, ) } 方式2：Jenkinsfile # pipeline { # 省略其他代码 stage(\u0026#39;config art\u0026#39;){ rtServer ( id: \u0026#34;art-1\u0026#34;, url: \u0026#34;http://art.company.com:8040/artifactory\u0026#34;, credentialsId: \u0026#34;d1cbab74-823d-41aa-abb7\u0026#34;) } post { # 这里只有在 Jenkins Job 成功的时候才将 build post 到 artifactory success { script { if (env.BRANCH_NAME == \u0026#39;develop\u0026#39;) { rtUpload ( serverId: \u0026#34;art-1\u0026#34;, spec: \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;files\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;/release/build/*.zip\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;demo/develop/\u0026#34; } ] }\u0026#34;\u0026#34;\u0026#34;, buildNumber: \u0026#34;${buildNumber}\u0026#34;, buildName: \u0026#34;${buildName}\u0026#34;, ) } else if (env.BRANCH_NAME.startsWith(\u0026#39;PR\u0026#39;)) { rtUpload ( serverId: \u0026#34;art-1\u0026#34;, spec: \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;files\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;/release/build/*.zip\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;demo/pull-request/\u0026#34; } ] }\u0026#34;\u0026#34;\u0026#34;, buildNumber: \u0026#34;${buildNumber}\u0026#34;, buildName: \u0026#34;${buildName}\u0026#34;, ) } } } } } Jenkins 与 Artifactory 集成成功 # 蓝色表示构建成功，绿色圆圈表示 Build 已经 Post 到 Artifactory 上了。\n点击绿色圆圈可以跳转到 Artifactory 看到制品。\nJenkins 与 Artifactory 打通了。完！\n","date":"2019-11-17","externalUrl":null,"permalink":"/posts/2019/artifactory-integrate-with-jenkins/","section":"Posts","summary":"本文介绍如何将 JFrog Artifactory 与 Jenkins 集成，实现持续集成和制品管理。","title":"Artifactory 与 Jenkins 集成","type":"posts"},{"content":" What is Artifactory # Artifactory is a product from JFrog that serves as a binary repository manager. A binary repository can unify the management of all these binaries, making team management more efficient and simpler.\nJust like you use Git to manage code, Artifactory is used to manage binary files, typically referring to jar, war, pypi, DLL, EXE, and other build files.\nI believe the biggest advantage of using Artifactory is that it creates a better continuous integration environment, helping other continuous integration tasks to call from Artifactory and then deploy to different test or development environments. This is crucial for implementing DevOps.\nTo learn more about Artifactory, please refer to the Chinese website and the English Website.\nInstalling Artifactory # Download the Open Source Artifactory from the official website. This demonstration shows installation on Linux, so click Download RPM to download. Upload the downloaded jfrog-artifactory-oss-6.14.0.rpm to Linux. # Create a file in the root directory. You can also create a folder in any directory. sudo mkdir /artifactory cd /artifactory # Upload the downloaded jfrog-artifactory-oss-6.15.0.rpm to your Linux. $ ls jfrog-artifactory-oss-6.14.0.rpm # Install artifactory sudo rpm -ivh jfrog-artifactory-oss-6.14.0.rpm Starting and Stopping the Artifactory Service # # Start the service sudo systemctl start artifactory.service # If you encounter the following error when starting the service using the above command: # Job for artifactory.service failed because a configured resource limit was exceeded. See \u0026#34;systemctl status artifactory.service\u0026#34; and \u0026#34;journalctl -xe\u0026#34; for details. # Details: https://www.jfrog.com/jira/browse/RTFACT-19988 # You can try starting with the following command cd /opt/jfrog/artifactory/app/bin \u0026amp;\u0026amp; ./artifactory.sh start \u0026amp; # Stop the service sudo systemctl stop artifactory.service # Check service status sudo systemctl status artifactory.service Accessing Artifactory # Artifactory\u0026rsquo;s default port is 8040. After successful installation, access http://hostname:8040 to log in (default username: admin, password: password). Upgrading Artifactory # Download the latest Artifactory from the official website.\nUpload the downloaded jfrog-artifactory-oss-6.15.0.rpm (currently the latest) to your Linux.\ncd /artifactory ls jfrog-artifactory-oss-6.14.0.rpm jfrog-artifactory-oss-6.15.0.rpm # Stop the service sudo systemctl stop artifactory.service # Perform the upgrade sudo rpm -U jfrog-artifactory-oss-6.15.0.rpm # Output log, showing successful upgrade warning: jfrog-artifactory-oss-6.15.0.rpm: Header V4 DSA/SHA1 Signature, key ID d7639232: NOKEY Checking if ARTIFACTORY_HOME exists Removing tomcat work directory Removing Artifactory\u0026#39;s exploded WAR directory Initializing artifactory service with systemctl... ************ SUCCESS **************** The upgrade of Artifactory has completed successfully. Start Artifactory with: \u0026gt; systemctl start artifactory.service Check Artifactory status with: \u0026gt; systemctl status artifactory.service NOTE: Updating the ownership of files and directories. This may take several minutes. Do not stop the installation/upgrade process. Uninstalling Artifactory # Stop the Artifactory service systemctl stop artifactory.service Use the root user to execute the RPM uninstall command # remove OSS version yum erase jfrog-artifactory-oss # remove PRO version, etc. yum erase jfrog-artifactory-pro For more information on uninstalling JFrog products, see: https://www.jfrog.com/confluence/display/JFROG/Uninstalling+JFrog+Products\nInstalling JFrog CLI # # ON MAC brew install jfrog-cli-go # WITH CURL curl -fL https://getcli.jfrog.io | sh # WITH NPM npm install -g jfrog-cli-go # WITH DOCKER docker run docker.bintray.io/jfrog/jfrog-cli-go:latest jfrog -v CLI for JFrog Artifactory\nHow to use the JFrog CLI on Artifactory\n","date":"2019-11-10","externalUrl":null,"permalink":"/en/posts/2019/artifactory-install-and-upgrade/","section":"Posts","summary":"JFrog Artifactory is a powerful binary repository manager. This article introduces its installation, upgrade, and usage.","title":"Getting Started with JFrog Artifactory","type":"posts"},{"content":"For example, I have a shared repository for code that\u0026rsquo;s very large (over 20G). Each product build requires this repository\u0026rsquo;s code (copying third-party libraries from it). If everyone needs to git clone this third-party repository, the network overhead is very large, git clone takes a long time, and it occupies a lot of physical space.\nThis can be solved by using NFS sharing.\nAdditionally, I want this code repository to update automatically. This introduces Jenkins. It checks for code commits to this large repository and automatically executes git pull to update the latest code to the shared server.\nWhat is NFS? NFS (Network File System) is a type of file system supported by FreeBSD that allows computers on a network to share resources. In NFS applications, local NFS clients can transparently read and write files located on a remote NFS server, just like accessing local files. On Windows, this is commonly known as a share.\nSetting Up NFS # # For example, on Linux, the shared server\u0026#39;s IP is 192.168.1.1 sudo vi /etc/exports # The following is the configuration of my exports file # Assume the internal network IP range is 192.168.1.1 ~ 192.168.1.250 # ro means read-only # all_squash means regardless of the user using NFS, their identity will be limited to a specified ordinary user identity (nfsnobody) /agent/workspace/opensrc 192.168.1.*(ro,all_squash) /agent/workspace/opensrc dev-team-a*.com(ro,all_squash) /agent/workspace/opensrc dev-team-b*.com(ro,all_squash) /agent/workspace/opensrc dev-ci*(ro,all_squash) NFS Operations # Starting the NFS Service # To start the NFS service, you need to start both the portmap and nfs services. portmap must be started before nfs.\nservice portmap start service nfs start # Check portmap status service portmap status Checking Service Status # service nfs status Stopping the Service # service nfs stop Exporting Configuration # After changing the /etc/exports configuration file, you don\u0026rsquo;t need to restart the NFS service; use exportfs directly.\nsudo exportfs -rv Mounting on Different Platforms # Windows # # Install the NFS Client (Services for NFS) # Step 1: Open Programs and Features # Step 2: Click Turn Windows features on or off # Step 3: Find and check option Services for NFS # Step 4: Once installed, click Close and exit back to the desktop. refer to https://graspingtech.com/mount-nfs-share-windows-10/ $ mount -o anon 192.168.1.1:/agent/workspace/opensrc Z: Linux/Unix # # Linux sudo mount -t nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc # AIX sudo nfso -o nfs_use_reserved_ports=1 # should only first time mount need to run this command sudo mount -F nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc # HP-UX sudo mount -F nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc # Solaris-SPARC # If you can\u0026#39;t execute mount directly from the command line sudo /usr/sbin/mount -F nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc ","date":"2019-09-10","externalUrl":null,"permalink":"/en/posts/2019/nfs/","section":"Posts","summary":"This article introduces the steps and commands for setting up NFS sharing and mounting it on different platforms (Windows/Linux/Unix).","title":"How to Set Up NFS Sharing and Mount on Different Platforms—Windows/Linux/Unix","type":"posts"},{"content":"","date":"2019-09-10","externalUrl":null,"permalink":"/en/tags/nfs/","section":"Tags","summary":"","title":"NFS","type":"tags"},{"content":"Recently, while running a Jenkins Job, I encountered this error during a git clone operation:\n$ git clone ssh://git@git.companyname.com:7999/repo/opensrc.git Cloning into \u0026#39;opensrc\u0026#39;... fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. I only encountered this error when I first started using Git, back when I didn\u0026rsquo;t know how to clone code using SSH. Why did this error appear now? I haven\u0026rsquo;t changed anything, which is very confusing.\nCommon Solutions # My Google searches didn\u0026rsquo;t yield a solution for my specific problem. Most results suggested that the issue was due to not having generated an SSH key, and that generating one and adding the public key to GitHub or another web Git management platform would solve the problem. Using GitHub as an example:\nFirst, generate an SSH key:\n# Remember to replace with your own email address ssh-keygen -t rsa -C xianpeng.shen@gmail.com Second, copy the SSH public key to your Git web platform, such as GitHub.\ncd %userprofile%/.ssh # Open id_rsa.pub and copy its contents notepad id_rsa.pub Finally, open https://github.com/settings/ssh/new and paste the copied content there to save it.\nThis solution was ineffective for my problem, because the same account worked without issue on other virtual machines. Since it was also an HP-UX virtual machine, I generated an SSH key using a different account and the git clone operation worked perfectly. This led me to suspect a difference between the two accounts.\nTroubleshooting via SSH Connection Test # First, I examined the .gitconfig files for both accounts and found differences. Copying the contents of the .gitconfig file from the working account to the problematic one didn\u0026rsquo;t resolve the issue.\nSecond, I noticed that a core file was generated in the current directory during the git clone execution, indicating a coredump. However, opening the core file mostly resulted in gibberish, making it difficult to pinpoint the error.\nFinally, I tested the SSH connection using commands. For GitHub, the command is:\nssh -T git@github.com My problematic clone was using Bitbucket, and its SSH connection test command is:\nssh -vvv git\\@bitbucket.org First, I tested with the account that successfully performed git clone. I\u0026rsquo;m omitting some of the output here for brevity.\n$ ssh -vvv git\\@bitbucket.org OpenSSH_6.2p1+sftpfilecontrol-v1.3-hpn13v12, OpenSSL 0.9.8y 5 Feb 2013 # Different OpenSSH version HP-UX Secure Shell-A.06.20.006, HP-UX Secure Shell version # Different call path debug1: Reading configuration data /opt/ssh/etc/ssh_config debug3: RNG is ready, skipping seeding debug2: ssh_connect: needpriv 0 debug1: Connecting to bitbucket.org [18.205.93.1] port 22. debug1: Connection established. ... ... debug2: we did not send a packet, disable method debug1: No more authentication methods to try. Permission denied (publickey). Then, I tested with the account that failed the git clone operation:\n$ ssh -vvv git\\@bitbucket.org OpenSSH_8.0p1, OpenSSL 1.0.2s 28 May 2019 # Different OpenSSH version debug1: Reading configuration data /usr/local/etc/ssh_config # Different call path debug2: resolving \u0026#34;bitbucket.org\u0026#34; port 22 debug2: ssh_connect_direct debug1: Connecting to bitbucket.org [180.205.93.10] port 22. debug1: Connection established. Memory fault(coredump) $ Clearly, different versions of OpenSSH were used, indicating differences in their environment variables. I had previously checked the environment variables, but the large number of variables made it difficult to immediately identify the culprit.\nFinal Solution # I revisited the .profile file for the account where the git clone failed. There was an extra /usr/bin entry in the environment variables, causing this account to use a different version of OpenSSH. HP-UX has very strict requirements on package dependencies and versions. After removing this environment variable, saving the changes, logging back into the virtual machine, and executing git clone, the operation returned to normal.\n","date":"2019-09-01","externalUrl":null,"permalink":"/en/posts/2019/could-not-read-from-remote-repository/","section":"Posts","summary":"Resolving the “Could not read from remote repository” error encountered when cloning code using Git, analyzing the causes, and providing solutions.","title":"Resolving the \"Could not read from remote repository\" Issue","type":"posts"},{"content":" If your commits on local not pushed to remote # combine local commits, you could follow this flow # Here is short video (only 3 minutes) and good explanation of git rebase -i usage.\nlist your local repository log\nIf you want to combine these 3 commits (add6152, 3650100, 396a652) to 1 commit, execute this command\ngit rebase -i HEAD~3 # last three commits Select which commit you want to squash (type s or squash are OK)\nthen press ESC, enter :wq! to save and exit.\nComment out some commits message you don\u0026rsquo;t need, press ESC, enter :wq! to save and exit.\nCheck log, you will see your local repository logs has combine to one commit\nIf your commits had pushed to remote # combine remote commits, you could follow this flow # list your repository logs\n# so you can create another branch from bugfix/UNV-1234 named bugfix/UNV-1234-for-squash xshen@dln-l-xs01 MINGW64 /c/U2GitCode/git-test (bugfix/UNV-1234) $ git checkout -b bugfix/UNV-1234-for-squash Switched to a new branch \u0026#39;bugfix/UNV-1234-for-squash\u0026#39; # combine last 2 commits $ git rebase -i HEAD~2 change one commit from pick to squash, see the screenshot below. press ESC, enter :wq! to save and exit.\nchange commit message, for example \u0026ldquo;UNV-1234 combine all commit to one commit\u0026rdquo;, then press ESC, enter :wq! to save and exit.\n# push your new create branch to remote. git push -u origin bugfix/UNV-1234-for-squash ","date":"2019-08-20","externalUrl":null,"permalink":"/en/posts/2019/git-commit-squash/","section":"Posts","summary":"How to squash multiple Git commits into a single commit, both locally and remotely, using interactive rebase and merge strategies in Bitbucket.","title":"Git Commit Squash","type":"posts"},{"content":"","date":"2019-08-20","externalUrl":null,"permalink":"/en/tags/squash/","section":"Tags","summary":"","title":"Squash","type":"tags"},{"content":" 业务场景 # 日常工作中需要切换到不同平台（包括 Linux, AIX, Windows, Solris, HP-UX）不同的版本进行开发和验证问题，但是由于虚拟机有限，并不能保证每个开发和测试都有所以平台的虚拟机并且安装了不同的版本，因此准备各种各样的开发和测试环境会花费很长时间。\n需求分析 # 对于这样的需求，一般都会首先想到 Docker；其次是从 Artifactory 取 Build 然后通过 CI 工具进行安装；最后从 Source Code 进行构建然后安装。\n先说 Docker，由于我们所支持的平台繁多，包括 Linux, AIX, Windows, Solris, HP-UX, Docker 只适用于 Linux 和 Windows，因此不能满足这样的需求。\n由于其他原因我们的 Artifactory 暂时还不能使用，最后只能选择用 Source Code 进行构建然后进行安装。这两种方式都需要解决锁定资源以及释放资源的问题。如果当前环境有人正在使用，那么这台虚拟机的资源应该被锁住，不允许 Jenkins 再去调用这台正在使用的 node，以保证环境在使用过程中不被破坏。\n本文主要介绍如何通过 Jenkins Lockable Resources Plugin 来实现资源的上锁和解锁。\n演示 Demo # 设置 Lockable Resources\nJenkins -\u0026gt; configuration -\u0026gt; Lockable Resources Manager -\u0026gt; Add Lockable Resource 这里的 Labels 是你的 node 的 Label，在 Jenkins -\u0026gt; Nodes 设置 查看 Lockable Resources 资源池\n测试锁资源\n这里我配置的是参数化类型的 Job，可以选择不同平台，不同仓库进行构建 build-with-parameters 运行第一个 Job 查看当前可用资源数量 Free resources = 1，看到已经被 #47 这个 Job 所使用 继续运行第二个 Job 查看当前可用资源数量 Free resources = 0，看到已经被 #48 这个 Job 所使用 最关键是这一步，如果继续运行第三个 Job，是否能够被继续行呢 可以看到这个任务没有开始执行，看下 log 是否真的没有被执行。通过日志发现，当前正在等待可用的资源 测试释放锁\n现在释放一个资源，看下第三个 Job 是否能拿到资源，并且执行 从下图可以看到 第三个 Job 已经运行成功了 Jenkins pipeline 代码 # 整个 pipeline 最关键的部分就是如何上锁和释放，这里是通过 lock 和 input message 来实现。\n当前 Job 只要用户不点击 Yes，就会一直处于没有完成的状态，那么的它的锁会一直生效中。直到点击 Yes， Job 结束，锁也就释放了。\n具体可以参考下面的 Jenkinsfile。\npipeline { agent { node { label \u0026#39;PreDevENV\u0026#39; } } options { lock(label: \u0026#39;PreDevENV\u0026#39;, quantity: 1) } parameters { choice( name: \u0026#39;platform\u0026#39;, choices: [\u0026#39;Linux\u0026#39;, \u0026#39;AIX\u0026#39;, \u0026#39;Windows\u0026#39;, \u0026#39;Solris\u0026#39;, \u0026#39;HP-UX\u0026#39;], summary: \u0026#39;Required: which platform do you want to build\u0026#39;) choice( name: \u0026#39;repository\u0026#39;, choices: [\u0026#39;repo-0.1\u0026#39;, \u0026#39;repo-1.1\u0026#39;, \u0026#39;repo-2.1\u0026#39;, \u0026#39;repo-3.1\u0026#39;, \u0026#39;repo-4.1\u0026#39;], summary: \u0026#39;Required: which repository do you want to build\u0026#39;) string( name: \u0026#39;branch\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Required: which branch do you want to build\u0026#39;) } stages { stage(\u0026#39;git clone\u0026#39;){ steps { echo \u0026#34;git clone source\u0026#34; } } stage(\u0026#39;start build\u0026#39;){ steps { echo \u0026#34;start build\u0026#34; } } stage(\u0026#39;install build\u0026#39;){ steps{ echo \u0026#34;installing\u0026#34; } } stage(\u0026#39;unlock your resource\u0026#39;){ steps { input message: \u0026#34;do you have finished?\u0026#34; echo \u0026#34;Yes, I have finished\u0026#34; } } } } ","date":"2019-08-10","externalUrl":null,"permalink":"/posts/2019/jenkins-lock-resource/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 的 Lockable Resources 插件来管理和锁定资源，确保在多任务环境中资源的独占性和安全性。","title":"如何通过 Jenkins 进行资源的锁定和释放","type":"posts"},{"content":"在持续集成中，你可能需要通过 Jenkins 来修改代码，并且将修改后的代码提交到Git仓库里。怎么做呢？最方便的做法还是 Jenkins 提供对应的插件，但是很遗憾我没找到合适的。另外我也觉得通过脚本的方式来实现会更加稳定，不用担心 Jenkins 以及插件升级带来潜在不好用的可能。\n以下 pipeline 片段供参考使用：\n// This pipeline is used for bumping build number pipeline { environment { MYGIT = credentials(\u0026#34;d1cbab74-823d-41aa-abb7\u0026#34;) } stages { stage(\u0026#39;Git clone repo\u0026#39;) { steps { sh \u0026#39;git clone -b develop --depth 1 https://$MYGIT_USR:\u0026#34;$MYGIT_PSW\u0026#34;@github.com/shenxianpeng/blog.git\u0026#39; } } stage(\u0026#39;Change code stage\u0026#39;){ steps { sh \u0026#39;\u0026#39; } } stage(\u0026#39;Git push to remote repo\u0026#39;) { steps { sh label: \u0026#39;\u0026#39;, script: \u0026#39;\u0026#39;\u0026#39; cd blog git add . git commit -m \u0026#34;Bld # 1001\u0026#34; git push https://$MYGIT_USR:\u0026#34;$MYGIT_PSW\u0026#34;@github.com/shenxianpeng/blog.git --all\u0026#39;\u0026#39;\u0026#39; } } } } 这里面我所遇到最大的坑，我之前脚本是这样写的：\nstage(\u0026#39;Git push to remote\u0026#39;) { // not works script steps { sh \u0026#39;cd blog\u0026#39; sh \u0026#39;git add .\u0026#39; sh \u0026#39;git commit -m \u0026#34;${JIRA_NO} Bld # ${BUILD_NO}\u0026#34;\u0026#39; sh \u0026#39;git push https://$MYGIT_USR:\u0026#34;$MYGIT_PSW\u0026#34;@github.com/shenxianpeng/blog.git --all\u0026#39; } } 在最后一个阶段提交代码时，shell 脚本不能使用单引号 \u0026lsquo;\u0026rsquo;，要使用三引号才行\u0026rsquo;\u0026rsquo;\u0026rsquo; \u0026lsquo;\u0026rsquo;\u0026rsquo;。我在这里花了很多时间，一直找不到问题所在，因为我在上面的shell脚本使用的时候用单引号 \u0026rsquo;\u0026rsquo; 可以正常 git clone 代码，但在提交代码时不行，最后我 Jenkins 的 Pipeline Syntax 生成的脚本，提交代码成功。\n","date":"2019-07-22","externalUrl":null,"permalink":"/posts/2019/git-push-by-jenkins/","section":"Posts","summary":"如何通过 Jenkins Pipeline 脚本来提交修改的代码到 Git 仓库，包括克隆仓库、修改代码和推送更改等步骤。","title":"通过 Jenkins 来提交修改的代码 git push by Jenkins","type":"posts"},{"content":"","date":"2019-07-16","externalUrl":null,"permalink":"/tags/os/","section":"标签","summary":"","title":"OS","type":"tags"},{"content":"在使用 Jenkins pipeline 的时候，在 Linux 需要用 root 来执行，我想通过 Jenkins pipeline 的语法来解决，但是只找到这种方式：SSH Pipeline Steps\ndef remote = [:] remote.name = \u0026#39;test\u0026#39; remote.host = \u0026#39;test.domain.com\u0026#39; remote.user = \u0026#39;root\u0026#39; remote.password = \u0026#39;password\u0026#39; remote.allowAnyHosts = true stage(\u0026#39;Remote SSH\u0026#39;) { sshCommand remote: remote, command: \u0026#34;ls -lrt\u0026#34; sshCommand remote: remote, command: \u0026#34;for i in {1..5}; do echo -n \\\u0026#34;Loop \\$i \\\u0026#34;; date ; sleep 1; done\u0026#34; } * command * Type: String * dryRun (optional) * Type: boolean * failOnError (optional) * Type: boolean * remote (optional) * Nested Choice of Objects * sudo (optional) * Type: boolean 从 example 来看需要提供的参数比较多，很多参数我已经在 Pipeline 的 environment 已经设置过了，这里再设置就显得不够优美，且限于没有足够的 example，你知道的 Jenkinsfile 调试非常痛苦和麻烦，我就没通过这种方式来尝试解决。\n通过 Linux 设置来解决\n// open a shell console and type $ sudo visudo // type your user name jenkins ALL=(ALL) NOPASSWD: ALL 但即使这样设置，通过 Jenkins 执行 shell 脚本的时候还是出现如下问题\nsudo: no tty present and no askpass program specified 最后通过如下脚本解决了我的问题\n// Jenkinsfile environment { JENKINS = credentials(\u0026#34;d1cbab74-823d-41aa-abb7-85848595\u0026#34;) } sh \u0026#39;sudo -S \u0026lt;\u0026lt;\u0026lt; \u0026#34;$JENKINS_PSW\u0026#34; sh test.sh\u0026#39; 如果你有更好的方式，欢迎留言评论，谢谢。\n","date":"2019-07-16","externalUrl":null,"permalink":"/posts/2019/execute-sudo-without-password/","section":"Posts","summary":"本文介绍了如何在 Jenkins Pipeline 中执行 sudo 命令而无需输入密码，提供了具体的实现方法和示例代码。","title":"在 Jenkins pipeline 中执行 sudo 的时候不需要输入密码","type":"posts"},{"content":"","date":"2019-07-07","externalUrl":null,"permalink":"/tags/disqus/","section":"标签","summary":"","title":"Disqus","type":"tags"},{"content":" 查找是否有遗漏提交 # 从一个分支找到所有的 commit 和 ticket 号，然后去另外一个分支去查找这些提交是否也在这个分支里。\n找一个分支的所有 commit 和 ticket 号\n# 从 develop 分支上获取所有的 commit 和 ticket 号，然后根据 ticket 号进行排序 git log origin/develop --pretty=oneline --abbrev-commit | cut -d\u0026#39; \u0026#39; -f2,1 | sort -t \u0026#39; \u0026#39; -k 2 \u0026gt;\u0026gt; develop_involve_tickets.txt --pretty=oneline # 显示为一行 --abbrev-commit # 显示短的提交号 cut --help # 切出来所需要的字段 -d # 字段分隔符, \u0026#39; \u0026#39;分隔空格 -f # 只选择某些字段 sort --help # 利用 sort 将剪出来的字段进行排序 -t # 字段分隔， \u0026#39; \u0026#39;分隔空格 -k # 通过键进行键定义排序;KEYDEF 给出位置和类型 然后去另外一个分支去找是否有次提交\n由于在 SVN 时代时，每次修改都会在描述里添加 ticket 号，所以切换到 master 分支后，直接搜索所有 ticket 号是否存在就好了.\n#!/bin/bash filename=\u0026#39;C:\\develop_involve_tickets.txt\u0026#39; while read line do echo $line var=`grep -ir $line src` if [[ -z $var ]];then echo \u0026#34;not found\u0026#34; echo $line \u0026gt;\u0026gt; ../not_found_in_master.txt else echo \u0026#34;found\u0026#34; echo $line \u0026gt;\u0026gt; ../found_in_master.txt fi done \u0026lt; \u0026#34;$filename\u0026#34; ","date":"2019-07-07","externalUrl":null,"permalink":"/posts/2019/git-management/","section":"Posts","summary":"本文介绍了 Git 的常见管理操作，包括分支管理、提交规范、代码审查等，帮助开发者更好地使用 Git 进行版本控制。","title":"Git 管理","type":"posts"},{"content":"","date":"2019-07-07","externalUrl":null,"permalink":"/categories/hexo/","section":"Categories","summary":"","title":"Hexo","type":"categories"},{"content":" 在你的 Hexo 网站添加 Disqus # 去 Disqus 创建一个账号，在这个过程中有需要选择一个 shortname，完成后，你可以在设置页码找到你的 shortname\nhttps://YOURSHORTNAMEHERE.disqus.com/admin/settings/general 在你 Hexo 博客里打开 _config.yml, 然后输入 disqus_shortnameand: YOURSHORTNAMEHERE，像这样：\ndisqus_shortname: myshortnamegoeshere comments: true 也需要更改 _config.yml 文件如下，例如我的：\n# 修改默认 url: http://yoursite.com 为： url: https://shenxianpeng.github.io 复制这段代码到 blog\\themes\\landscape\\layout\\_partial\\footer.ejs\n\u0026lt;% if (config.disqus_shortname){ %\u0026gt; \u0026lt;script\u0026gt; var disqus_shortname = \u0026#39;\u0026lt;%= config.disqus_shortname %\u0026gt;\u0026#39;; \u0026lt;% if (page.permalink){ %\u0026gt; var disqus_url = \u0026#39;\u0026lt;%= page.permalink %\u0026gt;\u0026#39;; \u0026lt;% } %\u0026gt; (function(){ var dsq = document.createElement(\u0026#39;script\u0026#39;); dsq.type = \u0026#39;text/javascript\u0026#39;; dsq.async = true; dsq.src = \u0026#39;//go.disqus.com/\u0026lt;% if (page.comments){ %\u0026gt;embed.js\u0026lt;% } else { %\u0026gt;count.js\u0026lt;% } %\u0026gt;\u0026#39;; (document.getElementsByTagName(\u0026#39;head\u0026#39;)[0] || document.getElementsByTagName(\u0026#39;body\u0026#39;)[0]).appendChild(dsq); })(); \u0026lt;/script\u0026gt; \u0026lt;% } %\u0026gt; 也需要复制这些文件到 footer.ejs 到最底部：\n\u0026lt;div id=\u0026#34;disqus_thread\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 最后 footer.ejs 文件是这样的：\n\u0026lt;% if (theme.sidebar === \u0026#39;bottom\u0026#39;){ %\u0026gt; \u0026lt;%- partial(\u0026#39;_partial/sidebar\u0026#39;) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;% if (config.disqus_shortname){ %\u0026gt; \u0026lt;script\u0026gt; var disqus_shortname = \u0026#39;\u0026lt;%= config.disqus_shortname %\u0026gt;\u0026#39;; \u0026lt;% if (page.permalink){ %\u0026gt; var disqus_url = \u0026#39;\u0026lt;%= page.permalink %\u0026gt;\u0026#39;; \u0026lt;% } %\u0026gt; (function(){ var dsq = document.createElement(\u0026#39;script\u0026#39;); dsq.type = \u0026#39;text/javascript\u0026#39;; dsq.async = true; dsq.src = \u0026#39;//go.disqus.com/\u0026lt;% if (page.comments){ %\u0026gt;embed.js\u0026lt;% } else { %\u0026gt;count.js\u0026lt;% } %\u0026gt;\u0026#39;; (document.getElementsByTagName(\u0026#39;head\u0026#39;)[0] || document.getElementsByTagName(\u0026#39;body\u0026#39;)[0]).appendChild(dsq); })(); \u0026lt;/script\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;footer id=\u0026#34;footer\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;outer\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;footer-info\u0026#34; class=\u0026#34;inner\u0026#34;\u0026gt; \u0026amp;copy; \u0026lt;%= date(new Date(), \u0026#39;YYYY\u0026#39;) %\u0026gt; \u0026lt;%= config.author || config.title %\u0026gt;\u0026lt;br\u0026gt; \u0026lt;%= __(\u0026#39;powered_by\u0026#39;) %\u0026gt; \u0026lt;a href=\u0026#34;http://hexo.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Hexo\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;div id=\u0026#34;disqus_thread\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 最后清理和构建\nhexo clean hexo generate \u0026amp;\u0026amp; hexo server 现在你可以看到我的博客已经可以添加评论了 : )\n","date":"2019-07-07","externalUrl":null,"permalink":"/posts/2019/hexo-add-disqus/","section":"Posts","summary":"在 Hexo 博客中集成 Disqus 评论系统，允许读者留言和互动。","title":"Hexo 添加 Disqus 留言功能","type":"posts"},{"content":"这个pipeline里包含了如下几个技术：\n如何使用其他机器，agent 如何使用环境变量，environment 如何在build前通过参数化输入，parameters 如何使用交互，input 如何同时clone多个repos 如何进行条件判断，anyOf pipeline { agent { node { label \u0026#39;windows-agent\u0026#39; } } environment { MY_CRE = credentials(\u0026#34;2aee7e0c-a728-4d9c-b25b-ad5451a12d\u0026#34;) } parameters { // Jenkins parameter choice( name: \u0026#39;REPO\u0026#39;, choices: [\u0026#39;repo1\u0026#39;, \u0026#39;repo2\u0026#39;, \u0026#39;repo3\u0026#39;, \u0026#39;repo4\u0026#39;], summary: \u0026#39;Required: pick a repo you want to build\u0026#39;) string( name: \u0026#39;BRANCH\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Required: chose a branch you want to checkout\u0026#39;) string( name: \u0026#39;BUILD_NO\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Required: input build number\u0026#39;) string( name: \u0026#39;JIRA_NO\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Optional: input jira ticket number for commit message\u0026#39;) } stages { stage(\u0026#34;Are you sure?\u0026#34;){ steps{ // make sure you want to start this build input message: \u0026#34;${REPO}/${BRANCH}:${BUILD_NO}, are you sure?\u0026#34; echo \u0026#34;I\u0026#39;m sure!\u0026#34; } } stage(\u0026#39;Git clone repos\u0026#39;) { steps { // git clone one repo source code checkout([ $class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;refs/heads/${BRANCH}\u0026#39;]], browser: [$class: \u0026#39;GitHub\u0026#39;, repoUrl: \u0026#39;https://github.com/${REPO}\u0026#39;], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;CleanBeforeCheckout\u0026#39;], [$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#39;**\u0026#39;], [$class: \u0026#39;RelativeTargetDirectory\u0026#39;, relativeTargetDir: \u0026#39;../${REPO}\u0026#39;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;2aee7e0c-a728-4d9c-b25b\u0026#39;, url: \u0026#39;https://github.com/${REPO}.git\u0026#39;]]]) // git clone another repo source code checkout([ $class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;refs/heads/${BRANCH}\u0026#39;]], browser: [$class: \u0026#39;GitHub\u0026#39;, repoUrl: \u0026#39;https://github.com/${REPO}\u0026#39;], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;CleanBeforeCheckout\u0026#39;], [$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#39;**\u0026#39;], [$class: \u0026#39;RelativeTargetDirectory\u0026#39;, relativeTargetDir: \u0026#39;../${REPO}\u0026#39;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;2aee7e0c-a728-4d9c-b25b\u0026#39;, url: \u0026#39;https://github.com/${REPO}.git\u0026#39;]]]) } } stage(\u0026#39;Build repo1 and repo2\u0026#39;) { when { // if REPO=repo1 or REPO=repo2, execute build_repo12.sh anyOf { environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo1\u0026#39; environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo2\u0026#39; } } steps { sh label: \u0026#39;\u0026#39;, script: \u0026#39;${REPO}/build_repo12.sh ${REPO} ${BUILD_NO} ${JIRA_NO}\u0026#39; } } stage(\u0026#39;Build repo3 and repo4\u0026#39;) { when { // if REPO=repo3 or REPO=repo4, execute build_repo34.sh anyOf { environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo3\u0026#39; environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo4\u0026#39; } } steps { sh label: \u0026#39;\u0026#39;, script: \u0026#39;${REPO}/build_repo34.sh ${REPO} ${BUILD_NO} ${JIRA_NO}\u0026#39; } } stage(\u0026#39;Git push to remote repo\u0026#39;) { steps { // commit code to remote repo sshagent([\u0026#39;2aee7e0c-a728-4d9c-b25b\u0026#39;]) { sh \u0026#34;git push https://%MY_CRE_USR%:%MY_CRE_PSW%@github.com/${REPO}.git\u0026#34; } } } } } ","date":"2019-07-07","externalUrl":null,"permalink":"/posts/2019/jenkinsfile-example/","section":"Posts","summary":"这个 Jenkinsfile 示例展示了如何在 Jenkins Pipeline 中实现交互式输入、克隆多个 Git 仓库，并在构建完成后将代码推送到远程仓库。","title":"Jenkinsfile example - 实现交互、clone 多个仓库以及 git push","type":"posts"},{"content":" Problems # Like database product, it runs on multi-platform, but for software enginner they may only works on one platform, how they could identify their code works on all platform? manually build the various platforms? NO!\nSolution # Most people would know we can use Jenkins pipeline, they may create multi Jenkins job for different stuation.\nHow to do it in an elegant way, I would want to share how to use multibranch pipeline to achieve.\nWhen create a pull request, auto parallel start simple build. Reviewers can decide whether to merge base on build results. After code merged, auto start full build. Benefits # What are the benefits:\nOne Jenkins job and one pipeline can manage multi branches. Do not need to compile platforms to verify, save huge time and machines. Stop looking for other people\u0026rsquo;s mistakes, no one can break the build. Builds can be generated quickly for QA testing Jenkinsfile example # In case of reduce simple build time and let PR creater and reviewer know the builds status as soon as possbile, you may need to do something different here, like below, used when condition and branch variable to check it is a develop branch or pull request branch.\nwhen { branch \u0026#39;PR-*\u0026#39; } when { branch \u0026#39;develop\u0026#39; } The entire code pipeline looks like this:\n// This Jenkinsfile is an explame for multibranch pipeline pipeline { agent none stages { stage(\u0026#34;All platform builds\u0026#34;) { parallel { stage(\u0026#34;Windows build\u0026#34;) { agent { node { label \u0026#39;windows-vm01\u0026#39; customWorkspace \u0026#39;C:\\\\agent\\\\workspace\\\\blog\u0026#39; } } stages { stage(\u0026#34;PR build\u0026#34;) { when { branch \u0026#39;PR-*\u0026#39; } steps { checkout scm dir(\u0026#39;src\\\\build\u0026#39;) { bat label: \u0026#39;\u0026#39;, script: \u0026#39;build.bat PR\u0026#39; } } } stage(\u0026#34;Release build\u0026#34;) { when { branch \u0026#39;develop\u0026#39; } steps { cleanWs() checkout scm dir(\u0026#39;src\\\\build\u0026#39;) { bat label: \u0026#39;\u0026#39;, script: \u0026#39;build.bat release\u0026#39; } } } stage(\u0026#34;Deploy\u0026#34;) { echo \u0026#34;====if you have more stage, can add stage like this===\u0026#34; } } } stage(\u0026#34;Linux build\u0026#34;) { agent { node { label \u0026#39;linux-vm01\u0026#39; customWorkspace \u0026#39;/agent/workspace/blog\u0026#39; } } stages { stage(\u0026#34;PR build\u0026#34;) { when { branch \u0026#39;PR-*\u0026#39; } steps { checkout scm dir(\u0026#39;src/build\u0026#39;) { bat label: \u0026#39;\u0026#39;, script: \u0026#39;build.sh PR\u0026#39; } } } stage(\u0026#34;Release build\u0026#34;) { when { branch \u0026#39;develop\u0026#39; } steps { cleanWs() checkout scm dir(\u0026#39;src/build\u0026#39;) { bat label: \u0026#39;\u0026#39;, script: \u0026#39;build.sh release\u0026#39; } } } } } stage(\u0026#34;AIX build\u0026#34;){ steps{ echo \u0026#34;====same as windows/Linux example, can write the code here you need ====\u0026#34; } } } } } } ","date":"2019-06-25","externalUrl":null,"permalink":"/en/posts/2019/jenkins-multi-branch-pipeline/","section":"Posts","summary":"Discusses the use of Jenkins Multibranch Pipeline to manage multiple branches in a project, enabling parallel builds for pull requests and efficient code review processes.","title":"Jenkins Multibranch Pipeline","type":"posts"},{"content":" Preparation # You need to ask for a free trial license and install You will receive a mail with username/password to login for downloading I test it on the Windows platform, so I download the Windows installer, then install Squish Coco and Add-in Installed Visual Studio 2010 or higher, I used VS2017 Professional Add-in # go to ..squishcoco\\Setup, see quishCocoVSIX2017.vsix, double click, reopen VS2017, squishcoco will be there Create a project # Start Visual Studio and create a new C++ application\nClick on \u0026ldquo;File→New→Project\u0026hellip;\u0026rdquo; to pop up the new project wizard. Choose a project type of \u0026ldquo;Visual C++2\u0026rdquo; and the \u0026ldquo;Win32 Console Application\u0026rdquo; template. Enter a project name of squishcoco_sample, then click the \u0026ldquo;OK\u0026rdquo; button. When the wizard’s second page appears, click the \u0026ldquo;Finish\u0026rdquo; button. At this stage the application is not yet instrumented, so now we will create a copy of the build. Open the configuration manager by clicking \u0026ldquo;Build→Configuration Manager\u0026hellip;\u0026rdquo;. In the \u0026ldquo;Configuration\u0026rdquo; column, select \u0026ldquo;New\u0026hellip;\u0026rdquo; in the combobox. In the \u0026ldquo;New Project Configuration\u0026rdquo; dialog: Enter Code Coverage in the \u0026ldquo;Name\u0026rdquo; field, Select Release or Debug in the \u0026ldquo;Copy settings from\u0026rdquo; selection dialog. Click the \u0026ldquo;OK\u0026rdquo; button. Add test code\nsquishcoco_sample.cpp\n// squishcoco_sample.cpp : Defines the entry point for the console application. // #include \u0026#34;stdafx.h\u0026#34; extern int myprint(); int _tmain(int argc, _TCHAR* argv[]) { int age; printf(\u0026#34;Enter your age: \u0026#34;); scanf(\u0026#34;%d\u0026#34;,\u0026amp;age); if (age \u0026gt; 0 \u0026amp;\u0026amp; age \u0026lt;=40) printf(\u0026#34;You\u0026#39;re young guys\\n\u0026#34;); else if (age \u0026gt;40 \u0026amp;\u0026amp; age \u0026lt;=70) printf(\u0026#34;You\u0026#39;re midle guys\\n\u0026#34;); else if (age \u0026gt; 70 \u0026amp;\u0026amp; age \u0026lt;=100) printf(\u0026#34;You\u0026#39;re old guys\\n\u0026#34;); else printf(\u0026#34;You\u0026#39;re awesome\\n\u0026#34;); myprint(); return 0; } myprint.cpp\n#include\u0026#34;stdafx.h\u0026#34; int myprint () { printf (\u0026#34;you have call printf function\\n\u0026#34;); return 0; } Activate instrumentation # use the Microsoft® Visual Studio® Add-In: Click \u0026ldquo;Tools→Code Coverage Build Mode\u0026hellip;\u0026rdquo; to pop up the Squish Coco wizard. In the \u0026ldquo;Project:\u0026rdquo; selection dialog, select squishcoco_sample. In the selection dialog \u0026ldquo;Configuration:\u0026rdquo;, select Code Coverage. In the Configuration section at the bottom, select the radio button \u0026ldquo;Modify\u0026rdquo;, and then click on the button, \u0026ldquo;Enable code coverage for C++ projects\u0026rdquo;. The Code Coverage configuration has now been modified to generate code coverage information. The \u0026ldquo;SquishCoco\u0026rdquo; output window summarizes all the modifications that have been made:\n... Modifying configuration \u0026#39;Code Coverage\u0026#39; for the project \u0026#39;squishcoco_sample\u0026#39; for the platform \u0026#39;Code Coverage|Win32\u0026#39; Compiler Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended Linker Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended Librarian Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended File Specific Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended for the file \u0026#39;squishcoco_sample.cpp\u0026#39; Modifying configuration \u0026#39;Code Coverage\u0026#39; for the project \u0026#39;squishcoco_sample\u0026#39; for the platform \u0026#39;Code Coverage|x64\u0026#39; Compiler Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended Linker Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended Librarian Configuration Additional command line arguments \u0026#39; --cs-exclude-file-abs-regex=\u0026#34;^.*\\\\squishcoco_sample\\\\[^\\\\]*$\u0026#34; --cs-on --cs-condition --cs-line --cs-function --cs-hit --cs-full-instrumentation --cs-no-qt3 --cs-no-qt4 --cs-no-boost\u0026#39; are appended File Specific Configuration ... Build project # Build project will cause the executable squishcoco_sample.exe to be built and the code coverage instrumentation file squishcoco_sample.exe.csmes to be generated\nDouble click on squishcoco_sample.exe.csmes to inspect this file in CoverageBrowser\nRight now there is no code coverage statistics visible in CoverageBrowser, this is because the application has not yet been executed. Click on squishcoco_sample.cpp in the source list to display the main function. All the instrumented lines are shown grayed out, to indicate that nothing has been executed.\nNow execute squishcoco_sample.exe by double clicking it. This will result in a file called squishcoco_sample.exe.csexe being generated. The file contains a code coverage snapshot which can be imported into Coverage Browser\nClick \u0026ldquo;File-\u0026gt;Load Execution Report\u0026hellip;\u0026rdquo;. Select the \u0026ldquo;File\u0026rdquo; item and enter the path of the squishcoco_sample.exe.csexe file. Click on the \u0026ldquo;Import\u0026rdquo; button. This will cause the code coverage statistics to be updated. Now, in the source code window, the main function’s return statement will be colored green to indicate that this line has been executed. Final result # ","date":"2019-05-21","externalUrl":null,"permalink":"/en/posts/2019/squishcoco/","section":"Posts","summary":"introduction to Squish Coco, a code coverage tool, with examples of how to set it up and use it in Visual Studio for C++ projects.","title":"A Code Coverage Tool - Squish Coco use examples","type":"posts"},{"content":"Code Coverage is a measurement of how many lines, statements, or blocks of your code are tested using your suite of automated tests. It’s an essential metric to understand the quality of your QA efforts.\nCode coverage shows you how much of your application is not covered by automated tests and is therefore vulnerable to defects. it is typically measured in percentage values – the closer to 100%, the better.\nWhen you’re trying to demonstrate test coverage to your higher-ups, code coverage tools (and other tools of the trade, of course) come in quite useful.\nList of Code Coverage Tools\nTools Support Language Cost Partners Squish Coco C, C++, C#, SystemC, Tcl and QML Not disclosed Botom of this page Selected Clients BullseyeCoverage C, C++ $800 for 1-year license and up Testwell C, C++, C#, Java Not disclosed Parasoft C/C++test C, C++ Not disclosed partners VECTOR Code Coverage C, C++ Not disclosed (free trial available) partners JaCoCo Java Open Source Most famous code coverage tool in Java area ","date":"2019-05-21","externalUrl":null,"permalink":"/en/posts/2019/code-coverage-tools/","section":"Posts","summary":"Code Coverage is a measurement of how many lines, statements, or blocks of your code are tested using your suite of automated tests. It’s an essential metric to understand the quality of your QA efforts.","title":"Code Coverage tools of C/C++","type":"posts"},{"content":"","date":"2019-05-21","externalUrl":null,"permalink":"/en/tags/squishcoco/","section":"Tags","summary":"","title":"SquishCoco","type":"tags"},{"content":"最近遇到一个 regression bug，是产品完成构建之后，build commit number 不对，显示的 HEAD 而不是常见的 97b34931ac HASH number,这是什么原因呢？ 我检查了 build 脚本没有发现问题，branch 的输出是正确的，那我怀疑是引入 Jenkins 的原因，果然登录到远程的 agent 上去查看分支名称如下：\nC:\\workspace\\blog\u0026gt;git branch * (HEAD detached at 97b3493) 果然问题出在了 Jenkins 上。这个问题有简单办法解决，就是直接使用git命令来clone代码，而不使用Git插件\ngit clone --depth 1 -b u2opensrc https://username:\u0026#34;passwowrd\u0026#34;@git.github.com/scm/blog.git blog 这种方式固然简单，不会出错，但它是明码显示，我岂能容忍这种不堪的处理方式吗？肯定还是要在 Git 插件上找到解决办法的。 随后google一下，果然有遇到和我一样问题的人，问题链接 这里。\n他说他做了很多调查，还跟专业的 Jenkins 人士联系，试了很多次，最后找到这个办法\ncheckout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/feature/*\u0026#39;]], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#34;**\u0026#34;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;99f978af-XXXX-XXXX-8147-2cf8f69ef864\u0026#39;, url: \u0026#39;http://TFS_SERVER:8080/tfs/DefaultCollection/Product/_git/Project\u0026#39;]]]) 主要是在 extensions:[] 中加入这句 [$class: \u0026lsquo;LocalBranch\u0026rsquo;, localBranch: \u0026ldquo;**\u0026rdquo;]\n这是 Jenkins 的 Bug 吗？带着这个疑问随后通过 Pipeline Syntax，找到 checkout: Check out from version control，在 Additional Behaviours 里有 Check out to specific local branch 这个配置项\nIf given, checkout the revision to build as HEAD on this branch. If selected, and its value is an empty string or \u0026ldquo;**\u0026rdquo;, then the branch name is computed from the remote branch without the origin. In that case, a remote branch origin/master will be checked out to a local branch named master, and a remote branch origin/develop/new-feature will be checked out to a local branch named develop/newfeature.\n看介绍原来 Jenkins 自带这个设置，只是它不是默认选项，所以才遇到刚才那个问题。随后选择这个设置，然后填入\u0026quot;**\u0026quot;，然后生成 Pipeline 脚本，就跟上面的脚本一样了。\n","date":"2019-05-14","externalUrl":null,"permalink":"/posts/2019/gitscm-clone-code-don-t-display-branch/","section":"Posts","summary":"如何在 Jenkins 中使用 GitSCM插件克隆代码时，确保正确显示分支信息，避免出现 HEAD detached 状态的问题。","title":"GitSCM clone code don't display branch","type":"posts"},{"content":"","date":"2019-05-13","externalUrl":null,"permalink":"/tags/automation/","section":"标签","summary":"","title":"Automation","type":"tags"},{"content":"","date":"2019-05-13","externalUrl":null,"permalink":"/tags/ftp/","section":"标签","summary":"","title":"FTP","type":"tags"},{"content":"实现 CI/CD 过程中，常常需要将构建好的 build 上传到一个公共的服务器，供测试、开发来获取最新的 build。如何上传 build 成果物到 FTP server，又不想把 FTP server登录的用户名和密码存在脚本里，想做这样的参数化如何实现呢？\nupload_to_ftp.bat [hostname] [username] [password] [local_path] [remote_pat] windows batch 由于它的局限性，在实现上是比较麻烦的，但还是有办法。如何用 windows batch 来实现呢？借助一个临时文件，把需要的参数写入到临时文件里，然后通过 ftp -s 参数读取文件，最后把临时文件删除的方式来实现。\n@echo off set ftp_hostname=%1 set ftp_username=%2 set ftp_password=%3 set local_path=%4 set remote_path=%5 if %ftp_hostname%! == ! ( echo \u0026#34;ftp_hostname not set correctly\u0026#34; \u0026amp; goto USAGE ) if %ftp_username%! == ! ( echo \u0026#34;ftp_username not set correctly\u0026#34; \u0026amp; goto USAGE ) if %ftp_password%! == ! ( echo \u0026#34;ftp_password not set correctly\u0026#34; \u0026amp; goto USAGE ) if %local_path%! == ! ( echo \u0026#34;local_path not set correctly\u0026#34; \u0026amp; goto USAGE ) if %remote_path%! == ! ( echo \u0026#34;remote_path not set correctly\u0026#34; \u0026amp; goto USAGE ) echo open %ftp_hostname% \u0026gt; ftp.txt echo user %ftp_username% %ftp_password% \u0026gt;\u0026gt; ftp.txt echo cd %remote_path% \u0026gt;\u0026gt; ftp.txt echo lcd %local_path% \u0026gt;\u0026gt;ftp.txt echo prompt off \u0026gt;\u0026gt;ftp.txt echo bin \u0026gt;\u0026gt; ftp.txt echo mput * \u0026gt;\u0026gt; ftp.txt echo bye \u0026gt;\u0026gt; ftp.txt ftp -n -s:ftp.txt del ftp.txt goto END :USAGE echo. echo. - ------------------------------------------------------------------------------- echo. - upload_to_ftp.bat [hostname] [username] [password] [local_path] [remote_pat] - echo. - Example: - echo. - upload_to_ftp.bat 192.168.1.1 guest guest D:\\Media\\* C:\\Builds\\ - echo. - ------------------------------------------------------------------------------- echo. :END ","date":"2019-05-13","externalUrl":null,"permalink":"/posts/2019/upload-to-ftp-parameterization-by-bat/","section":"Posts","summary":"本文介绍了如何使用 Windows Batch 脚本通过参数化的方式上传文件到 FTP 服务器，避免在脚本中硬编码 FTP 凭据。","title":"通过参数化上传文件到 FTP 服务器","type":"posts"},{"content":" Prepare Java runtime # Check if had installed java # $ java -version openjdk version \u0026#34;1.8.0_65\u0026#34; OpenJDK Runtime Environment (build 1.8.0_65-b17) OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode) if not Here is an article telling you how to install it # Create Node # 1. Jenkins home page-\u0026gt;Manage Node-\u0026gt;New Node, such as window-build-machine # 2. List Linux agent settings # Items Settings Name Linux-build-machine Description used for Linux build of executors 1 Remote root directory /home/agent Labels Linux, build Usage Use this node as much as possible Launch method Launch agent agents via SSH Host 192.168.1.112 Credentials username/password Host Key Verification Strategy Manually trusted key Verification Strategy Availability Keep this agent online as much as paossible 3. How to set credentials # credentials configuration Domain Global credentials (unrestricted) Kind Username with password Scope Global(Jenkins, nodes, items, all child items, etc) Username root Password mypassword Description Linux agent username \u0026amp; password 4. Save then Connect # Remoting version: 3.29 This is a Unix agent Evacuated stdout Agent successfully connected and online SSHLauncher{host=\u0026#39;192.168.1.112\u0026#39;, port=22, credentialsId=\u0026#39;d1cbab74-823d-41aa-abb7-8584859503d0\u0026#39;, jvmOptions=\u0026#39;\u0026#39;, javaPath=\u0026#39;/usr/bin/java\u0026#39;, prefixStartSlaveCmd=\u0026#39;\u0026#39;, suffixStartSlaveCmd=\u0026#39;\u0026#39;, launchTimeoutSeconds=210, maxNumRetries=10, retryWaitTime=15, sshHostKeyVerificationStrategy=hudson.plugins.sshslaves.verifiers.ManuallyTrustedKeyVerificationStrategy, tcpNoDelay=true, trackCredentials=true} [05/11/19 01:33:37] [SSH] Opening SSH connection to 192.168.1.112:22. [05/11/19 01:33:37] [SSH] SSH host key matches key seen previously for this host. Connection will be allowed. [05/11/19 01:33:37] [SSH] Authentication successful. [05/11/19 01:33:37] [SSH] The remote user\u0026#39;s environment is: Troubleshooting # Problem how to fix [04/22/19 23:15:07] [SSH] WARNING: No entry currently exists in the Known Hosts file for this host. Connections will be denied until this new host and its associated key is added to the Known Hosts file. ssh-keyscan HOSTNAME \u0026raquo; known_hosts /var/lib/jenkins/.ssh/known_hosts [SSH] No Known Hosts file was found at /var/lib/jenkins/.ssh/known_hosts. changing the Host key verification strategy in LAUNCH METHOD from \u0026ldquo;Known Hosts file verification strategy\u0026rdquo; to \u0026ldquo;Manually trusted key verification strategy\u0026rdquo; ","date":"2019-05-12","externalUrl":null,"permalink":"/en/posts/2019/jenkins-linux-agent/","section":"Posts","summary":"Provides a step-by-step guide on how to configure a Jenkins Linux agent, including setting up the Java runtime, creating the node, and troubleshooting common issues.","title":"Jenkins Linux agent configuration","type":"posts"},{"content":" Prepare Java runtime # 1. Download Java # 2. Configure Java Windows path # JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_201 CLASSPATH=.;%JAVA_HOME%\\lib;%JAVA_HOME%\\jre\\lib Create Node # 1. Jenkins home page-\u0026gt;Manage Node-\u0026gt;New Node, such as window-build-machine # 2. List windows agent settings # Items Settings Name window-build-machine Description used for windows build of executors 1 Remote root directory C:\\agent Labels windows, build Usage Use this node as much as possible Launch method Let Jenkins control this Windows slave as a Windows service Administrator user name .\\Administrator Password mypassword Host 192.168.1.111 Run service as Use Administrator account given above Availability Keep this agent online as much as paossible 3. Save then Connect # [2019-05-11 01:32:50] [windows-slaves] Connecting to 192.168.1.111 Checking if Java exists java -version returned 1.8.0. [2019-05-11 01:32:50] [windows-slaves] Copying jenkins-slave.xml [2019-05-11 01:32:50] [windows-slaves] Copying slave.jar [2019-05-11 01:32:50] [windows-slaves] Starting the service [2019-05-11 01:32:50] [windows-slaves] Waiting for the service to become ready [2019-05-11 01:32:55] [windows-slaves] Connecting to port 52,347 \u0026lt;===[JENKINS REMOTING CAPACITY]===\u0026gt;Remoting version: 3.29 This is a Windows agent Agent successfully connected and online Troubleshooting # The following issues I met and how I fixed them.\n1. ERROR: Message not found for errorCode: 0xC00000AC # You need need to install JDK, and config JAVA environment variable.\n2. How to fix add windows node as Windows service error # Ref to JENKINS-16418.\n3. org.jinterop.dcom.common.JIException: Message not found for errorCode: 0x00000005 # Fixed permission for the following registry keys\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Classes\\Wow6432Node\\CLSID{72C24DD5-D70A-438B-8A42-98424B88AFB8} HKEY_CLASSES_ROOT\\CLSID{76A64158-CB41-11D1-8B02-00600806D9B6} Steps to fix it\nOpen \u0026lsquo;regedit\u0026rsquo; (as Administrator), Find (Ctrl+F) the registry key: \u0026ldquo;{72C24DD5-D70A-438B-8A42-98424B88AFB8}\u0026rdquo; in HKEY_LOCAL_MACHINE\\SOFTWARE\\Classes\\Wow6432Node\\CLSID Right click and select \u0026lsquo;Permissions\u0026rsquo;, Change owner to administrators group (Advanced\u0026hellip;). Change permissions for administrators group. Grant Full Control。 Change owner back to TrustedInstaller (user is \u0026ldquo;NT Service\\TrustedInstaller\u0026rdquo; on local machine) Repeat the above steps to fix permission for HKEY_CLASSES_ROOT\\CLSID{76A64158-CB41-11D1-8B02-00600806D9B6}\nFinally, Restart Remote Registry Service (Administrative Tools / Services).\n4. ERROR: Unexpected error in launching an agent # This is probably a bug in Jenkins.\nLogin remote machine and open Services find jenkinsslave-C__agent Set startup type: Automatic Log On: select This account, type correct account and password start jenkinsslave-C__agent 5. Caused by: org.jinterop.dcom.common.JIRuntimeException: Message not found for errorCode: 0x800703FA # Slave under domain account, If your slave is running under a domain account and you get an error code 0x800703FA, change a group policy:\nopen the group policy editor (gpedit.msc) go to Computer Configuration-\u0026gt;Administrative Templates-\u0026gt;System-\u0026gt; UserProfiles, \u0026ldquo;Do not forcefully unload the user registry at user logoff\u0026rdquo; Change the setting from \u0026ldquo;Not Configured\u0026rdquo; to \u0026ldquo;Enabled\u0026rdquo;, which disables the new User Profile Service feature (\u0026lsquo;DisableForceUnload\u0026rsquo; is the value added to the registry) 6. ERROR: Message not found for errorCode: 0xC0000001 Caused by: jcifs.smb.SmbException: Failed to connect: 0.0.0.0\u0026lt;00\u0026gt;/10.xxx.xxx.xxx # Need to enable SMB1\nSearch in the start menu for ‘Turn Windows features on or off’ and open it. Find \u0026lsquo;SMB1.0/CIFS File Sharing Support\u0026rsquo; in the list of optional features that appears, and select the checkbox next to it. Click OK and Windows will add the selected feature. You’ll be asked to restart your computer as part of this process.\n7. .NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service # Need to upgrade your .NET Framework. Here is a link for update .NET Framework.\n6. more connect jenkins agent problem on windows # Please refer to this link https://github.com/jenkinsci/windows-slaves-plugin/blob/master/docs/troubleshooting.adoc\n","date":"2019-05-12","externalUrl":null,"permalink":"/en/posts/2019/jenkins-windows-agent/","section":"Posts","summary":"Provides a step-by-step guide on how to configure a Jenkins Windows agent, including setting up the Java runtime, creating the node, and troubleshooting common issues.","title":"Jenkins Windows agent configuration","type":"posts"},{"content":"Whenever I have free time at work, I often remember that I haven\u0026rsquo;t updated my WeChat official account article for a long time. I always want to share my experience and feelings of transitioning from testing to development with everyone when I\u0026rsquo;m not busy with work, but there\u0026rsquo;s always endless work, and I can\u0026rsquo;t stop for a moment.\nFinally, this week, I had two days where work wasn\u0026rsquo;t so busy, so I decided to finish updating the article I started writing a few days ago. These are the two most relaxing days I\u0026rsquo;ve had in these months. There are no bugs to investigate and test, no need to look at decades-old C code, and finally, I have a large chunk of time to write the Python Client-side code I\u0026rsquo;m responsible for. The feeling of refactoring, debugging, and modifying the Unit Test Suite while listening to music is truly blissful.\nHappy times are always short-lived. Today, two more bugs need investigating ε=(´ο｀*)))唉\u0026hellip;\nBack to reality. After investigating for a long time, I found out that it was actually a QA testing error. Phew, I can relax tonight and update my WeChat official account.\nFor the past five months, I\u0026rsquo;ve been working eight hours a day, then continuing to work at home with my laptop in the evening, occasionally going to the company on Sundays, and often studying at home. Due to the change in roles and new projects, there\u0026rsquo;s a lot to learn. From business to technology, plus the imminent product launch, as a new development member, I also bear the task of bug fixing. Ten-year-old code, all-English documentation, complex systems—if I don\u0026rsquo;t give it my all, I\u0026rsquo;m really worried about failing the transition, which would be quite embarrassing.\nThe busy work and pressure make me eat uncontrollably in the evening. Eating is the most relaxing time of my day. Last year, I bet with others to lose weight and win a prize; I won first prize without any resentment. But this year, when I bet with others to lose weight again, I haven\u0026rsquo;t even started yet. It\u0026rsquo;s almost the end of the year, and losing the prize is inevitable. In summary, it\u0026rsquo;s probably because I\u0026rsquo;ve been too busy and stressed this year, making it impossible for me to continue practicing guitar and using Keep outside of my eight hours. I haven\u0026rsquo;t even had time for my annual leave. Plans never quite work out as expected.\nAlthough I\u0026rsquo;m still a junior developer, the change in roles has also changed my perspective.\nAutomation Testing is the Foundation, DevOps is the Ladder # Over the past few years, you\u0026rsquo;ve probably realized that if a tester doesn\u0026rsquo;t understand automation testing and can\u0026rsquo;t write automation test scripts, not only will it be difficult to get promoted or change jobs, but they may also be eliminated by the company.\nI personally believe that DevOps is the path many companies will take in the near future. In general, there are very few people in second-tier cities who can explain and implement DevOps clearly. Therefore, those who master the implementation of DevOps early will have the opportunity to become DevOps coaches or testing architects.\nDon\u0026rsquo;t Become a Developer Without Preparing to Handle Pressure # I\u0026rsquo;ve encountered a lot of pressure in these months, from initially learning C language to the C language assessment; from learning the all-English business documentation to sharing the business documentation (which is also a kind of assessment); from investigating the code coverage of the C code and Git sharing to investigating and resolving bugs; from daily stand-up meetings to weekly meetings with foreign colleagues. Finally, in September, my title changed from Quality Assurance Engineer to Software Engineer. Only those who have experienced it know the pressure, pain, and fleeting joy involved.\nSkills Matching Your Age # This point is very important. If you\u0026rsquo;re asked now what advantages you have over young colleagues who just graduated two or three years ago, and you can\u0026rsquo;t confidently and clearly state your advantages, then you need to reflect on this.\nFrom a development perspective, I currently lack skills that match my age, so testing-related skills and DevOps-related knowledge are still lessons I need to master.\nLearn English # For domestic companies, English may not be used at work, but I want to say that if you want to have a longer-term development in the testing and development fields, English is very important. Generally, the most popular open-source automated testing frameworks, technologies, DevOps-related tools, and the most effective solutions for problem-solving are usually in English. If your English is not good, stick to it for a year and a half, diligently study English materials, form a habit, and you will benefit for life.\n","date":"2018-12-26","externalUrl":null,"permalink":"/en/misc/from-qa-to-dev/","section":"Miscs","summary":"Reflections and summaries of my five months transitioning from testing to development - work, learning, and life.","title":"From Tester to Developer","type":"misc"},{"content":"","date":"2018-08-07","externalUrl":null,"permalink":"/tags/functiontest/","section":"标签","summary":"","title":"FunctionTest","type":"tags"},{"content":"当你第一次开始接触测试这个行业的时候，首先听说的应该都是功能测试。\n功能测试是通过一些测试手段来验证开发做出的代码是否符合产品需求。这些年功能测试好像不太受欢迎了，不少同学开始尝试自动化测试，测试开发等等，结果是功能测试、自动化测试、测试开发一样都没做好。\n我们通常认为的功能测试是根据需求，采取以下测试流程：需求分析，用例编写，用例评审，提测验证，Bug回归验证，上线与线上回归等测试。如此日复一日，年复一年，可是等准备换工作的时候却得不到认可，你也遇到这种情况吗？\n那么如何做好功能测试？功能测试用到哪些知识？有哪些相关的建议呢？\n需求分析 # 业务方在提出需求的时候，产品是要分析这个需求的价值，影响范围和实现代价的。在需求评审的时候，作为一个测试人员必须了解这次需求的内容，影响到哪些现有的功能，涉及到的操作系统或是类别等，然后准确的评估出工作量，防止因评估不足造成后期测试不充分。\n再者，关注开发和产品的讨论，关注需求最后如何实现？其中做出的变动和难点就是测试的时候必须重点关注的部分，不能因为这些暂时和你没有关系就不去关注，防止欠债越来越多，不能做好充分的的测试。\n第三，需求评审结束后，要求产品更新此次评审过程中的所有改动部分，同时确保之后的任何需求变化都及时更新。\n第四，根据产品需求，同时与在会人员进行探讨，设计测试方案及时间安排，此时可以粗粒度考虑，时间上要合理。\n用例设计与评审 # 测试用例是每个测试人员工作过程中必须要完成的工作，它对测试工作起到指导作用，也是相关业务的一个文档沉淀。在以往面试的经验中，有许多人的测试用例写的没有章法，他们是凭着感觉去写测试用例，也没有从用户的角度来思考如何编写测试用例，对于测试用例设计较为常见的方法论也不清楚。\n假如面试的时候给你一个场景：一个全新的App要发布，如果让你来测试，你能想到哪些测试方案？如果你只能想到如何去测试app的功能的话，作为功能测试人员就考虑不够全面。此时除了App的功能以外，还应关注App的兼容性，易用性，接口的功能测试和性能测试，数据的存储以及容灾情况等等都应考虑在内。\n测试用例可设计为两类： 一类是开发自测和验收提测试标准的冒烟测试用例；一类是针对需求的全面测试用例。\n编写完测试用例后主动联系相关人员进行用例评审，在评审过程中及时修改不合适的用例。\n测试流程，注重项目控制 # 项目的流程控制在需求开始的时候就应该重视起来，只是很多时候我们没有意识到这是测试的工作，有的是产品来控制，有的是专门的项目经理来控制。\n测试人员需要有关注整体项目的意识。如果你不关注项目进度，什么时候提测什么时候开始测试，那么在测试过程中会遇到测试的内容和最初的需求不一致时候就会额外需要时间来解决，导致项目延期。另外主动关注项目，长此以往，你的这份主动性也会是你有效的竞争力。\n需求一旦明确了由你来负责的时候，就要时刻来关注项目的情况。中间变更需求的时候，要评估是否影响项目进度，如果影响了重新进行排期。如果开发提测试晚了，是否影响上线时间，如果影响需要及时跟相关的人员沟通，发风险邮件，通知大家详细的情况。\n同时在测试过程中，发现了bug需要详细描述问题，以方便开发去进行重现和修改。同时给bug准确分级，实时跟踪进度，保证项目高质量的按期完成。\n上线回归与项目总结 # 一个需求上线完成后，要及时进行线上回归，同时必须回归我们在需求评审的时候考虑到的可能影响到的原有的功能，以确保新功能完全上线成功。\n在一个项目完成后，最好有一份个人总结报告，总结整个项目过程中遇到的问题及最后的解决办法，有哪些需要注意的问题？有什么可以借鉴的方案或是改进策略？项目中有没有通用性的问题等等。\n能力的总结和沉淀 # 在找工作的时候，很多做功能测试多年的同学都遭遇过面试失败，究其原因，我觉得最核心的原因是：不具备相应工作年限应该具备的能力。\n我们应该时常问自己一句话：离开现有的平台，我还有什么？如果仅仅是对现在公司业务和工具的熟悉，那是没有任何优势可言的。\n对同类业务流程的掌握，项目的整体把控，快速了解业务并能根据需求选择测试方案，引入提高测试效率测试方案和工具，测试过程中遇到问题的预判和解决办法等才是功能测试人员必须具备的能力。\n这些方面你做到了吗？不要抱怨功能测试如何如何，认清行业现状和自己的优缺点，做好自己的职业规划。\n如果你不善于编码，那么做务专家也是功能测试人员一个很好的选择。\n","date":"2018-08-07","externalUrl":null,"permalink":"/posts/2018/how-to-do-functional-testing/","section":"Posts","summary":"介绍功能测试的基本流程、用例设计、项目控制、上线回归等方面的建议，帮助测试人员提升功能测试的质量和效率。","title":"如何做好功能测试","type":"posts"},{"content":"I haven\u0026rsquo;t updated my public account articles in the past few months because, starting in May, I had the opportunity to switch to development due to project needs. I cherish this opportunity and have been working hard to learn development-related skills.\nWhy did I switch to development after 9 years of testing?\nThree years ago, I planted the seed of becoming a developer. Years of writing automated test cases made me realize I enjoyed coding. I wanted to delve deeper into technology, and development was the most direct path. So I consistently worked on reading and writing more code, preparing for the day I could become a development test engineer or a developer.\nI was also inspired by a recent article by Ru Bingcheng, \u0026ldquo;Why I switched from development to testing and stuck with it for 16 years.\u0026rdquo; Our career paths are the opposite, but we both strive for better professional development. He mentioned many future possibilities for testers in his video (check it out if you\u0026rsquo;re unfamiliar; it broadens your perspective). I believe strong coding skills are fundamental for any engineering role. Testing isn\u0026rsquo;t just about meticulous clicking; it\u0026rsquo;s not a job better suited for women; it\u0026rsquo;s not easier than development; and it\u0026rsquo;s not a path to complacency. Successful testing requires more effort than it appears.\nSince I\u0026rsquo;m learning C, a language I haven\u0026rsquo;t touched since graduation, these past few months have highlighted the vast unknown in development. Learning a development language requires in-depth study, unlike learning automated testing. With automated testing, you can learn scripting languages on the fly. C is different; it demands systematic learning, mastering arrays, pointers, structures, linked lists, binary trees, data structures, and algorithms, operating systems, and compiler principles.\nAlthough I\u0026rsquo;ve switched to development, I will continue to focus on testing.\nI hope this role change allows me to view product quality, testing-related thinking, and technology from a more comprehensive perspective, and share more valuable content.\n","date":"2018-07-21","externalUrl":null,"permalink":"/en/posts/2018/why-i-move-to-development/","section":"Posts","summary":"This article documents my experience and insights from transitioning from testing to development. I share my learning and work arrangements during paternity leave, including reading books, participating in open-source projects, and physical exercise, emphasizing how to maintain learning and growth while caring for a family.","title":"Why I Switched from Testing to Development After 9 Years","type":"posts"},{"content":"如果你想在一台电脑上配置 github 和 bitbucket，如何配置多个 SSH git key？ 输入以下命令生成 SSH Key，注意在生成过程中最好输入新的名字，比如 id_rsa_github 和 id_rsa_bitbucket\nssh-keygen -t rsa -C \u0026#34;your_email@youremail.com\u0026#34; 然后将生成的 SSH key 文件内容复制到对应网址的个人用户设置中即可。但是明明按照官方教程做的但是在 git clone 的时候还是遇到以下问题： Error: Permission denied (publickey) 困恼了几天的错误终于解决了。\n参看这个文档\n由于我用的是macOS Sierra 10.13.3，文档这里写着如果是macOS Sierra 10.12.2 及以后的版本需要在 ~/.ssh 目录下创建一个 config 文件 congfig 文件的具体配置如下：\nHost * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa_github Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa_bitbucket 配置了这个文件之后，再次尝试\ngit clone git@github.com:shenxianpeng/blog.git 可以 download 代码了，两个 SSH git 都好用了 : )\n","date":"2018-05-06","externalUrl":null,"permalink":"/posts/2018/permission-denied-publickey/","section":"Posts","summary":"本文介绍了如何在配置多个 SSH Git Key 时解决 “Permission denied (publickey)” 错误，确保 GitHub 和 Bitbucket 的 SSH 连接正常工作。","title":"Error: Permission denied (publickey)","type":"posts"},{"content":"What kind of technical skills and experience define a senior engineer? Let me share my perspective on senior engineers.\nExtensive Industry Testing Experience # Ideally, experience in both traditional and internet companies is preferred. If not, at least engage with senior test engineers from these companies to understand their testing approaches and broaden your perspective.\nStrong Testing Fundamentals # Master necessary testing theories, be familiar with testing processes, requirements analysis, and test case design methods. Develop test plans based on project needs.\nStrong Business Acumen # Proficiency in functional testing requires strong business understanding. This allows you to design test cases from a product perspective, identifying issues beyond basic functionality and providing constructive feedback and suggestions to the product team.\nFamiliarity with Relevant Testing Tools # Many tools are used in software testing. Understanding and using these tools allows you to effectively choose the appropriate tools based on company requirements and project needs, improving work efficiency.\nManagement Tools: e.g., JIRA, Testlink, Wiki, Confluence Continuous Integration: Jenkins, Bamboo, Travis CI, etc. Understand their differences and implementation. Automated Testing: Understand how to perform automated testing for web and mobile platforms, and the tools involved. Familiarity with Selenium, WebDriver, Appium, Robotium testing frameworks, and programming languages used for developing automated test cases (Python? Java? JavaScript?). Know how to choose and implement these tools. Performance Testing: Understand Jmeter and LoadRunner, two major performance testing tools, and how to conduct performance testing. Strong Coding Skills # Strong coding skills enable rapid mastery of automated testing and even the development of testing platforms. Furthermore, this allows for quick familiarization with automated test cases written in Java, Python, Javascript, etc., when transitioning to a new company.\nLanguage Proficiency # This includes communication and foreign language skills. Communication is an essential skill for testers. Good communication ensures developers understand the issues and accept your suggestions, and allows you to better understand requirements from product personnel. While English is mainly required in foreign companies, learning English is beneficial because many technical resources and materials from the open-source community are in English.\nTherefore, becoming an excellent senior test engineer requires many skills. Let\u0026rsquo;s work hard together! 💪\n","date":"2018-05-06","externalUrl":null,"permalink":"/en/posts/2018/senior-test-engineer/","section":"Posts","summary":"This article outlines the skills and experience required for a senior test engineer, including testing theories, business acumen, tool proficiency, and coding abilities, helping readers understand how to become an excellent senior test engineer.","title":"My Perspective on Senior Test Engineers","type":"posts"},{"content":"我想大多数的团队都面临这样的问题：\n发布周期长\n开发和测试时间短\n开发和测试是两个独立的团队\n不稳定的交付质量\n低收益难维护的UI自动化测试脚本\n不合理的测试权重分配\n解决方法：\n引入 DevOps 和分层自动化\n组件化产品 产品开发引入模块化，数据驱动会使得产品更加容易实施 Unit，Server，UI 自动化测试 优化工程师 开发和测试在未来将没有界限，他们都是开发者，都会产品的质量和客户负责 分层自动化 更合理的测试权重分配，更底层的测试收益越高 引入工具 实施DevOps引入必要的工具，Bitbucket, Jenkins, Sonar, Pipelines, Docker, test framework … ","date":"2018-04-14","externalUrl":null,"permalink":"/posts/2018/devops-practice/","section":"Posts","summary":"本文介绍了 DevOps 实践的核心概念、目标和实施方法，强调了持续集成、持续交付和自动化的重要性。","title":"DevOps 实践","type":"posts"},{"content":"最近在做有关 DevOps Build 的时候，学习了 Jenkins 的 Pipeline 的功能，不得不提到的就是 Jenkinsfile 这个文件。\n以下面是我配置的 Jenkinsfile 文件及简单说明，更多有关 Pipeline 请看官方文档。\npipeline { agent any stages { // Build 阶段 stage(\u0026#39;Build\u0026#39;) { steps { echo \u0026#39;Building...\u0026#39; bat \u0026#39;npm run build webcomponent-sample\u0026#39; } } // 单元测试阶段 stage(\u0026#39;Unit Test\u0026#39;) { steps { echo \u0026#39;Unit Testing...\u0026#39; bat \u0026#39;npm test webcomponent-sample\u0026#39; } post { success { // 执行成功后生产报告 publishHTML target: [ allowMissing: false, alwaysLinkToLastBuild: false, keepAll: true, reportDir: \u0026#39;components/webcomponent-sample/coverage/chrome\u0026#39;, reportFiles: \u0026#39;index.html\u0026#39;, reportName: \u0026#39;RCov Report\u0026#39; ] } } } // E2E 测试阶段 stage(\u0026#39;E2E Test\u0026#39;) { steps { bat \u0026#39;node nightwatch e2e/demo/test.js\u0026#39; } } stage(\u0026#39;Release\u0026#39;) { steps { echo \u0026#39;Release...\u0026#39; } } } post { // 执行成功是触发 success { mail bcc: \u0026#39;email@qq.com\u0026#39;, body: \u0026#34;\u0026lt;b\u0026gt;Project: ${env.JOB_NAME} \u0026lt;br\u0026gt;Build Number: ${env.BUILD_NUMBER} \u0026lt;br\u0026gt;Build URL: ${env.BUILD_URL} \u0026#34;, cc: \u0026#39;\u0026#39;, charset: \u0026#39;UTF-8\u0026#39;, from: \u0026#39;jenkins@qq.com\u0026#39;, mimeType: \u0026#39;text/html\u0026#39;, replyTo: \u0026#39;\u0026#39;, subject: \u0026#34;SUCCESS: Project Name -\u0026gt; ${env.JOB_NAME}\u0026#34;, to: \u0026#34;\u0026#34;; } // 执行失败时触发 failure { mail bcc: \u0026#39;email@qq.com\u0026#39;, body: \u0026#34;\u0026lt;b\u0026gt;Project: ${env.JOB_NAME} \u0026lt;br\u0026gt;Build Number: ${env.BUILD_NUMBER} \u0026lt;br\u0026gt;Build URL: ${env.BUILD_URL} \u0026#34;, cc: \u0026#39;\u0026#39;, charset: \u0026#39;UTF-8\u0026#39;, from: \u0026#39;jenkins@qq.com\u0026#39;, mimeType: \u0026#39;text/html\u0026#39;, replyTo: \u0026#39;\u0026#39;, subject: \u0026#34;FAILURE: Project Name -\u0026gt; ${env.JOB_NAME}\u0026#34;, to: \u0026#34;\u0026#34;; } } } ","date":"2018-04-14","externalUrl":null,"permalink":"/posts/2018/jenkinsfile-configure/","section":"Posts","summary":"本文介绍了如何使用 Jenkinsfile 配置 Jenkins Pipeline，包括构建、测试和发布阶段的示例，以及如何处理邮件通知。","title":"Jenkinsfile 配置","type":"posts"},{"content":" Workplace frustrations I\u0026rsquo;ve experienced as someone not good at expressing myself # Overinterpreting the meaning of emails and chats with colleagues; Continuously pondering disagreements with colleagues after the fact; Being overly concerned about my superiors\u0026rsquo; opinions of my work successes and failures. Approaches that have worked for me # Focusing more of my energy on the work itself; Focusing on the work, not the people—doing the right thing; Taking a long-term perspective, remembering my initial goals, and concentrating on what I should be doing; My leader\u0026rsquo;s words, \u0026ldquo;Look forward,\u0026rdquo; have left a deep impression on me. Let the past be the past; don\u0026rsquo;t dwell on it, look forward. ","date":"2017-11-23","externalUrl":null,"permalink":"/en/misc/weather-setbacks-at-work/","section":"Miscs","summary":"How to adjust your mindset and focus on work itself when facing setbacks, rather than getting overly entangled in interpersonal relationships.","title":"Overcoming Workplace Frustrations","type":"misc"},{"content":"","date":"2017-11-20","externalUrl":null,"permalink":"/en/tags/hexo/","section":"Tags","summary":"","title":"Hexo","type":"tags"},{"content":"Hexo\u0026rsquo;s default theme uses black code highlighting. Want a different style? Here\u0026rsquo;s how to do it:\n# Modify the highlight.styl file, located at themes/landscape/source/css/_partial/highlight.styl Modify the default code theme to Tomorrow Night Eighties:\nhighlight-background = #2d2d2d highlight-current-line = #393939 highlight-selection = #515151 highlight-foreground = #cccccc highlight-comment = #999999 highlight-red = #f2777a highlight-orange = #f99157 highlight-yellow = #ffcc66 highlight-green = #99cc99 highlight-aqua = #66cccc highlight-blue = #6699cc highlight-purple = #cc99cc For the Tomorrow theme:\nhighlight-background = #ffffff highlight-current-line = #efefef highlight-selection = #d6d6d6 highlight-foreground = #4d4d4c highlight-comment = #8e908c highlight-red = #c82829 highlight-orange = #f5871f highlight-yellow = #eab700 highlight-green = #718c00 highlight-aqua = #3e999f highlight-blue = #4271ae highlight-purple = #8959a8 For more details, please refer to the tomorrow-theme modifications.\n","date":"2017-11-20","externalUrl":null,"permalink":"/en/posts/2017/change-hexo-code-highlight/","section":"Posts","summary":"Hexo’s default theme uses black code highlighting. Want a different style? This article explains how to modify Hexo theme code highlighting styles.","title":"Modifying Hexo Theme Code Highlighting","type":"posts"},{"content":" Hexo 配置 rss 订阅功能 # 安装 hexo-generator-feed 插件\nnpm install hexo-generator-feed --save 如果国内 npm 安装不成功，可以先安装 cnpm\nnpm install -g cnpm --registry=https://registry.npm.taobao.org 然后再\ncnpm install hexo-generator-feed --save 在 _config.yml 中配置这个插件\nfeed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: \u0026#39; \u0026#39; Hexo 博客文章中插入图片 # 如果想在 Hexo 文章中插入图片怎么做？\n网络上很容易搜到 Markdown 的语法是 ![Alt text](/path/to/img.jpg) 前面 Alt text 是指在图片下面命名，后面是图片的地址。那么如何配置？\n经过几番尝试得知：在你的 hexo 项目根目录下面 source 创建一个 images 文件夹， 把你以后用的到图片都放在这个目录下面就 OK 了。\n![示例图1](../images/color.png) ","date":"2017-10-25","externalUrl":null,"permalink":"/posts/2017/hexo-practice/","section":"Posts","summary":"本文介绍如何在 Hexo 博客中配置 RSS 订阅功能，包括安装插件和使用。","title":"Hexo 的配置和使用","type":"posts"},{"content":"title: 2025 Year-End Summary tags:\nThoughts Summary author: shenxianpeng date: 2025-12-31 Time flies quickly. At the end of each year, I write a running account to record my life and work from the past year. I hope my future self will still remember what happened this year.\nYear-Beginning Flag Completion Status # Looking back at the flags I set at the beginning of the year, some were achieved, some were completed but not ideally, and some were not done at all.\nTravel to other European countries, watch a football match\nHalf completed. I went to Austria, but didn\u0026rsquo;t visit football powerhouses like Spain or Italy, nor did I get to watch a match.\nObtain Azure certification\nWith the rapid development of AI, this goal seemed less important, so I temporarily put it aside.\nParticipate in a DevOps or Python conference\nIn April, I went to PyCon LT and wrote three conference articles: Day 1, Day 2, Day 3.\nJoin PyPA or Python GitHub Organization\nThis year, I created a new open-source project Explain Error Plugin, and thus joined the Jenkins GitHub Organization. The goal of joining PyPA and Python GitHub Organization has not been achieved yet, but I hope to have the opportunity to join in the future.\nDon\u0026rsquo;t gain weight, keep exercising\nI didn\u0026rsquo;t gain much weight this year, but my exercise volume wasn\u0026rsquo;t high either. I hope to do better in 2026.\nIn summary, although not all were completed, it\u0026rsquo;s still a decent achievement.\nLooking Back at 2025 # I actually don\u0026rsquo;t remember what happened in 2025, so I looked through my calendar and photos to recall the bits and pieces of the year.\nJanuary: Celebrated New Year in Vilnius, prepared to go back home to visit relatives at the end of the month. February: Went back to my hometown for Chinese New Year, but caught a cold for several days due to insufficient rest, and later got chickenpox. Afterwards, I worked while spending time with family, and only started meeting friends after recovering from chickenpox. March: Started intermittent holidays, went to the beach, Saint-Ya, zoo, and other places to spend time with my child and family. April: Returned to Vilnius. Each departure brings reluctance, and it\u0026rsquo;s only after returning to Vilnius that I can slowly get back into my life and work rhythm. Afterwards, I attended PyCon LT and took my child to a basketball game. May: Drove to Kaunas, where I celebrated my child\u0026rsquo;s third birthday, and visited landmarks such as Kaunas Zoo and Kaunas Old Town. At the end of the month, I went to see Vilnius\u0026rsquo; Pink Soup Festival. June: Trip to Austria, went to Wolfgang (the lake and mountain scenery is beautiful), Vienna, and other places. At the end of the month, I visited a colleague\u0026rsquo;s home for the first time. July: Took a train to Klaipėda, visited the Maritime Museum, mainly wanting to play with sand and water at the beach, but unfortunately, the weather wasn\u0026rsquo;t great those days. August: Explored the Old Town, preparing for my child\u0026rsquo;s kindergarten to start. September: My child successfully started kindergarten. Although they cried twice, they gradually adapted well. I took my child to Vilnius\u0026rsquo; Capital Days festival. October: Business as usual for work and life. Went out more on weekends, as it would be hard to see such good weather next month. November: Still the same. Originally planned to go to Poland, but canceled due to being too busy with work. I\u0026rsquo;ll see if there\u0026rsquo;s a chance this year. December: As the holidays approached, I became even busier. I attended Vilnius\u0026rsquo; Christmas market and my child\u0026rsquo;s kindergarten Christmas event, then embarked on the journey back home. These bits and pieces form the precious memories of 2025.\nLooking Ahead to 2026 # In 2026, I don\u0026rsquo;t plan to set too many flags (goals), I just hope to do the following things well:\nKeep exercising, and ensure both my family and I are healthy. Spend more time with family, go out more, and communicate more with others. Persist in writing and open-source, especially the newly started weekly update project 《攻城狮周刊》. Pay more attention to AI technologies and applications, integrating them deeper into work and study. If I want to set a goal for my future self 1000 days from now, it would probably be:\nDevelop my own \u0026ldquo;second leg\u0026rdquo; to explore more possibilities. Have more flexible and free work and life, and always maintain passion. I hope my future self, when seeing these words, can still smile while recalling this year.\nI also wish you good health, smooth work, and a happy life in the new year!\nPast Year-End Summaries # 2024 Year-End Summary 2023 Year-End Summary 2022 Year-End Summary 2020 Year-End Summary 2019 Year-End Summary 2018 From QA to Dev Please attribute the author and source when reprinting articles from this site. Do not use for any commercial purposes. Welcome to follow the official account「DevOps攻城狮」\n","externalUrl":null,"permalink":"/en/misc/2025-summary/","section":"Miscs","summary":"title: 2025 Year-End Summary tags:\nThoughts Summary author: shenxianpeng date: 2025-12-31 Time flies quickly. At the end of each year, I write a running account to record my life and work from the past year. I hope my future self will still remember what happened this year.\n","title":"","type":"misc"},{"content":"title: Understand DevOps in One Article—This is How Packer, Terraform, Docker, and K8s Divide Their Work! summary: In the world of DevOps, with so many tools, many people get confused about their responsibilities. Using a car industry analogy, this article helps you understand the positioning and collaboration of Packer, Terraform, Ansible, Docker, and Kubernetes all at once. tags:\nDevOps date: 2025-12-10 authors: shenxianpeng In the world of DevOps, with so many tools, many people get confused about their responsibilities. The most typical questions are: What exactly do Packer, Terraform, and Ansible do? And where do Docker and Kubernetes fit in? What about VMs?\nIn fact, if you imagine them all as a set of processes for \u0026ldquo;manufacturing cars, buying cars, tuning cars, driving cars, and operating a fleet,\u0026rdquo; it becomes very clear.\nToday, using a more complete car factory analogy, I\u0026rsquo;ll help you understand it all at once:\nDevOps Tools = A Car Life Cycle Industry Chain # To make the analogy natural, we\u0026rsquo;ll divide the entire IT Infra life cycle into three stages:\nBuild Time: Car Manufacturing Provision Time: Car Purchasing / Allocating Parking Spaces Run Time: Driving Cars / Managing the Fleet Then, mapping each DevOps tool to these stages will be very intuitive.\n1. Packer — The \u0026ldquo;Pre-assembly Stage\u0026rdquo; in a Car Factory # Role: Car Factory Production Line Stage: Build Time\nDo you need a car that\u0026rsquo;s \u0026ldquo;fueled up, seats adjusted, and navigation configured\u0026rdquo;? Packer is what assembles all of these in the factory, then packages them into an image (AMI, VMDK, Docker Image, QCOW2, etc.).\nAnalogy: Packer is the factory worker who \u0026ldquo;pre-assembles the car\u0026rdquo;. The engine is installed, seats are adjusted, and software is pre-installed.\nThis way, when the car gets to you, it\u0026rsquo;s \u0026ldquo;ready to use out of the box\u0026rdquo;.\n2. Terraform — The Fleet Manager and Parking Lot Administrator # Role: Managing the Fleet, Scheduling Resources, Building Parking Lots Stage: Provision Time\nTerraform doesn\u0026rsquo;t build cars or drive them; its job is:\nDeciding how many cars to buy Which garage to put them in How big of a parking lot to build (VPC, Subnet) Assigning parking spaces for each car (VM, EC2, K8s nodes, LB, and other resources) It is responsible for the planning and creation of the entire fleet and infrastructure environment.\nAnalogy: Terraform is the fleet manager, saying: \u0026ldquo;We need three trucks, two sedans, parked in Garage Zone A, and build a charging station next to it.\u0026rdquo;\n3. Ansible — The Good Driver / Mechanic # Role: Vehicle Adjustment and Configuration Stage: Provision Time\nThe car is built and purchased, but before hitting the road, you still need to:\nAdjust the rearview mirrors Set up navigation Install extra accessories on the car Replace software, start services This is Ansible\u0026rsquo;s job.\nAnalogy: Ansible is the driver, also the mechanic, responsible for \u0026ldquo;all configurations after the car starts\u0026rdquo;.\nOf course, if you use Packer to do the work in advance within the image, then Ansible\u0026rsquo;s runtime work will be even less.\n4. Docker — Your \u0026ldquo;Personal Car\u0026rdquo; (Application Container) # Role: Single Car Stage: Build \u0026amp; Run\nDocker is not a fleet; it\u0026rsquo;s individual cars (containers). Each container is a \u0026ldquo;mini-car\u0026rdquo;:\nIndependent, secure Fast startup Comes with a \u0026ldquo;car shell\u0026rdquo; (image) Does not affect each other Can be driven on any \u0026ldquo;road\u0026rdquo; (different environments) Analogy: Docker is a \u0026ldquo;lightweight, unified, standard\u0026rdquo; small car; each car comes with its own operating instructions, no need to adapt to the ground environment.\n5. Virtual Machine (VM) — Renting a Piece of Land to Build Parking Spaces # Role: Parking Space / Plot Stage: Provision Time\nA virtual machine is a \u0026ldquo;designated parking space and road\u0026rdquo;. On this plot, you can:\nInstall an operating system Run applications Run Docker Run databases VM is the most fundamental layer of infrastructure.\nAnalogy: A VM is an \u0026ldquo;individual parking space in a garage\u0026rdquo;. The car must be parked here before further configuration and operation can proceed.\n6. Kubernetes — Intelligent Dispatch System for Large-Scale Fleets # Role: Fleet Operation Platform Stage: Run Time\nIf you have one car, Docker is enough; If you have 1000 cars, you need an intelligent system to:\nAutomatically assign vehicles Automatically repair cars, restart faulty vehicles Scale up, scale down Assign roads, traffic rules for vehicles Ensure continuous operation of the entire fleet This is Kubernetes.\nAnalogy: Kubernetes is a \u0026ldquo;super intelligent fleet management platform\u0026rdquo;. Responsible for the operation, traffic, and health of all cars in the entire city.\nHow Do These Tools Collaborate? (Complete Workflow) # Looking at it from the \u0026ldquo;Build Car → Buy Car → Drive Car\u0026rdquo; life cycle:\nStage Action Tool Analogy Build Time Build Standard Car Packer Factory Pre-assembly of Vehicles Provision Time Buy Car, Build Parking Lots Terraform Fleet Manager Selects Cars \u0026amp; Plans Garages Provision/Run Adjust Vehicles, Install Software Ansible Driver/Mechanic Build \u0026amp; Run Run Application Car Docker Single Standardized Small Car Run Time Manage the Entire Fleet Kubernetes Intelligent Fleet Dispatch System Infra Base Parking Space VM Individual Parking Space in a Garage With this table, you can clearly understand the responsibilities and collaboration methods of each tool.\nConclusion # Each DevOps tool has its own positioning; there\u0026rsquo;s no question of \u0026ldquo;which is more advanced,\u0026rdquo; only \u0026ldquo;which is more suitable for that stage.\u0026rdquo;\nIf this article helped you clarify the relationships between these tools, feel free to share it with your teammates so everyone can avoid some pitfalls.\nIf you\u0026rsquo;d like me to write more similar everyday explanations of DevOps in the future, please let me know.\n","externalUrl":null,"permalink":"/en/posts/2025/devops-tools/","section":"Posts","summary":"title: Understand DevOps in One Article—This is How Packer, Terraform, Docker, and K8s Divide Their Work! summary: In the world of DevOps, with so many tools, many people get confused about their responsibilities. Using a car industry analogy, this article helps you understand the positioning and collaboration of Packer, Terraform, Ansible, Docker, and Kubernetes all at once. tags:\n","title":"","type":"posts"},{"content":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. \u0026#x1f680;\nYou can also use these content pages to define Hugo metadata like titles and descriptions that will be used for SEO and other purposes.\n","externalUrl":null,"permalink":"/en/tags/advanced/","section":"Tags","summary":"This is the advanced tag. Just like other listing pages in Blowfish, you can add custom content to individual taxonomy terms and it will be displayed at the top of the term listing. 🚀\n","title":"Advanced","type":"tags"},{"content":"","externalUrl":null,"permalink":"/en/archive/","section":"Archive","summary":"","title":"Archive","type":"archive"},{"content":"","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/en/portfolio/","section":"Portfolios","summary":"","title":"Portfolios","type":"portfolio"},{"content":"","externalUrl":null,"permalink":"/en/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":" About Me DevOps/Build/Release Engineer, built the DevOps practice from scratch and drove best practices adoption across teams and departments. Specialized in DevOps since 2018, experienced with Windows, Linux, AIX, Solaris, HP-UX, and the full software development lifecycle. Strong in the Scan → Try → Scale approach to adopt and scale industry best practices. Proficient in Python / Shell / Groovy for developing DevOps tools and automation solutions. Open Source Creator: Founded and maintain cpp-linter, commit-check, conventional-branch, and devops-maturity. cpp-linter-action is used by Microsoft, Apache, NASA, and other notable projects. Tech Writer: Published hundreds of original articles on my blog and WeChat public account DevOps Engineer, reaching and inspiring a broad developer audience. Work Experience Senior DevOps Engineer | Rocket Software, Lithuania | Jul 2024 – Present\nDriving advanced DevOps initiatives and scaling delivery best practices across teams. DevOps Engineer | Rocket Software, Dalian | 2015 – Jun 2024\nLed CI/CD transformation from manual/Bamboo builds to Jenkins with shared libraries. Built IaC with Ansible for provisioning Jenkins, development environments. Dockerized MVAS products using buildx, health checks, and Kubernetes deployments. Proposed and scaled DevOps maturity badge and conventional commits. Automated VM management with Jira + Python, adopted company-wide. Enabled code coverage reporting for multiple product lines. Won Rocket Build Awards and added to product roadmap. Built Rocket Discover automation framework from scratch. Software Engineer in Test | JD.COM, Beijing | 2012 – 2014\nDeveloped automated test scripts and maintained CI pipelines. QA Engineer | SIMCOM (Shanghai) \u0026amp; Neusoft (Beijing) | 2009 – 2011\nDesigned and executed test cases; led small QA teams and shared best practices. Key Projects Internal pipeline-library Jenkins shared library for CI/CD as code, improving SDLC consistency. docker-images Dockerized MVAS with buildx, pytest, health checks, and Kubernetes. ansible-playbooks Managed build/dev infrastructure as code for fast recovery. U2Box CLI Go-based tool to set up MV dev/testing environments quickly. MV Intelligent Terminal CLI with auto-completion via UOPY API. Won Rocket Build First Prize \u0026amp; CPO Award (2019). VM Management with JIRA Automated VM lifecycle with Jira dashboards \u0026amp; Python scripts. Open Source cpp-linter – C/C++ linting solutions using clang-format and clang-tidy, used by Microsoft, Apache, NASA. commit-check – Ensures consistent commit messages, branch names, and more. conventional-branch – Git branch naming conventions for clear workflows devops-maturity – Specs and tools for assessing DevOps maturity explain-error-plugin – Explains Jenkins job failures with AI gitstats – Visualize Git repo insights and contribution analytics. Skills DevOps, CI/CD ★★★★★ Jenkins, Docker, Ansible ★★★★☆ Python, Shell ★★★★☆ Go, Groovy ★★★☆☆ Languages Chinese — Native English — Professional Working Proficiency Lithuanian — Beginner (A1) Education College Degree, Software Technology — Liaoning Provincial College of Communications (2006 – 2009) ","externalUrl":null,"permalink":"/en/hireme/","section":"","summary":"About Me DevOps/Build/Release Engineer, built the DevOps practice from scratch and drove best practices adoption across teams and departments. Specialized in DevOps since 2018, experienced with Windows, Linux, AIX, Solaris, HP-UX, and the full software development lifecycle. Strong in the Scan → Try → Scale approach to adopt and scale industry best practices. Proficient in Python / Shell / Groovy for developing DevOps tools and automation solutions. Open Source Creator: Founded and maintain cpp-linter, commit-check, conventional-branch, and devops-maturity. cpp-linter-action is used by Microsoft, Apache, NASA, and other notable projects. Tech Writer: Published hundreds of original articles on my blog and WeChat public account DevOps Engineer, reaching and inspiring a broad developer audience. Work Experience Senior DevOps Engineer | Rocket Software, Lithuania | Jul 2024 – Present\n","title":"Xianpeng Shen's Resume","type":"page"}]
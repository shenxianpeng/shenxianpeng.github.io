[{"content":"","date":"2025-08-03","externalUrl":null,"permalink":"/tags/ai/","section":"标签","summary":"","title":"AI","type":"tags"},{"content":"沈显鹏， shenxianpeng：高级 DevOps 工程师，cpp-linter、commit-check、conventional-branch、devops-maturity 组织的创建人，开源贡献者。\n","date":"2025-08-03","externalUrl":null,"permalink":"/","section":"DevOps 工程师 | Python 爱好者 | 开源贡献者","summary":"沈显鹏的博客，关于DevOps、CI/CD、开源等技术的探索与实践","title":"DevOps 工程师 | Python 爱好者 | 开源贡献者","type":"page"},{"content":"","date":"2025-08-03","externalUrl":null,"permalink":"/tags/jenkins/","section":"标签","summary":"","title":"Jenkins","type":"tags"},{"content":"上周我发布了 Jenkins Explain Error Plugin，旨在帮助 Jenkins 用户通过内置 AI 来更快地分析和解决 Jenkins 构建中的错误。\n有读者朋友在评论区提到，希望插件支持 Google Gemini 模型进行错误分析，因为他们公司只能使用 Google 的 AI 服务。\n今天，我很高兴地宣布，这个插件现在已经支持 Google Gemini 模型了！🎉\n插件更新 # 新增对 Google Gemini 模型的支持 优化文档并补充了示例视频 如何使用 Google Gemini # 在开始使用之前，请确保插件已更新到最新版本。 你可以在 Jenkins 插件管理器中找到 Explain Error Plugin，并将其升级到最新版\n更新后，在插件配置中，你可以选择使用 Google Gemini 模型进行错误分析。只需在 Manage Jenkins → Configure System 页面下的 Explain Error Plugin Configuration 中，将模型设置为 Google Gemini，并提供相应的 API 地址和密钥。\n点击 Test Configuration，确保你的 Google Gemini API Key、URL 和 Model 均已正确填写且可正常访问。\n插件示例视频 # 考虑到不少用户可能对插件的使用还不够熟悉，我录制了一段简短的视频，演示如何在 Jenkins 中使用 Explain Error Plugin 进行错误分析。\n你可以在 YouTube 上观看这个视频。\n结语 # 如果你在使用过程中有任何问题或建议，欢迎在 GitHub 上提交 issue，或在评论区留言。\n仓库地址：jenkinsci/explain-error-plugin\n欢迎 Star ⭐️ 支持一下！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-08-03","externalUrl":null,"permalink":"/posts/explain-error-plugin-support-gemini/","section":"Posts","summary":"本文介绍了 Jenkins Explain Error Plugin 的新功能，支持 Google Gemini 模型进行错误分析，并提供了配置方法和示例视频。","title":"Jenkins Explain Error Plugin 支持 Google Gemini 了！🤖","type":"posts"},{"content":"","date":"2025-08-03","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"Blowfish 支持基于 Hugo 的所有分类方法。同时，当前的标签预览页也支持展示自定义内容。\n在这里可以为每个分类添加额外的描述信息。查看下面的高级标签页面，了解更多。\n","date":"2025-08-03","externalUrl":null,"permalink":"/tags/","section":"标签","summary":"\u003cp\u003eBlowfish 支持基于 Hugo 的所有分类方法。同时，当前的标签预览页也支持展示自定义内容。\u003c/p\u003e","title":"标签","type":"tags"},{"content":"正如标题所说，我最近上线了我的第一个 Jenkins 插件！🎉\n这个插件的主要功能，是让 Jenkins 构建过程中遇到的错误不再需要复制错误信息到 ChatGPT 等 AI 工具中进行分析，而是直接在 Jenkins 的构建日志中提供一个按钮，点击后可以自动将错误信息发送到 OpenAI 进行分析。你也可以在 pipeline 中添加 explainError() 步骤来对错误进行解释，从而帮助开发者更快地定位和解决问题。\n这是我在 Jenkins 社区的第一个插件项目。之前之所以没有尝试，是因为我认为很多功能完全可以通过 pipeline 脚本来实现，没必要单独开发插件。\n但随着 AI 的使用越来越普及，我发现 Jenkins 插件中心居然还没有类似的插件，于是决定自己动手实现这个功能。在 AI 的加持下，利用晚上时间断断续续地开发与测试，以及 Jenkins Hoster 多轮认真细致的 Code Review，终于在上周末提交到了 Jenkins 插件中心，正式上线。\n插件介绍 # Explain Error Plugin 是一个基于 OpenAI 的 Jenkins 插件，它可以自动解析构建失败的日志信息，并生成可读性强的错误解释，适用于流水线（Pipeline）和自由风格（Freestyle）等常见的 Jenkins 作业类型。\n🔍 一键分析控制台输出中的错误 ⚙️ 可在 Pipeline 中使用，只需一个简单的 explainError() 步骤 💡 基于 OpenAI GPT 模型的 AI 智能解释 🌐 提供两种 Web UI 展示 AI 生成的分析结果 🎯 可定制：支持设置模型、API 地址、日志过滤器等配置 插件亮点功能 # 功能 说明 ✅ 控制台一键分析 控制台页面顶部新增 “Explain Error” 按钮 ✅ 流水线支持 提供 explainError() 步骤，可自动在失败时触发 ✅ 支持模型配置 自定义使用 GPT-3.5 或其他模型 ✅ 支持 Jenkins CasC 支持配置即代码（Configuration as Code） ✅ 日志筛选 支持正则过滤日志，聚焦报错内容 使用前提 # Jenkins 版本 ≥ 2.479.3 Java 版本 ≥ 17 拥有一个 OpenAI API Key（可在 OpenAI官网 申请） 快速安装 # 你可以通过 Jenkins 插件管理器安装：\nManage Jenkins → Manage Plugins → Available → 搜索 “Explain Error Plugin”\n插件配置 # 安装后，在 Manage Jenkins → Configure System 页面下找到 Explain Error Plugin Configuration，配置你的 OpenAI API key 和模型即可：\n设置项 描述 默认值 Enable Explanation 是否启用 AI 解析功能 ✅ 启用 API Key 你的 OpenAI Key 必填 API URL OpenAI 接口地址 https://api.openai.com/v1/chat/completions AI Model 使用的模型 gpt-3.5-turbo 也可以通过 Jenkins Configuration as Code 管理这部分配置，例如：\nunclassified: explainError: enableExplanation: true apiKey: \u0026#34;${AI_API_KEY}\u0026#34; apiUrl: \u0026#34;https://api.openai.com/v1/chat/completions\u0026#34; model: \u0026#34;gpt-3.5-turbo\u0026#34; 如何使用 # 在流水线中使用 # 推荐在 post { failure { ... } } 语句中调用 explainError()，在构建失败时自动分析错误：\npost { failure { explainError() } } 也支持设置日志长度和匹配关键词：\nexplainError( maxLines: 500, logPattern: \u0026#39;(?i)(error|failed|exception)\u0026#39; ) 在控制台中使用 # 适用于任意类型的 Jenkins 作业：\n打开失败构建的控制台输出页面 点击顶部的 “Explain Error” 按钮 AI 分析结果会展示在按钮下方 效果预览 # 构建失败后，在作业页面会出现一个侧边栏入口，点击即可查看 AI 分析结果：\n或者直接在控制台页面中点击“Explain Error”来查看分析内容：\n未来计划 # 错误缓存：同一个错误多次分析时避免重复调用 OpenAI，节省调用次数。（已经实现，等待合并） 多模型支持：支持其他 AI 平台，例如 Google Gemini、Claude、DeepSeek 等 这些功能仍在开发、构思和完善中，也非常欢迎你的反馈和建议。\n写在最后 # 插件是完全开源的，欢迎试用和贡献代码！\n如果你在使用过程中遇到任何问题，或者有好的建议，欢迎在 GitHub 上提交 issue 或 pull request：\nGitHub 地址：https://github.com/jenkinsci/explain-error-plugin\n如果你觉得这个插件对你有帮助，欢迎转发文章；也可以在 GitHub 上点个 Star ⭐ 支持一下！\n这是对开源项目最直接的支持 🙌\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-07-29","externalUrl":null,"permalink":"/posts/explain-error-plugin/","section":"Posts","summary":"介绍我开发的第一个 Jenkins 插件：Explain Error Plugin。它可以自动解析构建失败的日志信息，并生成可读性强的错误解释，帮助开发者更快地定位和解决问题。","title":"Jenkins 插件中心居然还没有 AI 插件？我写了一个！🤖","type":"posts"},{"content":"","date":"2025-07-13","externalUrl":null,"permalink":"/misc/","section":"Miscs","summary":"","title":"Miscs","type":"misc"},{"content":"","date":"2025-07-13","externalUrl":null,"permalink":"/tags/thoughts/","section":"标签","summary":"","title":"Thoughts","type":"tags"},{"content":"今晚孩子睡得早，我坐在阳台的沙发上，打开昨天和同事聚餐剩下的啤酒，听着窗外邻居用我听不懂的语言聊天，心里有点感慨。\n前段时间，有人问我：“你怎么出国的？什么途径？孩子以后就在那边读书吗？”\n我也了解到，一些朋友正在纠结是否要出国生活，最后还是选择留在国内。\n今天刚和房东签了第二年的租约，想想自己一个人出来生活也满一年了，算是积累了一些经验。\n如果用一句话来概括我的感受，那就是：\n出不出国，是个非常个人化的决定，没有对错。 但对于打工人来说——打磨一项核心技能，学好英语，养好身体，做长期有价值的事。\n或许是这两年就业形势愈发严峻，大家越来越多地在思考一个问题：\n我适合出国吗？ # 有人是为了孩子教育； 有人想逃离内卷、追求更高的生活质量。\n但想法与现实之间，常常横亘着语言、文化、教育体系、效率与生活习惯。\n我自己作为一名在欧洲工作的程序员，也经常被问到这些问题：\n“欧洲适合养娃吗？” “不会英语，能活下来吗？” “孩子上学怎么办？光有英语够不够？” “工资能攒下钱吗？” 今天，我想以“程序员”和“孩子爸爸”的双重视角，结合自己的观察，聊聊：\n什么样的人适合来欧洲，什么样的人可能会不适应？ # 适合来欧洲的人群 # 1. 英语能力尚可，能应对日常沟通 # 在欧洲，尤其是技术领域，英语是最常用的工作语言。 你不需要说得像母语者一样流利，但至少得能看懂邮件、开会能跟上、和同事能沟通。\n2. 愿意学习当地语言和文化 # 欧洲不像英国、美国、加拿大那样是纯英语国家。 各国有自己的官方语言。掌握当地语言，会让你在超市购物、看病、租房、办事时省下不少麻烦，也能更快融入社会。\n当然，也有人并不想再学一门新语言，那可能生活上的不便就会多一些。\n3. 能接受“慢节奏 + 高质量”的生活 # 欧洲很多国家推崇“工作-生活平衡”：\n准时下班，周末不加班 假期随时请，没有人追问“项目怎么办” 网购慢、办事需预约，但节奏慢不代表低效，习惯就好 如果你追求“年入百万”或热衷于“996冲刺项目上线”，可能会觉得不适应； 但如果你希望生活稳定、有时间陪家人、有精力关注自身成长，那欧洲或许正适合你。\n4. 带学龄前小孩的家庭 # 小朋友的语言适应能力非常强，尤其是0–5岁之间。 这个阶段来欧洲，语言习得几乎是“自动完成”的，教育质量不错，还有政府补贴。\n对家庭来说，这是一个“软着陆”的黄金阶段。\n5. 追求长期发展与生活质量的人 # 在国内，程序员35岁之后经常面临职业焦虑； 但在欧洲，35岁依然是职场的黄金期，跳槽加薪、稳定成长都很常见。\n如果你关注长期发展、工作稳定、生活质量、教育与医疗保障—— 欧洲确实是一个值得考虑的选择。\n而对于想拿欧盟绿卡或长期居留的人来说，欧洲的移民政策整体比美国要友好。\n可能不太适应的人群 # 1. 英语较差，且不愿意学习当地语言 # 你可能会觉得“我写代码就行了”，但现实中：\n签证、银行、医院、驾照、孩子上学、日常生活……处处都需要语言。\n靠 Google 翻译应急可以，靠它生活几年，很容易焦虑和挫败。\n2. 急性子、习惯有服务配套的人 # 如果你习惯了“今天提需求，明天上线”的国内节奏，那么欧洲的办事效率可能会让你抓狂。\n这边很多事都需要预约、等待、排队，甚至很多服务根本没有——比如便宜的外卖、按摩、理发、搓澡等，要么很贵，要么根本没有。\n3. 对天气特别敏感的人 # 很多欧洲国家的天气对中国人来说确实不算友好：\n冬天漫长、阴冷、日照时间短，容易产生季节性抑郁 夏天短，但白天特别长，有时晚上10点天还没黑 雨水多、湿度大、衣服不好晒干 如果你特别需要阳光和四季分明的天气，这可能是个痛点。\n4. 孩子已经上初中或高中 # 这个阶段孩子的语言学习能力下降，学业压力却上来了。\n如果是在非英语国家（比如德国、法国、立陶宛等），课程是本地语言，考试也要本地化，孩子压力很大，家长也会跟着焦虑。\n5. 主要目标是“赚钱” # 如果你的目标是“积累原始资本”或者“挣快钱”，那么：\n欧洲的税高 消费也不低 程序员工资虽然还不错，但远不如美国，而且很多时候低于国内大厂 尤其是如果你成家了，考虑只身来欧洲拼几年，一定要谨慎衡量代价和收益。\n孩子的年龄，对家庭决策也很关键： # 年龄段 适应程度 建议 0–5 岁 适应最强 语言学习快，教育压力小，建议来 6–11 岁 中等适应 语言学习难度略高，需额外辅导 12 岁以上 适应较难 升学压力大，语言负担重，建议慎重考虑 18 岁以上 适应轻松 读大学或硕士是很好的选择，教育资源多、费用低 一眼判断适不适合你 # 特征 适合来欧洲 不太适合来欧洲 英语能力 能日常沟通 英语差、也不想学 性格 平和，愿意慢节奏适应 急性子，习惯高效率与被服务 天气接受度 能接受冬天漫长、阳光少 极度依赖阳光，怕冷 家庭阶段 孩子年幼（0–5 岁） 孩子已进入初高中 职业规划 追求稳定发展、生活质量 主要目标是赚钱或快速积累资本 文化态度 愿意融入、愿意学语言 抵触本地文化，只想待在“华人圈” 写在最后 # “出不出国”没有标准答案，每个人的背景、追求、资源都不一样。\n我个人更看重生活环境、教育资源、医疗保障和工作节奏。 最大的缺憾，就是离家太远，每年只能回国一次，哪怕呆上一个月也觉得时间不够。\n语言是生活中不便的来源之一，但我也在慢慢适应和学习。\n如果未来能实现“远程+回国工作几个月”的自由，那对我来说，就是最理想的状态。\n出国生活是一个重要的决定，尤其是有孩子的家庭。开启一段新的人生，并不轻松。\n希望这篇文章，能帮你更清晰地判断自己是否适合来欧洲。 如果你正在考虑移居，欢迎留言交流，也欢迎转发给身边的朋友～\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-07-13","externalUrl":null,"permalink":"/misc/run-or-not/","section":"Miscs","summary":"哪些人适合来欧洲发展，哪些人可能会感到不适？从程序员和孩子爸爸的双重视角，聊聊出国生活的真实体验。","title":"出国一年，我看清了：不是所有人都适合来欧洲","type":"misc"},{"content":"","date":"2025-06-28","externalUrl":null,"permalink":"/tags/devops/","section":"标签","summary":"","title":"DevOps","type":"tags"},{"content":"","date":"2025-06-28","externalUrl":null,"permalink":"/tags/jfrog/","section":"标签","summary":"","title":"JFrog","type":"tags"},{"content":"大家好，我是DevOps攻城狮。\n最近读了一份挺有意思也挺震撼的报告：JFrog发布的《2025软件供应链现状报告》。这是JFrog连续多年基于其平台数据、CVE趋势分析、安全研究团队研究和1400位开发/安全从业者调查所形成的一份行业报告。\n我挑了一些对我们DevOps从业者、尤其是负责CI/CD和软件交付的人来说非常有参考价值的内容，分享如下：\n软件供应链，真的变了 # 报告开头就给出了几个数字，让人警觉：\n64% 的企业开发团队使用 7种以上编程语言，44% 使用10种以上； 一个普通组织 每年引入458个新包，也就是每月平均38个； Docker Hub 和 Hugging Face 上的镜像和模型数量仍在指数级增长； npm依然是恶意包的“重灾区”，但 Hugging Face 上的恶意模型增长了6.5倍。 如果说过去我们担心“你用的包有没有CVE”，现在可能得先问一句：\n你真的知道自己用了哪些包、拉了哪些模型吗？\n风险激增，不只来自漏洞 # 2024年，全球共披露了 33,000多个CVE，比2023年多了27%。但这只是“冰山一角”。\n报告揭示了一个更加令人担忧的趋势：“漏洞≠风险”，而“风险”正在从更多方向袭来：\n秘钥泄露：JFrog在公开仓库中扫描出 25,229个秘密token，其中 6,790个是“活的”； XZ Utils后门：攻击者假装是OSS维护者，潜伏多年后埋入后门，影响OpenSSH； AI模型的投毒：某些Hugging Face模型在加载时自动执行恶意代码（Pickle注入），悄无声息入侵机器； 误配置的代价：比如微软Power Pages因权限配置问题泄露大量用户数据，Volkswagen旗下公司因SaaS误配置泄露了80万台电动车的定位数据。 说实话，这些问题和“有没有扫描CVE”没什么关系。很多时候，是我们根本没意识到“这里也会出事”。\nAI爆发，风险也同步升级 # 今年Hugging Face新增了超过100万个模型和数据集，但同时，恶意模型也增长了 6.5倍。很多组织都开始将AI模型纳入业务，但：\n有 37%的企业靠“人工白名单”筛选模型使用； 很多AI模型使用的是Pickle格式，加载即执行，一不小心就中招； Hugging Face等平台上也出现了“挂羊头卖木马”的开源模型。 对于我们DevOps或者平台团队来说，这意味着：\n“模型”正在变成新的“依赖包”，也应该纳入供应链治理和安全扫描的范畴。\n安全实践现状：工具变多，人却更焦虑 # 报告里还有一个很现实的观察：\n安全工具越多，反而让人越看不清真相。\n73%的企业使用了 7个以上安全工具，但只有 43%同时扫描代码和二进制； 71%的组织允许开发者直接从公网拉包； 52%的组织一边允许公网拉包，一边又靠自动规则追踪来源； CVSS评分“虚高”问题越来越严重，JFrog分析后发现，88%的Critical CVE其实并不适用。 作为一线DevOps，我看到的是：工具越来越多，但我们似乎并没有真正“安心”下来。\n我们能做什么？ # 报告里没有提供“万无一失”的解决方案，但给出了一些务实建议，我结合自己的理解补充几点：\n治理从源头做起：不要再让开发者从公网自由拉包，使用内部代理仓库如 Artifactory/Nexus/Harbor； 扫描不止于代码：二进制扫描、容器镜像扫描和SBOM（软件物料清单）都需要纳入CI流程； 引入“适用性评估”：别光看CVE得分，更重要的是它是否真的适用于你的场景； 把AI模型当“依赖”管理：构建模型白名单、扫描模型安全性，甚至做模型SBOM； 限制新“匿名包”引入：不要因为某个库“突然火了”就引入，回顾XZ事件足够令人警醒。 写在最后 # 2025年的软件供应链比以往更大、更快、更复杂，也更脆弱。\n安全问题不是“有没有风险”，而是“你知道风险藏在哪里吗”。 一不小心，风险可能来自一个新同事引入的PyPI包，或者一位AI工程师下载的模型。\nJFrog这份报告虽然没有解决所有问题，但给我们敲了一个不小的警钟。\n如果你也在构建自己的DevOps流程、AI平台、或者仅仅是维护日常构建环境， 建议你认真想一想：\n你的“依赖”到底靠不靠谱？ 你的“扫描”真的能发现问题吗？ 你的“策略”是否跟得上变化了？\n欢迎在评论区聊聊你看到的“供应链怪现象”。也希望这篇分享对你有所启发。\n🧡 欢迎关注，一起做更好的技术实践。 📥 如果你需要这份《JFrog Software Supply Chain Report 2025》原报告， 可以在公众号后台回复关键词：“jfrog report 2025” 来获取报告下载链接。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-06-28","externalUrl":null,"permalink":"/posts/jfrog-report/","section":"Posts","summary":"JFrog发布的《2025软件供应链现状报告》揭示了软件供应链的变化和风险，尤其是AI模型的安全问题。本文分享了报告中的关键发现和对DevOps从业者的启示。","title":"🧊2025软件供应链现状报告：开源时代，我们究竟在和谁打交道？","type":"posts"},{"content":" 问题 # 我在 Jenkins 中使用以下 Groovy 代码发布文档：\npublishHTML([ allowMissing: false, alwaysLinkToLastBuild: false, keepAll: false, reportDir: \u0026#34;docs/build/html/\u0026#34;, reportFiles: \u0026#39;index.html\u0026#39;, reportName: \u0026#34;Documentation\u0026#34;, useWrapperFileDirectly: true ]) 然而，一些来自 Shields.io 的徽章在已发布的文档中无法正常显示。\n解决方法 # ✅ 在 Script Console 中更新 Jenkins CSP 的可用脚本\n以下是应在 “Manage Jenkins → Script Console” 中运行的精简且正确的 Groovy 脚本：\nSystem.setProperty( \u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;default-src \u0026#39;self\u0026#39;; img-src * data:;\u0026#34; ) 该设置允许从任意域加载图片（img-src *），包括 Shields.io。 如果你希望更安全地限制来源，可以这样设置：\nSystem.setProperty( \u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;default-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; https://img.shields.io data:;\u0026#34; ) 🟡 注意：此更改是临时的（仅在内存中生效），Jenkins 重启后会失效。\n✅ 永久生效的方法\n修改 Jenkins 启动参数（取决于你的 Jenkins 运行方式）： 如果使用 /etc/default/jenkins（Debian/Ubuntu）：\nJENKINS_JAVA_OPTIONS=\u0026#34;-Dhudson.model.DirectoryBrowserSupport.CSP=\\\u0026#34;default-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; https://img.shields.io data:;\\\u0026#34;\u0026#34; 如果使用 systemd（CentOS/Red Hat 或现代系统）：\n编辑或覆写 jenkins.service 文件：\nEnvironment=\u0026#34;JAVA_OPTS=-Dhudson.model.DirectoryBrowserSupport.CSP=default-src \u0026#39;self\u0026#39;; img-src \u0026#39;self\u0026#39; https://img.shields.io data:;\u0026#34; 重启 Jenkins： sudo systemctl restart jenkins 结果 # 现在，Shields.io 徽章在 Jenkins 中可以正常显示了。\n转载本文时请注明作者与出处。禁止商业用途。你可以通过 RSS 订阅我的博客。\n","date":"2025-06-23","externalUrl":null,"permalink":"/posts/jenkins-show-badge/","section":"Posts","summary":"如果你的 Shields.io 徽章（例如构建状态或文档状态）在 Jenkins 仪表盘中无法显示，很可能是因为 Jenkins 的严格内容安全策略（CSP）限制所致。本文将演示如何通过 Script Console 临时修复，以及如何通过修改 Jenkins 启动参数永久解决。该方法适用于内部 Jenkins 环境，并已在现代 Jenkins 安装中测试验证。","title":"如何修复 Shields.io 徽章在 Jenkins 中无法显示","type":"posts"},{"content":"","date":"2025-06-12","externalUrl":null,"permalink":"/tags/bitbucket/","section":"标签","summary":"","title":"Bitbucket","type":"tags"},{"content":"","date":"2025-06-12","externalUrl":null,"permalink":"/tags/git/","section":"标签","summary":"","title":"Git","type":"tags"},{"content":" 背景 # 最近和同事讨论了一个看似简单但很重要的问题：\n我们如何确保 PR 中有价值的信息不会随着时间和工具的更替而丢失？\n虽然我们日常使用 Bitbucket 来协作开发，但未来也许会迁移到 GitHub、GitLab 等平台。这些托管平台可能会变，但 Git 本身作为代码历史的记录核心，很可能还会长期存在。\n问题也就来了：\nPR 页面里描述的变更背景、解决思路和关键讨论，如果只保存在 PR 工具里，就很可能在平台迁移后“消失”。而这些信息，本应是 commit message 的一部分。\n我们讨论过的一些方案： # 在 git commit -m 时手动记录问题的解决方式 —— 但容易被忽略或写得不完整。 模仿 pip 项目使用 NEWS 文件 来记录每次变更 —— 虽然能保留信息，但这种方式更适合用于生成 release note，而不是记录修改的动机或原因。 强制要求成员把内容写在 Jira ticket 中 —— 工具间割裂，不利于在代码上下文中快速理解历史。 最终我们决定：使用 Bitbucket 的 Commit Message Templates 功能，把 PR 的描述直接写入 Git commit 中。\nBitbucket 的 Commit Message Templates 功能 # Bitbucket 支持在合并 PR 时自动生成 commit message，并允许通过模板插入有用的信息。文档地址如下： 🔗 Pull request merge strategies - Bitbucket Server\n我看到了 GitLab 也有类似的功能，但 GitHub 似乎没有。\n参考 GitLab Commit Templates 官方文档\n你可以在模板中使用以下变量：\n变量名 说明 title PR 标题 id PR ID description PR 描述 approvers 当前已批准的审阅人 fromRefName / toRefName 源 / 目标分支名称 fromProjectKey / toProjectKey 源 / 目标项目 fromRepoSlug / toRepoSlug 源 / 目标仓库 crossRepoPullRequestRepo 跨仓库 PR 的源仓库信息 我们是怎么用的？ # 在 Bitbucket 的仓库设置中，你可以配置 PR 的合并提交模板，找到设置：\nRepository settings -\u0026gt; Merge strategies -\u0026gt; Commit message template\n配置后，当你在 PR 页面合并时，Bitbucket 会自动把 PR 的标题、描述、相关 ID 等内容写入最终的 merge commit message 中。\n这样，不管将来团队是否继续使用 Bitbucket，PR 的关键信息都将永远保存在 Git 历史中。\n下面是实际效果：\n📥 配置模板界面： 📤 最终生成的 Git commit message： 总结 # 这项改动虽然小，却帮助我们从流程上保护了代码变更的上下文。它让 PR 不再是“临时信息容器”，而是自然地成为 Git 历史的一部分。\n如果你也在使用 Bitbucket，不妨试试这个功能。\n让 Git commit message，不只是代码提交，更是决策和演进的记录。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-06-12","externalUrl":null,"permalink":"/posts/commit-message-template/","section":"Posts","summary":"在 Bitbucket 中使用 Commit Message Templates 功能，将 PR 描述自动写入 Git commit message，确保关键信息不会随工具变更而丢失。","title":"提升代码可追溯性：一招把 PR 描述写入 Git commit","type":"posts"},{"content":" 背景介绍 # 在我们的 Jenkins 流水线中，经常会遇到这样一种情况：当同一个分支或 PR 已有一个构建任务正在运行时，如果此时有新的提交进来，当前构建会被自动中断，转而执行新的构建任务。\n这是由于我们需要合理控制资源使用，尤其是对于那些构建时间较长的任务。如果允许同一个分支或 PR 同时触发多个构建，Jenkins Agents 很快就会被大量占用，导致后续任务只能排队等待。\n因此，在 Jenkinsfile 中，我们经常会设置这样的选项：\n// 禁止并发构建，允许中断前一个任务 disableConcurrentBuilds abortPrevious: true 这段代码你可能在不少 Jenkinsfile 中看到，就算是 Jenkins 团队他们自己的 CI 也是这样设置的。\n这样做的好处是显而易见的，但在某些情况时也带来了一个新的问题，比如：\n当一个 release 分支正在运行的构建任务，会被新的合并触发的构建中断。尤其是当它即将完成时，却因为新的合并代码触发了新的构建任务，导致前一个任务被中断。\n这让 QA 同事很抓狂：一个马上就要交付测试的 Build，结果因为新的合并被中断了，还得重新等待……\n于是他们提出了一个需求：\nPR 构建可以中断来节省资源没问题，但像 devel 或 release 分支上的构建，如果已经在运行，就不应该被中断。新的合并触发的构建应该排队等待，直到前一个任务完成后再运行。\n起初我认为这个设置是全局生效的，没办法按分支进行区分。查了一下 ChatGPT 和 Google，也确实没发现特别简单的方法能做到“某些分支不中断，而是排队”。\n后来我想到了一个非常简单的做法：\n可以通过一段简单的判断逻辑，在 pipeline 的起始位置判断当前构建是基于哪个分支，然后根据分支类型动态设置 abortPrevious 的值：\ndef call() { def isAboutPrevious = true if (env.BRANCH_NAME == \u0026#39;devel\u0026#39; || env.BRANCH_NAME.startsWith(\u0026#39;release/\u0026#39;)) { isAboutPrevious = false // devel 和 release 分支，不中断 } pipeline { options { disableConcurrentBuilds abortPrevious: isAboutPrevious } stages { // ... 构建流程 } } } 最终效果 # 这段逻辑已经合并到了我们的共享 Jenkins 库中。上线后的表现完全符合预期：\n✅ 同一个 devel / release 分支上的构建任务会排队执行，不再中断 ✅ 同一个 PR 的构建依然会中断前一个任务，从而节省资源 ✅ 无需单独创建 Jenkinsfile，也没有引入复杂逻辑，维护成本极低\n改动之前是这样的：\nJob #104：还没跑完就被中断了 Job #105：同样的命运，又被新合并触发的构建终止了 我们希望达到的效果是：\nJob #106：即使有新的合并，也能继续正常运行 Job #107：在队列中等待，等 #106 完成后再开始执行 总结一下 # 如果你也在使用 Jenkins 进行多分支构建，并且对某些特定分支有类似的需求，这个方法值得一试：\n简单的一段条件判断，就可以实现按需控制构建是否会被中断。\n欢迎大家在留言区交流你们的 Jenkins 构建优化经验！👋\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-06-05","externalUrl":null,"permalink":"/posts/jenkins-concurrent-build/","section":"Posts","summary":"在 Jenkins 中，\u003ccode\u003edisableConcurrentBuilds\u003c/code\u003e 选项用于管理并发构建。本文介绍了如何根据分支类型动态设置 \u003ccode\u003eabortPrevious\u003c/code\u003e 的值，从而实现更灵活的构建管理。","title":"如何优雅地控制 Jenkins 构建是否中断？只看这篇","type":"posts"},{"content":"","date":"2025-06-02","externalUrl":null,"permalink":"/tags/ci/","section":"标签","summary":"","title":"CI","type":"tags"},{"content":"","date":"2025-06-02","externalUrl":null,"permalink":"/tags/ci/cd/","section":"标签","summary":"","title":"CI/CD","type":"tags"},{"content":"今天这篇文章，起因是我看到前同事发的一条朋友圈。\n我感觉他这是在赤裸裸地夸我 ：） （不好意思，得瑟一下~）\n不过说实话，这几年在 CI/CD 这块，我确实一直在做两件事：\n一边关注行业里最新的实践； 一边筛选出适合我们业务的部分落地实施，或者写文章进行分享。 但我发现一个常见的误区是：\n很多人觉得 CI/CD 做完了就完了，是一劳永逸的事情。\n其实并不是。\n罗马不是一天建成的，CI/CD 也不是 # 不论是 CI/CD 流水线，还是底层的程序、工具、库、平台，它们都属于基础设施的一部分。基础设施的一个显著特点是：\n需要长期投入与持续维护。\n否则，哪怕你现在搭建得再好，几年之后，也会因为无人维护而变成一个臃肿、失控的“技术债堆积场”。最终不得不推翻重来。\n举几个例子你就会明白：\n如果不维护，风险很快就会显现 # 1. 安全风险： # 如果你没关注 CVE-2024-23897，可能根本不知道你的 Jenkins 存在任意文件读取漏洞，有被攻击者入侵的风险。\n2. 功能缺失与兼容性问题： # 不看 Jenkins 的更新日志，可能会错过一些关键修复或新特性； 不更新 CI/CD 工具链，代码可能突然就编译不了了，测试也不再通过。\n3. 技术债积累与维护成本上升： # 没有引入新的实践和工具，流水线就会越来越复杂、难维护，开发体验也会越来越差。\n再来看 Python 项目这边 # 1. 依旧使用 setup.py？ # 那你可能错过了 PEP 518 推出的 pyproject.toml 所带来的统一构建生态。\n2. 依旧用 pip install 安装 CLI 工具？ # 那你可能还不了解 pipx 和 uvx 能如何更方便、高效地管理工具依赖。\n3. 不了解 PEP、不了解包结构和发布规范？ # 那很容易写出不规范、难以维护、别人无法复用的代码。\n这些问题，表面上看似无伤大雅，但背后隐藏的是维护成本、团队协作效率以及项目未来发展的可持续性。\n所以说，重构是软件工程的日常功课\n很多时候，这些工作并不显眼，甚至容易被认为是“做多了”“瞎折腾”或“浪费时间”。\n但实际上，恰恰是这些“隐性的工作”，才是软件工程真正的根基。\n没有重构，系统只会越来越臃肿； 没有持续进化，CI/CD 迟早变成一个烂摊子； 没有持续学习和探索，就会离最佳实践越来越远。\n所以，希望这篇文章能给你一点启发：\nCI/CD 不是一次性的建设项目，而是一个持续演进、不断重构和成长的 DevOps 系统。\n如果你也正在做这些“看不见的工作”，别灰心。你正在为整个系统的长期可持续打下基础。\n你是否也遇到过相关的坑？欢迎留言分享你的故事。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-06-02","externalUrl":null,"permalink":"/posts/code-refactor/","section":"Posts","summary":"在 DevOps 中，CI/CD 流水线需要持续维护和重构。本文探讨了为什么 CI/CD 不是一次性的建设项目，而是一个需要长期投入和持续演进的系统。","title":"CI/CD 不是一次性的项目，而是一个不断演进的系统","type":"posts"},{"content":"","date":"2025-05-29","externalUrl":null,"permalink":"/tags/asdf/","section":"标签","summary":"","title":"Asdf","type":"tags"},{"content":"最近，我在 cpp-linter 组织下发布了一个名为 asdf-clang-tools 的全新仓库。这个项目是从 amrox/asdf-clang-tools fork 而来。由于原作者多年没有维护，我对其进行了修复、升级和功能扩展，使其焕然一新。简单来说，asdf-clang-tools 是一个 asdf 插件，用于安装和管理 Clang Tools 相关工具（如 clang-format、clang-tidy、clang-query 和 clang-apply-replacements 等）。\n新的安装方式：除了 pip 还有 asdf # 在此之前，我曾推出过 clang-tools-pip 工具包，用户可以通过 pip install clang-tools 的方式安装包括 clang-format、clang-tidy、clang-query、clang-apply-replacements 在内的一整套 Clang 可执行工具。\n而 asdf-clang-tools 则提供了另一种途径——利用 asdf 版本管理器来安装这些工具。简而言之，这为喜欢用 asdf 管理工具版本的开发者多了一个选择。\n这两种方式并不是互斥的：你可以通过 pip 或 asdf 轻松安装和管理 Clang 工具。至于选择哪种方式取决于你的工作流和个人喜好。\n什么是 asdf 版本管理器 # 很多开发者可能还不太熟悉 asdf。asdf 是一个多语言、多工具的版本管理器。\n它可以用一个命令行工具管理多种运行时环境的版本，支持插件机制。\n举例来说，你可以通过 asdf 来管理 Python、Node.js、Ruby 等语言的版本，也可以管理 Clang 工具（像我介绍的 asdf-clang-tools）。\n所有工具的版本信息都记录在一个共享的 .tool-versions 文件中，这样团队可以轻松在不同机器间同步配置。\n总之，asdf 的好处就是“一个工具管理所有的依赖”，让项目所需的各类工具版本统一起来，免去在每个工具里使用不同版本管理器的麻烦。\n安装与使用示例 # 使用 asdf-clang-tools 安装 Clang 工具非常简单。假设你已经安装好了 asdf，只需按照官方仓库的说明进行操作：\n首先 添加插件：以 clang-format 为例，在终端运行：\nasdf plugin add clang-format https://github.com/cpp-linter/asdf-clang-tools.git 类似地，clang-query、clang-tidy、clang-apply-replacements 等工具也使用相同的仓库地址，只需把插件名改为对应的名称即可。\n查看可用版本：添加插件后，可以运行 asdf list all clang-format 来列出所有可安装的 clang-format 版本。\n安装工具：选择一个版本（例如最新的 latest），执行：\nasdf install clang-format latest 这会下载并安装指定版本的 clang-format 二进制文件。\n设置全局版本：安装完成后，可以执行：\nasdf set clang-format latest 这会把版本写入 ~/.tool-versions 文件，实现全局可用。此后，你就可以直接在命令行中使用 clang-format 等命令了。\n以上操作完成后，clang-format、clang-tidy 等工具就已集成到 asdf 管理下。更多细节可参考 asdf 官方文档。\n欢迎试用并反馈建议 # 总的来说，asdf-clang-tools 为需要 Clang Tools 的开发者提供了一种新的安装方式。\n它与 cpp-linter 组织的其它工具（如 clang-tools-pip）互为补充。\n我诚挚欢迎大家尝试 cpp-linter 提供的整个 C/C++ lint 解决方案，选择最适合自己工作流的工具。\n同时，如果在使用过程中有任何问题或改进建议，欢迎通过 GitHub Issues、讨论区等渠道提出，一起完善 Cpp Linter 工具链，让 C/C++ 格式化和静态分析工作更加便捷高效！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-05-29","externalUrl":null,"permalink":"/posts/asdf-clang-tools/","section":"Posts","summary":"asdf-clang-tools 是一个 asdf 插件，用于安装和管理 Clang Tools 相关工具（如 clang-format、clang-tidy、clang-query 和 clang-apply-replacements 等）。","title":"asdf-clang-tools：使用 asdf 安装 Clang 工具的新选择","type":"posts"},{"content":"","date":"2025-05-29","externalUrl":null,"permalink":"/tags/clang-tools/","section":"标签","summary":"","title":"Clang-Tools","type":"tags"},{"content":" 分享两个最近的体会 # AI 让人感到“虚” # 老同事来欧洲出差，和他聊了很多 —— 从工作到生活，从技术到 AI。\n当我问他：“你对 AI 有什么感受？”\n他说了一个字：虚！\n这让我有些意外。他可是我心中的技术大牛，工作了十几年，积累了很多经验和技能。但如今，他却觉得自己在 AI 面前变得“虚”了。\n我也有同感。可能在一年前，我还觉得，自己在工作、博客、开源上的不断积累，会在下一份工作中起到很大的帮助；但 AI 的出现，让我开始怀疑：\n我过去四五年的努力是否还有意义？\nAI 的确让每个人的工作效率都有了显著提升。\n有同事开玩笑说：“我路过每个人的桌面，都看到 ChatGPT 开着。”；另一位同事接着说：“我都同时开好几个。”\n这样的对话一度让我感到沮丧。\n我开始怀疑：\n自己在工作中是否还有不可替代的价值？ 自己在开源社区的贡献是否还有意义？ 自己还在坚持写博客是否还有用处？ 直到我在 LinkedIn 上看到还有不少岗位在招聘，意识到手头的工作依然需要推进，这才慢慢把我拉回现实：AI 不会取代我们的职位，但一定会改变我们的做事方式。\n每天一小步 # 另外还有一个小的体会是：每天一小步。\n无论是工作还是生活，有目标感是好事。但如果目标太大，往往难以坚持，甚至会被拖垮。\n过去我常常给自己设定理想化的目标，比如“写一本书”或“翻译一本书”。但这样的目标需要投入大量时间和精力，最后常常因为现实的工作和生活节奏被搁置。\n后来我发现，其实偶尔写一篇小文章，或者维护一个小项目，更容易坚持。\n当一件事不再那么“重”，它也就没那么难开始了。\n所以，我们既要抬头看路，树立目标，也要低头走好每一步。\n这两点体会，虽然简单，却是我在日常工作和生活中逐渐领悟到的。\nAI 带来的冲击还在继续，我们每个人都要拥抱变化；而“每天一小步”的坚持，也在悄悄积累力量。\n愿我们都能在变化中找到节奏，在前行中保持方向。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-05-26","externalUrl":null,"permalink":"/posts/ai/","section":"Posts","summary":"AI 的出现让很多人感到“虚”，但它不会取代我们的职位，而是改变我们的做事方式。本文分享了对 AI 的体会和每天一小步的坚持。","title":"ChatGPT 一开，谁还去“努力”？","type":"posts"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/confluence/","section":"标签","summary":"","title":"Confluence","type":"tags"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/documentation/","section":"标签","summary":"","title":"Documentation","type":"tags"},{"content":"","date":"2025-05-14","externalUrl":null,"permalink":"/tags/wiki/","section":"标签","summary":"","title":"Wiki","type":"tags"},{"content":"不知道你是否也在企业中使用过 Confluence，或者其他类似的 Wiki 工具。刚接触它时，我觉得这玩意儿真不错：功能强大、支持各种排版样式、可以插入图片、视频、图标，还能查看历史版本，使用体验比 Git 轻松太多了。\n但慢慢地，我发现了它的一个巨大问题：每个人都可以创建并维护自己的 Wiki 页面。\n一开始，这种自由看起来是优势。但时间一长，问题就来了：同一个主题的内容，可能会被不同的人写了多个版本。尤其是在项目或产品从一个团队交接到另一个团队时（在外企这是很常见的操作），新团队成员可能不会在原有文档上继续维护，而是习惯另起炉灶，记录自己的理解。\n于是，旧的 Wiki 随着人员流动逐渐失效（原作者可能早已离职），新的 Wiki 内容又不够完善甚至有误。这样一来，知识沉淀不仅没有统一，反而更混乱了。\n我始终认为 Wiki 工具本身是好的，但如果缺乏统一的管理机制、没有像 Git 那样的 Pull Request 审批流程，那最终它就容易沦为垃圾信息的生产场。\n相比之下，开源社区在这方面反而做得很好。\n以 Python 社区为例：\nPython 官网 https://www.python.org 的内容，是通过 GitHub 上的 python/pythondotorg 仓库维护的； 开发者指南 https://devguide.python.org 的内容，托管在 GitHub 上的 python/devguide 仓库。 不管是谁，只要对这些文档有修改建议，都必须通过 PR 提交，经过审查、通过 CI 检查后才能合并。而且因为是开源项目，社区用户也会主动参与反馈和改进，这也帮助文档长期保持高质量。\n但反观企业内部，就完全不同了：\n多人各写各的 Wiki，质量参差不齐； 内容孤岛多，缺乏维护，一旦人员流动，旧文档就“失效”； 更重要的是，内部文档缺乏公开审核机制，也没有外部反馈入口，错误不容易被发现或纠正。 还有一点可能更真实也更残酷：在企业内部，员工缺乏维护文档的动力。因为一个写得面面俱到、毫无遗漏的“满分文档”，可能在某天就意味着你可以被“无缝替换”。相比之下，把关键细节掌握在自己脑子里，才更有“工作安全感”。\n所以文档治理这件事，本质上和工具无关，关键在人。在没有文化和流程支撑的前提下，再先进的工具也可能变成信息垃圾的堆放地。\n文档和代码其实不分家。在开源社区的经历让我深有感触：真正优秀的人，往往能独立撑起一个团队的质量与节奏。他们热爱技术、主动负责、乐于分享，推动项目健康发展。\n反观一些企业项目，问题往往出在相反的方向。当团队成员缺乏主人翁意识，人员流动频繁，或者有些人能力一般却意见最多，最终只会在屎山代码上继续堆。\n最后，你是否也有类似的经历？你所在的公司又是如何管理内部文档和代码的呢？欢迎在评论区留言交流。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-05-14","externalUrl":null,"permalink":"/posts/docs-and-code/","section":"Posts","summary":"在企业中，Wiki 和 Confluence 等文档工具如果缺乏统一管理和审核机制，可能会导致信息混乱和知识沉淀失败。本文探讨了如何避免这种情况，并借鉴开源社区的成功经验。","title":"还在用 Wiki/Confluence？你可能在生产垃圾","type":"posts"},{"content":"","date":"2025-05-05","externalUrl":null,"permalink":"/tags/pip/","section":"标签","summary":"","title":"Pip","type":"tags"},{"content":"","date":"2025-05-05","externalUrl":null,"permalink":"/tags/uv/","section":"标签","summary":"","title":"Uv","type":"tags"},{"content":"如果你已经习惯了 pip install、手动创建虚拟环境、自己管理 requirements.txt，那你可能会对 uv 有些惊喜。\n这是一个由 Astral 团队开发、用 Rust 写的 Python 包管理工具，它不仅能替代 pip、venv、pip-tools 的功能，还带来了更快的依赖解析速度，以及一套更现代的项目管理方式。\n从 uv init 开始，一键创建项目骨架 # 我们不从 “怎么装 uv” 讲起，而是从 “怎么用 uv 创建一个项目” 开始。\nuv init 运行这条命令后，uv 会帮你：\n创建 .venv 虚拟环境； 初始化 pyproject.toml 配置文件； （可选）添加依赖； 生成 uv.lock 锁定文件； 将 .venv 设置为当前目录默认环境（无需手动激活）； 整个过程只需一个命令，就能完成以往多个步骤，是构建 Python 项目的更优起点。\n使用 uv add 安装依赖（而不是 pip install） # 传统的写法是：\npip install requests 但在 uv 的世界里，添加依赖是这样的：\nuv add requests 这样做的好处是：\n自动写入 pyproject.toml 的 [project.dependencies]； 自动安装到 .venv 中； 自动更新 uv.lock 锁定文件； 无需再维护 requirements.txt。 如果你想添加开发依赖（比如测试或格式化工具），可以：\nuv add --dev pytest ruff 想移除依赖？\nuv remove requests 运行项目脚本或工具：uv venv + uvx # uv 的虚拟环境默认安装在 .venv 中，但你不需要每次 source activate，只需执行：\nuv venv 这会确保 .venv 存在并自动配置成当前 shell 的默认 Python 环境。之后你运行脚本或工具时，不用担心路径问题。\n更妙的是，uv 提供了一个 uvx 命令，跟 pipx 以及 Node.js 的 npx 类似，可以直接运行安装在项目里的 CLI 工具。\n比如我们用 ruff 来检查或格式化 Python 代码：\nuvx ruff check . uvx ruff format . 现在用 uvx，你不需要再装一堆全局工具，也不用再通过 pre-commit 来统一调用命令——项目内直接用，跨平台还省心。\n项目结构示例 # 经过 uv init 和一些 uv add 之后，一个干净的 Python 项目结构可能是这样的：\nmy-project/ ├── .venv/ ← 虚拟环境 ├── pyproject.toml ← 项目配置（依赖、元数据等） ├── uv.lock ← 锁定的依赖版本 ├── main.py ← 项目入口脚本 使用体验如何？ # 我最近已经把 uv 作为新项目的默认工具：\n不再手动写 requirements.txt 不再纠结 poetry.lock 和 pyproject.toml 的混用 依赖安装速度非常快，第一次装大型项目时速度能快 3～5 倍 搭配 ruff 作为 Lint + Format 工具，完全不需要再装 black、flake8 在 CI 中，我也逐步用它替代 pip install -r requirements.txt，用锁定文件构建环境，一致性更强。\n总结 # 如果你：\n对 pip install 安装慢感到不满； 不想再写一堆 requirements 文件； 想要更现代、更快、更自动化的 Python 项目结构； 那你应该试试 uv。它是更快、更更现代的包管理器工具集。\n从下个项目开始，不妨从 uv init 开始吧。\n项目地址：https://docs.astral.sh/uv/\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-05-05","externalUrl":null,"permalink":"/posts/uv/","section":"Posts","summary":"uv 是一个由 Astral 团队开发的 Python 包管理工具，它能替代 pip、venv、pip-tools 的功能，提供更快的依赖解析速度和更现代的项目管理方式。","title":"还在用 pip 和 venv？那你可真落伍了，赶紧体验 uv！","type":"posts"},{"content":"","date":"2025-04-29","externalUrl":null,"permalink":"/tags/lithuania/","section":"标签","summary":"","title":"Lithuania","type":"tags"},{"content":"","date":"2025-04-29","externalUrl":null,"permalink":"/tags/pycon/","section":"标签","summary":"","title":"PyCon","type":"tags"},{"content":"","date":"2025-04-29","externalUrl":null,"permalink":"/tags/python/","section":"标签","summary":"","title":"Python","type":"tags"},{"content":"今天是我参加 PyCon LT 2025 的第三天：AI and ML Day，主题是 LLM 和 Neural Networks。\n全部日程可以在这里查看：PyCon LT 2025\nAI 的发展确实很快，我也想借这次大会的机会，听一听关于 AI 的相关讨论。虽然因为下午有工作需要回公司处理，只听了上午的议程，但依然有收获。\n主要的收获是：AI 已经成为每个人都不能不学习和应用的技术了。\n依旧是流水账记录一下今天的感受 # 今天同样是早上九点多到的会场，第一场演讲开始前，打上一杯咖啡就进去听了。\n第一场的主题是《Open-source Multimodal AI》。这位演讲嘉宾来自法国，是 Hugging Face 团队的成员，也许大家都见过这个表情 🤗 —— 著名的 Transformers、Diffusers 开源库就是他们开发的，另外他们还有 Hugging Face Hub 网站，用户可以上传、分享、下载各种 AI 模型和数据集。 本次演讲主要分享了多模态人工智能的内容，介绍了多模态和开源的一些背景知识、相关库、基本 API，以及如何开始使用开源模型，并提到了目前流行的一些开源模型。\n第二场听的是《Knowledge Bases \u0026amp; Memory for Agentic AI》，介绍了 Agent 人工智能（Agentic AI）的起源。演讲内容包括如何设计提示（prompt），指导 LLM 使用工具并规划解决复杂查询的方法；学习函数调用，以及如何利用 LLM 的这一特性作为代理（Agent）的基础。演讲者同样是一位女性，来自荷兰。\n第三场听的是《EGTL data-processing model prototype using Python》，分享了如何在传统 ETL（提取、转换、加载）流程中，新增由生成式 AI 支持的“生成”步骤，扩展为 EGTL（提取、生成、传输、加载）。演示了基于数据仓库的 Python 流水线如何自动提取数据、生成新洞察并实现优化转换，探讨了实用工作流程、实际用例和最佳实践，帮助数据项目应用 EGTL 方法。\n上午最后一场听的是《AI 360: From Theory to Transformation》，分享了 AI 的发展历程。从数据和模型的双重视角，回顾了人工智能的演变，讲述了从早期的符号系统到今天先进的数据驱动技术的发展过程。与会者了解了不断增长的数据集和日益复杂的模型架构如何相互作用，推动 AI 从理论好奇心变为全球创新催化剂的过程。\n后面因为下午要回公司处理工作，就在吃了午饭后离开了。\n启发 # 今天听了几位演讲，女性讲者的比例依旧很高，前两位讲得尤其好，逻辑清晰、表达有力。\n而今天关于 AI 的主题，更是让我意识到一件事：**AI 的发展根本不等人。**谁还在观望、犹豫，谁就已经落后了。唯一的出路，是尽快拥抱它，而不是浪费时间抗拒或怀疑。\n说白了，现在最该做的不是去学怎么“用” AI，而是反过来问：**哪些事情应该让 AI 重新来做一遍？**你过去那一整套工作方式，可能早已效率低下，只是 AI 还没来得及替你动刀而已。\n顺带一提，现在市面上 99% 的 AI 课程本质上都是智商税——用 DeepSeek、ChatGPT 摘点资料、套个 Notion 模板，然后卖给你。看似在教你用 AI，本质上是在‘用 AI 来教你交钱’。\n真正该做的，是盯紧你所在的行业和岗位，认真思考两个问题：AI 是否正在取代我的工作？我是否能用 AI 做出比别人更好的结果？\n总结 # 以上就是我参加 PyCon 第三天的简单记录。\n整体来看，这次 PyCon LT 之行非常值得。虽然谈不上收获了太多具体的技术细节，但能听到来自同行的分享，无论是从技术视角还是人际交流上，都是有价值的。\n这次也和几位同事一同参会，有更多时间深入交流，感觉非常难得。\n如果以后还有机会参加 PyCon，我希望能更多关注一些核心项目，比如 CPython、PyPA，也希望能接触更多 DevOps 和 AI 相关的话题。\n这次的 PyCon 随记就分享到这里，期待下一次再见！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-29","externalUrl":null,"permalink":"/misc/pycon-lt-d3/","section":"Miscs","summary":"今天是 PyCon LT 2025 的最后一天，主题是 AI and ML Day。分享了几位女性讲者的精彩演讲，以及对数据科学和 AI 的新认识。","title":"全程记录｜PyCon LT 2025 第三天：AI 能取代你吗？","type":"misc"},{"content":"今天是我参加 PyCon LT 2025 的第二天：Data Day，主题是 Dataframes、Databases、Orchestration。\n全部日程可以在这里查看：PyCon LT 2025\n这确实是我不太熟悉的领域，但还是希望来听听，看看能不能有所收获，毕竟门票里也包含了这一天的议程。没想到，今天真正打动我的，竟然是几位技术演讲者中的女性讲者——不仅讲得有料，还有热情、风格、甚至让我顺手关注了几个博客……\n依旧是流水账记录一下今天的感受 # 早上也是九点多到的会场，刚刚好赶上第一场演讲开始。打上一杯咖啡就进去听了。\n这场的主题是《Build Your Own (Simple) Static Code Analyzer》。这位演讲嘉宾来自纽约，是 numpydoc 的核心开发者，也是《Hands-On Data Analysis with Pandas》一书的作者。确实很有干货，她讲的是如何使用 AST（抽象语法树）构建一个自己的静态代码分析器，这也是我第一次了解到这些 Linter 工具的工作原理，非常有启发。\n另外我也觉得她不仅有技术实力，而且乐于公开演讲，打造个人影响力。看了她的博客 https://stefaniemolin.com/ 也很值得参考。\n接下来听了一场主题是《Data Warehouses Meet Data Lakes》，是一位意大利人带着浓重的意大利口音讲数据仓库面临的挑战，以及他们使用哪些技术来构建数据架构。这个领域我确实不太熟悉，但也了解到了，和 DevOps 一样，最终也需要构建 pipeline 来完成数据的收集和分析。\n然后又去听了《Cutting the price of Scraping Cloud Costs》，讲述了他们用哪些技术构建 pipeline 来计算云定价。\n上午最后一场我听了《cluster-experiments: A Python library for end-to-end A/B testing workflows》，主要介绍了他开发的这个 Python 库用来做 A/B 测试。我怀疑他的这个演讲多少有点是在宣传自己的开源项目。\n等我听完演讲已经是 12:30 以后了，看到很多人都在一楼排队点餐，我索性就去三楼找了个地方先处理工作。\n工作差不多处理完了，同事发消息说大家一起拍张照吧，于是我才下楼去点餐。\n点餐依然等了很久，吃完饭差不多快两点了，下午的演讲也要开始了。\n下午第一场我听的是《Accelerating privacy-enhancing data processing》，关于在真实世界中进行癌症研究的数据处理挑战。演讲者也介绍了他们用到的技术栈，而且特别有心地在开头准备了一个乐高积木，因为他在 PPT 里也使用乐高元素来表达观点。最后在提问环节，说谁答对了就送给谁，结果这个乐高被我同事答对并领走了。\n第二场我听了《Working for a Faster World: Accelerating Data Science with Less Resources》，这个分享里主要了解到一个工具叫 Panel，可以用来做数据探索和构建 Web 应用。其他的收获就不多了。\n接着参加了《Organize your data stack using Dagster》，介绍了一个开源的数据编排工具 Dagster。讲得非常好，我觉得作为 Dagster 的开发者应该感谢她。估计很多之前没听说过这个工具的人，现在也想上手试试看它能带来哪些好处，以及在实际工作中该怎么使用。值得一提的是，这位演讲者还是一位热爱技术的女性，在这次 PyCon 上她一共有两个演讲，令人佩服！\n最后一场我听了《Top 5 Lessons from a Senior Data Scientist》，这场就更有启发了，演讲者也是一位女性，目前是自由职业的数据科学家。我发现数据领域的女性确实不少呢？她的演讲不涉及技术，纯讲经验，也就是作为一名资深数据科学家的五大经验教训，其中的观点对我们这些在职场中的人都很适用。她的个人网站也维护得非常好，值得学习。\n启发 # 我发现今天我听的几位演讲者里，女性讲者在个人网站方面普遍比男性做得更好。\n但怎么说呢，所有的演讲者都很棒，至少他们都有热情、有想法，愿意站在台上把自己的所知所想分享给台下的听众，这一点就很值得我们学习了。\n尤其是那些技术过硬、表达自信、又乐于分享的女性讲者，确实有点让我圈粉了，也有启发。\n这就是我参加 PyCon 的第二天“流水账”。\n希望明天的 AI and ML Day 能带来更多启发。那就明天见了！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-24","externalUrl":null,"permalink":"/misc/pycon-lt-d2/","section":"Miscs","summary":"今天是 PyCon LT 2025 的 Data Day，主题是 Dataframes、Databases、Orchestration。虽然不太熟悉这些领域，但几位女性讲者的演讲让我印象深刻，收获颇丰。","title":"全程记录｜PyCon LT 2025 第二天：被几位女性开发者圈粉了","type":"misc"},{"content":"第一次参加了 Python 的大会，没想到是在国外。\n虽然这次的规模和演讲嘉宾的知名度不及即将六月份在捷克举办的 PyCon 欧洲大会，但这也是一次不错的体验。\n这次 PyCon LT 2025 一共是三天：\n第一天（4月22日）是 Python Day（Web、Cloud、IoT、Security） 第二天（4月23日）是 Data Day（Dataframes、Databases、Orchestration） 第三天（4月24日）是 AI and ML Day（LLM、Neural Networks） 全部的日程可以在这里查看：PyCon LT 2025\n先说说第一次参加的感受吧： # 我是早上九点刚过到的会场，扫了二维码后，工作人员给我一张写有我名字的参会证，然后我就去参加开幕式了。\n接着就是当天的第一场演讲，主题是《Ethics, Privacy and a Few Other Words》。说实话，我对这个话题并不太了解，主要讲的是道德、隐私等相关问题，听得不是特别明白。旁边的同事倒是听得津津有味，还时不时给我补充一些内容，这让我切实感受到自己在英语方面的不足——技术相关的内容还可以听懂，但一旦话题偏离技术，就有点跟不上了。\n这位讲者确实很敢说，有些观点我甚至担心可能会影响他以后进入一些大公司，甚至在前往某些国家时可能会受到限制。\n之后就是茶歇时间，大家出来喝水、喝咖啡，吃点甜点。\n接下来，会场会同时有多个主题在不同的房间进行，可以根据自己的兴趣自由选择。\n我接着听了一个主题是《Code Review the Right Way》。这个演讲没有讲太多技术细节或工具，而是分享了一些关于代码审查的最佳实践，以及如何建立代码审查文化。我觉得说得挺好的，特别是在审查别人代码时，要做到谦逊、委婉、有礼貌，避免冲突。\n然后我又去了另一场，主题是《What We Can Learn from Exemplary Python Documentation》。这个主题刚好也是我最近在做的事情之一，主要是将 Python 项目中的 Markdown 文档转换为 reStructuredText，并使用 Sphinx 生成 HTML 文档。\n听了一会儿之后，我觉得信息密度不是很高，就闪人去了另一场：《Using Trusted Publishing to Ansible Release》。\n主要是因为我对 Trusted Publishing 和 Ansible 都比较了解，去看看有没有什么新鲜的内容。演讲者是 PFS 的研究员、Ansible 的发布经理，但这个主题确实没有什么新意，可能因为我已经在使用这些内容了，她演示了一个 Demo ，通过 GitHub 使用 Trusted Publishing 发布一个 Python package 到 PyPI，确实没什么新鲜。\n听完这场已经 12:30 了，刚好是吃午饭的时间。这个会议是包含午餐的，主办方大概包下了会场内的一家餐厅，大家可以随便在餐厅的点餐机器上点单，然后直接拿着打出的单子等着取餐，就像麦当劳、肯德基那样。当然你想多吃几份也没人拦着你，自助点餐机就在那里。\n吃完饭后，两点钟才有第一场下午的分享。无所事事之下，就和同事一起去参加会场里的集章活动。\n主要是主办方联合赞助商安排了一些小游戏，比如玩贪吃蛇，还有其他小活动，通过参与可以收集邮戳。另外，还需要去找演讲者合影来获得特定邮戳。\n最终我集齐了 4 个邮戳，可以去工作人员那里参与转盘抽奖。我一共抽了两次，第一次抽中“再转一次”，第二次抽到了一件 PyCon T-shirt，把一起的同事羡慕坏了。\n下午参加的第一场是《Python Containers: Best Practices》。讲得很好，但大部分内容我都了解，因为我之前写过一篇关于 Docker 容器最佳实践的文章，内容差不多，感觉可以跳过。\n后面我又听了一场叫《Do Repeat Yourself》。这场稍微有点被标题“骗”了，但演讲者确实花了很大心思来准备。他没有用 PPT，而是用 FastAPI + CSS + HTML 做了一个网站来演讲，还配了音乐，整体很用心，也挺酷的。虽然技术内容不算新，但让我觉得 PPT 还能这么玩，启发是可以有空尝试用 FastAPI 做个 Web 网站练练手。\n接着是《Coding Aesthetics: PEP 8, Existing Conventions, and Beyond》，主要讲的是 Linter、Pythonic 等相关内容，这些我都挺熟的，就不展开说了。\n然后又到了下午的咖啡时间，和同事们聊了聊他们都听了哪些主题。期间还被主办方的工作人员随机采访了，说不定会出现在后续发布的视频中。\n最后一个主题是《Skip the Design Patterns: Architecting with Nouns and Verbs》，算是当天的“重磅”嘉宾，看着得有五六十岁了，现场也有很多人认识他。\n这位讲者主要讲的是：Python 程序员很少花时间去考虑设计模式，以及为什么设计模式在 Python 中似乎不再适用。他还带着大家一起重构一个 Python 项目，引出三种思维方式，并介绍了如何在架构上进行理解和应用。\n这就是我参加 PyCon 的第一天“流水账”。\n希望明天的 Data Day 能学习到更多新内容。那就明天再见啦！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-23","externalUrl":null,"permalink":"/misc/pycon-lt-d1/","section":"Miscs","summary":"参加了 PyCon LT 2025 的第一天，分享了演讲内容、个人感受以及对 Python 社区的思考。","title":"全程记录｜PyCon LT 2025 第一天：我在异国 Python 大会上的见闻","type":"misc"},{"content":"","date":"2025-04-16","externalUrl":null,"permalink":"/tags/life/","section":"标签","summary":"","title":"Life","type":"tags"},{"content":"上个月回国休假，结束之后从北京转机飞欧洲。因为我们三口之家行李比较多，就订了首都机场附近的酒店，方便第二天出发。趁着在北京停留的这个晚上，就约了几位大学同学小聚一下。\n那天是星期二，我大概下午四五点钟到酒店。感谢北京的同学们的热情招待，也挺不好意思的，让他们下班后还大老远跑来机场这边见我一面。\n有一个小感慨就是：北京人下班真的挺晚的。\n最早到的一位同学是晚上八点，最晚的则到了十点钟。因为第二天还要上班，加上有娃在场，需要早点休息，我们就聚到了十一点就散了。\n其实我离开北京已经十几年了，差不多都快忘了自己当年也是晚上九十点才下班，甚至偶尔加班到半夜才回家。\n这十年在大连工作，主要在外企，工作节奏相对温和。一般如果晚上有安排，我会早点去公司、早点下班，或者请个小时假，就算偶尔早走了，没人会去计算你在办公室到底坐了满不满八小时。\n现在再看到北京同学的生活节奏，我真的很难想象，要怎么平衡这么晚的下班时间和家庭生活，尤其是当你有了孩子。\n当然，每个人都有自己的应对方式。\n比如有的同学请父母过来帮忙带孩子；还有的因为孩子上学的问题，让孩子暂时在老家生活，只有放假的时候再团聚。\n另外他们还提到了北京幼儿园入园的问题。同学说，北京各类幼儿园在招生时会有一定的优先顺序，比如是否为本地户籍、是否在本区有房产等，综合考量户籍、房产、甚至是居住证等情况，顺位安排比较复杂。这也再次让我感受到：在北京，生活确实不容易，尤其是当了父母之后，再也承受不起北京的生活节奏。\n“逃离”未尝不是一种选择。避开这些现实的压力，让生活变得简单一点、纯粹一点。\n和队友聊天时，我们也不觉得外国的月亮就一定比中国圆。\n怎么说呢？可能这就是人口大国的特征吧。资源有限、人口多，哪怕你再努力，也时常感受到那种“被推着往前走”的无力感。\n想想看，如果欧洲哪个国家也有我们这么庞大的人口，竞争恐怕也会一样激烈吧。\n作为个体，我们所能做的，大概就是始终保持准备，继续前行，在适合的时机遇见并把握属于自己的机会。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-16","externalUrl":null,"permalink":"/misc/one-night-in-beijing/","section":"Miscs","summary":"在北京停留的一晚，和同学小聚，感受到北京人下班的晚节奏，以及生活在大城市的压力和挑战。","title":"回国休假的一点感慨：北京的夜，好晚","type":"misc"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/tags/clang/","section":"标签","summary":"","title":"Clang","type":"tags"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/tags/clang-format/","section":"标签","summary":"","title":"Clang-Format","type":"tags"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/tags/clang-tidy/","section":"标签","summary":"","title":"Clang-Tidy","type":"tags"},{"content":"","date":"2025-04-15","externalUrl":null,"permalink":"/tags/cpp-linter/","section":"标签","summary":"","title":"Cpp-Linter","type":"tags"},{"content":"上周，我创建并维护的开源项目 cpp-linter-action 迎来了一个小小的里程碑：\n🌟 GitHub Star 数突破 100！\n虽然这个数字不算大，但对我来说是一个小小的里程碑——这是我第一次有项目在 GitHub 上获得超过 100 个 Star，也可以算是对这个项目的认可，给了我持续维护的动力。\n我在这个项目的第一次提交是在 2021 年 4 月 26 日，一晃将近 4 年过去了。回头看看这段时间，挺庆幸自己一直没有闲着，也留下了一些对别人有用的东西。\n随着项目不断发展，用户也越来越多。根据粗略估算，目前已经有上千个项目在使用这个 Action。\n其中不乏一些知名组织和开源项目，比如：\nMicrosoft Apache NASA CachyOS nextcloud Jupyter 最重要的是，这个过程让我收获了很多新的技能和知识，也让我保持了一个「业余习惯」：\n手机上除了刷微信、抖音，还有 GitHub。\n这个项目也成为了我后续工作的一个起点。我后来创建了 cpp-linter 组织，和其他开发者一起维护并发布 clang-tools 的二进制版本和 Docker 镜像。同时也开发了 cpp-linter-hooks，为用户提供 clang-format 和 clang-tidy 的 pre-commit hook，使用起来更加方便。\n不谦虚的说：\n如果你的项目是用 C/C++ 开发，如果想使用 clang-format 和 clang-tidy，那么 cpp-linter 会是一个绕不开的选项。\n最后，欢迎大家提出意见或建议，也欢迎通过 Issue 或 Discussions 与我交流！\n如果你觉得这个项目对你有帮助，也欢迎在公众号「DevOps攻城狮」留言或者去 GitHub 点个 Star，支持下这个项目～\n—— 写于 2025-04-15 12:49 AM\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-15","externalUrl":null,"permalink":"/posts/cpp-linter-action-milestone/","section":"Posts","summary":"cpp-linter-action 是一个 GitHub Action，提供 C/C++ 代码的格式化和静态分析功能。它使用 clang-format 和 clang-tidy，支持多种配置和自定义规则。项目自 2021 年创建以来，已被多个知名组织和开源项目使用。","title":"微软、NASA 都在用？我用业余时间维护了 4 年的项目破百了","type":"posts"},{"content":"","date":"2025-04-13","externalUrl":null,"permalink":"/tags/github/","section":"标签","summary":"","title":"GitHub","type":"tags"},{"content":"昨天想必有些人可能已经遇到 GitHub 无法访问的情况了。\n网上有人调侃，说是因为美国加征关税，GitHub 的响应码从 200 升级到了 403。\n玩笑归玩笑，GitHub 官方后来给出了说明——是配置错误，目前问题已经修复。\n刚好前段时间回国了一趟，再次体验到了快递、外卖、出行、支付等方面的便捷和实惠。唯一不太方便的，就是网络。\n更准确地说，是那些很多程序员日常离不开的网站和服务，比如 GitHub、Docker Hub、Docker Desktop，还有 ChatGPT。当然，现在我们也有 DeepSeek 这样的替代品了。\n就我个人体验来说，GitHub 在国内是可以访问的，但稳定性不太行。经常是刚打开还能用，过一会就彻底加载不出来了。\n工作之外，本来每天晚上能用来自由安排的时间本就不多，但却经常不得不花时间来解决网络问题。\n虽然也知道可以改 hosts、或者折腾下科学上网，但我的电脑配置起来始终不是很顺利。\n这一通折腾下来，感觉真是累了。有时候甚至不想再打开电脑，只想躺着刷会儿手机。\n访问 GitHub 的那点坚持，快磨没了。\n算了，就当是放个假吧。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-13","externalUrl":null,"permalink":"/posts/visit-github/","section":"Posts","summary":"GitHub 的访问问题让很多程序员感到困扰，尤其是在国内。本文分享了个人的体验和对网络问题的思考。","title":"访问 GitHub 的那点坚持，快磨没了","type":"posts"},{"content":"","date":"2025-04-12","externalUrl":null,"permalink":"/tags/readthedocs/","section":"标签","summary":"","title":"ReadTheDocs","type":"tags"},{"content":"","date":"2025-04-12","externalUrl":null,"permalink":"/tags/rst/","section":"标签","summary":"","title":"RST","type":"tags"},{"content":"","date":"2025-04-12","externalUrl":null,"permalink":"/tags/sphinx/","section":"标签","summary":"","title":"Sphinx","type":"tags"},{"content":"在日常的开源项目或团队协作中，我们经常会需要一个易于维护、可自动部署的文档系统。\n最近在维护自己的开源项目时，我尝试用 Sphinx 来生成文档，并通过 ReadTheDocs 实现了自动构建和托管，整体体验还不错。\n记录一下配置过程，希望能帮到有类似需求的朋友。\n为什么选择 Sphinx 和 ReadTheDocs # Sphinx 是一个用 Python 编写的文档生成工具，最初为 Python 官方文档设计，支持 reStructuredText 和 Markdown（通过插件）。 ReadTheDocs 是一个文档托管平台，可以自动从你的 Git 仓库拉取代码、构建并发布文档，支持 webhook 自动触发。 这两个工具的组合非常适合持续维护和更新文档，而且社区成熟、资料丰富。\n基础配置步骤 # 以下是一个实际项目中配置的完整流程。\n1. 安装 Sphinx 和相关依赖 # 推荐使用虚拟环境，然后安装：\n# docs/requirements.txt sphinx==5.3.0 sphinx_rtd_theme==1.1.1 # 如果你需要支持 Markdown，可以添加: myst_parser==0.18.1 安装依赖：\npip install -r docs/requirements.txt 说明：\nsphinx-rtd-theme 是 ReadTheDocs 使用的默认主题 myst-parser 用于支持 Markdown 2. 初始化文档项目结构 # 在项目根目录执行：\nsphinx-quickstart docs 建议将 source 和 build 目录分开\n执行完后，docs 目录下会生成 conf.py 和 index.rst 等文件。\n3. 修改 conf.py 配置文件 # 几个关键设置如下：\n# Read the Docs configuration file # See https://docs.readthedocs.io/en/stable/config-file/v2.html for details from datetime import datetime project = \u0026#34;GitStats\u0026#34; author = \u0026#34;Xianpeng Shen\u0026#34; copyright = f\u0026#34;{datetime.now().year}, {author}\u0026#34; html_theme = \u0026#34;sphinx_rtd_theme\u0026#34; 如果你需要支持 Markdown，需要在 conf.py 中添加:\nextensions = [ \u0026#39;myst_parser\u0026#39;, # 支持 Markdown ] 配置 ReadTheDocs 自动构建 # 只要项目结构清晰，ReadTheDocs 基本可以一键跑通。\n1. 导入项目到 ReadTheDocs # 登录 https://readthedocs.org/ 点击 \u0026ldquo;Import a Project\u0026rdquo;，选择你的 GitHub 或 GitLab 仓库 确保仓库中包含 docs/conf.py，系统会自动识别 2. 添加 .readthedocs.yml 配置文件 # 为了更好地控制构建过程，建议在项目根目录添加 .readthedocs.yml：\n# Read the Docs configuration file # See https://docs.readthedocs.io/en/stable/config-file/v2.html for details version: 2 build: os: ubuntu-24.04 tools: python: \u0026#34;3.12\u0026#34; sphinx: configuration: docs/source/conf.py python: install: - requirements: docs/requirements.txt 配置完成后，每次提交 Pull Request 时，ReadTheDocs 都会自动拉取并构建最新文档供你预览，确保文档如你所期望的那样。\n最终效果 # 构建完成后，ReadTheDocs 会提供一个类似 https://your-project.readthedocs.io/ 的文档地址，方便团队协作和用户查阅。\n我目前的开源项目也采用了这套方案，例如：GitStats 文档\n小结 # 只要做好上述的配置，几乎可以实现“写完文档，提交即上线”，大大提高了文档维护的效率。\n如果你正在写开源项目文档，或者想给团队项目（尤其是 Python 项目）补上文档系统，不妨试一试 Sphinx + ReadTheDocs。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-12","externalUrl":null,"permalink":"/posts/sphinx-readthedoc/","section":"Posts","summary":"在开源项目或团队协作中，Sphinx + ReadTheDocs 是一个易于维护、可自动部署的文档系统。本文记录了配置过程和注意事项。","title":"从零配置 Sphinx + ReadTheDocs：快速部署自动化文档","type":"posts"},{"content":"","date":"2025-04-11","externalUrl":null,"permalink":"/tags/markdown/","section":"标签","summary":"","title":"Markdown","type":"tags"},{"content":"在日常工作中，无论是写 README、写博客，还是写项目文档，我们总要选择一种标记语言来排版内容。\n目前主流的有两种：Markdown 和 reStructuredText（简称 RST）。\n那它们之间到底有什么区别？又该在什么场景下选哪个呢？\n最近我将 gitstats 项目的文档从 Markdown 转换为 RST，并发布到 ReadTheDocs 上，这篇文章就来聊聊我的一些实践体会。\nMarkdown 是什么？ # Markdown 是一种轻量级标记语言，最早由 John Gruber 和 Aaron Swartz 在 2004 年设计，目标是让文档尽可能「可读、可写、可转换为结构良好的 HTML」。\n优点：\n语法简单、容易上手 社区支持广泛（GitHub、GitLab、Hexo、Jekyll 等等都支持） 渲染速度快，格式一致性好 常见用途：\nREADME.md 博客文章 简单的项目文档 RST 是什么？ # RST 是 Python 社区使用较多的一种标记语言，由 Docutils 项目维护。相比 Markdown，语法更丰富，也更严格一些。\n优点：\n原生支持脚注、交叉引用、自动索引、代码文档化等高级功能 是 Sphinx 的首选格式，适合大型项目的文档编写 对结构化文档更友好 常见用途：\nPython 项目的文档（如官方文档） 使用 Sphinx 生成的技术手册 多语言文档（结合 gettext） 语法对比小结 # 功能 Markdown RST 标题 # 开头 ===== 或 ----- 下划线 粗体 / 斜体 **text** / *text* **text** / *text* 超链接 [text](url) `text \u0026lt;url\u0026gt;`_ 表格 简单表格（扩展支持） 需要严格缩进，写法复杂 脚注 / 引用 不支持 / 限制较多 原生支持 交叉引用 不支持 原生支持 Markdown 更轻松，RST 更专业。\n什么时候用 Markdown？ # 项目较小，只需写简单说明文档 团队成员不熟悉 RST，想快速写文档 写博客、日常笔记更推荐 Markdown 使用平台（如 GitHub Pages、Hexo）默认支持 Markdown 一句话：轻量文档首选 Markdown。\n什么时候用 RST？ # 使用 Sphinx 生成 API 文档或技术手册 项目结构复杂，需要自动索引、交叉引用、模块文档等高级功能 需要与 Python 工具链（如 autodoc, napoleon 等）集成 要发布到 ReadTheDocs（虽然现在也支持 Markdown，但 RST 体验更好） 一句话：Python 项目或结构化技术文档推荐 RST。\n我个人的建议 # 如果你是：\n开发工程师，日常文档写 Markdown 足够 Python 开发者，建议学一下 RST，特别是用 Sphinx 写文档时 开源项目维护者，项目规模不大，README 用 Markdown 就好；但文档站点可以考虑用 RST + Sphinx 构建 总结一句话 # Markdown 更像是写字的便利贴，RST 更像是写书的排版系统。\n选哪个，其实就看你的目标是写“随笔”还是写“手册”。\n如果你对 Markdown、RST 或文档构建工具有自己的心得，欢迎留言交流。\n下期我打算分享下 Sphinx 配置和 ReadTheDocs 自动发布的经验～\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-04-11","externalUrl":null,"permalink":"/posts/md-vs-rst/","section":"Posts","summary":"Markdown 和 reStructuredText（RST）是两种常用的标记语言。本文对比了它们的优缺点，并分享了在不同场景下的使用建议。","title":"Markdown 不香了吗？为什么越来越多 Python 项目用 RST？","type":"posts"},{"content":"","date":"2025-03-12","externalUrl":null,"permalink":"/tags/nutanix/","section":"标签","summary":"","title":"Nutanix","type":"tags"},{"content":"","date":"2025-03-12","externalUrl":null,"permalink":"/tags/vmware/","section":"标签","summary":"","title":"VMware","type":"tags"},{"content":" 背景 # 如果你是 VMware 的企业用户，或许你正在考虑脱离 VMware。目前，许多企业用户正在积极寻找替代方案，以降低成本和减少对 VMware 生态的依赖。\n很多企业在考虑脱离 VMware，主要原因是：\nBroadcom（博通）收购 VMware 带来的影响。2023 年 Broadcom 完成了对 VMware 的收购，并进行了一系列调整，包括：\n取消部分产品的永久许可证，强推订阅模式（Subscription）。 价格上涨，使得许多企业的成本大幅增加。 砍掉了一些非核心产品和合作计划，让部分企业和合作伙伴感到不安。 这些变化导致很多企业开始寻找替代方案，以降低成本并减少对 VMware 生态的依赖。\n替代方案 # 今天就分享一个对于企业用户的潜在替换方案 —— Nutanix。\n它是一个超融合基础设施（HCI）（替代 VMware vSAN / vSphere），适用于需要整合计算、存储、网络资源的企业，希望减少对 VMware vSAN 或 vSphere 的依赖。\nNutanix 是 VMware vSphere 的主要竞争对手，支持 KVM，提供企业级 HCI 解决方案。 提供简洁的管理界面，与 VMware vSAN 类似，但成本更低。 适用于希望从 VMware 迁移，但仍然想要企业支持的公司。 Nutanix 有哪些优势 # Nutanix 相比 VMware 在架构、管理、可扩展性、性价比等方面具有一些优势，具体如下：\n1. 统一的超融合架构（HCI） # 🔹VMware： # 传统上采用 vSphere + vSAN + NSX 组合，需要多个组件协同工作。 需要单独配置 vSAN 存储，与计算和网络资源分离，管理复杂度较高。 🔹Nutanix： # 采用 超融合架构（HCI, Hyper-Converged Infrastructure），计算、存储、网络融合在一起。 通过 Nutanix AHV（Acropolis Hypervisor） 取代 VMware ESXi，无需额外的 Hypervisor 许可费用。 存储即服务（Distributed Storage Fabric，DSF），存储性能随集群扩展线性增长。 ✅ 优势：\n简化管理，不需要额外的 SAN/NAS 存储设备，存储和计算资源统一管理。 更好的横向扩展（Scale-out），可以通过增加节点来提升计算和存储能力。 降低软件授权成本（VMware vSphere/vSAN 费用较高，而 Nutanix AHV 免费）。 2. 更灵活的超融合存储 # 🔹VMware： # 依赖 vSAN 提供存储，扩展时需要与 vSphere 结合进行配置。 vSAN 需要额外授权，且对硬件有较严格的兼容性要求。 🔹Nutanix： # 内置 Nutanix AOS（Acropolis Operating System），提供分布式存储（Nutanix Files, Volumes, Objects）。 存储弹性更高，支持 混合云存储（AWS/Azure/Nutanix 自有云）。 支持 本地 NVMe SSD、HDD 组合，根据数据冷热程度自动分层存储。 ✅ 优势：\n无需额外存储授权费用，相比 vSAN 成本更低。 自动数据优化，存储性能随集群扩展而提高。 支持外部云存储，适合混合云部署。 3. 内置免费 Hypervisor（AHV），降低授权成本 # 🔹VMware： # 需要付费使用 ESXi，并通过 vCenter 进行管理。 许多企业在使用 VMware 时需要额外购买 vSphere Enterprise Plus 或 vSAN 许可证，成本较高。 🔹Nutanix： # 提供 Nutanix AHV（Acropolis Hypervisor），基于 KVM，免费 使用。 直接通过 Prism Central 进行管理，无需 vCenter。 兼容 VMware VM，支持直接迁移 VMware 现有 VM 到 Nutanix AHV。 ✅ 优势：\n免费 Hypervisor，节省 VMware vSphere/ESXi 许可费用。 无需 vCenter，管理更简单。 兼容 VMware VM，迁移更容易。 4. 更高的自动化和可扩展性 # 🔹VMware： # 依赖 vSphere 和 vSAN 进行 VM 资源管理。 需要 vRealize Automation（vRA） 进行自动化操作，授权费用较高。 🔹Nutanix： # 提供 Prism Central 统一管理 VM、存储、网络，操作更简洁。 提供 Calm（自动化编排工具），支持一键部署应用（K8s, Jenkins, DevOps 相关工具）。 支持 API 直接调用，可通过 Nutanix Prism API 实现自动化。 ✅ 优势：\n自动化程度更高，适合 DevOps 场景。 管理界面更简洁，易用性更好。 API 支持更强，适合 CI/CD 自动化集成。 5. 更强的混合云支持 # 🔹VMware： # 需要额外购买 VMware Cloud on AWS、Azure VMware Solution 才能扩展到公有云。 VMware vSAN 和 NSX 在公有云环境的支持需要额外授权。 🔹Nutanix： # 内置 Nutanix Clusters，支持混合云部署（AWS、Azure、本地数据中心）。 自动数据同步，允许本地 Nutanix 与云端 Nutanix 互相迁移数据。 提供 Nutanix Xi 作为 SaaS 解决方案，支持跨云管理。 ✅ 优势：\n无需额外许可即可支持混合云，相比 VMware 成本更低。 数据迁移更简单，支持自动同步本地与云端数据。 公有云和私有云统一管理，减少运维复杂度。 6. 成本优势 # 🔹VMware： # 需要 vSphere + vSAN + NSX 组合，整体授权费用较高。 VMware 采用 CPU 核心计费，而且 2024 年 VMware 可能进一步提高授权成本（受 Broadcom 收购影响）。 🔹Nutanix： # 提供 免费 AHV Hypervisor，相比 VMware vSphere 省去授权费用。 按实际使用付费，不像 VMware 需要捆绑多个产品。 存储、计算、网络一体化，不需要额外购买 vSAN 或 NSX。 ✅ 优势：\n整体 TCO（总拥有成本）更低，相比 VMware 可节省 30%-50% 成本。 降低 Hypervisor 许可费用，免费 AHV 替代 vSphere。 更少的组件，提高管理效率。 总结：Nutanix vs VMware # 特性 VMware Nutanix Hypervisor ESXi（付费） AHV（免费） 管理工具 vCenter（额外收费） Prism Central（免费） 存储 vSAN（额外收费） Nutanix Files/Volumes（内置） 自动化 vRealize Automation（付费） Calm（内置） 混合云 需要 VMware Cloud 方案 Nutanix Clusters（内置支持） 成本 vSphere + vSAN + NSX 费用高 省去 ESXi/vSAN 费用 扩展性 需要手动扩展 vSphere/vSAN HCI 模式，存储计算统一扩展 什么时候选择 Nutanix？ # 你希望 减少 VMware 许可费用（省去 vSphere/vSAN/NSX 费用）。 你需要 更简单的管理，不想配置 vCenter、vSAN、NSX。 你计划 使用混合云，希望本地与云端无缝迁移。 你希望 自动化能力更强，适合 DevOps 场景。 什么时候选择 VMware？ # 你已经有 大量 VMware 生态（vSphere, NSX, vSAN），不想切换架构。 你的企业应用 依赖 VMware 生态，如 Horizon（桌面虚拟化）。 你不介意 额外的授权费用，并且已有成熟的 VMware 运营团队。 如果你主要关注降低成本、管理简化、混合云能力，对于企业用户 Nutanix 或许是更好的选择！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-03-12","externalUrl":null,"permalink":"/posts/nutanix/","section":"Posts","summary":"Broadcom 收购 VMware 后，许多企业用户开始寻找替代方案。Nutanix 作为一个超融合基础设施（HCI）解决方案，提供了更低的成本和更简洁的管理界面，是一个不错的选择。","title":"为什么越来越多的企业用户开始放弃 VMware？","type":"posts"},{"content":"","date":"2025-02-27","externalUrl":null,"permalink":"/tags/cpython/","section":"标签","summary":"","title":"CPython","type":"tags"},{"content":"昨晚，哄完女儿睡觉已经是午夜十二点了。我回到自己的屋里，打开 GitHub，看看当晚有没有什么可以贡献的项目。\n这次，我决定去 CPython 的 Issue 区找找有没有适合自己的贡献机会。\nCPython 就是大名鼎鼎的 Python 编程语言的官方代码仓库。\n其实，早就想找机会为 CPython 贡献代码，但一直没能迈出第一步。这次，我想用自己的方式寻找突破口。\n这种想法的启发，来自 Tian Gao（GitHub ID：gaogaotiantian），他是 Python 的 Core Developer（核心开发者），专注于维护 pdb，并曾跻身 Python 贡献排行榜 #94 名。他可能是唯一一个前 100 名的中国开发者。\n于是，我筛选了一些自己感兴趣的类别，当然就是 Infra 和 DevOps 相关的问题。很快，我找到了一张合适的 Issue，修改代码、测试、提交 Pull Request，然后就去睡觉了。\n今天早上醒来，我发现我的 PR 已经被 Merge 到 CPython 主分支了！\n虽然这算不上什么了不起的成就，但却是一个很好的学习过程。比如，通过参与优秀的开源项目，了解他们是如何管理 Issue 和 Pull Request 的，学习他们做得好的地方。这些经验都有可能应用到自己的工作或项目中。\n在贡献优秀开源项目的过程中，不仅能提升相关技能，还能与这些优秀的开发者交流，学到新的知识。\n从短期来看，或许不会带来直接的收益，但如果这是你真正热爱的事情，那么长期投入一定是值得的。\n假如拥有 Python Core Developer 这样的身份认可，在国内可能有助于获得更理想的工作机会。然而，并非所有公司都青睐这种“双时区开发者”（白天工作，晚上开源）。\n但如果你的目标是寻找远程工作，或者申请欧美国家的签证，这样的经历无疑会成为一个重要的加分项。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-02-27","externalUrl":null,"permalink":"/posts/my-first-pr-to-cpython/","section":"Posts","summary":"在 CPython 的 Issue 区找到一个合适的 PR，修改代码、测试、提交 Pull Request，第二天醒来发现已经被 Merge 到主分支了！这是一个很好的学习过程，也是对开源社区的贡献。","title":"一觉醒来，我的 PR 已经被 Merge 到 CPython 主分支了！","type":"posts"},{"content":"","date":"2025-02-14","externalUrl":null,"permalink":"/tags/europython/","section":"标签","summary":"","title":"EuroPython","type":"tags"},{"content":"","date":"2025-02-14","externalUrl":null,"permalink":"/tags/reviewer/","section":"标签","summary":"","title":"Reviewer","type":"tags"},{"content":"最近工作之余没怎么贡献代码，主要把时间都用在了评审 EuroPython 2025 （欧洲 Python 2025 大会）的提案上了。\n在国内的时候，我从未想过去参与某个活动，并当志愿者服务大家。但是来到欧洲，我突然想多参加点这样的活动。\n下面就说说我为什么选择参加志愿者工作，以及最近一周的支援活动有哪些收获和感悟。\n为什么选择参与活动 # 第一，其实就是想锻炼一下自己，逼自己去主动说，去跟别人交流，去听别人在讲什么。\n其次，这件事情是可以写进简历里或博客里。\n第三，我不是什么活动都想参加，对我来说想参加的活动类别就两大类，要么是 Python，要么是 DevOps。\n这就是我要不要去做一件事情思考的底层逻辑。\n其实产生这一想法就为最重要的，有了想法，去年年底我就去搜索欧洲 2025 年有什么活动，然后就关注到了 EuroPython 2025，在布拉格举办并报名参加组委会工作。\n不管怎么样，就是很想参与一下，去当志愿者，如果能去现场就更好了，只要门票不要钱就可以，飞机+酒店我自己购买，就当全家人一起过去旅游一下，顺便还能会一会老同事。一举三得。\n当评审能学到什么 # 第一点，我从参与中窥见了一些如何组织这样活动的流程，包括一些文档、职责、评审流程、要求、工具等等。总之，能感觉组织者很有经验，活动是在有条不紊地推进中。\n第二点，作为评审，我可以阅读到上百条的演讲提案，分享我的观点，帮助组委会评分，最终选择出来哪些提案最终可以呈现在 2025 年 7 月的大会上。这同时拓宽了我的视野。\n一个感触 # 开源是个人的强大背书。\n我们在公司内部比如分享一个 Python Linter 工具，像 uv，pre-commit 等，只要你足够了解，没有问题。\n当你要去一个大会去分享的时候，你或许可以讲出很多有用的内容来，但不一定能获得评委的认可让你去讲。如果你是 uv，pre-commit 不用说是作者，就是其中的维护者，你能去讲的可能性就是别人的几倍。\n举例来说，我在 review 提案的时候，我看到了 PyPI 一名成员的提案，当然他的提案很有内容，加上他是 PyPI 成员，一下就让这个分享的含金量拉满。\n另外还有一些是 Python Core Dev 和 PyPy Cor Dev。怎么说呢，这些人但凡想来分享点他们贡献的功能，那真是的太容易了，主办方求之不得。\n最后 # 虽然这会每天需要多花几个小时来做这件事，但收获还是挺多的，至少拓宽了视野，看到了更多的想法。\n所以接下来会去试试之前没上手的项目，比如 PyPy，Pydandic，FastAPI 等。\n我现在完成了 129 个 review，还剩一天时间，我希望我能完成近 150 ~ 200 个，虽然组委只建议每个人评审 30 个。\n总之，这是一个非常正向的学习过程，从中我能学到很多的想法，锻炼我的英文和沟通能力，并且对我的成长和简历都是有帮助的。\n这件事情起初是有一点难度的，但并不是不可跨越的，这就是成长过程中最好的力度了。\n你觉得呢？\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-02-14","externalUrl":null,"permalink":"/posts/euro-python-review/","section":"Posts","summary":"最近工作之余没怎么贡献代码，主要把时间都用在了评审 EuroPython 2025 （欧洲 Python 2025 大会）的提案上了。","title":"为什么我选择参与 EuroPython 2025 评审？","type":"posts"},{"content":"大家好！自从上次发文宣布开始维护 gitstats 以来，我一直在不断地改进这个项目，下面是这两个月的主要更新内容：\n✨ 新增功能与改进 # 支持生成 JSON 文件 除了原有的 HTML 报告，现在 gitstats 还可以生成 JSON 文件！ 用途：方便开发者进行二次开发或编程使用，满足更多定制化需求。 来源：根据用户反馈，迅速实现了这一功能。\n代码重构 对原本混杂在一起的代码进行了大量拆分和优化。 好处：代码结构更清晰，易于维护，同时为编写单元测试奠定了基础。\n替换 getopt 使用更现代的 argparse 替换了已弃用的 getopt。 优势：提升了代码的可读性和可维护性。\n多平台支持 除了 Linux，gitstats 现在已全面对 Windows 和 macOS 进行了测试。 测试：我在这三个平台上进行了充分测试，确保实时可用。\n📅 下一步计划 # 支持主题切换 除了默认主题，计划增加 黑暗模式（Dark Mode），满足不同用户的视觉偏好。\n单元测试与覆盖率 将增加单元测试，并将覆盖率提升至 100%（小目标），避免回归 Bug。\n💡 你的需求很重要！ # 如果你有其他需求或功能建议，欢迎随时访问以下仓库地址，提 Issue 告诉我：👉 https://github.com/shenxianpeng/gitstats\n🌟 欢迎使用与支持 # 如果你觉得 gitstats 对你有帮助，欢迎 Star🌟 支持！你的认可是我持续改进的动力！\n你希望 gitstats 增加哪些新功能？欢迎在评论区留言，或直接到 GitHub 提 Issue！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-02-05","externalUrl":null,"permalink":"/posts/gitstats-update/","section":"Posts","summary":"gitstats 经过两个月的持续改进，现已支持 JSON 输出、代码重构、argparse 替换 getopt，并全面兼容 Windows 和 macOS。欢迎使用和 Star 支持！","title":"🚀 gitstats 升级来袭：支持 JSON 输出、多平台兼容、代码重构！","type":"posts"},{"content":"","date":"2025-02-05","externalUrl":null,"permalink":"/tags/gitstats/","section":"标签","summary":"","title":"GitStats","type":"tags"},{"content":"","date":"2025-01-25","externalUrl":null,"permalink":"/tags/cloud/","section":"标签","summary":"","title":"Cloud","type":"tags"},{"content":"","date":"2025-01-25","externalUrl":null,"permalink":"/tags/docker/","section":"标签","summary":"","title":"Docker","type":"tags"},{"content":"最近我在进行 Jenkins 实例迁移，这次我选择使用 Jenkins Docker Cloud，而不是在 Jenkinsfile 中直接使用 docker { ... }。\nJenkins Docker Cloud 插件 # 首先，你需要安装 Jenkins Docker Cloud 插件：\nhttps://plugins.jenkins.io/docker-plugin/\nJenkins Docker Cloud 是一个允许 Jenkins 使用 Docker 容器作为构建节点的插件。\n所以，你需要配置一个启用 Remote API 的 Docker 主机，如下所示。\n启用 Docker Remote API # Jenkins Controller 通过 REST API 连接到 Docker 主机。\n启用 Docker 主机的远程 API，请按照以下步骤操作：\nStep 1：启动一个虚拟机并安装 Docker。你可以根据所使用的 Linux 发行版，参考官方文档进行安装。确保 Docker 服务已启动并正常运行。\nStep 2：登录服务器，打开 Docker 服务文件 /lib/systemd/system/docker.service，找到 ExecStart 这一行，并替换为：\nExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock Step 3：重新加载并重启 Docker 服务：\nsystemctl daemon-reload service docker restart Step 4：执行以下命令验证 API 是否可用（将 myhostname 替换为主机名或 IP 地址）：\ncurl http://localhost:4243/version curl http://myhostname:4243/version 创建自定义 Docker 镜像 # 我在使用自定义 Docker 镜像时，选择了 launch via JNLP。\n如果你的镜像基于 jenkins/inbound-agent，可以参考以下 Dockerfile：\nFROM jenkins/inbound-agent RUN apt-get update \u0026amp;\u0026amp; apt-get install XXX COPY your-favorite-tool-here ENTRYPOINT [\u0026#34;/usr/local/bin/jenkins-agent\u0026#34;] 在 Jenkinsfile 中使用 # 当 Docker Cloud 配置完成后，你就可以在 Jenkinsfile 中像普通 agent 一样使用它，例如：\n// Jenkinsfile (Declarative Pipeline) pipeline { agent { node { \u0026#34;docker\u0026#34; } } } 这种方式和直接使用 docker { ... } 是不同的。 例如直接在 Jenkinsfile 中使用 docker { ... }：\n// Jenkinsfile (Declarative Pipeline) pipeline { agent { docker { image \u0026#39;node:22.13.1-alpine3.21\u0026#39; } } stages { stage(\u0026#39;Test\u0026#39;) { steps { sh \u0026#39;node --eval \u0026#34;console.log(process.platform,process.env.CI)\u0026#34;\u0026#39; } } } } 更多关于在 Pipeline 中使用 Docker 的内容，请参考： https://www.jenkins.io/doc/book/pipeline/docker/\n如有疑问，欢迎留言交流。\n转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2025-01-25","externalUrl":null,"permalink":"/posts/jenkins-docker-cloud/","section":"Posts","summary":"本文介绍如何使用 Jenkins Docker Cloud 来构建和部署应用，包括配置 Docker 主机以及创建自定义 Docker 镜像。","title":"如何使用 Jenkins Docker Cloud","type":"posts"},{"content":"","date":"2025-01-20","externalUrl":null,"permalink":"/tags/copyright/","section":"标签","summary":"","title":"Copyright","type":"tags"},{"content":"最近逛 CPython 的仓库发现了这个 Issue gh-126133:\nHugo van Kemenade 他作为 Python 3.14 \u0026amp; 3.15 发布经理提出是否可以停止更新 Copyright。\n在工作中，我其中的职责之一也是发布，因此我这个想法的提出也非常感兴趣，跟 CPython 项目一样，我们的项目在每年的年初都要更新 Copyright。\n下面我们就一起来看看 Hugo 提出的理由以及最终这个想法被 Python 项目法律团队采纳并最终合并到 CPython 的主分支的过程。\nHugo 提出问题的内容是这样的 # 每年一月，我们会在代码库中更新 PSF 的版权年份：\nCopyright © 2001-2024 Python Software Foundation. All rights reserved. 2025年即将到来。\n我们能否获得 PSF 的许可，采用一种无需每年更新的版权声明？\n我建议使用如下之一：\nCopyright © Python Software Foundation. All rights reserved. Copyright © 2001 Python Software Foundation. All rights reserved. Copyright © 2001-present Python Software Foundation. All rights reserved. 然后他列出了许多以前很多关于更新 Copyright 的 PR 的例子。\n其中很多还有重复的，浪费了开发者的时间。并表示不仅这些工作枯燥无趣，而且可能完全没有必要。\n许多大型项目已经停止更新版权年份，比如：\nCopyright (c) 2009 The Go Authors. All rights reserved. Copyright (c) 2015 - present Microsoft Corporation Copyright 2013 Netflix, Inc. Copyright (c) Meta Platforms, Inc. and affiliates. Copyright (c) Microsoft Corporation. All rights reserved. Copyright (c) .NET Foundation and Contributors 这些要么只有首年，要么完全省略年份，要么以“present”结尾。\n我们可以效仿类似的做法吗？\n这里面有其他人发表的观点。\nGregory P. Smith（Python 指导委员会成员）给出了很好的建议：\n他说：我不会从任何地方删除整个版权声明。Hugo 应该向 PSF（作为版权持有者）寻求建议。我们的目标是简化我们的生活，制定一项可参考的政策，减少劳累。我们应该将这些建议编入开发指南，并简单地回复任何试图修改版权声明的人，并提供该文档的链接。\n最终 Hugo 给 PFS 法律发了邮件，并得到如下回复 # 首先要理解的是，版权声明是可选的/信息性的。曾经有一段时间，版权所有者必须包含一份声明，以保护他们的版权；根据国际条约，这一要求已被废除。因此，版权声明的形式不会影响版权所有者的权利。\n然而，版权声明仍然可以发挥有用的信息作用。根据美国版权局的说法，有效的版权声明的要素包括：\n版权符号 ©；“copyright”一词；或缩写 “copr.”； 作品首次出版的年份；和 版权所有者的姓名。\n示例：© 2024 John Doe\n对于只出版过一次的作品，首次出版的年份很简单。另一方面，开源项目中的文件本质上是修订或衍生作品的累积，每个作品都有不同的首次出版年份。这就是为什么您会在相应的版权声明中看到多个日期或日期范围。例如，如果某个文件是在 2015 年创建的，并在 2017 年、2018 年、2019 年和 2022 年进行了修订，则您可以采用以下方式之一呈现版权声明：\n© 2015、2017-2019、2022 John Doe © 2015、2017、2018、2019、2022 John Doe 在这种情况下，使用“2015-2022”作为日期范围可能相当普遍，尽管这提供的有关各个修订的首次发布日期的信息较少，并且可能错误地暗示该文件在该范围内的每一年都进行了修订。\n无论相应文件是否最近更新过，在每个版权声明的日期范围末尾更新声明以添加当前年份都没有特别的意义。但它也不会影响任何人的权利，也不会使用“-present”来代替。\n所以我认为你提出的包含年份的两种格式都可以。我不建议完全省略首次出版的年份，因为它是有效版权声明的要素之一。\n我还要指出的是，“Python 软件基金会”可能不是 PSF 项目作者的准确陈述，因为 PSF 不需要（据我所知）版权转让。话虽如此，为文件的所有作者包含声明也是不切实际且容易出错的。一些常见的替代方案是 “FOO 项目贡献者”或“贡献者中列出的作者”。这些都不是理想的，因为它们要么实际上没有命名作者，要么随着作者的贡献随着时间的推移而消失，它们可能会变得不准确。\n简而言之，没有理想的解决方案，但幸运的是声明并不那么重要。选择一些合理的内容并尽力坚持下去。\n以上就是 PFS 法律的回复。\n期间还有人提议 Hugo 要争得 PFS 董事会的同意。\n但有人这样回复这个提议：PSF 法律列表由 PSF 员工（包括我自己）监控，是解决此类问题的正确途径。需要董事会全员参与的主题会被提交，不需要董事会全员参与的主题会在那里决定，根据主题的不同，是否需要法律顾问。在这个特定情况下，我们确实请来了律师，以确保 PSF 履行我们作为版权持有人的法律义务。\n此回答并获得了 Hugo 及其他人的点赞👍和❤️。\n最终 Hugo 的提议 “只包含首次出版的年份，删除结束年份” 修改被合并到了 CPython 的主分支了。详见：https://github.com/python/cpython/pull/126236\n总结 # 从这个问题我们可以学到，关于 Copyright 中的年份，我们可以仿照 Python，谷歌，微软和 Netflix，只包含首次出版的年份即可。\n我不是律师，所以不要把这当作我的法律建议。\n我只想说，如果这对 Python，谷歌、微软和 Netflix 的律师来说是可以的，那么对我来说也是可以的。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2025-01-20","externalUrl":null,"permalink":"/posts/copyright/","section":"Posts","summary":"CPython 停止更新 Copyright 的原因和过程。了解 Python 项目法律团队的建议，以及如何处理开源项目中的版权声明。","title":"CPython 停止更新 Copyright 了，为什么？","type":"posts"},{"content":"时间过得很快，2024 年即将过去。如果不记录下在这一年里发生的事情，过不了多久就很难回想起这一年都发生了什么。按照惯例，这篇年终总结如期而至。\n回看自己年初写下的 Flag，有些实现了，有些做了但不是很好，还有一些没有做到。\n职业发展：希望能顺利度过职业发展期，并迎来一个崭新的、充满挑战的开始。✅ 家庭健康：家人身体健康，工作和家庭的平衡，多带他们出去走走。✅ 提升技能：补足 DevOps 领域的一些不足，争取通过实际项目提升自己。⏳ 分享与成长：通过博客和公众号持续分享知识，以教促学。✅ 保持运动：不论是跑步还是踢球，努力减重。❌ 总结来说，2024 年的目标除了没有减重之外，总体完成得不错。\n回顾 2024 # 回顾这一年，最大的变化就是工作，生活因此也做出了改变。这也是我结束沪漂、北漂回连之后十年来最大的变动。\n珍惜每一天 # 从 2013 年 12 月份，我知道自己将在 2024 年 6 月底离开工作了快10年的公司，并有可能去欧洲工作。\n从那一刻起，我开始更加珍惜工作和生活中的每一天。有时间就尽可能带着家人多出去走走。\n接下来是一些流水账，记录我无比珍惜在国内的七个月：\n一月：我的脚踝在打羽毛球时受伤了，活动参加一场少一场，所以我还是拖着脚参伤加了公司在金石滩鲁能温泉的活动。 二月：孩子意外烫伤，对我们全家来说是至暗时刻。整个春节都是在医院里度过的，现在回想起来依然觉得痛心。 三月：月初去了姐姐家，月中去了大连森林动物园，月末带爸妈去了丹东，看了鸭绿江大桥和抗美援朝纪念馆。 四月：多次走亲访友，动物园，还参加了公司组织的小黑山登山、摘蓝莓和草莓活动。 五月：月初父母来大连一起给孩子过了一个生日；月底我们去了日本东京办理签证。 六月：月初和孩子的小伙伴家长们一起露营；月中回了趟老家，带爸妈去了一趟省会转转；又去了动物园；月底正式结束在大连公司的工作。 七月：月初和朋友们聚餐告别；独自带孩子回了一趟老家，与父母分别开上车的那一刻，我就哭了；随后去日本取签证，顺便去了东京迪士尼；月底启程去欧洲，临行时心情沉重。 这些点滴构成了2024年上半年的珍贵记忆。\n享受每一天的工作 # 上半年，我一边准备交接，一边完成本职工作，始终不遗余力地践行 DevOps 最佳实践。尽管结果尚不可知。\n但我真的很享受在岗的每一天，包括咖啡、午休散步、听喜欢的内容、偶尔跑步，以及每周二、四的羽毛球时光。\n业余时间，坚持开源和写作 # 和去年一样，业余时间我还做着相同的事情：\n开源项目 cpp-linter-action：已有上千用户，包括微软、Jupyter、Linux Foundation 等知名项目。 commit-check：优化并新增用户需求，用户数稳步增长。 gitstats：这是我今年新纳入维护的一个开源项目，希望能帮助更多人。 写作 — 全年更新 27 篇博客、41 篇公众号文章，大幅超出预期。 学习 — 加入知名社区学习和贡献的计划仍在继续，但工作之余带孩子确实让业余时间变得很少。 展望 2025 # 2024 年对于很多人来说都是特殊的一年。无论身处何地，都能感受到不稳定的政治因素和不乐观的经济形势。\n2025 年会更好吗？应该不会，但仍怀抱希望：希望未来能有工作，有饭吃，有地方住，家人健康平安。\n如果有什么愿望，那就是：\n祝愿家人身体健康。 去其他欧洲国家旅行，争取看一场喜欢的球队比赛。 争取获得 Azure 认证。 争取参与 DevOps 或 Python 大会。 争取加入 PyPA 或 Python GitHub Organization。 争取别长胖，保持运动。 没有信心完成的愿望，一律使用“争取”来描述。:）\n过去的年终总结 # 2023 年终总结 2022 年终总结 2020 年终总结 2019 年终总结 2018 从测试转开发\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-12-28","externalUrl":null,"permalink":"/misc/2024-summary/","section":"Miscs","summary":"\u003cp\u003e时间过得很快，2024 年即将过去。如果不记录下在这一年里发生的事情，过不了多久就很难回想起这一年都发生了什么。按照惯例，这篇年终总结如期而至。\u003c/p\u003e","title":"2024 年终总结","type":"misc"},{"content":"","date":"2024-12-28","externalUrl":null,"permalink":"/tags/summary/","section":"标签","summary":"","title":"Summary","type":"tags"},{"content":"女儿已经两岁七个月了，我还没有单独写过一篇关于她的文章。\n我的女儿是个好动的小姑娘，每天充满活力，以至于经常我从下班到家六点半一直陪她到半夜十二点多才能停下来睡觉。\n今天是周五晚上，十二点半了还是蹦蹦跳跳不想睡觉，终于在凌晨一点钟熄灯睡觉了。但她突然下床要吃苹果。\n放在以前晚上十一点为了哄她睡觉我会尽量满足她，但今天实在是太晚了，我就严厉地再次告诉她：熄灯之后只能喝水，其他的都不能吃和喝了。\n显然她对我的拒绝表示抗拒，开始又哭又闹！但这次我不想让她得逞，坐在床上任凭她怎么推我拉我去厨房，我都一动不动。\n她被我的举动气得不行了，哭得上气不接下气。\n说实在我心里是很心疼的，同时也是很犹豫的。\n心疼是哭得实在太厉害；犹豫是我这样做真的就是对吗？\n最终结果是，在我拿了卫生间的纸准备给她擦眼泪和鼻涕的时候，两岁多的女儿因“秩序敏感期”，更是哭不成泣了，而不再想着吃苹果的事情了。\n最后，她就趴在我的身上，不跟我说话，不让我离开厨房。我就这么抱着她大概有五分钟。\n然后我问她：你爱爸爸吗？她不说话。\n我又问她：你爱妈妈吗？她还不说话。\n不记得是哪句话，她同意跟我回卧室睡觉了。\n当她躺在我的身边，我搂着她，听着她的呼吸声。当我的手臂离开她的时候，她说：爸爸抱抱！\n那时我的心都要融化了。\n我再次问她：你爱爸爸吗？她说爱！你爱妈妈吗？她说爱！\n我说爸爸妈妈也爱你！\n随着她的呼吸声加重，我知道了她应该已经睡着了。\n此时，我回想刚刚发生的一切，觉得是我错了。是我从一开始就没有把睡觉前可以做的和不可以做的事情清楚地让她理解。\n以前她可以吃苹果，而今天爸爸突然不让吃苹果了，显然她是不能理解了。（虽然从我的角度是因为实在是太晚了）。\n我在心中默念，女儿，对不起！也请给爸爸一点耐心，爸爸也是第一次当爸爸，还需要学会如何做一个好爸爸。\n写下这篇文章的时候，女儿就熟睡在我的身边。 —— 写于 2024 年 12 月 28 日凌晨 2:41\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-12-28","externalUrl":null,"permalink":"/misc/my-daughter/","section":"Miscs","summary":"女儿已经两岁七个月了，我还没有单独写过一篇关于她的文章。记录下我和女儿的点滴生活。","title":"我的女儿","type":"misc"},{"content":"最近的晚上的时间（一般要等到孩子睡了）我正在做一件事：复活已经沉寂多年的 GitStats 项目。\n此前，我曾写过两篇关于 GitStats 的文章，如果感兴趣，可以查阅了解。\nGit 历史统计信息生成器 GitStats 通过 Jenkins 定期自动给老板提供 Git 仓库的多维度代码分析报告 什么是 GitStats # GitStats 是一个用 Python 写的工具，用来分析 Git 仓库的历史记录，并生成 HTML 报告。\n然而，它只支持 Python 2，且作者已经停止维护（最后一次提交还停留在 9 年前）。\n在当前开发环境中，兼容性和使用便捷性都受到很大限制，但不可否认，它的价值依旧存在。\n因此，我决定对这个项目进行现代化改造。\n我已经完成的工作 # 迁移到 Python 3.9+：重构代码以支持 Python 3 版本。✅ 创建现代化流水线：增加 CI/CD 等工具，便于持续开发和发布。✅ 发布到 PyPI：用户现在可以通过 pip install gitstats 来安装。✅ 提供 Docker 镜像：用户无需自行处理依赖，运行 gitstats 更加便捷。✅ 提供在线预览：创建一个展示页面，让用户直观了解 GitStats 的功能。✅ 特别感谢 # 在这里我要特别感谢 Sumin Byeon（@suminb）。从他的介绍来看，他应该是一位生活在韩国的程序员。\n原本 GitStats 在 PyPI 上的 Owner 是他，因此我无法直接使用这个名字。我尝试过其他名字，例如 git-stats 和 gitstory，但都因与其他项目的名字相近而被 PyPI 拒绝。\n看到他的项目已经有五年没有维护了，我抱着试一试的心态给他发了一封邮件，问他是否愿意将 GitStats 的名字转让给我，因为我正在重振这个项目。\n没想到，他很快回复了，并最终同意将 GitStats 的名字交给我。他唯一的条件是，如果将来我停止维护 GitStats，而其他人需要这个名字时，我也能像他一样将名字交给对方。\n我答应了他，也承诺将长期维护 GitStats。（希望我能够做到）\n未来计划 # 解决有价值的 Issue：梳理原仓库中未解决的问题，挑选具有价值的进行修复。 审查现有 Pull Requests：评估原仓库的 PR，视情况合并到当前项目中。 更新文档：完善文档，使其更清晰易懂。 添加新功能：增加功能，使项目更强大、更有用。 优化 UI：提升界面美观性，改善用户体验。 如何参与 # 如果你对 GitStats 的改进有兴趣，欢迎参与这个项目！你可以：\n建议功能：提出想法或功能请求，帮助项目更贴近用户需求。 贡献代码: 修复 Bug 或添加功能，为项目直接出力。 分享推广：将 GitStats 推荐给可能感兴趣的朋友或社区。 最后，让我们携手合作，让 GitStats 再次焕发活力！\n写于 2024 年 11 月 28 日凌晨 2:50\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-11-28","externalUrl":null,"permalink":"/posts/gitstats/","section":"Posts","summary":"介绍 GitStats 项目的复活过程，包括迁移到 Python 3、创建现代化流水线、发布到 PyPI 和 Docker，以及未来的改进计划。","title":"复活 GitStats：让 Git 历史分析焕发新活力","type":"posts"},{"content":" pip vs pipx 的区别 # 在 Python 的生态中，pip 和 pipx 都是用于管理包的软件工具，但它们有不同的设计目标和使用场景。有些同学可能会疑惑，两者到底有什么区别？该如何选择？\n1. pip: 通用的 Python 包管理工具 # pip 是 Python 官方推荐的包管理工具，用于安装和管理 Python 包（libraries）。\n主要特点：\n适用于任何 Python 包：可以安装库和命令行工具。\n安装在全局或虚拟环境：包默认安装到全局 Python 环境，或者虚拟环境（如 venv、virtualenv）中。\n命令简单：\npip install package-name 适用场景：\n安装开发所需的依赖（如 requests、flask）。 创建项目特定的环境（通常结合虚拟环境使用）。 局限性：\n如果直接安装到全局环境，容易导致版本冲突。 对于命令行工具（CLI）工具的安装和管理较繁琐，因为它们共享相同的环境。 2. pipx: 专注于隔离安装命令行工具 # pipx 是一个专门为 Python 命令行工具（CLI）设计的工具，提供隔离的安装环境。\n主要特点：\n为每个工具创建独立环境：每个 CLI 工具都在自己的虚拟环境中运行，避免冲突。 自动管理依赖：安装工具时，它会自动处理依赖的版本管理。 简化使用体验：CLI 工具直接可用，无需额外配置路径。 命令简单： pipx install package-name 适用场景：\n安装和管理 Python CLI 工具（如 black、httpie、commit-check）。 避免工具之间的依赖冲突。 对开发工具或脚本运行环境要求高的用户。 局限性：\n仅适用于 CLI 工具，不适合安装普通的 Python 库。 需要先安装 pipx 工具： python -m pip install pipx 对比总结 # 特性 pip pipx 用途 安装和管理所有 Python 包 安装和管理 CLI 工具 安装范围 全局环境或虚拟环境 每个工具独立的虚拟环境 依赖隔离 需要手动管理（结合虚拟环境更好） 自动隔离，工具互不影响 适用场景 开发项目的依赖管理 CLI 工具的独立安装和使用 示例 pip install flask pipx install black 如何选择？ # 如果你正在构建一个 Python 项目，需要安装项目依赖，使用 pip。 如果你需要安装 Python CLI 工具，如 pytest 或 pre-commit，建议用 pipx，以确保独立性和稳定性。 简单来说：pip 是通用工具，pipx 是针对 CLI 工具的专用解决方案。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-11-26","externalUrl":null,"permalink":"/posts/pip-vs-pipx/","section":"Posts","summary":"本文介绍了 pip 和 pipx 的区别，帮助开发者选择合适的工具来管理 Python 包和命令行工具。","title":"pip vs pipx 的区别","type":"posts"},{"content":"","date":"2024-11-18","externalUrl":null,"permalink":"/tags/ansible/","section":"标签","summary":"","title":"Ansible","type":"tags"},{"content":"最近在配置一台新的 Windows Server 2022 时，我遇到了一个问题：之前运行正常的 Ansible playbook 无法执行了。\n以下是我在 Ansible inventory 中为 Windows 主机配置的内容：\n[jenkins-agent-windows:vars] ansible_user= ansible_ssh_pass= ansible_connection=winrm ansible_winrm_transport=ntlm ansible_winrm_server_cert_validation=ignore 但是，当我运行 playbook 时，出现了如下错误：\nwinrm send_input failed; stdout: stderr \u0026#39;PowerShell\u0026#39; is not recognized as an internal or external command, operable program or batch file. 问题原因 # 这种情况通常是由于 SYSTEM 用户的 PATH 环境变量被修改，导致系统无法找到 PowerShell.exe 的路径。\n请检查 PATH 环境变量中是否包含以下路径：\nC:\\Windows\\System32\\WindowsPowerShell\\v1.0 解决方法 # 依次执行：\n右键 此电脑 → 属性 点击 高级系统设置 进入 环境变量 在 PATH 中添加： C:\\Windows\\System32\\WindowsPowerShell\\v1.0 添加完成后，重新运行 Ansible playbook，问题即可解决。\n","date":"2024-11-18","externalUrl":null,"permalink":"/posts/ansbile-playbook-issue/","section":"Posts","summary":"介绍在 Windows Server 2022 上执行 Ansible playbook 时出现 PowerShell 无法识别的问题原因及解决方法。","title":"PowerShell 不是内部或外部命令","type":"posts"},{"content":"很多人可能会好奇，作为一名 DevOps 工程师，每天究竟忙些什么呢？今天就来简单聊聊，作为 DevOps/Build/Release 工程师，我的日常工作节奏是怎样的。\n工作准备 # 每天早上九点半到公司，第一件事就是打开 Slack 和邮箱，优先处理那些紧急或容易回复的消息。遇到比较复杂的内容，就会设置提醒，以防漏掉。之后，会把当天的任务列入 To-Do List，再检查 Jenkins 上是否有失败的任务需要关注。这一系列动作大概会花半小时左右。\n咖啡时间 # 十点左右是咖啡时间——一天的正式开始。如果十点半有站会，那就是一个快速的回顾和计划环节，主要是分享昨天的进展、当天的任务安排，也和团队同步一下各自的状态。\n日常工作 # 开始工作后，我会打开 VSCode，接着前一天没完成的任务。平时常用的代码仓库包括 pipeline-library、ansible-playbook、docker-images 和 infra，它们分别负责管理流水线、自动化脚本、容器和基础设施。几乎每天都会对这些仓库进行一些更新或优化。\nBuild 和 Release 也是我的主要工作之一。构建任务已经实现了自动化，团队成员通过 Multibranch Pipeline 自行构建；我主要负责分支管理、发布时的合并和冲突解决，确保发布信息的准确和版本的合规性。\n此外，还有一些日常任务，比如：\n维护和升级构建环境 收集代码覆盖率，生成报告 升级编译器，处理相关问题 管理虚拟机分配，帮团队解决环境问题 上午的主要工作还是消息回复和问题处理，之后再逐一处理 To-Do List。\n午餐与休息 # 中午十二点半左右和同事一起午餐。吃饭时聊聊天，也是练习英语的机会。饭后，有时会和同事一起在附近散步，或者自己跑步运动一下。\n下午 # 下午是主要的产出时间。从一点半到四点半，专心处理 To-Do List 上的任务，尽量推进工作进度。四点半之后，美国同事上线，可能会有会议或讨论需求。\n晚间时光 # 晚上是家人时间。天气渐冷，不方便带孩子出门散步，我们偶尔会去超市采购。如果孩子自己看书或看动画片，我会利用时间给开源社区做点贡献，或是写文章。\n这就是我在 DevOps 岗位上的一天，一边忙工作，一边兼顾家庭和爱好。希望这个小分享能让大家更了解这个岗位的日常。\n​你更喜欢我分享一些技术，还是更偏爱这种工作、生活的日常？欢迎留言告诉我！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-10-27","externalUrl":null,"permalink":"/posts/devops-everyday/","section":"Posts","summary":"本文介绍了作为 DevOps 工程师的日常工作节奏，从早上到晚上，涵盖工作准备、会议、代码管理、构建发布等环节。","title":"从早到晚，我的 DevOps 一天","type":"posts"},{"content":" 背景 # 我最近在做的一件事情是迁移并升级 Jenkins。主要动机是因为这个漏洞 CVE-2024-23897\nJenkins 2.441 及更早版本、LTS 2.426.2 及更早版本未禁用其 CLI 命令解析器的一项功能，该功能会将参数中文件路径后的“@”字符替换为文件内容，从而允许未经身份验证的攻击者读取 Jenkins 控制器文件系统上的任意文件。\n因此需要将 Jenkins 至少升级到 2.442 或 LTS 2.426.3 及以上版本，也趁此机会重新重构之前没有做满意的部分。\n升级之前的 Jenkins # 升级之前我们使用的是 Jenkins 2.346.3，这是最后一个支持 Java 8 的版本。由于老的操作系统不支持 Java 17，导致无法升级 Jenkins。\n实际上，在升级之前我们已经做得还不错：\n遵循了 Infrastructure as Code 原则，我们通过 Docker Compose 部署 Jenkins。 遵循了 Configuration as Code 原则，使用 Jenkins Shared Library 来管理所有的 Jenkins Pipeline。 使用 Jenkins Multibranch Pipeline 来构建和测试项目。 但也存在一些不足之处，比如：\nJenkins 服务器没有一个固定的域名，比如 jenkinsci.organization.com，而是 http://234.345.999:8080 这样的格式。所有配置到这台 Jenkins 上的 Webhook 当 IP 或 hostname 变化时，需要从 Git 仓库同时修改。 没有使用 Docker Cloud，虽然很多任务已经使用 Docker 构建，但没有使用 Docker JNLP，也就是动态创建 Agent 来进行构建，完成后自动销毁。 Jenkins Shared Library 的名字和代码结构需要重构。当初创建时只是为一个团队而做，因此仓库名字比较受限。 没有使用 Windows Docker Container。 有的 Jenkins 插件可能已经不再使用，但仍然存在。 由于 Jenkins Agent 版本的限制，Jenkins 和插件没有及时更新。 升级后的 Jenkins # 在保留了之前好的实践的基础上，我们进行了以下优化：\n继续遵循 Infrastructure as Code 原则，同时通过 Nginx 作为代理，使用 Docker Compose 与 Jenkins 一起部署，确保拥有一个固定的域名。 继续遵循 Configuration as Code 原则。 继续使用 Jenkins Multibranch Pipeline 来构建和测试项目。 尽可能使用 Docker Cloud 来构建。 将 Jenkins Shared Library 重命名为 pipeline-library（与 Jenkins 官方命名一致），并对大量 Jenkinsfile 和 Groovy 文件进行了重构。 引入 Windows Docker Container 构建 Windows 部分。 使用 Jenkins Configuration as Code 插件，定期备份配置。 仅安装必要的 Jenkins 插件，并通过 plugin 命令导出当前的插件列表。 尝试在升级前自动备份插件，确保升级失败时能够快速回滚。 总结 # 我希望通过这些努力，将 Infrastructure 的维护和 Pipeline 的开发通过 GitOps 方式进行管理。\n不断探索、尝试和应用最佳实践，使 CI/CD 成为一个健康的、可持续维护的、自我改进和成长的 DevOps 系统。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-10-24","externalUrl":null,"permalink":"/posts/jenkins-upgrade/","section":"Posts","summary":"本文介绍了在升级 Jenkins 的过程中所做的优化，包括使用 Docker Compose 部署、重构 Jenkins Shared Library、引入 Windows Docker Container 等，以提升 CI/CD 流程的效率和安全性。","title":"从 Jenkins 升级，我做了哪些优化","type":"posts"},{"content":"偶尔深夜躺下时，我常常在想，我是怎么就走到这了？这都是源于毕业后的一系列选择吧！\n也时常感慨，选择往往比努力更重要。回顾过去十余年，这几个决定对我走到今天起到了至关重要的影响。\n1. 毕业之后去上海 # 2009年刚毕业时，我的目标很简单，就是找到一份和专业相关的工作，不论在哪，不论具体做什么，没任何计划，只要能提供一份能养活自己的工作就行。\n当时，有同学已经找到手机测试的工作，并被派到上海出差，我得知之后他们还在招聘，也应聘了这家公司。\n还记得应聘完不久，我就坐上了回家的火车。列车从沈阳北站出发时，我接到了公司的电话，通知我面试通过，可以来签合同了。就在火车到达沈阳站时，我毫不犹豫地下了车，还退掉了火车票，开始了人生的第一次真正选择。\n我的第一份测试工作就这样开始了。那时候真的很敢闯，说走就走，然后就独自一个人去了上海。\n上海对我来说是一个新鲜的、充满魔力的城市，即使是住在公司郊区的宿舍，生活依然快乐和精彩。\n在上海，我去过世博会、首家苹果店开业，当然还有南京路、外滩、杭州西湖；还看过刘翔在钻石联赛跑110米跨栏，看周杰伦的上海演唱会。周末的晚上还会和同事一起聚餐到半夜，那时候大家都是刚毕业的小伙子，同事之间相处起来更像是同学。\n2. 从上海回到沈阳 # 在上海出差了一年多，又到了选择的时候，是回到沈阳分公司还是辞职在上海找一份新的工作？我和一部分同事选择回到沈阳分公司。\n回到沈阳，那里的办公室、食堂、工厂和宿舍都在一个大大的厂区里，周围是郊区，只有周末才能出去逛逛。\n我感觉到在这个郊区，我就是这个工厂里的螺丝钉，看到了那种一眼望到头的生活。\n日子虽然过得平稳，但不应该是我这个二十来岁的年龄该追求的，我知道这不会持续太久，如果不及时调整，等到有一天没什么一技之长时会很被动。我开始思考：如果将来我想去大连，我现在的这份手机测试是很难找到工作的，因此我要转做 Web 测试，哪里要我，我就去哪里，我需要的就是相关工作经验。\n后来我是在沈阳东软面试上了北京东软的岗位，去北京的工资是3000，于是我毅然决然去了北京。\n3. 从沈阳到北京 # 那是2011年3月，来之前我联系了大学同学刚哥。我可以在找到房子之前在他那里借宿一段时间。\n就这样，我就坐上了去北京的火车。\n我如愿做上了我心中真正的软件测试工作，也从那时候开始，我才开始上手了一点脚本、数据库相关的知识。\n在北京东软，我经历了两个外派项目，发现我不喜欢这样的外派方式。一是上班无定所，二是定期更换并认识新同事。不到一年时间，我打算换工作了。\n我第一个投递的简历是百度，面试了但不知道为什么反正就是没有通过，可能那时候确实是太菜了，才做了不到一年的 web 测试新手，还想面试百度？想想也不太可能。\n后来又投了几个简历，误打误撞我就面试了京东，且最终通过了面试。记得当时的工资是6500，这对我来说真的是一笔巨款啊，足足比我刚来北京的时候翻了一倍。\n在京东工作的两三年，是我真正入行测试的几年。身边确实有一些值得学习的同事，在他们身上我学到了 Python、Jenkins、性能测试、以及功能测试。\n虽然我学到了这些技能，但我知道如果我想回到大连找到一份不错的工作，光会点技术还不够，我还得会英语，因为在大连只有外企的待遇还不错。\n因此我在北京就开始搜索大连的外企，另外也开始准备学习英语。\n我报名了新东方，学费好几千块钱，我记得上一堂课可能都要几十到上百，另外我觉得这样性价比不高。听了一两次试听课后，我果断退了学费。\n我觉得最好的学习英语的环境就是加入外企。我开始在北京尝试去面试大连的外企，正好有一家在北京有分公司，我就去面试了，但可惜面试没通过。\n4. 从北京回到大连 # 没有办法，在北京想找到大连外企实在不太现实，我辞掉了京东的工作回到大连找。\n回到大连之后休息了几天，发现自己没有工作实在没心情做其他任何事情，然后就开始了找工作。\n去面试过花旗的外派，也没有成功。那时候我还有一个想法就是转开发！\n这是受在京东的同事影响。当时有一个测试同事，他会开发且技术不错，负责给开发团队写单元测试。我当时觉得这个很厉害，我也想做那种被认为有技术含量的工种。\n因此我当时的另一个计划是：如果有任何一家公司愿意招聘我这个只有测试工作经验的人，培养我做开发，我其实愿意少拿钱，甚至免费为他们工作。我当时离开京东时的工资已经有一万二了，尽管如此但我也想要转开发，即使是从一个实习生开始。可惜当时市场没有给我这个机会，我没有在短期内找到这样的开发岗位。因为我需要一份工作，后来就找到一个6000多薪资的测试小主管工作。在这个岗位上我工作了几个月，发现这不是我想继续工作的环境，并在骑驴找马继续寻找一份外企工作。\n后来我又误打误撞投了我之前在北京投递过的外企，这次面试得挺不错的，并正式被录用进入了外企。在这里开启了一段十年的职业生涯，是我最为努力的十年。\n5. 在外企从测试转开发然后DevOps # 上面说过我想转开发。在我加入到新公司后，组内有机会可以从测试转前端开发的机会。我当时极度羡慕这样的机会，可惜晚了一步，人够了，因此我就只能在测试岗位上继续发光发热。\n直到2018年，因为公司业务调整，我又有了转开发的机会，但需要考核。\n虽然对我来说是一个很大的挑战，但我认为这对我来说是在做对的事情。两个原因：\n如果编程技能比较强，我可以成为测试里技术最好的人； 通常开发比测试有更多的话语权，更多的机会，比如说技术移民。 我毫不犹豫地申请去做开发！ 那是一段压力非常大的日子，我之前在#码农随笔系列里写过文章记录过那段日子。但最后我还是成功做了开发。再后来团队需要一个Build工程师，说真的，真是为我准备的角色，太适合我了！\n之前这个角色还叫Build工程师，因为我非常有热情把这个工作做好，因此我做了很多Build之外的事情，因此我的职责也在慢慢扩大到DevOps相关领域，之后大家都称我为DevOps工程师，负责DevOps/Build/Release相关的事情。\n6. 从大连到欧洲 # 早在几年前，我开始有了想走出去看看的想法，想在日本、欧美范围内找一个可以提供签证的DevOps工作。如果再走不出去，到了四十岁我可能就没什么机会了。\n这个想法的产生可能源自于我此前自由行去过泰国、日本、美国的缘故。之后我喜欢听《静说日本》、《随口说美国》这些音频节目。\n我更新了自己的LinkedIn，期间偶尔有人联系过我新加坡的机会。说实话，能不能去成还不好说，但我对那里不是很感冒，给我的感觉那边比较卷。\n在经历了疫情、孩子出生后，这个想法就搁置了，也没有任何进展。在经历了众所周知的大环境后，突然有了机会。\n但说实话这时候我顾虑比当初去上海，回沈阳、去北京、回大连时多得多。\n因为家庭、因为孩子、因为父母，做出决定是不容易的。但在过往的所有选择里，没有完美的机会，在这个不确定的时代，唯一不变的就是变化。况且这样的机会以后几乎都不会再有了。\n留下来可能需要被迫去卷无意义的事情，就像大人卷工作、教育，小孩卷学习。想到此，我知道又到了该选择的时候，那就是走出去。\n最后 # 自毕业以来，特别是近十年的外企工作，我挺努力的。努力工作、不妥协地去完成；坚持业余时间以教促学的写了七八年的博客和公众号，贡献开源项目。\n我坚信努力的价值，但同时也提醒自己不要陷入“自我感动”的误区。努力固然重要，但不能让战术上的勤奋掩盖战略上的懒惰。我们必须抬头看路，抓住关键的机会，做出明智的决策。毕竟，如果方向错误，再多的努力也可能是徒劳的。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-27","externalUrl":null,"permalink":"/misc/power-of-choice/","section":"Miscs","summary":"偶尔深夜躺下时，我常常在想，我是怎么就走到这了？这都是源于毕业后的一系列选择吧！","title":"选择往往比努力更重要","type":"misc"},{"content":"想象一下，你是一名 DevOps 工程师，不论初级、中级还是高级，老板总有一天拍拍你的肩膀说：“加油干，兄弟，未来你就是我们的首席 DevOps 工程师了！”你可能会心想：“啥是首席 DevOps？这是个什么新饼？”\n今天就带你了解一下，所谓的“首席 DevOps 工程师”到底干啥，职责是什么？\n我们一起看看，顺便找准未来的职业发展方向。毕竟，谁都希望能进阶到高级角色嘛，对吧？\n首席 DevOps 工程师是干啥的？ # 在今天这个技术跑得比人快的世界里，首席 DevOps 工程师是关键角色，帮助企业搞定基础设施，让软件交付又快又稳。这可不光是架个服务器那么简单，真正的大活儿是当好开发和运营团队之间的“桥梁”，推动 DevOps 文化在公司生根发芽。\n那么他们的日常是啥呢？总结起来，有三个主要工作：\n设计并维护基础设施 —— 和开发团队配合，搭建弹性、可扩展的基础设施，满足业务需求。 自动化所有能自动化的事情 —— 减少手动操作，提升代码发布和测试的效率。 推动团队文化变革 —— 推广 CI/CD 最佳实践，优化大家的工作方式。 核心职责 # 1. 协调开发和运营 # 在你成为首席 DevOps 工程师后，你的头号任务就是让开发和运营两边配合得像一个人。持续集成、持续部署这些词你得说得像背诗一样顺溜，同时，基础设施得稳如老狗。\n2. 实现自动化和流程优化 # 自动化是 DevOps 的灵魂，首席 DevOps 工程师就是得把那些繁琐的手动任务尽可能自动化，不断优化，让所有事情跑得又快又稳。\n3. 保证系统可靠性和效率 # 系统跑不稳，CI/CD就得停摆，所以你要设计出能撑得住风浪的基数设施。遇到高负载或者故障，系统照样得稳住。定期监控、优化，是你的日常功课。\n成为首席 DevOps 工程师需要哪些技能？ # 1. 技术要硬 # 技术基础是标配，Python、Bash 这些脚本语言得熟悉，Docker 这种容器技术也得懂，Ansible、Chef 这些配置管理工具是你日常操作。最重要的是，云平台（比如 AWS 和 Azure）管理经验不能少。\n2. 领导力和管理能力 # 技术大牛不稀奇，领导力大牛才是硬通货。你得激励团队，帮他们成长，创造出协作的氛围。别忘了，技术再牛，不会跟不同层级的利益相关者沟通，那也白搭。\n3. 解决问题的能力 # 技术上碰到过问题可以迅速找到问题的根源，给出靠谱的解决方案。更重要的是，做决策时得能平衡技术需求和业务目标，让公司上下都买账。\n对公司有啥好处？ # 1. 加强沟通协作 # 作为首席 DevOps，你不仅仅写代码，还得跨团队沟通协调，让大家更默契，工作更顺畅。减少内部摩擦，提升效率。\n2. 加快产品交付 # 优化流程、自动化任务，你能让产品的交付速度快得像坐火箭。市场变化快，你得让公司跟得上，产品能快速迭代，企业才能有竞争力。\n3. 提高系统稳定性和安全性 # 稳定性和安全性很重要，你得建立起强大的监控体系，防止潜在的威胁，在安全和稳定方面给公司打下坚实基础。\n总结一下 # 首席 DevOps 工程师可不是单纯的技术专家，你要靠技术提升效率，推动合作，保证产品交付和系统的稳定性。这过程里，你不仅是一个技术领袖，更是团队文化的推动者。\n想成为首席 DevOps？那就不仅要技术过硬，还要培养领导力和解决问题的能力。\n希望这篇轻松的文章能帮你找到未来的努力方向，毕竟，我们都在为更高的目标努力着！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-26","externalUrl":null,"permalink":"/posts/principal-devops-engineer/","section":"Posts","summary":"本文介绍了首席DevOps工程师的职责、核心技能和对公司的价值，帮助你了解如何在DevOps领域实现职业发展。","title":"DevOps进阶：揭秘首席DevOps工程师的职责与技能","type":"posts"},{"content":"上周正式发布《约定式分支规范》后，受到了广泛关注，不少人询问何时推出中文版。\n经过一周末的努力，中文版已正式上线，详情请访问：https://conventional-branch.github.io/zh/。\n此外，我们对原始版本进行了部分细微调整。以下为《约定式分支规范》中文版的完整内容。\n概述 # 约定式分支是指 Git 分支的结构化和标准化命名约定，旨在使分支更具可读性和可操作性。我们建议了一些您可能想要使用的分支前缀，但您也可以指定自己的命名约定。一致的命名约定使按类型识别分支变得更加容易。\n要点 # 以目的为导向的分支名称：每个分支名称都清楚地表明了其目的，使所有开发人员都能轻松了解该分支的用途。 与 CI/CD 集成：通过使用一致的分支名称，它可以帮助自动化系统（如持续集成/持续部署管道）根据分支类型触发特定操作（例如，从发布分支自动部署）。 团队协作：它通过明确分支目的来鼓励团队内部的协作，减少误解并使团队成员更容易在任务之间切换而不会产生混淆。 规范 # 分支命名前缀 # 分支规范通过 feature/，bugfix/，hotfix/，release/和chore/ 来描述，其结构应如下：\n\u0026lt;type\u0026gt;/\u0026lt;description\u0026gt; main：主要开发分支（例如 main、master 或 develop） feature/：用于新功能（例如 feature/add-login-page） bugfix/：用于错误修复（例如 bugfix/fix-header-bug） hotfix/：用于紧急修复（例如 hotfix/security-patch） release/：用于准备发布的分支（例如 release/v1.2.0） chore/：用于非代码任务，如依赖项、文档更新（例如 chore/update-dependencies） 基本规则 # 小写字母和连字符分隔：始终使用小写字母，并使用连字符分隔单词。例如，feature/new-login 或 bugfix/header-styling。 仅使用字母数字和连字符：仅使用小写字母 (a-z)、数字 (0-9) 和连字符。避免使用空格、标点符号和下划线等特殊字符。 无连续连字符：确保连字符单独使用，没有连续的连字符（例如，feature/new-login，而不是 feature/new--login）。 无尾随连字符：请勿在分支名称末尾添加连字符。例如，使用 feature/new-login 而不是 feature/new-login-。 清晰简洁：使分支名称具有描述性但简洁。名称应清楚地表明正在完成的工作。 包括 Jira（或其他工具）票号：如果适用，请包括项目管理工具中的票号，以便于跟踪。例如，对于票号 T-123，分支名称可以是 feature/T-123-new-login。 结论 # 清晰的沟通：仅凭分支名称就能清楚地了解代码更改的目的。 自动化友好：轻松挂接自动化流程（例如，针对 feature, release 等的不同工作流程）。 可扩展性：适用于许多开发人员同时处理不同任务的大型团队。 总之，约定式分支旨在改善 Git 工作流程中的项目组织、沟通和自动化。\n常见问题 # 如果团队成员不符合此规范，可以使用哪些工具来自动识别？ # 你可以使用 commit-check 来检查分支规范，或者如果你的代码托管在 GitHub 上，则使用 commit-check-action。\n感谢大家的关注！最后，欢迎Star、转发或建议。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-24","externalUrl":null,"permalink":"/posts/conventional-branch/","section":"Posts","summary":"本文介绍了约定式分支规范的中文版，旨在通过结构化的命名约定提高 Git 分支的可读性和可操作性。","title":"约定式分支规范中文版正式发布！","type":"posts"},{"content":"PyPA（Python Packaging Authority）是管理和维护 Python 包相关工具的一个社区组织。PyPA 管理的知名项目包括 pip、packaging、setuptools、wheel、twine、build 等等。了解这些项目的关于有助于我们更好的了解 Python 的生态系统。\n以下是这些项目的介绍以及它们之间的关系：\npip 作用：pip 是 Python 的包管理工具，用于安装和管理 Python 库和依赖项。通过 pip，用户可以从 Python Package Index (PyPI) 或其他包源下载并安装所需的 Python 包。 关系：pip 依赖于 setuptools 和 wheel 来处理包的构建和安装。它是最常用的 Python 包管理工具，也是官方推荐的包安装方法。\nsetuptools 作用：setuptools 是一个用于打包 Python 项目的工具，可以创建分发包（distribution packages）并发布到 PyPI。它扩展了 Python 标准库中的 distutils，提供了更多功能，如依赖管理、插件系统等。 关系：setuptools 是创建 Python 包时常用的工具，pip 使用 setuptools 来安装源代码形式的 Python 包。setuptools 生成的分发包通常是 .tar.gz 或 .zip 文件格式。\npackaging 作用：packaging 提供了用于与 Python 包打包和分发相关的核心实用工具和标准实现。它实现了一些与包版本、依赖关系解析等有关的 PEP（Python Enhancement Proposals）。 关系：packaging 是 setuptools 和 pip 等工具的底层依赖，用于处理包的版本比较、依赖解析等低层次操作。\nwheel 作用：wheel 是一种 Python 包的打包格式，作为 setuptools 打包格式 .egg 的替代方案。它是目前推荐的发布格式，可以避免编译步骤，安装速度更快。 关系：pip 优先安装 wheel 格式的包，因为它可以直接安装，而不需要像 .tar.gz 那样进行编译。setuptools 可以生成 wheel 格式的包。\nvirtualenv 作用：virtualenv 用于创建独立的 Python 环境，可以避免不同项目之间的包依赖冲突。每个虚拟环境都有自己独立的 Python 可执行文件和库集合。 关系：pip 被用于管理 virtualenv 中的包。virtualenv 是创建隔离环境的工具，但近年来 Python 标准库中的 venv 模块也能提供类似功能。\ntwine 作用：twine 是用于将 Python 包上传到 PyPI 的工具。与 setuptools 的 python setup.py upload 方法不同，twine 更加安全，支持上传 wheel 文件和 .tar.gz 文件。 关系：twine 通常与 setuptools 或 wheel 一起使用，负责将构建好的包上传到 PyPI。\nbuild 作用：build 是一个简单的命令行工具，用于构建 Python 项目。它可以使用 PEP 517 接口构建包，而不依赖于 setuptools。 关系：build 提供了比 setuptools 更简洁的构建方式，但它依赖于 setuptools 或其他构建后端（如 flit、poetry）来实际完成构建过程。\npyproject.toml 作用：pyproject.toml 不是一个工具，而是一种配置文件格式。它被用来描述项目的构建需求，并支持使用不同的构建后端，如 setuptools、flit、poetry 等。 关系：pip 和 build 等工具会读取 pyproject.toml 文件，了解如何构建和安装项目。\n他们之间的关系总结 # pip 作为包管理工具，与所有这些项目都有交互关系，尤其依赖 setuptools 和 wheel 来安装包。 setuptools 负责包的创建和打包，并使用 packaging 处理版本和依赖。 wheel 是打包格式，pip 更倾向于安装 wheel 格式的包。 virtualenv 用来创建隔离的环境，pip 被用来在这些环境中安装依赖。 twine 用来安全地上传包到 PyPI，通常与 setuptools 和 wheel 结合使用。 build 是一个新兴的构建工具，使用 PEP 517 接口，可以使用 setuptools 作为构建后端。 这些工具共同构成了 Python 包管理和分发的完整生态系统，简化了 Python 开发者的工作流程。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-09-05","externalUrl":null,"permalink":"/posts/pypa/","section":"Posts","summary":"本文介绍了 PyPA（Python Packaging Authority）下的知名项目，包括 pip、setuptools、wheel 等，并分析了它们之间的关系，帮助读者更好地理解 Python 包管理和分发的生态系统。","title":"初步了解 PyPA（Python Packaging Authority）下的知名项目和关系","type":"posts"},{"content":"","date":"2024-07-09","externalUrl":null,"permalink":"/tags/work/","section":"标签","summary":"","title":"Work","type":"tags"},{"content":"最近开车的时候我总爱听赵雷的歌，尤其是那首《我记得》。\n我觉得这首歌就是回忆的交响曲，每每听到就仿佛看到了一部老电影。它像是一种声音，一种触动人心的触感，让我们回忆起那些已经消逝的时光，反思我们的生活方式和人生的方向。\n歌词和旋律中都充满了满满的怀旧情感，总是让我回想起来这一段时间来的所有人多我的问候和祝福。\n趁着他们都还很鲜活我想把他们全都记录一下，也叫《我记得》：\n6月14日（周五）我所在的1024足球队就为我准备了聚餐，当时还只有我的签证获批，这已经是一个好的开始了，当晚把我这几年的啤酒全喝了，还好酒精度数不是很大，回到家也没有迷糊。 6月27日（周四）与公司的同事和领导去吃了钱库里自助，当时的心情还是复杂的，因为明天就要毕业了，我们在一起照了一张欢快的合影。 6月28日（周五）毕业当天很多同事过来跟我告别、以及他们把我送出公司的门和大楼门的情景，最后我和大超一起下山，他回家，我等公交车。 6月30日（周日）我去小平岛恰巧碰到了同事大叔，我去送洗鞋，他去买菜。我们就站在马路边站着聊了很久，最后分别前拍了一张合影。 7月1日（周一）跟当年的发小和老同桌一起吃了午饭，绕着腾飞走了一圈，最后他陪我一起去4s保养、补胎，聊了一下午，好久没有单独和一个人聊这么久了，感觉很好，拍了合影，留作纪念 7月4日（周四）我独自带着女儿回到老家让二老再看看他们的大孙女，整个往返车程6个小时，女儿都是坐在宝宝安全座椅上，表现的非常棒。 7月7日（周日）和妻子、女儿去老丈人家吃了一个美味的午饭；还是当天，下午和教练去市馆游泳，回到腾飞后喝了杯咖啡、吃了点小食、最后绕着腾飞走了半圈，聊到意犹未尽。 7月8日（周一）去跟原公司同事蹭场打羽毛球， 7月9日（周二）跟1024足球队踢了我在这边的最后一场球了，对方今天有一个超强的守门员，给人印象深刻，另外一个深刻的地方可能就是我的一记后场吊射进球了吧。 7月10日（周三）启程再次去日本拿卡，另外还去了一趟东京迪斯尼，尽管当天下了雨，但感受还是很好。 7月14日（周日）从日本回来，当天下午前同事朋友去羊汤馆吃了一顿饭，聊了几个小时。 7月15日（周一）见了发小，他在海边钓鱼，我也去了，那是我第一次使用鱼竿钓鱼，并且钓到了一条小小小鱼。 7月16日（周二）启程出发了，虽然心中有万般不舍，但到了改走的时候了。我知道此时的难过是真的，我也知道我一定会走出这一步也是真的。当晚做高铁到了北京，见到机场见到大学同学，一起聊了一会，但因为已经很晚了，就叙旧到此。 7月17日（周三）经过了十几个小时的飞行，我们一家人终于到了维村。至此，我短暂的毕业季就结束了。 很怀念，因此写下只言片语，记录一切发生过的美好。\n这些记忆会提醒我，无论生活如何变迁，我们都不会忘记过去，但也要对未来充满希望和期待，体验到了生活的美好和希望。\n生活里虽然充满了困难和挑战，但只要我们对过去有回忆，对未来有期待，那么生活就充满了意义和价值。\n美好的音乐也会触动着我们的情感，引发我们的思考，带给我们力量和希望。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-07-09","externalUrl":null,"permalink":"/misc/remember/","section":"Miscs","summary":"最近开车的时候我总爱听赵雷的歌，尤其是那首《我记得》。这篇文章记录了我在Rocket中国分公司的毕业季和同事们的祝福。","title":"我记得","type":"misc"},{"content":"​今天（6月28日）是我在Rocket中国分公司的最后一天，也是效力的第十个年头。这是我职业生涯中效力过最长的一家公司，想借此机会写下一些文字，为这十年画上一个句号。\n光阴似箭，岁月如梭。转眼间，十年悄然逝去。唯有认真努力地生活，才能不被时光的流逝所感叹。我一直很喜欢的一句话是：“种一棵树最好的时间是十年前，其次是现在。\n受同事的影响，从加入到公司以后我开始爱上软件这个行业，也想成为别人眼中的技术专家。我从最初的测试工程师转为开发人员，最终投身于DevOps领域，在这个过程中收获了快乐和成就。\n十年征程，感恩有你。在Rocket中国的十年里，我结识了众多优秀的同事和朋友，从他们身上我学到了很多，也受到了他们的深刻影响。\n无数次的团队聚餐、活动、旅游、足球、羽毛球、游泳等等，感谢你们一路的相伴，让这段旅程更加精彩和难忘。\n也感谢我的家人，一直以来对我的包容、理解、支持和爱。是你们给了我坚强的后盾，让我无畏前行。感谢这十年里所有美好的回忆。\n三十年风雨，五座城市。三十年来，我辗转生活过五个城市：出生在辽宁庄河，大学就读于沈阳，第一份工作在上海，后来又在北京工作了三年。直到2014年，我从京东裸辞，回到了家乡大连。\n大连是我最喜欢的城市之一，这里风景优美、气候宜人。我在这里买房、结婚、生子，完成了人生中许多重要的阶段。\n天下没有不散的宴席。随着最后一天日期的临近，我的心情也变得复杂。\n其中既有对未来的期待和憧憬，也有对挑战和未知的担忧，以及对家人的牵挂。我期待着自己、妻子和孩子都能在未来的道路上变得更好、更强；但从工作到生活，这无疑将充满挑战。我无法确定这一过程是否会顺利，也不确定需要多长时间。\n选择欧洲，开启新篇章。或许有人会问，在国内难道不能养家糊口吗？为什么要跑到那么远的地方？其实，这并非对错之分，而是选择。一个人的选择往往与他的经历息息相关。\n以前，我从未想过出国，直到加入Rocket中国之后，身边优秀的同事陆续带着家人和孩子去了澳大利亚、欧洲、日本等地。此外，我也有机会去美国出差、去日本旅游，看到了更加广阔精彩的世界。\n这些经历逐渐在我心中萌生了一个想法：如果有机会走出去，那将是一次充满挑战和乐趣的人生体验。\n我钦佩那些主动打破舒适圈的人，而我则一直被动地等待着机会的降临。如果机会来临，我愿意勇敢地尝试。\n恰逢公司业务调整，项目要移至欧洲。对于其他人来说，这可能是一个坏消息，但对我而言，这是一个难得的机会。虽然这并非一个完美的契机，但人生哪有那么多完美呢？\n这次机会将让我有机会在全英文的工作环境中锻炼自己；我的两岁女儿也将有机会接受不同的教育，开启她的第二甚至第三外语学习之旅。\n对于我们一家来说，这都是一次全新的挑战和开始。虽然我们已经过了35岁，但我始终觉得，我们依然年轻，可以不断学习和探索，保持对世界的好奇心。\n选择欧洲的另一个原因是，如果留在国内，程序员想要延续职业生涯到四十岁甚至五十岁、六十岁会非常困难，而在欧美国家相对容易一些。\n此外，随着年龄的增长，我对“好公司”的定义也在不断变化。\n比如刚毕业去上海工作时，我觉得SIMCom是最好的公司；后来去北京东软工作时，我觉得东软也还不错；再后来，我加入了京东，觉得这可能是我这辈子能加入的最好的公司了；直到我加入到现在这家外企，这十年来我一直觉得这才是一家好公司。\n如果还按照现在的标准在大连甚至其他城市来寻找新的工作，恐怕只能在梦里才能实现了。\n最后，感谢在Rocket中国这十年里遇到的每一位朋友和同事。\n衷心感谢每位同事的临别前的问候和祝福。\n希望未来我们在人生的旅途上能够再次相聚。\n再见！👋\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-06-28","externalUrl":null,"permalink":"/misc/farewell/","section":"Miscs","summary":"2024年6月28日是我在Rocket中国分公司的最后一天，回顾十年征程，展望未来新篇章。","title":"告别Rocket中国，回连十年再启程","type":"misc"},{"content":"","date":"2024-06-13","externalUrl":null,"permalink":"/tags/devsecops/","section":"标签","summary":"","title":"DevSecOps","type":"tags"},{"content":"","date":"2024-06-13","externalUrl":null,"permalink":"/tags/slsa/","section":"标签","summary":"","title":"SLSA","type":"tags"},{"content":"软件真是个有趣又深奥的东西，它由看似神奇的代码片段组成，这些代码运行在最终的终端上，本身却并非生命体，但拥有自己的生命周期。\n软件最初是源代码的形式，仅仅是存放在某个仓库的文本文件，然后通过独特的构建过程，这些源代码会转变为其他形式。例如交付到 web 服务器的压缩 JavaScript 代码块、包含框架代码和业务逻辑的容器镜像，或者针对特定处理器架构编译的原始二进制文件。\n这种最终的形态转变，也就是源代码生成的其他形式，通常被称为“软件制品”。在创建之后，制品通常会处于休眠状态，等待被使用。它们可能会被放置在软件包注册表（例如 npm、RubyGems、PyPI 等）或容器注册表（例如 GitHub Packages、Azure Container Registry、AWS ECR 等）中，也可能作为附加到 GitHub 版本发布的二进制文件，或者仅仅以 ZIP 文件的形式存储在某个 Blob 存储服务中。\n最终，有人会决定拾取该制品并使用它。他们可能会解压缩包、执行代码、启动容器、安装驱动程序、更新固件 - 无论采用何种方式，构建完成的软件都将开始运行。\n这标志着生产生命周期的顶峰，该周期可能需要大量人力投入、巨额资金，并且鉴于现代世界依赖软件运行，其重要性不言而喻。\n然而，在许多情况下，我们并不能完全保证所运行的制品就是我们构建的制品。制品经历的旅程细节要么丢失，要么模糊不清，很难将制品与其来源的源代码和构建指令联系起来。\n这种缺乏对制品生命周期的可见性是当今许多最严峻的安全挑战的根源。在整个软件开发生命周期 (SDLC) 中，有机会保护代码转换为制品的流程 - 如此一来，可以消除威胁行为者破坏最终软件并造成严重后果的风险。一些网络安全挑战似乎难以成功应对，但这种情况并非如此。让我们深入了解一些背景知识。\n哈希值和签名 # 假设你的目录中有一个文件，并且你想要确保它明天与今天完全相同。你该怎么做？一个好的方法是通过安全的哈希算法生成文件的哈希值。\n以下是如何使用 OpenSSL 和 SHA-256 算法完成此操作：\nopenssl dgst -sha256 ~/important-file.txt 现在，你拥有了一个哈希值（也称为散列值），它是一个由字母和数字组成的 64 字符字符串，代表该文件的唯一指纹。只要更改该文件中的任何内容，然后再次运行哈希函数，你就会得到不同的字符串。你可以将哈希值写在某个地方，然后第二天回来尝试相同的过程。如果你两次没有得到相同的哈希值字符串，则文件中的某些内容已发生更改。\n到目前为止，我们可以确定某个文件是否被篡改。如果我们想要对制品进行声明怎么办？如果我们想说“我今天看到了这个制品，我（系统或人）保证这个东西就是我看到的东西”，该怎么办？此时，你需要的是软件制品签名；你需要将哈希值字符串通过加密算法进行处理，以生成另一个字符串，代表使用唯一密钥“签名”该指纹的过程。 如果你随后希望其他人能够确认你的签名，则需要使用非对称加密：使用你的私钥签名哈希值，并提供相应的公钥，以便任何获取你文件的人都可以进行验证。\n你可能已经知道，非对称加密是互联网上几乎所有信任的基础。它可以帮助你安全地与银行互动，也可以帮助 GitHub 安全地交付你的存储库内容。我们使用非对称加密来支持诸如 TLS 和 SSH 等技术，以创建可信赖的通信通道，但我们也使用它通过签名来创建信任软件的基础。\nWindows、macOS、iOS、Android 等操作系统都具有用于确保可执行软件制品的可信来源的机制，方法是强制要求存在签名。这些系统是现代软件世界中极其重要的组件，构建它们非常困难。\n不仅仅是签名 - 还要证明 # 当我们思考如何展示关于软件制品的更多可信赖信息时，签名是一个好的开端。它表示“某个可信赖的系统确实看到了这个东西”。但是，如果你真的想在整个软件开发生命周期 (SDLC) 的安全性方面取得重大进步，那么你就需要超越简单的签名，而是要考虑证明。\n证明是一种事实断言，是对制品或制品所做的声明，并由可被认证的实体创建。之所以可以进行认证，是因为声明已签名，并且用于签名的密钥是可信的。\n最重要和最基础的证明类型之一是断言有关制品来源和创建的事实 - 它来自的源代码和将源代码转换为制品的构建指令，我们称之为来源证明。\n我们选择的来源证明规范来自 SLSA 项目。SLSA 是考虑软件供应链安全性的绝佳方式，因为它为软件的生产者和消费者提供了一个通用的框架，用于推理安全保证和边界，而这与特定的系统和技术栈无关。\nSLSA 基于 in-toto 项目的工作，提供了一种用于生成软件制品来源证明的标准化架构。in-toto 是一个 CNCF 毕业项目，其存在目的之一是提供一系列有关供应链和构建过程的相关信息的标准化元数据架构。\n构建这样的东西需要什么？ # GitHub 作为托管大量代码和构建管道的全球最大软件开发平台，对此进行了大量的思考。构建认证服务需要许多活动部件。\n这样做意味着有一种方法可以：\n颁发证书（本质上是绑定到某个经过身份验证的身份的公钥）。 确保这些证书不会被滥用。 在众所周知的上下文中启用工件的安全签名。 以最终用户可以信任的方式验证这些签名。 这意味着设置证书颁发机构 (CA) 并拥有某种客户端应用程序，你可以使用它来验证与该颁发机构颁发的证书相关联的签名。\n为了防止证书被滥用，你需要 (1) 维护证书吊销列表或 (2) 确保签名证书是短期的，这意味着需要某种时间戳机构的反签名（可以提供权威印章，表明证书仅在其有效期内用于生成签名）。\n这就是 Sigstore 的作用所在，它是一个开源项目，提供 X.509 CA 和基于 RFC 3161 的时间戳机构。它还允许你使用 OIDC 令牌进行身份验证，许多 CI 系统已经生成了令牌并将其与其工作负载相关联。\nSigstore 对软件签名的作用与 Let\u0026rsquo;s Encrypt 对 TLS 证书的作用相同：使其简单、透明且易于采用。\nGitHub 通过在技术指导委员会中的席位帮助监督 Sigstore 项目的治理，是服务器应用程序和多个客户端库的维护者，并且（与来自 Chainguard、Google、RedHat 和 Stacklok 的人员一起）组成了 Sigstore 公共物品实例的运营团队，该团队的存在是为了支持 OSS 项目的公共证明。\nSigstore 需要符合更新框架 (TUF) 规定的标准的安全信任根。这允许客户端跟上 CA 底层密钥的轮换，而无需更新其代码。TUF 的存在是为了缓解在现场更新代码时可能出现的大量攻击媒介。许多项目都使用它来更新长期运行的遥测代理、提供安全的固件更新等。\n有了 Sigstore，就可以创建防篡改的纸质跟踪，将工件与 CI 联系起来。这一点非常重要，因为以无法伪造的方式签署软件和捕获出处细节，意味着软件消费者有办法执行他们自己的规则，以确定他们正在执行的代码的来源。\n原文：https://github.blog/2024-04-30-where-does-your-software-really-come-from/\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-06-13","externalUrl":null,"permalink":"/posts/where-does-your-software-come-from/","section":"Posts","summary":"本文介绍了软件制品的来源证明，强调了在软件开发生命周期中保护代码转换为制品的流程的重要性，并介绍了 SLSA 项目和 Sigstore 的作用。","title":"你的软件究竟从哪里来？","type":"posts"},{"content":"上次我在 代码签名（Code Signing）的文章中时候提到了 GaraSign，这是我在工作中使用到的另一个代码签名工具。\n鉴于关于 GaraSign 的使用并没有多少中文资料，本篇我将介绍关于 GaraSign 的一些实线，希望对你有帮助。\n代码签名 # 这里再次说明什么是代码签名。代码签名证书用于对应用程序、驱动程序、可执行文件和软件程序进行数字签名，客户可通过这种方式验证他们收到的代码未被网络罪犯和黑客篡改或破坏。签名后的交付产品结合了加密令牌和证书，用户可在安装产品前对其进行验证。代码签名可确认谁是软件作者，并证明代码在签名后未被修改或篡改。\nGarasign 解决方案 # GaraSign 是一个基于 SaaS 的安全协调平台，可对企业基础设施、服务和数据进行集中管理。GaraSign 可与所有主要操作系统、平台和工具的本地客户端集成，确保现有工作流不受干扰，同时改善其整体安全态势和合规性。\nGaraSign 由以下组件组成： # 加密令牌 - 存储签名密钥的加密设备（通常是一个或多个 HSM - Hardware Security Modules） GaraSign 签名服务器 - 位于存储签名密钥的加密令牌前的 REST 服务器 GaraSign Signing 客户端 - 允许与之集成的签名工具在本地散列数据并将签名生成脱载至 GaraSign Signing 服务器的客户端。 Garasign 代码签名散列方法 - 大幅提高速度\n安装 GaraSign # 关于如何安装 GaraSign 这里不过去介绍，可以到官网找相关的安装文档。这里要注意目前 GaraSign 对操作系统版本的要求还是很高的，比如\nWindows 最低要求是 Windows 2019, Win10 and Win11 Linux 最低要求是 RHEL 7.9, 8.0, 9.0，CentOS 7.9, 8.0, 9.0，Rocky 8.0 如果你的构建环境还是比较旧的或是不符合其支持的版本，建议你向我一样设置一台专用的 GaraSign 机器（推荐 Linux）。\n如果你使用 Jenkins 来构建，可以将这台机器设置为一台 Jenkins agent，通过创建一个 Jenkins pipeline，这样其他所有的要需要发布的包都可以通过这个 pipeline 来进行签名。\n如何签署独立签名 # 如果你已经设置好了 GaraSign 环境，以 Linux 为例，那么就可以通过下面的命令进行签署。\n注：在 Windows 与 Linux 在签署不同的类型文件所使用到的命令不同，因此推荐在 Linux 进行签名会更加简单。\nopenssl dgst -sign \u0026lt;private key file\u0026gt; -keyform PEM -sha256 -out \u0026lt;signature-file-name.sig\u0026gt; -binary \u0026lt;binary file to sign\u0026gt; 具体的实施 # 加入你的 Artifacts 存在 Artifactory 上面，下面就 Jenkins 为例，来实施一个可以自动签名的 pipeline。包括：\n从 Artifactory 上下载需要签名的 Artifacts 使用 GaraSign 进行签名 验证 GaraSign 是否成功 上传签名文件和公钥到 Artifactory 上的同一个目录下 pipeline{ agent { node { label \u0026#39;garasign\u0026#39; } } parameters { string( name: \u0026#39;REPO_PATH\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Repository Path on Artifactory. eg. generic-stage/test_repo/devel/54/mybuild_1.1.0_752d0821_64bit.exe\u0026#39; ) } environment { BOT = credentials(\u0026#34;BOT-credential\u0026#34;) ART_URL = \u0026#34;https://my.org.com/artifactory\u0026#34; } stages { stage(\u0026#39;GaraSign\u0026#39;){ steps { script { if (! params.REPO_PATH){ error \u0026#34;REPO_PATH can not empty, exit!\u0026#34; } // Update Job description def manualTrigger = true currentBuild.upstreamBuilds?.each { b -\u0026gt; currentBuild.description = \u0026#34;Triggered by: ${b.getFullDisplayName()}\\n${REPO_PATH}\u0026#34; manualTrigger = false } if (manualTrigger == true) { currentBuild.description = \u0026#34;Manual sign: ${REPO_PATH}\u0026#34; } sh \u0026#39;\u0026#39;\u0026#39; # download artifacts curl -u${BOT_USR}:${BOT_PSW} -O ${ART_URL}/${REPO_PATH} file_name=$(basename ${REPO_PATH}) repo_folder=$(dirname ${REPO_PATH}) # garasign openssl dgst -sign grs.privkey.pem -keyform PEM -sha256 -out $file_name.sig -binary $file_name # verify grs.pem.pub.key output=$(openssl dgst -verify grs.pem.pub.key -keyform PEM -sha256 -signature $file_name.sig -binary $file_name) if echo \u0026#34;$output\u0026#34; | grep -q \u0026#34;Verified OK\u0026#34;; then echo \u0026#34;Output is Verified OK\u0026#34; else echo \u0026#34;Output is not Verified OK\u0026#34; exit 1 fi # upload signature file (.sig) and public key (.pem.pub.key) curl -u${BOT_USR}:${BOT_PSW} -T $file_name.sig ${ART_URL}/${repo_folder}/ curl -u${BOT_USR}:${BOT_PSW} -T grs.pem.pub.key ${ART_URL}/${repo_folder}/ \u0026#39;\u0026#39;\u0026#39; } } } } } 如何验证独立签名 # 还是以 Linux 为例，使用如下命令可以进行签名的验证。\nopenssl dgst -verify \u0026lt;public key file\u0026gt; -signature \u0026lt;signature\u0026gt; \u0026lt;file to verify\u0026gt; 当你的 Artifacts 已经进行了签名，在提供给客户的时候，你不但需要提供发布的包，而且需要提供签名文件 (.sig) 和公钥 (.pem.pub.key)。\n举个例子，如下 CLI 产品分别提供了 Windows，Linux 和 AIX 三个平台的安装包，客户可以参考如下进行签名验证。\n# 下载安装包、签名文件和公钥 $ ls cli.pem.pub.key CLI_AIX_1.1.0.zip CLI_AIX_1.1.0.zip.sig CLI_LINUXX86_1.1.0.zip CLI_LINUXX86_1.1.0.zip.sig CLI_WINDOWS_1.1.0.zip CLI_WINDOWS_1.1.0.zip.sig # 验证签名 openssl dgst -verify cli.pem.pub.key -signature CLI_AIX_1.1.0.zip.sig CLI_AIX_1.1.0.zip Verified OK openssl dgst -verify cli.pem.pub.key -signature CLI_LINUXX86_1.1.0.zip.sig CLI_LINUXX86_1.1.0.zip Verified OK openssl dgst -verify cli.pem.pub.key -signature CLI_WINDOWS_1.1.0.zip.sig CLI_WINDOWS_1.1.0.zip Verified OK # 当包和签名文件不符时会验证失败 openssl dgst -verify cli.pem.pub.key -signature CLI_AIX_1.1.0.zip.sig CLI_LINUXX86_1.1.0.zip Verification Failure 以上就是关于 GaraSign 的实现分享，如有任何问题或是建议咱们评论区见。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-06-10","externalUrl":null,"permalink":"/posts/garasign/","section":"Posts","summary":"本文介绍了 GaraSign 代码签名工具的安装、使用和验证方法，帮助开发者实现安全的代码签名。","title":"代码签名（Code Signing） - GaraSign","type":"posts"},{"content":"","date":"2024-05-28","externalUrl":null,"permalink":"/tags/infrastructure/","section":"标签","summary":"","title":"Infrastructure","type":"tags"},{"content":"Python 软件基金会 (PFS) 或许大家比较熟知，它是开源 Python 编程语言背后的组织，致力于为 Python 和 Python 社区的发展壮大创造条件。\n继上次我们看完了 Apache 的基础设施介绍，本篇文章我们一起来看看 Python 软件基金会 (PFS) 的基础设施，看看可以从中学到哪些。\nPSF 基础设施概述 # PSF 运行各种基础设施服务来支持其使命，从 PyCon 站点 到 CPython Mercurial 服务器。本页的目标是列举所有这些服务，它们在哪里运行，以及主要联系人是谁。\n基础架构团队 # 基础架构团队最终负责维护 PSF 基础设施。但是，它不需要成为运行 PSF 服务的基础设施。事实上，大多数的日常运营服务由不在基础设施团队中的人员处理。这基础设施团队可以协助建立新服务并为维护人员提供建议的个别服务。其成员通常还处理对敏感的更改全球系统，例如 DNS。目前的团队成员是：\nAlex Gaynor (has no responsibilities) Benjamin Peterson Benjamin W. Smith Donald Stufft Ee Durbin (PSF Director of Infrastructure) Noah Kantrowitz 联系基础架构团队的最佳方式是发送邮件 infrastructure-staff@python。也经常有人在 Libera 的 #python-infra 频道联系他们。\n基础设施提供商 # PSF 为其基础架构使用多个不同的云提供商和服务。\nFastly Fastly 慷慨捐赠其内容分发网络（CDN）到 PSF。我们最高的流量服务（即 PyPI, www.python.org, docs.python.org）使用此 CDN 来改善最终用户延迟。\nDigitalOcean DigitalOcean 是当前的主要托管对于大多数基础设施，此处部署的服务由 Salt 管理。\nHeroku Heroku 托管了许多 CPython 核心工作流机器人，短暂的或概念验证的应用程序，以及其他适合部署在 Heroku 的 Web 应用程序。\nGandi Gandi 是我们的域名注册之星。\nAmazon Route 53 Amazon Route 53 托管所有域的 DNS，它目前由基础设施人员手动管理。\nDataDog DataDog 提供指标、仪表板和警报。\nPingdom Pingdom 提供监控，当服务中断时向我们投诉。\nPagerDuty PagerDuty 用于 PSF 的待命轮换基础设施员工在一线，志愿者作为后援。\nOSUOSL 俄勒冈州立大学开源实验室举办一个 PSF 的硬件服务器，speed.python.org 用于运行基准测试，此主机是使用 Chef 和他们的配置管理位于 PSF-Chef Git 存储库中。\n数据中心 # PSF DC Provider Region ams1 Digital Ocean AMS3 nyc1 Digital Ocean NYC3 sfo1 Digital Ocean SFO2 各种服务的详细信息 # 本部分列举了 PSF 服务、有关其托管的一般情况以及所有者的联系信息。\nBuildbot buildbot master 是由 python-dev@python 运行的服务。特别是 Antoine Pitrou and Zach Ware.\nbugs.python.org bugs.python.org 由 PSF 在 DigitalOcean 上托管，由 Roundup 提供支持。它还部署了 bugs.jython.org 和 issues.roundup-tracker.org。\ndocs.python.org Python 文档托管在 DigitalOcean 上，通过 Fastly 提供。负责人是 Julien Palard。\nhg.python.org CPython Mercurial 存储库托管在 Digital Ocean VM 上。负责人是 Antoine Pitrou 和 Georg Brandl。\nmail.python.org python.org Mailman 实例托管在 https://mail.python.org 和 SMTP（Postfix）上。所有查询都应定向到 postmaster@python。\nplanetpython.org 和 planet.jython.org 它们托管在 DigitalOcean VM 上。Planet 代码和配置托管在 GitHub 上，并由团队在 planet@python。\npythontest.net pythontest.net 托管 Python 测试套件。python-dev@python 对其最终负责维护。\nspeed.python.org speed.python.org 是一个跟踪 Python 性能的 Codespeed 实例。Web 界面托管在 DigitalOcean VM 上，基准测试在 strongfy 上运行机器在 OSUOSL 上，由 Buildbot 主节点调度。由 speed@python 和 Zach Ware 维护。\nwiki.python.org 它托管在 DigitalOcean VM 上。负责人是 Marc-André Lemburg。\nwww.jython.org 这是从 Amazon S3 存储桶托管的。设置非常简单，不应该需要很多调整，但基础设施工作人员如有需要可以对它进行调整。\nwww.python.org 主要的 Python 网站是一个 Django 应用程序，托管在 Heroku。它的源代码可以在 GitHub 上找到，并且该网站的问题可能是报告给 GitHub 问题跟踪器。 Python 下载 （即 https://www.python.org/ftp/ 下的所有内容）都托管在单独的 DigitalOcean 虚拟机。整个网站都在 Fastly 后面。还有用于测试站点的 https://staging.python.org。http://legacy.python.org 是从静态镜像托管的旧网站。\nPyCon PyCon 网站托管在 Heroku 上。联系地址是 pycon-site@python。\nPyPI Python 包索引的负载最多 任何 PSF 服务。它的源代码可在 GitHub 上找到。 它的所有基础设施都在 由 pypi-infra 配置的 AWS，它以 Fastly 为首。基础设施是由 Ee Durbin, Donald Stufft, 和 Dustin Ingram 维护的，联系地址是 admin@pypi。\nPyPy properties PyPy 网站托管在 DigitalOcean VM 上并进行维护作者：pypy-dev@python。\n如需要参看原文。可访问地址。\n最后 # 从 PFS 基础设施来看，他们更多的使用了 Cloud 服务商和技术来部署他们的应用。因此想要参与到 PFS 基础设施管理用，需要对 CDN，DigitalOcean，Heroku，Amazon Route 53，Amazon S3，DataDog，Pingdom 这些技术有比较深入的使用经验。\n我们羡慕那些可以为开源软件全职工作的人，他们拿着薪水做着很多程序员都羡慕的事情。\n但拥有这样的工作需要我们自己的技能可以匹配的上，并且积极主动的去寻求这些机会，才有可能得到一份全职开源软件工程师的职位。共勉！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-05-28","externalUrl":null,"permalink":"/posts/psf-infra/","section":"Posts","summary":"本文介绍了 Python 软件基金会 (PFS) 的基础设施，包括其服务、提供商和团队成员，帮助读者了解 PFS 如何支持 Python 社区。","title":"Python 软件基金会 (PFS) 基础设施概览","type":"posts"},{"content":"当谈到软件开发和安全性时，Code Signing（代码签名）是一个至关重要的概念。在这篇文章中，我们将探讨什么是代码签名，为什么它重要，以及两个代码签名工具的对比。\n什么是代码签名？ # 代码签名是一种数字签名技术，用于验证软件或代码的真实性和完整性。它通过使用加密技术，为软件文件附加一个数字签名，证明该文件是由特定的开发者创建并未被篡改过。\n代码签名的过程通常涉及以下步骤：\n生成数字证书：开发者使用数字证书来创建数字签名。证书通常由可信任的第三方证书颁发机构（CA）签发。 对软件进行签名：开发者使用专门的工具，如 Microsoft 的 SignTool 或 Apple 的 codesign 工具，对软件进行数字签名。 分发已签名的软件：带有数字签名的软件可以被分发到用户设备上。 为什么代码签名重要？ # 代码签名的重要性体现在以下几个方面：\n完整性验证：代码签名可以确保软件在分发过程中未被篡改。用户可以通过验证签名来确认软件的完整性，确保软件来自可信任的来源。\n身份验证：签名附带的数字证书可以用来验证软件的发布者身份。用户可以查看证书，了解软件的制造商，并评估其可信度。\n安全性增强：通过数字签名，可以防止恶意软件插入合法软件包中，确保用户下载的软件是安全可靠的。\n操作系统信任：大多数操作系统和应用商店要求开发者对软件进行代码签名才能发布。没有签名的软件可能会被视为不安全或不可信。\n代码签名工具 # 我在工作中主要接触到两个 Code signing 工具，分别是代码签名证书Code Signing Certificates 和 GaraSign，它们是比较有代表性的两类工具。\n这里简单介绍一下代码签名证书与 GaraSign 的区别：\n代码签名证书 和 GaraSign 都是用于验证软件完整性和来源的解决方案，但它们在工作方式和功能方面存在一些关键差异。\n功能 代码签名证书 GaraSign 颁发者 受信的证书颁发机构 (CA) GaraSign 形式 数字证书 云服务 验证方法 加密哈希 加密哈希 功能 验证软件完整性、确保软件未被篡改 验证软件完整性、确保软件未被篡改、提供集中式管理、支持审计和合规性 成本 每年 100 美元到数千美元不等 按使用付费 管理 需要购买和管理证书 无需管理证书 可扩展性 适用于需要签署大量软件的组织 适用于任何规模的组织 总的来说，代码签名证书和 GaraSign 都是有效的解决方案，可用于验证软件完整性和来源。 最适合你的选择将取决于你的特定需求和预算。以下是一些需要考虑的因素：\n成本： 如果你需要签署大量软件，GaraSign 可能更具成本效益。 可扩展性： 如果你需要签署大量软件，GaraSign 可能是更好的选择。 审计和合规性： 如果你需要满足严格的审计和合规性要求，GaraSign 可能是更好的选择。 易用性：从我目前使用体感来说代码签名证书更容易设置和使用。GaraSign 则需要搭建服务和安装、配置客户端，非常繁琐1。 其他签名工具还包括微软的 SignTool.exe，Docker trust sign等，这里就不一一介绍了。\n结论 # 通过代码签名，开发者可以增加软件的安全性和可信度，同时帮助用户避免恶意软件和篡改的风险。 在今天的数字化环境中，代码签名是保障软件完整性和安全性的重要一环，提高软件供应链安全。\n更过关于软件供应链安全内容可以参考这一系列文章。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n目前我还在初步使用 GaraSign 后续如果有必要，我会单独写一些篇关于它的使用体验文章。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-04-29","externalUrl":null,"permalink":"/posts/code-signing/","section":"Posts","summary":"本文介绍代码签名的概念、重要性以及两种常见的代码签名工具，强调代码签名在软件供应链安全中的作用。","title":"代码签名（Code Signing）","type":"posts"},{"content":"今天翻译一篇我在 Jenkins Contributors 页面上看到的一篇文章。\n其作者是 Hervé Le Meur，我早在关注 Jenkins-Infra 的项目的时候就关注到他，他是一个法国人。以下是关于他如何通过 Jenkins-X 社区最终进入到 Jenkins 基础设施团队成为 SRE 的经历。\n说实话有点羡慕。希望这个分享能给每一个想加入开源、并且在开源组织中找到一份全职工作带来一点启发。\n以下是我对原文的翻译：\nHervé Le Meur 是一名 SRE（网站可靠性工程师），目前是 Jenkins 基础设施团队的成员。他是通过 Jenkins X 进入开源社区的，随后转到 Jenkins 基础设施工作。\nHervé 的父亲是一名木匠，母亲是一名室内装潢师，他出身于一个模拟技术背景的家庭，但在六岁时就通过 Amstrad CPC 464 电脑第一次真正接触到了技术。\n如今，在不从事 Jenkins 任务的时候，Hervé 喜欢和家人一起到户外散步、阅读和观看自己喜欢的节目。\n在加入 Jenkins 之前，你的背景是什么？ # 大学毕业后，我在一家小型 B2B 营销咨询公司工作了 10 年，当时我是开发我们所用工具的万事通，但那里既没有 CI/CD，也没有开源。\n之后，我进入一家建筑信息建模（BIM）软件公司，从软件开发人员做起。有些团队在使用 Jenkins，但对他们来说，当时的 Jenkins 看起来既麻烦又缓慢。\n随着我逐渐成长为软件架构师，我的任务是基于 Jenkins X 建立一个新的 CI/CD，这花了我几个月的时间。 由于 Jenkins X 刚刚出炉，而我又是 Kubernetes 的新手，这项任务比预期的要困难得多，尤其是 Jenkins X 进入测试阶段后，我不得不多次重做大部分工作。\n通过这项工作，我学到了很多关于 Kubernetes 和 CI/CD 的知识，同时也为 Jenkins X 做出了不少贡献。 被这份工作解雇后，我联系了 James Strachan 和 James Rawlings，他们给了我一个链接，让我从 CloudBees 公司的 Oleg Nenashev 那里获得一个职位空缺，也就是我现在的职位。\n在我的脑海中，我是一名程序员，而不是系统管理员。因此，当 Mark Waite 解释说最后一轮面试将与人际网络有关时，我有点害怕。 我以为我会因此失去机会，因为这可能是不可能完成的任务。然而，当我与 Mark、Damien Duportal 和 Olivier Vernin 面谈时，他们却问我如何将 CI/CD 与 Jenkins X 集成：这真是一次奇妙的经历。我们进行了有意义的讨论，这让我感觉更舒服，也让我更容易做出决定。\n面试前15分钟，我收到了另一家公司（ Damien 和 Jean-Marc Meessen 之前恰好在这家公司工作过）的最终录用通知，当时我犹豫了一下，但结果是最好的，因为我现在仍然是 Jenkins 项目的一员，这可以说是我梦寐以求的工作。\n我还有过主持在线论坛的经验，所以社区部分的工作对我来说很熟悉。\n你使用 Jenkins 多久了？ # 我从 Jenkins X 开始，但从未接触过 Jenkins 本身。除了共享 Jenkins 的名称外，它们没有任何共同之处。 我对 Jenkins 的预想是负面的。我认为它是一个笨重、过时、复杂的 Java 程序。这些印象都是我在以前的公司里从其他使用它的人那里听来的。然而，当我开始使用 Jenkins 后，这种对比简直是天壤之别。 我的先入之见是，与其他程序相比，它既笨重又缓慢。我并不是唯一一个认为 Jenkins 不一定是最好、最快或最新的项目的人，但事实证明，一旦我开始使用这个项目，我就错了。\n为什么选择 Jenkins 而不是其他项目？ # 我并不一定选择 Jenkins，因为它是我工作的主要部分。当我开始查看 Tyler、Olivier、Damien 和 Mark 为 Jenkins 基础设施所做的工作时，我意识到 Jenkins 比我想象的要完善和高效得多。 我还喜欢我们使用 Jenkins 开发和发布 Jenkins 的事实。这种用法是独一无二的，因为大多数开源工具都不具备转发成功的能力。 Jenkins 花费了大量的时间和精力，以配合开发人员的流程和功能。在我看来，这是 Jenkins 成功的主要原因之一。Jenkins 还集成了 Terraform、Puppet、Kubernetes 和 Helmfile 等其他工具，但 Jenkins 仍然是这些工具的协调者。\n对我来说，为 Jenkins 工作是我的最高成就，因为我一直喜欢创建和开发工具，即使不是开发 Jenkins 的核心。\n加入 Jenkins 项目后，你看到 Jenkins 有哪些增长？ # 我们已经有越来越多的实例被定义为代码。因此，我们可以重新创建任何实例，而无需手动配置设置，这大大提高了我们的恢复能力。我们还在慢慢实现将 ci.jenkins.io 定义为代码并进行管理。\n你对 Jenkins 的哪方面特别感兴趣？ # 现在，我正在重构 Jenkins 控制器和代理的官方 Docker 镜像的构建过程，这让我感到非常有趣。 我也喜欢在贡献者方面工作，因为这就像一个谜题，知道我需要达到什么目标以及我的起点会让我更愉快。\n什么样的贡献最成功或最有影响力？ # Basil Crow 提出了使用 SQLite 代替文件系统的有趣想法。改用 JDK 17 非常成功，随着 JDK 21 的推出，Jenkins 可以在更新的平台上运行，并跟上时代的进步。 由于我们喜欢让 Jenkins 基础架构保持领先（例如始终运行最新的 Jenkins Weekly），下一步将引入 JDK22。插件健康分数标识对于可视化整个插件生态系统的健康状况非常有用。\n给新开发人员和开源社区新成员的建议 # 首先要提醒我的是项目的庞大性，并指示我一开始要选择一件事来专注。\n不要犹豫，大胆尝试；开源意味着对所有人开放。不要害怕提交 pull request，它并不需要完美无缺。\n你可能最终会喜欢它，并继续提交贡献！\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-04-21","externalUrl":null,"permalink":"/posts/jenkins-contributors/","section":"Posts","summary":"本文介绍了 Hervé Le Meur 如何通过 Jenkins-X 社区的贡献，最终成为 Jenkins 基础设施团队的一名 SRE，并分享了他的经历和对 Jenkins 的看法。","title":"《分享》通过 Jenkins-X 社区最终进入到 Jenkins 基础设施团队成为 SRE 的经历","type":"posts"},{"content":"","date":"2024-04-21","externalUrl":null,"permalink":"/tags/contributor/","section":"标签","summary":"","title":"Contributor","type":"tags"},{"content":"相信大家最近都总会看到这样或那样的新闻：哪个科技巨头又裁员了。裁员潮似乎成为了这个时代的常态，让许多打工人感到焦虑和不安。\n身在大连的我确实深有感触，外企和私企都有在裁员，与前两年相比，岗位越来越少，失业的人越来越多，因此想找到一个满意的岗位将会变得越来越难。\n再加上随着人工智能（AI）的发展，作为 DevOps 打工人常常在想，需要掌握哪些关键技能和能力才能让自己保持竞争力。\n以下是我认为在 2024 年至关重要的关键技能和能力：\n深入理解 DevOps 理念和工具：\n熟练掌握持续集成/持续交付（CI/CD）工具和流程。如 Jenkins，GitLab CI/CD，GitHub Actions。 能够设计和优化自动化部署流程，包括自动化测试、构建和发布。 精通容器化技术，如 Docker，以及容器编排工具，如 Kubernetes，Helm。 云计算和基础设施：\n对主流云服务提供商（如 AWS、Azure、Google Cloud）的基础设施和服务有深入了解。 能够进行云原生架构设计和实施，包括使用云原生服务和技术。 自动化和编程能力：\n精通至少一种编程语言（如 Python、Go、Java 等），能够编写脚本和工具来实现自动化。 对基础架构即代码（IaC）工具有熟练掌握，例如 Terraform、Ansible 等。 监控和日志管理：\n熟悉监控和日志管理工具，能够建立完善的监控系统和日志分析平台。 掌握应用性能监控和故障排除技术。如 Prometheus，Grafana，ELK Stack。 安全和合规性：\n了解容器和云安全最佳实践，能够设计安全的部署架构。 理解数据隐私和合规性要求，能够实施符合法规的解决方案。如 HashiCorp Vault，Chef InSpec。 持续学习和技术更新：\n持续关注新技术和行业趋势，参与培训和研讨会，多于同行交流。 不断学习和提升自身的技能，保持适应快速变化的技术环境。 团队协作和沟通能力：\n良好的团队合作和沟通能力，能够与开发团队、运维团队和其他利益相关者有效地协作。 熟练使用版本控制系统和协作工具。 问题解决和创新思维：\n具备快速定位和解决问题的能力，善于思考创新解决方案。 鼓励并参与团队中的持续改进和创新活动。 业务理解和领导能力（对于高级岗位）：\n具备对业务需求的理解和洞察，能够为业务提供技术支持和解决方案。 如果担任领导职务，需要具备领导团队和推动项目的能力。 只有通过不断学习和拓展技能，保持对最新技术的了解，注重团队协作和创新，才能够在市场不好，AI崛起的环境中继续保持竞争力。\n最后，希望大家都能在 2024 年工作顺利，不被裁员；裁员 N+x (x\u0026gt;=1)，并顺利过渡到下一份更好的工作 💪\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-04-08","externalUrl":null,"permalink":"/posts/devops-skills-2024/","section":"Posts","summary":"本文介绍了在2024年DevOps工程师需要掌握的关键技能和能力，以应对裁员潮和人工智能的挑战，保持竞争力。","title":"2024年如何保持竞争力：DevOps工程师的关键技能","type":"posts"},{"content":" 什么是 Reusable Workflows # 如果你使用过 GitHub Actions，那么你一定要了解 Reusable Workflows 这个特性，它允许你定义工作流并在多个仓库中重复使用它们。\nGitHub Actions 是 GitHub 自家的 CI/CD 工具。其他主流的 CI/CD 工具还有 Jenkins，Azure DevOps，Travis CI 等。\n通过 GitHub Reusable Workflows 你可以将常见的工作流程定义在单独的 Git 仓库，然后在其他仓库中引用这些工作流，而无需在每个仓库中重复定义它们，这样做带来的好处包括：\n一致性: 确保你的团队或组织在不同的仓库中使用相同的标准化工作流程，保持一致性。 维护性: 对工作流程进行更改或更新你只需在一个地方进行修改，而不必修改多个仓库中的代码。 重用性: 将通用的工作流程分离出来，在需要时可以在任何项目中重用，提高了代码的重用性和可维护性。 总的来说，GitHub Reusable Workflows 使得在 GitHub Actions 中管理和组织工作流程变得更加灵活和可维护。\n如何使用 Reusable Workflows # 使用 GitHub Reusable Workflows 可以让你在 .github 或是其他仓库创建一个工作流，然后在其他仓库中调用该工作流。\n以下是使用 GitHub Reusable Workflows 的一般步骤：\n创建可重用工作流程：\n在你的 GitHub 账户下创建一个新的仓库用于存储你的可重用工作流程。 在仓库中创建一个名为 .github/workflows 的目录（如果不存在的话）。 在该目录下创建一个 YAML 文件，用于定义你的工作流程。 定义参数化工作流程（可选）：\n如果你希望你的工作流程是可参数化的，可以在 workflows 中使用 inputs 关键字定义参数。 将工作流程提交到仓库：\n将你创建的工作流程 YAML 文件提交到仓库，并确保它位于 .github/workflows 目录中。 在其他仓库中使用工作流程：\n打开你希望使用该工作流程的其他仓库。在 .github/workflows 目录下创建一个 YAML 文件，指向你之前创建的可重用工作流程的 YAML 文件。 提交更改并触发工作流程：\n将对仓库的更改提交到 GitHub，并将它们推送到远程仓库。 GitHub 将自动检测到新的工作流程文件，并根据触发器（例如 push、pull_request 等）来触发工作流程的执行。 以下是一个简单的示例，演示如何创建和使用可重用工作流程：\n假设你在名为 reuse-workflows-demo 的仓库中 .github/workflows 目录下创建了一个名为 build.yml 的工作流程文件，用于构建你的项目。该文件的内容如下：\n如果不在 .github/workflows 目录下，你会遇到这个错误 invalid value workflow reference: references to workflows must be rooted in '.github/workflows'\nname: Build on: workflow_call: inputs: target: required: true type: string default: \u0026#34;\u0026#34; jobs: build: strategy: matrix: target: [dev, stage, prod] runs-on: ubuntu-latest steps: - name: inputs.target = ${{ inputs.target }} if: inputs.target run: echo \u0026#34;inputs.target = ${{ inputs.target }}.\u0026#34; - name: matrix.targe = ${{ matrix.target }} if: matrix.target run: echo \u0026#34;matrix.targe = ${{ matrix.target }}.\u0026#34; 然后，在你的其他仓库中的 .github/workflows 目录下你可以创建一个 workflow build.yml 指向该文件，例如：\nname: Build on: push: pull_request: workflow_dispatch: jobs: call-build: uses: shenxianpeng/reuse-workflows-demo/.github/workflows/build.yml@main with: target: stage 更多关于 Reusable Workflows 的实际项目示例可以参考 cpp-linter 组织下的 .github 仓库。\n# cpp-linter/.github/.github/workflows . ├── codeql.yml ├── main.yml ├── mkdocs.yml ├── pre-commit.yml ├── py-coverage.yml ├── py-publish.yml ├── release-drafter.yml ├── snyk-container.yml ├── sphinx.yml └── stale.yml GitHub Reusable Workflows 的 7 点最佳实践： # 模块化设计：将工作流程分解为小的、可重用的模块，每个模块专注于执行特定的任务。这样可以提高工作流程的灵活性和可维护性，并使其更容易被重用和组合。 参数化配置：使工作流程能够接受参数，以便在每次使用时进行配置。这样可以使工作流程更通用，适应不同项目和环境的需求。 版本控制：确保你的可重用工作流程受到版本控制，并定期更新以反映项目需求的变化。可以使用 GitHub 的分支或 tag 来管理工作流程的不同版本，并在需要时轻松切换或回滚。 文档和注释：为工作流程提供清晰的文档和注释，以帮助其他开发人员理解其目的和操作步骤。注释关键步骤的目的和实现细节，以便其他人可以轻松理解和修改工作流程。 安全性：谨慎处理包含敏感信息（如凭据、密钥等）的工作流程文件，确保它们不会意外地泄露。将敏感信息存储在 GitHub 的 Secrets 中，并在工作流程中使用 Secrets 来访问这些信息。 测试和验证：在引入可重用工作流程到项目之前，进行测试和验证，确保它们能够正确地集成和执行所需的操作。可以在单独的测试仓库中模拟和测试工作流程，以确保其正确性和可靠性。 优化性能：优化工作流程的性能，尽量减少不必要的步骤和资源消耗，以确保工作流程能够在合理的时间内完成任务，并尽量减少对系统资源的占用。 遵循这些最佳实践可以帮助你更好地利用 GitHub Reusable Workflows 并为你的项目和团队提供更高效和可维护的自动化工作流程。\nReusable Workflows 和 Jenkins Shared Library 有什么不同和相同 # 最后，说一下我对 GitHub Reusable Workflows 和 Jenkins Shared Library 的理解和总结。有一些相似之处，但也有一些区别。\n相同点：\n可重用性: 两者都旨在提供一种机制，使得可以将通用的自动化工作流程定义为可重用的组件，并在多个项目中共享和重用。 组织性: 都有助于更好地组织和管理自动化工作流程，使其易于维护和更新。 参数化: 都支持参数化，使得可以根据需要在不同的上下文中定制和配置工作流程。 不同点：\n平台: Reusable Workflows 是 GitHub Actions 的一部分，而 Shared Library 是 Jenkins 的功能。它们在不同的平台上运行，具有不同的生态系统和工作原理。 语法: Reusable Workflows 使用 YAML 语法来定义工作流程，而 Shared Library 使用 Groovy 语言来定义共享库。 易用性: Reusable Workflows 在 GitHub 平台上使用起来相对较简单，特别是对于那些已经在 GitHub 上托管代码的项目。Shared Library 则需要配置 Jenkins 服务器和相关插件，并且需要对 Jenkins 构建流程有一定的了解。 综上所述，尽管 GitHub Reusable Workflows 和 Jenkins Shared Library 都旨在提供可重用的自动化工作流程，并且具有一些相似之处，但是它们在平台、语法、易用性等方面存在显著的差异。\n具体选择使用哪种取决于你的需求、工作流程和所需要使用的平台。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-03-25","externalUrl":null,"permalink":"/posts/reusable-workflows/","section":"Posts","summary":"本文介绍了 GitHub Actions 的可重用工作流（Reusable Workflows）特性，帮助开发者和团队更高效地管理和复用 CI/CD 流程。","title":"你一定要了解的 GitHub Action 特性：可重用工作流（Reusable Workflows）","type":"posts"},{"content":"最近看到一篇非常有信息量的关于人工智能、云原生、开源的趋势报告，出自于GitHub，翻译并分享给大家，以下是报告全文。\n英文原文在这里：https://github.blog/2023-11-08-the-state-of-open-source-and-ai/?utm_source=banner\u0026amp;utm_medium=github\u0026amp;utm_campaign=octoverse\n新技术成为主流意味着什么？\nGit 于 2005 年首次发布，当我们创建 GitHub 时，Git 还是一个新的开源版本控制系统。如今，Git 已成为现代开发人员体验的基本元素 — 93% 的开发人员使用它在各地构建和部署软件。\n2023 年，GitHub 数据凸显了另一种技术如何迅速开始重塑开发者体验：人工智能。去年，越来越多的开发人员开始使用人工智能，同时也尝试构建人工智能驱动的应用程序。 Git 从根本上改变了当今的开发人员体验，现在人工智能正在为软件开发的下一步奠定基础。\n在 GitHub，我们知道开发人员喜欢边做边学，开源可以帮助开发人员更快地采用新技术，将其集成到他们的工作流程中，并构建下一代技术。开源还为几乎所有现代软件提供动力——包括大部分数字经济。当我们探索技术如何成为主流时，GitHub 在弥合开源技术实验与广泛采用之间的差距方面继续发挥着关键作用，这些技术支撑着我们软件生态系统的基础。\n在今年的报告中，我们将研究围绕人工智能、云和 Git 的开源活动如何改变开发人员体验，并日益增强对开发人员和组织的影响。\n我们发现了三大趋势:\n开发人员正在大量使用生成式人工智能进行构建。 我们看到越来越多的开发人员尝试使用 OpenAI 和其他 AI 参与者的基础模型，开源生成式 AI 项目甚至会在 2023 年进入按贡献者数量计算的前 10 个最受欢迎的开源项目。几乎所有开发人员 (92%) 都在使用或试验借助 AI 编码工具，我们期望开源开发人员能够在 GitHub 上推动下一波 AI 创新浪潮。 开发人员正在大规模运营云原生应用程序。 我们看到使用基于 Git 的基础设施即代码 (IaC) 工作流程的声明性语言有所增加，云部署的标准化程度更高，开发人员使用 Dockerfile 和容器、IaC 和其他云原生的速度急剧增加技术。 2023 年首次开源贡献者数量最多。 我们继续看到商业支持的开源项目在首次贡献者和总体贡献中占据最大份额，但今年，我们还看到生成式 AI 项目进入了首次贡献者最受欢迎的前 10 个项目。我们还看到 GitHub 上的私人项目显著增长，同比增长 38%，占 GitHub 上所有活动的 80% 以上。 在 GitHub 上构建的全球开发者社区 # 在全球范围内，开发人员正在使用 GitHub 来构建软件并进行比以往更多的协作，而且涉及公共和私人项目。这不仅证明了 Git 在当今开发者体验中的基础价值，也展示了全球开发者社区使用 GitHub 构建软件的情况。\n美国拥有 2020 万开发者，过去一年开发者增长 21%，继续拥有全球最大的开发者社区。\n但自 2013 年以来，我们不断看到其他社区在整个平台上实现了更多增长，我们预计这种情况会持续下去。 GitHub 上开发人员的全球分布显示了哪些地区拥有最多的开发人员。\n亚太地区、非洲、南美洲和欧洲的开发者社区逐年扩大，其中印度、巴西和日本处于领先地位。\n预测未来五年排名前 10 的开发者社区 # 为了了解哪些开发者社区将在未来五年内增长最快，我们根据当前的增长率进行了预测。在此标题下，我们预计到 2027 年印度将取代美国成为 GitHub 上最大的开发者社区。\n亚太地区发展最快的开发者社区 # 我们继续看到，在印度、日本和新加坡等经济中心的推动下，亚太地区出现了可观的增长。\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 表 1：2023 年开发者总数增长，较 2022 年增长百分比。\n印度的开发者社区继续实现同比大幅增长 # 在去年的 Octoverse 中，我们预测印度的开发者总数将超过美国。这仍然有望发生。印度的开发者人数同比增长 36%，2023 年有 350 万新开发者加入 GitHub。\n作为联合国支持的数字公共产品联盟的一部分，印度一直在利用开放材料（从软件代码到人工智能模型）建设数字公共基础设施，以改善数字支付和电子商务系统。以下是印度开发人员在 GitHub 上构建并贡献的开源软件 (OSS) 项目列表。\n新加坡今年是亚太地区开发者人数增长最快的国家 # 并且以开发者占总人口的比例最高而位居全球第一。新加坡国立大学计算机学院将 GitHub 纳入其课程，高增长也可能归因于该国在东南亚的监管重要性。\n由于对技术和初创公司的投资，我们还可能在明年看到日本的开发人员持续增长。\n非洲发展最快的开发者社区 # 非洲地区拥有世界上增长最快的人口和不断增加的开发人员，已被认为是有前途的科技公司中心。（例如，在肯尼亚，小学和中学必须教授编程。）\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 表 2：2023 年开发者总数增长，较 2022 年增长百分比。\n尼日利亚是 OSS 采用和技术投资的热点，其 45% 的同比增长率（全球增幅最高）反映了这一点。GitHub 上还有至少 200 个由尼日利亚开发者制作的项目集合，可以在“非洲制造”集合下找到。\n南美洲发展最快的开发者社区 # 南美洲的开发者增长率与亚太和非洲一些增长最快的开发者社区相当。\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 表 3：2023 年开发者总数增长，较 2022 年增长百分比。\n2023年，巴西的开发者人数是该地区最多的，并继续以两位数增长，同比增长30%。此前，巴西的私人和公共组织持续投资。查看巴西开发人员在 GitHub 上创建和贡献的 OSS 项目列表。\n我们还看到阿根廷和哥伦比亚的持续增长，这两个国家在过去几年中已成为组织的热门投资目标。\n欧洲发展最快的开发者社区 # 整个欧洲的社区开发人员总数继续增加，但他们的发展现在更接近于美国的总体发展，因为南美洲、非洲和亚太地区的社区增长超过了他们。\n开发人员数量 同比增长 01 新加坡 \u0026gt;100 万开发者 39% 02 印度 \u0026gt;1320 万开发者 36% 03 香港（特别行政区） \u0026gt;160 万开发者 35% 04 越南 \u0026gt;150 万开发者 34% 05 印度尼西亚 \u0026gt;290 万开发者 31% 06 日本 \u0026gt;280 万开发者 31% 07 菲律宾 \u0026gt;130 万开发者 31% 08 泰国 \u0026gt;857K 开发者 25% 09 韩国 \u0026gt;190 万开发者 22% 10 澳大利亚 \u0026gt;140 万开发者 21% 值得注意的是，法国的增长是在政府推动吸引更多科技初创企业之后实现的。我们还看到西班牙和意大利的增长有所上升，这说明这两个地区为支持其国内技术市场所做的努力。\n2023 年生成式 AI 爆发式增长 # 虽然生成式人工智能在 2023 年引起了轰动，但对于 GitHub 上的开发者来说，它并不是全新的。事实上，过去几年我们已经在 GitHub 上看到了几个生成式 AI 项目的出现，以及许多其他专注于 AI 的项目。\n但 2023 年的 GitHub 数据反映了这些人工智能项目如何从更面向专业的工作和研究发展到更主流的采用，开发人员越来越多地使用预先训练的模型和 API 来构建由人工智能驱动的生成应用程序。\n就在去年过半的时候，我们看到 2023 年的生成式 AI 项目数量是 2022 年全年的两倍多。 我们知道这只是冰山一角。\n随着越来越多的开发人员尝试这些新技术，我们期望他们能够推动软件开发中的人工智能创新，并继续将该技术快速发展的功能带入主流。\n开发人员越来越多地尝试人工智能模型。 在过去的几年里，我们看到开发人员使用 tensorflow/tensorflow、pytorch/pytorch 等机器学习库构建项目，而现在我们看到更多的开发人员尝试使用 AI 模型和LLM（例如 ChatGPT API）。\n保持聪明： 我们预计企业和组织也将利用预先训练的人工智能模型，特别是随着越来越多的开发人员熟悉如何使用它们进行构建。\n开源人工智能创新多种多样，顶级人工智能项目由个人开发者拥有。 分析 GitHub 上排名前 20 的开源生成式 AI 项目，其中一些顶级项目归个人所有。这表明 GitHub 上的开源项目继续推动创新，并向我们所有人展示行业的未来发展，社区围绕最令人兴奋的进步而构建。\n生成式人工智能正在推动生成式人工智能项目的个人贡献者在全球范围内大幅增长，同比增长 148%，生成式人工智能项目总数也同比增长 248%。 值得注意的是，美国、印度和日本在开发者社区中处于领先地位，其他地区（包括香港特别行政区）、英国和巴西紧随其后。\n了解生成式人工智能的开发人员数量大幅增加将对企业产生影响。 随着越来越多的开发人员熟悉构建基于人工智能的生成式应用程序，我们预计不断增长的人才库将支持寻求开发自己的基于人工智能的产品和服务的企业。\n底线： 在过去的一年里，我们看到基于基础模型（例如 ChatGPT）构建的应用程序呈指数级增长，因为开发人员使用这些 LLM 来开发面向用户的工具，例如 API、机器人、助手、移动应用程序和插件。全球开发人员正在帮助为主流采用奠定基础，而实验正在帮助组织建立人才库。\n最流行的编程语言 # 自从我们在 2019 年看到云原生开发的巨大增长以来，IaC 在开源领域也持续增长。 2023 年，Shell 和 Hashicorp 配置语言 (HCL) 再次成为开源项目中的顶级语言，这表明运维和 IaC 工作在开源领域越来越受到重视。\nHCL 采用率同比增长 36%，这表明开发人员正在为其应用程序利用基础设施。 HCL 的增加表明开发人员越来越多地使用声明性语言来指示他们如何利用云部署。 JavaScript 再次夺得第一大最受欢迎语言的桂冠，并且我们继续看到 Python 和 Java 等熟悉的语言逐年保持在前五名语言之列。\nTypeScript 越来越受欢迎。 今年，TypeScript 首次取代 Java，成为 GitHub 上 OSS 项目中第三大最受欢迎的语言，其用户群增长了 37%。 TypeScript 是一种集语言、类型检查器、编译器和语言服务于一体的语言，它于 2012 年推出，标志着渐进类型的到来，它允许开发人员在代码中采用不同级别的静态和动态类型。\n用于数据分析和操作的流行语言和框架显著增加。 T-SQL 和 TeX 等古老语言在 2023 年不断发展，这凸显了数据科学家、数学家和分析师如何越来越多地使用开源平台和工具。\n底线： 编程语言不再仅仅局限于传统软件开发领域。\n与 GitHub 上使用的总体最流行语言相比，我们发现 2023 年创建的项目中使用的最流行语言具有显著的一致性。一些值得注意的异常值包括 Kotlin、Rust、Go 和 Lua，它们在 GitHub 上的新项目中出现了更大的增长。\nRust 和 Lua 都以其内存安全性和效率而闻名，并且都可以用于系统和嵌入式系统编程，这可以归因于它们的增长。Go 最近的增长是由 Kubernetes 和 Prometheus 等云原生项目推动的。\n开发者活动是新技术采用的领头羊 # 2023 年初，我们庆祝了超过 1 亿开发者使用 GitHub 的里程碑 —— 自去年以来，我们看到 GitHub 上的全球开发者帐户数量增长了近 26%。更多的开发人员跨时区协作并构建软件。私人和公共存储库中的开发人员活动强调了哪些技术正在被广泛采用，以及哪些技术有望得到更广泛的采用。\n开发人员正在自动化更多的工作流程。 在过去的一年里，开发人员使用 GitHub Actions 分钟数增加了 169%，用于自动化公共项目中的任务、开发 CI/CD 管道等。\n平均而言，开发人员在公共项目中每天使用 GitHub Actions 的时间超过 2000 万分钟。随着 GitHub Marketplace 中的 GitHub Actions 数量在 2023 年突破 20,000 个大关，社区不断发展。 这凸显了开源社区对 CI/CD 和社区管理自动化的认识不断增强。 超过 80% 的 GitHub 贡献都贡献给私有存储库。 其中，私人项目贡献超过 42 亿美元，公共和开源项目贡献超过 3.1 亿美元。这些数字显示了通过免费、团队和 GitHub Enterprise 帐户在公共、开源和私人存储库中发生的活动的巨大规模。大量的私人活动表明了内部源代码的价值，以及基于 Git 的协作不仅有利于开源代码的质量，而且也有利于专有代码的质量。\n事实上，在最近 GitHub 赞助的一项调查中，所有开发人员都表示他们的公司至少采用了一些内部源实践，超过一半的开发人员表示他们的组织中有活跃的内部源文化。\nGitHub 是开发人员操作和扩展云原生应用程序的地方。 2023 年，430 万个公共和私有存储库使用 Dockerfile，超过 100 万个公共存储库使用 Dockerfile 来创建容器。过去几年，我们看到 Terraform 和其他云原生技术的使用量不断增加。 IaC 实践的增加也表明开发人员正在为云部署带来更多标准化。\n生成式人工智能进入 GitHub Actions。 人工智能在开发者社区中的早期采用和协作能力在 GitHub Marketplace 中的300 多个人工智能驱动的 GitHub Actions和40 多个 GPT 支持的 GitHub Actions中显而易见。开发人员不仅继续尝试人工智能，还通过 GitHub Marketplace 将其带入开发人员体验及其工作流程的更多部分。\n底线： 开发人员尝试新技术并在公共和私人存储库中分享他们的经验。这项相互依赖的工作凸显了容器化、自动化和 CI/CD 在开源社区和公司之间打包和发布代码的价值。\n开源的安全状况 # 今年，我们看到开发人员、OSS 社区和公司等通过自动警报、工具和主动安全措施更快地响应安全事件，这有助于开发人员更快地获得更好的安全结果。我们还看到 GitHub 上共享了负责任的 AI 工具和研究。\n更多的开发人员正在使用自动化来保护依赖关系。 与 2022 年相比，2023 年开源开发人员合并的针对易受攻击包的自动化Dependabot拉取请求数量增加了 60%，这凸显了共享社区对开源和安全性的奉献精神。得益于 GitHub 上的免费工具（例如 Dependabot、代码扫描和密钥扫描），开源社区的开发人员正在修复更多易受攻击的软件包并解决代码中的更多漏洞。\n**更多的开源维护者正在保护他们的分支机构。**受保护的分支为维护者提供了更多方法来确保其项目的安全性，我们已经看到超过 60% 的最受欢迎的开源项目 使用它们。自从我们今年早些时候在 GA 中在 GitHub 上推出存储库规则以来，大规模管理这些规则应该会变得更加容易。\n开发人员正在 GitHub 上分享负责任的 AI 工具。 在实验性生成人工智能时代，我们看到人工智能信任和安全工具的发展趋势。开发人员正在围绕负责任的人工智能、人工智能公平、负责任的机器学习和道德人工智能创建和共享工具。\n乔治城大学安全与新兴技术中心也在确定哪些国家和机构是值得信赖的人工智能研究的顶级生产者，并在 GitHub 上分享其研究代码。\n底线： 为了帮助 OSS 社区和项目保持更加安全，我们投资了 Dependabot、受保护的分支、CodeQL 和秘密扫描，免费向公共项目提供。2023 年的新采用指标显示这些投资如何成功帮助更多开源项目提高整体安全性。我们还看到软件开发人员和机构研究人员对创建和共享负责任的人工智能工具感兴趣。\n开源状态 # 2023 年，开发者为 GitHub 上的开源项目做出了总计 3.01 亿的贡献，其中包括 Mastodon 等热门项目到 Stable Diffusion 和 LangChain 等生成式 AI 项目。\n商业支持的项目继续吸引一些最开源的贡献，但 2023 年是生成式 AI 项目也进入 GitHub 上十大最受欢迎项目的第一年。说到生成式 AI，几乎三分之一拥有至少一颗星的开源项目都有一位使用 GitHub Copilot 的维护者。\n商业支持的项目继续领先。 2023 年，贡献者总数最大的项目获得了压倒性的商业支持。这是去年以来的持续趋势，microsoft/vscode、flutter/flutter 和 vercel/next.js 在 2023 年再次跻身前 10 名。\n生成式人工智能在开源和公共项目中快速发展。 2023 年，我们看到基于 AI 的生成式 OSS 项目，如 langchain-ai/langchain 和 AUTOMATIC1111/stable-diffusion-webui，在 GitHub 上按贡献者数量跃居榜首。越来越多的开发人员正在使用预先训练的人工智能模型构建法学硕士应用程序，并根据用户需求定制人工智能应用程序。\n开源维护者正在采用生成式人工智能。 几乎三分之一拥有至少一颗星的开源项目都有使用 GitHub Copilot 的维护者。这是我们向开源维护人员免费提供 GitHub Copilot 的计划，并表明生成式 AI 在开源领域的采用日益广泛。\n开发人员看到了组合包和容器化的好处。 正如我们之前指出的，2023 年有 430 万个存储库使用了 Docker。另一方面，Linux 发行版 NixOS/nixpkgs 在过去两年中一直位居贡献者开源项目的榜首。\n首次贡献者继续青睐商业支持的项目。 去年，我们发现，与其他项目相比，围绕流行的、商业支持的项目的品牌认知度吸引了更多的首次贡献者。这种情况在 2023 年继续出现，一些在 Microsoft、Google、Meta 和 Vercel 支持的首次贡献者中最受欢迎的开源项目。\n但社区驱动的开源项目（从 home-assistant/core 到 AUTOMATIC1111/stable-diffusion-webui、langchain-ai/langchain 和Significant-Gravitas/Auto-GPT）也见证了首次贡献者的活动激增。这表明，基础模型的开放实验增加了生成人工智能的可及性，为新的创新和更多合作打开了大门。\n2023 年，首次为开源项目做出贡献的贡献者数量最多。 新的开发人员通过 freeCodeCamp、First Contributions 和 GitHub Education 等计划参与到开源社区中。我们还看到大量开发人员参与了 Google 和 IBM 等公司的在线开源教育项目。\n**底线是：**开发人员正在为开源生成式人工智能项目做出贡献，开源维护者正在采用生成式人工智能编码工具，而公司则继续依赖开源软件。这些都表明，公开学习并分享新技术实验的开发人员提升了整个全球开发人员网络 - 无论他们是在公共存储库还是私人存储库中工作。\n总结 # 正如 Git 已成为当今开发人员体验的基础一样，我们现在也看到了人工智能成为主流的证据。仅在过去一年，就有高达 92% 的开发人员表示在工作内外使用基于人工智能的编码工具。在过去的一年里，GitHub 上托管的各种开源项目的人工智能实验也出现了爆炸性增长。\n我们给您留下三个要点：\nGitHub 是生成式 AI 的开发者平台。 生成式 AI 将于 2023 年从专业领域发展成为主流技术，开源活动的爆炸式增长反映了这一点。随着越来越多的开发人员构建和试验生成式 AI，他们正在使用 GitHub 进行协作和集体学习。 开发人员正在 GitHub 上大规模运行云原生应用程序。 2019 年，我们开始看到开源中使用基于容器的技术的开发人员数量大幅增加，并且越来越多的开发人员使用基于 Git 的 IaC 工作流程、容器编排和其他云原生技术的速度急剧增加2023 年。如此大量的活动表明开发人员正在使用 GitHub 来标准化他们将软件部署到云的方式。 GitHub 是开源社区、开发人员和公司构建软件的地方。 2023 年，我们看到私有存储库的数量增加了 38%，占 GitHub 上所有活动的 81% 以上。但我们看到开源社区持续增长，他们使用 GitHub 来构建未来并推动行业向前发展。数据显示新的开源开发人员的增加以及开放社区可能实现的快速创新步伐，很明显开源从未如此强大。 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-02-22","externalUrl":null,"permalink":"/posts/open-source-state/","section":"Posts","summary":"本文介绍了 GitHub 发布的 2023 年开源状况和人工智能的崛起报告，分析了开发者社区的增长、生成式 AI 的应用以及云原生技术的发展趋势。","title":"2023 年开源状况和人工智能的崛起（GitHub）","type":"posts"},{"content":"","date":"2024-02-22","externalUrl":null,"permalink":"/tags/oss/","section":"标签","summary":"","title":"OSS","type":"tags"},{"content":"作为 cpp-linter 的创建者和贡献者，我很高兴地宣布 —— cpp-linter-action 从 v2.9.0 版本开始支持 Pull Request Review 功能了 👏\n以下是 cpp-linter-action 的 release notes。\n其中 Bump cpp-linter from 1.6.5 to 1.7.1 by @dependabot in #191 是其中最重要的变化。注：cpp-linter 包是​ cpp-linter-action 的核心依赖。\n什么是 cpp-linter-action # 如果你还不了解 cpp-linter-action 可以查看我之前的文章。\n简单来说，cpp-linter-action 是 cpp-linter 组织下的一个 GitHub Action，针对 C/C++ 代码做代码格式、诊断和修复典型的编程错误。\n目前 cpp-linter-action 大概被超过 500 个开源项目所使用（闭源项目无法统计到），这其中包括 Microsoft、Linux Foundation、CachyOS、nextcloud、Jupyter 等知名组织或项目。\n可以说目前 cpp-linter-action 是 GitHub 上 C/C++ 项目的最佳 Linter 选择。\n关于 Pull Request Review 功能 # 此次新增的 Pull Request Review 功能可以直接在 cpp-linter-action 检查完成后给出 review 建议，开发者无需本地修改检查到的错误，并提交到远程。 而是可以直接点击 GitHub 上的 Commit suggestions 按钮，就可以把建议修改直接 merge 到当前的 pull request 中，省去了人为的修改和推送。\n一旦所有的 suggestions 都已经修复了，github-action bot 会自动你 approve 你的 pull request。\ncpp-linter-action 其他已经支持的功能 # 除了 Pull Request Review 功能之外，cpp-linter-action 目前还支持另外三个选项：Annotations，Thread Comment 和 Step Summary。\nGitHub Annotations # 即在指定的需要修改的代码行位置提示执行结果​。\nThread Comment # 即在 Pull Request 上以评论形式添加执行结果。​\nStep Summary # ​即在 GitHub Action job 运行界面添加并显示执行结果。\n关于本次发布背后的故事 # 我终于在大年初八的晚上孩子睡着了之后有时间坐下来写一篇文章了，来记录一下本次发布背后的故事。\n这次发布要特别感谢 cpp-linter 联合创建者 @2bndy5 的贡献。他和我素未谋面，但却与我一起“共事”了三年。 我们的相识是因为他的一次主动贡献而开始的，但一直以来的交流仅限于 GitHub 上的 issue 和 pull request 的讨论，只有不公开信息才会通过邮件来传达。\n正如 @2bndy5 的个人介绍那样：热衷于编程，喜欢 DIY 电子产品，坚持写易懂的文档。他是我认识的人当中少有的技术全面且文档十分友好的极客。\n不久前我收到了他的邮件说：因家中变故，他要休息一段时间，他没有动力坐下来写代码了，并告诉我 Pull Request Review 所有改动似乎都通过测试了。如果我想主导发布，他可以提供支持。\n在此，我想对发生在他身上的事情再次表示最深切的同情和慰问🙏\n继续他的工作我需要先认真阅读他的修改并搞清楚这部分功能，但年底了迟迟没有一个充足的时间来开始，想等着春节假期再来补作业吧。\n可是还没有到春节放假，就在腊月二十七孩子生病了，并告知需要住院治疗，因为我们发现得早，病症不严重，孩子在除夕当天恢复得很好，期待着再观察两天就可以出院了。 哎，可是意外发生了，孩子不小心胳膊被烫伤了，作为父母非常痛心，这是我永远都不想回忆的至暗时刻。总之就是雪上加霜。就这样我们从年前腊月二十七一直住院到正月初六，住了 10 天医院。 孩子出院的第二天我和妻子都生病了，可能是当放松下来，人一下子就累病了。\n在这段时间里 @2bndy5 完成了对 Pull Request Review 功能测试、文档修改和发布。他在评论里说花时间在编码上会让他短暂地逃离现实。\n终于上班的前一天我差不多恢复了体力，就迫不及待的打开 GitHub，审查并测试别人对项目的贡献代码，这次是一个 Title 是来自于德国多特蒙德大学的天体物理学家的贡献者，确实有点惊讶到我了。\n但这就是开源的有趣之处，它能让你有机会和任何高手近距离免费过招。如果你给 Linux 内核提交代码，那你极有可能得到 Linus 的指导 ：）\n最后，欢迎使用 cpp-linter 组织下的任何项目并提出您的宝贵意见、建议、或贡献代码。\n———— 于 2024 年 2 月 17 日 23:34\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-02-17","externalUrl":null,"permalink":"/posts/cpp-linter-action/","section":"Posts","summary":"本文介绍了 cpp-linter-action 的新功能：Pull Request Review，允许开发者直接在 GitHub 上提交代码修改建议，提升代码质量和协作效率。","title":"cpp-linter-action 最新版支持 Pull Request Review 功能了 👏","type":"posts"},{"content":"","date":"2024-01-21","externalUrl":null,"permalink":"/tags/apache/","section":"标签","summary":"","title":"Apache","type":"tags"},{"content":"本篇介绍的是大名鼎鼎的开源软件基金会 Apache 所使用的服务(Services)和工具(Tools)，这或许能帮助你打开视野，在选择工具的时候提供参考。如果你是一名 DevOps、SRE 或是 Infra 工程师，通过本篇文章内容结果帮助你更好的展示团队所提供的服务有哪些，以及窥探到 Apache Infra 是怎样组织和管理他们的。\nApache 是谁 # 如果你不太了解 Apache，下面是关于 Apache 的简要介绍。\nApache 是一个开源软件基金会（Apache Software Foundation，简称 ASF）的缩写。ASF 是一个非营利性的组织，致力于支持和发展开源软件项目。Apache 软件基金会通过提供法律、财务和基础设施支持，帮助开发者共同合作创建和维护开源软件。其中，Apache 软件基金会最为著名的项目之一是 Apache HTTP 服务器，也称为 Apache Web 服务器。此外，ASF 还托管了许多其他流行的开源项目，像 ECharts，Superset，Dubbo，Spark，Kafka 等等。\n服务与工具 # Apache Infra 团队维护着供 PMC（项目管理委员会）、项目提交者和 Apache 董事会使用的各种工具。这些工具中的部分工具只提供给有特定职责或角色的人员使用。其他工具，如显示 Apache 基础设施各部分状态的监控工具，则向所有人开放。\n为顶级项目（TLP）提供的服务 网站 电子邮件 ASF 自助服务平台 ASF 账户管理 支持 LDAP 的服务 项目孵化服务 ASF 项目工具 版本控制 问题跟踪和功能请求 将版本库与 Jira 票据集成 源码库发布者/订阅者服务 构建服务 产品命名 代码签名 代码质量 代码分发 虚拟服务器 在线投票 其他工具 DNS URL 短缩器 共享片段 机器列表 奇思妙想 为顶级项目（TLP）提供的服务 # 网站 # www.apache.org 这是 Apache 的主要网站。 Apache 项目网站检查器 会定期检查所有为顶级项目（TLP）提供的网站，并报告它们是否符合 Apache 的 TLP 网站政策。 这里只列出了几个挺有意思的连接，比如项目网址检查器，它会检查顶级项目是否有 License, Donate, Sponsors, Privacy 等正确的连接。\n电子邮件 # 所有新建电子邮件列表的申请都应通过自助服务系统进行。 电子邮件服务器 - QMail/QSMTPD ASF自助服务平台 # Infra 的目标之一是让 ASF 成员、PMC 和提交者有能力完成他们需要做的大部分工作，而无需向 Infra 求助。例如，自助服务平台提供了许多方便的工具，拥有 Apache 电子邮件地址的人（基本上是项目提交者、PMC 成员和 ASF 成员）可以使用这些工具：\n创建 Jira 或 Confluence 项目、Git 仓库或电子邮件列表（PMC 主席和 Infra 成员）。 编辑你的 ASF 身份或更新你的 ASF 密码。如果要更新密码，则需要访问与 Apache 帐户相关联的电子邮件帐户。重置密钥的有效期只有 15 分钟，因此请务必在收到密钥后立即使用。 同步 Git 仓库。 使用 OTP 计算器为 OTP 或 S/Key 一次性密码系统生成一次性密码（一般用于 PMC 成员）。 将 Confluence Wiki 空间存档并设置为只读。 不属于 ASF 社区但希望提交有关 ASF 项目产品的 Jira 票据的人员可使用该平台申请 Jira 账户。\nASF账户管理 # 如果你想更新账户详情或丢失了账户访问权限，ASF 账户管理可为你提供指导。\n支持LDAP的服务 # Infra 支持许多 LDAP 的 ASF 服务。你可以使用 LDAP 凭据登录这些服务。\n孵化项目服务 # Infra 支持孵化项目。\nInfra 孵化器介绍，展示了建立孵化项目的步骤。 项目或产品名称选择指南 ASF项目工具 # Infra 支持一系列工具和服务，以帮助项目开发和支持其应用程序及其社区，包括\n每个项目都可以在 Confluence 维基上使用专用空间。 如何管理项目维基空间的用户权限。 如何授予用户编辑维基空间的权限。 Reporter 提供有关项目的活动统计和其他信息，并提供编辑工具，帮助你撰写和提交项目的季度董事会报告。 你可以创建并运行项目博客。 你可以建立一个 Slack 频道，用于团队实时讨论。一旦你建立了 Slack 频道，Infra 就可以建立 Slack-Jira 桥接，这样你就可以在频道中收到新的或更新的 Jira 票据通知。 团队可以使用 ASFBot 通过 Internet Relay Chat (IRC) 进行并记录会议。不过，你必须按照 Apache 投票流程，在相应的项目电子邮件列表中对决策进行正式投票。 本地化工具。 Apache 发布审核工具 (RAT) 可帮助你确认所提议的产品发布符合 ASF 的所有要求。 ASF OAuth 系统为希望使用身份验证的服务提供了一个协调中心，而不会对存储敏感用户数据造成安全影响。许多 Apache 服务使用它来验证请求访问的用户是否是项目中的提交者，以及是否拥有对相关系统的合法访问权限。了解更多有关 Apache OAuth 的信息。 版本控制 # Apache 提供并由 Infra 维护代码库，Apache 项目可使用这些代码库来保证项目代码的安全、团队成员的可访问性以及版本控制。\n关于使用 【Git 的信息](https://infra.apache.org/git-primer.html)\nSVN 代码库的只读 Git 镜像 可写的 Git 代码库 Apache 与 GitHub GitHub 仓库的访问角色 关于使用 Subversion 的信息\nSubversion (SVN) 版本库 ViewVC（SVN 主版本库的浏览器界面） 问题跟踪和功能请求 # ASF 支持以下用于跟踪问题和功能请求的选项： * Jira * GitHub 问题跟踪功能\n由于历史原因，一些项目使用 Bugzilla。我们将继续支持 Bugzilla，但不会为尚未使用它的项目设置。\nApache Allura 是另一个问题跟踪选项。如果你觉得它可以满足你的项目需求，请通过 users@allura.apache.org 邮件列表直接咨询 Allura 项目。\n请参阅 issues.apache.org，查看各项目使用的问题列表。\n以下是为你的项目申请 bug 和问题跟踪器的方法。\n以下是撰写优秀错误报告的指南。\n将你的版本库与Jira票据集成 # Infra 可以为你的项目激活 Subversion 和 Git 与 Jira 票据的集成。\n源码库发布者/订阅者服务 # SvnPubSub PyPubSub 构建服务 # Apache 支持并模拟持续集成和持续部署（或 CI/CD）。ASF 构建和支持的服务页面提供了有关 ASF 提供和/或支持的 CI 服务的信息和链接。\n其他可考虑的工具:\nTravis CI Appveyor 产品命名 # 请参阅产品名称选择指南\n代码签名 # 数字证书源码库发布者/订阅者服务\n请求访问 Digicert 代码签名服务 使用 Digicert 通过苹果应用程序商店发布\n关于代码签名和发布的更多信息\n代码质量 # SonarCloud 是一款代码质量和安全工具，开源项目可免费使用。它允许对代码质量进行持续检查，因此你的项目可以通过对代码进行静态分析来执行自动审查，以检测 20 多种编程语言中的错误、代码气味和安全漏洞。\n你可以检查许多 Apache 项目软件源的状态。\n有关在 ASF 项目中使用 SonarCloud 的指导，请点击此处。\n代码分发 # 使用 ASF Nexus Repository Manager 浏览和审查 ASF 项目的代码发布。\n发布 # 当前发布 历史发布存档 Rsync 分发镜像 Nexus 虚拟服务器 # Infra 可为项目提供 Ubuntu 虚拟机。\n虚拟机策略 申请虚拟机的流程 使用nightlies.a.o # nightlies.a.o 如其名称所示，是一种 \u0026ldquo;短期 \u0026ldquo;存储解决方案。请参阅 nightlies 使用政策。\n在线投票 # 项目可使用 Apache STeVe 投票系统实例（不使用时离线）。工具名称指的是作为投票选项之一的单一可转移投票系统。为 Infra 开立 Jira 票单，以便为你的项目使用 STeVe 做好准备。\n其他工具 # DNS # Infra 管理在 Namecheap 注册的 ASF DNS。\nURL短缩器 # URL 短缩器\n分享代码片段 # Paste 是一项服务，ASF 成员可以发布代码片段或类似的文件摘要，以说明代码问题或供重复使用，通常是与其他项目成员共享。你可以以纯文本形式发布内容，也可以以多种编码和格式发布内容。\n机器列表 # 主机密钥和指纹\n奇思妙想 # Apache Whimsy 自称为 \u0026ldquo;以易于使用的方式提供有关 ASF 和我们项目的组织信息，并帮助 ASF 实现企业流程自动化，使我们众多的志愿者能够更轻松地处理幕后工作\u0026rdquo;。\nWhimsy 有许多对项目管理委员会和个人提交者有用的工具，例如提交者搜索。\n总结 # 以上就是 Apache 开源软件基金会用到的一些服务和工具，总体的感觉就是写的很全面，并且每个连接都对应着完整的文档，这也是这种开源协作方式最重要的地方：通读文档。另外这种组织方式对于想参与的人来说很清晰，值得学习。\n另外我们看到了一些常见的服务和工具，像是 Jira，Confluence，Slack，Git，GitHub，SonarCloud，Digicert，Nexus。 也看到了不太常见的工具，像在 CI 工具上的选择是 Travis CI 和 Appveyor。 还有一些有意思的工具，像是 URL缩短器，代码片段分享，奇思妙想等工具，从访问的网址来看它们是部署在内部。 由于历史原因，还有项目还在使用 Bugzilla 和 SVN 等工具。 以上 Apache 所使用的服务和工具，借用理财中风险评估等级划分是属于稳健型，而非一味的追求“新”、“开源”和“免费”。\n为了文章的可读性，本文做了部分修改和删减。原文在这里。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2024-01-21","externalUrl":null,"permalink":"/posts/apache-services-and-tools/","section":"Posts","summary":"本篇介绍的是大名鼎鼎的开源软件基金会 Apache 所使用的服务(Services)和工具(Tools)，这或许能帮助你打开视野，在选择工具的时候提供参考。","title":"看看顶级的开源组织都在用哪些服务和工具","type":"posts"},{"content":"时间过得很快，2023 年转瞬即逝。如果不记录下自己在这一年里发生的事情，过不了多久就很难回想起来这一年都发生过什么。\n因此按照惯例还是要先回顾 2023 年，然后展望 2024 年。\n回顾 2023 # 回顾这一年，我想用三个关键词来形容生活、工作以及业余。\n生活中，是“奶爸” # 从孩子一出生就是我和我的队友独立带娃，白天我上班，她带娃；晚上下班我火速回去接班、陪玩、家务直到孩子睡觉。周末至少有一天会带孩子出去溜达。\n带娃的过程中会觉得辛苦，漫长，然而回顾这一年会发现孩子一眨眼就长大了。年初时候还只会爬、扶站；到了年末孩子已经能爬桌子能跑了，有消耗不完的能量。\n接下来就是流水账的记录了，记录一下平凡一年中的小事件：\n四月：跟公司活动去了一趟发现王国、爬了童牛岭、玩了三国杀、在渔公码头吃了海鲜自助 五月：在星海和庄河分别给孩子办了生日宴 九月：第一次带娃住酒店，去了金石滩希尔顿住了一晚、吃了一顿酒店自助早餐 十月：国庆回庄河住了一晚，吃了一顿家庭烧烤；回来去了一趟横山寺，吃了一顿自助素食餐；假期踢了一场球；办了大连森林动物园的卡开始打卡动物园 十一月：去了大连森林动物园、旅顺太阳沟、大连自然博物馆、参加了公司在钱库里的聚餐 十二月：收到非常突发的消息（以后有机会再细说）、参加组内团建、大连博物馆、陪爸爸住院检查、逛旅顺博物馆 工作中，是“最佳实践”的坚守着 # 这一年我依旧坚持 DevOps 最佳实践：\n继续拓展 Ansible Playbook 仓库的应用范围 创建了 Infrastructure as Code 仓库，用 Code 来管理 Infra 创建了 Docker Image 仓库来容器化产品，在 Bitbucket 仓库中应用了很多我的 DevOps 实践 承担了更多产品的 Build 和 Release 工作 提出并实施了 Software Supply Chain Security 想法 在团队内部分享 DevOps 实践，以 Jenkins 共享库的形式分享给其他团队 业余时间，是“开源和写作”的爱好者 # 和去年一样，业余时间我还做同样的事情：\n开源项目 cpp-linter-action 截至目前已经有 400 多个用户在使用，包括微软、Jupyter、Waybar、libvips、desktop、chocolate-doom 等知名开源项目，希望之后还有更多的想法去改进 commit-check 目前处在初期发展阶段，目前的用户很少，还需要优化它以及引入更多好的想法 写作 - 2023 年我一共更新了 20 篇博客 + 6 篇公众号文章，距离年初设定的 24（博客）+ 12（公众号）打了一些折扣 学习 - 加入的知名社区里学习和贡献这件事没有做到，有了孩子之后确实没有多少业余时间了，希望 2024 娃能早睡我能有更多自己的时间 展望 2024 # 每年的愿望或是 Flag 还是要设置的，万一实现了呢？\n希望能顺利的度过职业发展期，希望会是一个崭新的、充满挑战的开始 家人身体健康，工作和家庭的平衡，带她们多出去走走 补足在 DevOps 一些方面的不足，争取参与到实际的项目中来获得提升 以教促学，持续在博客、公众号等分享，坚持个人成长 保持运动，不论是跑步还是踢球，把体重降下来 过去的年终总结 # 2022 年终总结 2020 年终总结 2019 年终总结 2018 从测试转开发\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-12-31","externalUrl":null,"permalink":"/misc/2023-summary/","section":"Miscs","summary":"\u003cp\u003e时间过得很快，2023 年转瞬即逝。如果不记录下自己在这一年里发生的事情，过不了多久就很难回想起来这一年都发生过什么。\u003c/p\u003e","title":"2023 年终总结","type":"misc"},{"content":"如果你使用过 GitHub 发布过项目，你会知道 GitHub 可以自动生成 Release Notes。\n就像这样 GitHub 自动生成的 Release Notes。\n这个截图里的 Release Notes 内容很少，看起来还很清晰。但如果内容很多，以 Jenkinsci 组织下的 configuration-as-code-plugin 项目为例，可以看出来这里的 Release Notes 中的内容是按照标题进行分类的，假如这些内容混在一起将会非常糟糕的体验。(不要误以为这是手动进行分类的，程序员才不愿意干这种事😅)\n本文将分享针对需要对 GitHub Release Notes 的内容按照标题进行自动分类的两种方式。\n方式一：使用 GitHub 官方提供的功能 # 方式一是通过 GitHub 提供的功能对 Release Notes 进行自动分类，即在仓库下面创建配置文件 .github/release.yml。这个功能与 GitHub 的 Issue Template 和 Pull Request Template 类似。具体的配置选项可以参考官方文档\n以下我是在 commit-check-action 项目的配置\nchangelog: exclude: labels: - ignore-for-release categories: - title: \u0026#39;🔥 Breaking Changes\u0026#39; labels: - \u0026#39;breaking\u0026#39; - title: 🏕 Features labels: - \u0026#39;enhancement\u0026#39; - title: \u0026#39;🐛 Bug Fixes\u0026#39; labels: - \u0026#39;bug\u0026#39; - title: \u0026#39;👋 Deprecated\u0026#39; labels: - \u0026#39;deprecation\u0026#39; - title: 📦 Dependencies labels: - dependencies - title: Other Changes labels: - \u0026#34;*\u0026#34; 针对上面的示例，在添加了 .github/release.yml 配置文件之后，当再次生成 Release Notes 时就会自动将其内容进行自动归类（下图中的标题 📦 Dependencies 是自动添加的）\n方式二：使用 Release Drafter # 方式二是使用 Release Drafter，即在仓库创建配置文件 .github/release-drafter.yml。\n从 Release Drafter 项目提供的配置参数可以看出来它提供的功能更多，使用也更加复杂。另外它还支持将配置文件放到组织下的中央仓库 .github 来实现统一的配置、并将其共享给其他仓库。\n目前方式一 .github/release.yml 不支持通过中央仓库 .github 来实现统一的配置，详见这个讨论。\n这里还以 jenkinsci/configuration-as-code-plugin 为例看到它的 .github/release-drafter.yml 的配置。\n_extends: .github 这个配置的 _extends: .github 表示从中央仓库 .github/.github/release-drafter.yml 继承过来的配置。\n# Configuration for Release Drafter: https://github.com/toolmantim/release-drafter name-template: $NEXT_MINOR_VERSION tag-template: $NEXT_MINOR_VERSION # Uses a more common 2-digit versioning in Jenkins plugins. Can be replaced by semver: $MAJOR.$MINOR.$PATCH version-template: $MAJOR.$MINOR # Emoji reference: https://gitmoji.carloscuesta.me/ # If adding categories, please also update: https://github.com/jenkins-infra/jenkins-maven-cd-action/blob/master/action.yaml#L16 categories: - title: 💥 Breaking changes labels: - breaking - title: 🚨 Removed labels: - removed - title: 🎉 Major features and improvements labels: - major-enhancement - major-rfe - title: 🐛 Major bug fixes labels: - major-bug - title: ⚠️ Deprecated labels: - deprecated - title: 🚀 New features and improvements labels: - enhancement - feature - rfe - title: 🐛 Bug fixes labels: - bug - fix - bugfix - regression - regression-fix - title: 🌐 Localization and translation labels: - localization - title: 👷 Changes for plugin developers labels: - developer - title: 📝 Documentation updates labels: - documentation - title: 👻 Maintenance labels: - chore - internal - maintenance - title: 🚦 Tests labels: - test - tests - title: ✍ Other changes # Default label used by Dependabot - title: 📦 Dependency updates labels: - dependencies collapse-after: 15 exclude-labels: - reverted - no-changelog - skip-changelog - invalid template: | \u0026lt;!-- Optional: add a release summary here --\u0026gt; $CHANGES replacers: - search: \u0026#39;/\\[*JENKINS-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[JENKINS-$1](https://issues.jenkins.io/browse/JENKINS-$1) - \u0026#39; - search: \u0026#39;/\\[*HELPDESK-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[HELPDESK-$1](https://github.com/jenkins-infra/helpdesk/issues/$1) - \u0026#39; # TODO(oleg_nenashev): Find a better way to reference issues - search: \u0026#39;/\\[*SECURITY-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[SECURITY-$1](https://jenkins.io/security/advisories/) - \u0026#39; - search: \u0026#39;/\\[*JEP-(\\d+)\\]*\\s*-*\\s*/g\u0026#39; replace: \u0026#39;[JEP-$1](https://github.com/jenkinsci/jep/tree/master/jep/$1) - \u0026#39; - search: \u0026#39;/CVE-(\\d{4})-(\\d+)/g\u0026#39; replace: \u0026#39;https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-$1-$2\u0026#39; - search: \u0026#39;JFR\u0026#39; replace: \u0026#39;Jenkinsfile Runner\u0026#39; - search: \u0026#39;CWP\u0026#39; replace: \u0026#39;Custom WAR Packager\u0026#39; - search: \u0026#39;@dependabot-preview\u0026#39; replace: \u0026#39;@dependabot\u0026#39; autolabeler: - label: \u0026#39;documentation\u0026#39; files: - \u0026#39;*.md\u0026#39; branch: - \u0026#39;/docs{0,1}\\/.+/\u0026#39; - label: \u0026#39;bug\u0026#39; branch: - \u0026#39;/fix\\/.+/\u0026#39; title: - \u0026#39;/fix/i\u0026#39; - label: \u0026#39;enhancement\u0026#39; branch: - \u0026#39;/feature\\/.+/\u0026#39; body: - \u0026#39;/JENKINS-[0-9]{1,4}/\u0026#39; 以上是中央仓库的 .github/.github/release-drafter.yml 配置，可以看到 Jenkins 官方使用了很多特性，比如模板、替换、自动加 label 等，需要在通读 Release Drafter 的文档之后能更好的理解和使用。\n总结 # 以上两种方式都可以帮助你在自动生成 Release Notes 的时候自动进行标题分类，但两者有一些如下差别，了解它们可以帮助你更好的进行选择。\nGitHub 官方提供的方式更容易理解和配置，可以满足绝大多数的项目需求。主要的不足是不支持从中央仓库 .github 中读取 .github/release.yml。 Release Drafter 提供了更为强大的功能，比如模板、排序、替换、自动对 pull request 加 label 等等，尤其是可以通过中央仓库来设置一个模板，其他项目来继承其配置。 如果是大型的开源组织，Release Drafter 是更好的选择，因为它提供了强大的功能以及支持继承中央仓库配置。如果是个人项目，GitHub 官方提供的方式基本满足需求。\n以上就是我对两个生成 GitHub Release Notes 并进行自动分类的分享。\n如果有任何疑问或建议欢迎在评论区留言。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-12-27","externalUrl":null,"permalink":"/posts/automatic-categorize-release-notes/","section":"Posts","summary":"本文将分享针对需要对 GitHub Release Notes 的内容按照标题进行自动分类的两种方式。","title":"如何把 GitHub Release Notes 按照 New features、Bug Fixes ... 进行自动分类","type":"posts"},{"content":"有时候，我们并不希望 Jenkins Pipeline 因为某个特定错误而直接失败。\n这时可以使用 catchError 来捕获错误，并将阶段（stage）或构建（build）结果更新为 SUCCESS、UNSTABLE 或 FAILURE（如果需要的话）。\nJenkinsfile 示例 # pipeline { agent { node { label \u0026#39;linux\u0026#39; } } stages { stage(\u0026#39;Catch Error\u0026#39;) { steps { catchError(buildResult: \u0026#39;UNSTABLE\u0026#39;, stageResult: \u0026#39;UNSTABLE\u0026#39;, message: \u0026#39;abc: command not found\u0026#39;) { sh \u0026#34;abc\u0026#34; } } } } } Pipeline 输出示例 # 17:14:07 [Pipeline] Start of Pipeline 17:14:08 [Pipeline] node 17:14:08 Running on linux in /agent/workspace/Stage-Job/catch-error 17:14:08 [Pipeline] { 17:14:08 [Pipeline] stage 17:14:08 [Pipeline] { (Catch Error) 17:14:08 [Pipeline] catchError 17:14:08 [Pipeline] { 17:14:08 [Pipeline] sh 17:14:08 + abc 17:14:08 /agent/workspace/Stage-Job/catch-error@tmp/durable-303b03ca/script.sh: line 1: abc: command not found 17:14:08 [Pipeline] } 17:14:08 ERROR: abc: command not found 17:14:08 ERROR: script returned exit code 127 17:14:08 [Pipeline] // catchError 17:14:08 [Pipeline] } 17:14:08 [Pipeline] // stage 17:14:08 [Pipeline] } 17:14:08 [Pipeline] // node 17:14:08 [Pipeline] End of Pipeline 17:14:08 Finished: UNSTABLE 转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2023-12-15","externalUrl":null,"permalink":"/posts/jenkins-catch-error/","section":"Posts","summary":"介绍如何在 Jenkins Pipeline 中使用 \u003ccode\u003ecatchError\u003c/code\u003e 捕获特定错误，并在不中断整个构建的情况下更新阶段或构建结果，从而实现更灵活的错误处理。","title":"如何让 Jenkins Pipeline 在特定错误发生时不中断失败","type":"posts"},{"content":"","date":"2023-12-02","externalUrl":null,"permalink":"/tags/witness/","section":"标签","summary":"","title":"Witness","type":"tags"},{"content":" 为什么软件供应链安全很重要？ # 软件供应链安全是指保障软件开发中涉及的组件、活动与流程的安全。\n近年来，软件供应链攻击愈发频繁。SonaType 报告称，从 2019 年到 2022 年，开源软件供应链攻击增长了 700% 以上。\n在 Google 安全博客 中，列举了许多真实的供应链攻击案例。Google 于 2021 年提出了名为 SLSA 的解决方案。\n同时，Linux Foundation 与 CNCF 等知名组织也提出了相关标准与工具，用于帮助生产可信的软件与生成证明文件（attestation）。\n基于此背景，许多组织希望将开源社区的最佳实践引入到自身的 CICD 流水线中。\n如何在 GitHub 与非 GitHub 项目中采用软件供应链安全 # 下面我将分别以 GitHub 与 Rocket Bitbucket 为例，介绍如何集成软件供应链安全。\nGitHub 项目 # 在 GitHub 上，最简单且最常用的方式是使用 slsa-github-generator。\n这是由 SLSA 官方提供的工具，用于在构建过程中生成证明文件（attestation），并将签名的证明文件上传到由 Sigstore 创建的透明日志系统 Rekor。\n示例仓库：slsa-provenance-demo\n在安装你的产品包之前，用户可先下载包与其 .intoto.jsonl 结尾的证明文件，并手动或在 CI 流水线中运行以下命令来验证构件是否被篡改：\nslsa-verifier verify-artifact test-1.0.0-py3-none-any.whl \\ --provenance-path test-1.0.0-py3-none-any.whl.intoto.jsonl \\ --source-uri github.com/shenxianpeng/slsa-provenance-demo Verified signature against tlog entry index 49728014 at URL: https://rekor.sigstore.dev/api/v1/log/entries/24296fb24b8ad77af7063689e8760fd7134f37e17251ec1d5adc16af64cb5cb579493278f7686e77 Verified build using builder \u0026#34;https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.9.0\u0026#34; at commit fb7f6df9f8565ed6fa01591df2af0c41e5573798 Verifying artifact test-1.0.0-py3-none-any.whl: PASSED PASSED: Verified SLSA provenance 非 GitHub 项目 # 对于代码托管在非 GitHub 平台的组织，可以使用 Witness（CNCF in-toto 提供的工具）。 Witness 可在构建过程中生成与验证证明文件，且容易集成到现有构建流程中。\n只需在构建命令中加入 witness 命令，即可在生成构建产物的同时生成证明文件、签名策略文件（policy-signed.json）以及公钥文件。\n参考示例仓库：witness-demo\n在安装产品包之前，用户可手动或在 CI 流水线中运行以下命令验证构件是否被篡改：\nwitness verify \\ -f dist/witness_demo-1.0.0-py3-none-any.whl \\ -a witness-demo-att.json \\ -p policy-signed.json \\ -k witness-demo-pub.pem INFO Using config file: .witness.yaml INFO Verification succeeded INFO Evidence: INFO 0: witness-demo-att.json 转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2023-12-02","externalUrl":null,"permalink":"/posts/supply-chain-security/","section":"Posts","summary":"本文介绍如何使用 SLSA 与 Witness 在 GitHub 与非 GitHub 项目中实现软件供应链安全，以提升软件开发与部署流程的安全性。","title":"如何在 GitHub 与非 GitHub 项目中采用软件供应链安全","type":"posts"},{"content":"由于近些年针对软件的供应链的攻击越来越频繁，因此 Google 在 2021 提出的解决方案是软件工件供应链级别（Supply chain Levels for Software Artifacts，\u0026ldquo;SLSA\u0026rdquo;）\n本篇将介绍在非 GitHub 生态系统中，我们如何生成和验证软件工件的来源，从而提高你的项目的 SLSA Level。\nWitness 是一个可插拔的软件供应链风险管理框架，它能自动、规范和验证软件工件出处。它是 in-toto 是 CNCF 下的项目之一。它的最初作者是 Testifysec，后来捐赠给了 in-toto。\n什么是 Witness # Witness 是一个可插拔的供应链安全框架，可创建整个软件开发生命周期（SDLC）的证据（Provenance）跟踪，确保软件从源代码到目标的完整性。它支持大多数主要的 CI 和基础架构提供商，是确保软件供应链安全的多功能、灵活的解决方案。\n安全 PKI (Public Key Infrastructure - 公钥基础设施)分发系统的使用和验证 Witness 元数据的能力进一步增强了流程的安全性，并有助于减少许多软件供应链攻击向量。\nWitness 的工作原理是封装在持续集成流程中执行的命令，为软件开发生命周期（SDLC）中的每个操作提供证据跟踪，这样就可以详细、可验证地记录软件是如何构建的、由谁构建以及使用了哪些工具。\n这些证据可用于评估政策合规性，检测任何潜在的篡改或恶意活动，并确保只有授权用户或机器才能完成流程中的某一步骤。\n总结 - Witness 可以做什么\n验证软件由谁构建、如何构建以及使用了哪些工具 检测任何潜在的篡改或恶意活动 确保只有经授权的用户或机器才能完成流程的每一步 分发证词（Attestations）和策略（Policy） 如何使用 Witness # 主要分三步：\nwitness run - 运行提供的命令并记录有关执行的证词。 witness sign - 使用提供的密钥签署提供的文件。 witness verify - 验证 witness 策略。 快速开始 # 这是我创建的 Witness Demo 仓库用于演示 witness 的工作流程，具体可以根据如下步骤进行。\n准备环境 # 安装 witnesss，下载 demo 项目\nbash \u0026lt;(curl -s https://raw.githubusercontent.com/in-toto/witness/main/install-witness.sh) Latest version of Witness is 0.1.14 Downloading for linux amd64 from https://github.com/in-toto/witness/releases/download/v0.1.14/witness_0.1.14_linux_amd64.tar.gz expected checksum: f9b67ca04cb391cd854aec3397eb904392ff689dcd3c38305d38c444781a5a67 file checksum: f9b67ca04cb391cd854aec3397eb904392ff689dcd3c38305d38c444781a5a67 witness v0.1.14-aa35c1f Witness v0.1.14 has been installed at /usr/local/bin/witness git clone https://github.com/shenxianpeng/witness-demo.git 创建钥匙对 # openssl genpkey -algorithm ed25519 -outform PEM -out witness-demo-key.pem openssl pkey -in witness-demo-key.pem -pubout \u0026gt; witness-demo-pub.pem 准备 Witness 配置文件 .witness.yaml # run: signer-file-key-path: witness-demo-key.pem trace: false verify: attestations: - \u0026#34;witness-demo-att.json\u0026#34; policy: policy-signed.json publickey: witness-demo-pub.pem 将构建步骤进入到证词（Attestation）中 # witness run --step build -o witness-demo-att.json -- python3 -m pip wheel --no-deps -w dist . INFO Using config file: .witness.yaml INFO Starting environment attestor... INFO Starting git attestor... INFO Starting material attestor... INFO Starting command-run attestor... Processing /tmp/witness-demo Building wheels for collected packages: witness-demo Running setup.py bdist_wheel for witness-demo: started Running setup.py bdist_wheel for witness-demo: finished with status \u0026#39;done\u0026#39; Stored in directory: /tmp/witness-demo/dist Successfully built witness-demo INFO Starting product attestor... 即在之前的 build command 中加入 witness run 的相关命令。\n查看已签名的 Attestation 的验证数据 # 因为 Attestation 数据是进行的 base64 编码，因此需要解码进行查看\ncat witness-demo-att.json | jq -r .payload | base64 -d | jq # 以下是部分输出 { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/command-run/v0.1\u0026#34;, \u0026#34;attestation\u0026#34;: { \u0026#34;cmd\u0026#34;: [ \u0026#34;python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;pip\u0026#34;, \u0026#34;wheel\u0026#34;, \u0026#34;--no-deps\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;dist\u0026#34;, \u0026#34;.\u0026#34; ], \u0026#34;stdout\u0026#34;: \u0026#34;Processing /tmp/witness-demo\\nBuilding wheels for collected packages: witness-demo\\n Running setup.py bdist_wheel for witness-demo: started\\n Running setup.py bdist_wheel for witness-demo: finished with status \u0026#39;done\u0026#39;\\n Stored in directory: /tmp/witness-demo/dist\\nSuccessfully built witness-demo\\n\u0026#34;, \u0026#34;exitcode\u0026#34;: 0 }, \u0026#34;starttime\u0026#34;: \u0026#34;2023-11-29T05:15:19.227943473-05:00\u0026#34;, \u0026#34;endtime\u0026#34;: \u0026#34;2023-11-29T05:15:20.078517025-05:00\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/product/v0.1\u0026#34;, \u0026#34;attestation\u0026#34;: { \u0026#34;dist/witness_demo-1.0.0-py3-none-any.whl\u0026#34;: { \u0026#34;mime_type\u0026#34;: \u0026#34;application/zip\u0026#34;, \u0026#34;digest\u0026#34;: { \u0026#34;gitoid:sha1\u0026#34;: \u0026#34;gitoid:blob:sha1:b4b7210729998829c82208685837058f5ad614ab\u0026#34;, \u0026#34;gitoid:sha256\u0026#34;: \u0026#34;gitoid:blob:sha256:473a0f4c3be8a93681a267e3b1e9a7dcda1185436fe141f7749120a303721813\u0026#34;, \u0026#34;sha256\u0026#34;: \u0026#34;471985cd3b0d3e0101a1cbba8840819bfdc8d8f8cc19bd08add1e04be25b51ec\u0026#34; } } }, \u0026#34;starttime\u0026#34;: \u0026#34;2023-11-29T05:15:20.078579187-05:00\u0026#34;, \u0026#34;endtime\u0026#34;: \u0026#34;2023-11-29T05:15:20.081170078-05:00\u0026#34; } 创建 Policy 文件 # policy.json 用来定义或要求一个步骤由具有特定属性或满足一些特殊的值，从而要求验证 Attestation 才能成功。比如这里的 expires 字段过期了即小于当前的时间，那么在执行 witness verify 的时候就会失败。\n{ \u0026#34;expires\u0026#34;: \u0026#34;2033-12-17T23:57:40-05:00\u0026#34;, \u0026#34;steps\u0026#34;: { \u0026#34;build\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;attestations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/material/v0.1\u0026#34;, \u0026#34;regopolicies\u0026#34;: [] }, { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/command-run/v0.1\u0026#34;, \u0026#34;regopolicies\u0026#34;: [] }, { \u0026#34;type\u0026#34;: \u0026#34;https://witness.dev/attestations/product/v0.1\u0026#34;, \u0026#34;regopolicies\u0026#34;: [] } ], \u0026#34;functionaries\u0026#34;: [ { \u0026#34;publickeyid\u0026#34;: \u0026#34;{{PUBLIC_KEY_ID}}\u0026#34; } ] } }, \u0026#34;publickeys\u0026#34;: { \u0026#34;{{PUBLIC_KEY_ID}}\u0026#34;: { \u0026#34;keyid\u0026#34;: \u0026#34;{{PUBLIC_KEY_ID}}\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;{{B64_PUBLIC_KEY}}\u0026#34; } } } 更多关于 policy 的属性和设置可以参考这里：https://github.com/in-toto/witness/blob/main/docs/policy.md\n给 Policy 文件做签名 # 在签名之前需要先替换到 Policy 文件的变量\nid=`sha256sum witness-demo-pub.pem | awk \u0026#39;{print $1}\u0026#39;` \u0026amp;\u0026amp; sed -i \u0026#34;s/{{PUBLIC_KEY_ID}}/$id/g\u0026#34; policy.json pubb64=`cat witness-demo-pub.pem | base64 -w 0` \u0026amp;\u0026amp; sed -i \u0026#34;s/{{B64_PUBLIC_KEY}}/$pubb64/g\u0026#34; policy.json 然后使用 witness sign 来做签名\nwitness sign -f policy.json --signer-file-key-path witness-demo-key.pem --outfile policy-signed.json INFO Using config file: .witness.yaml 验证二进制文件是否符合政策要求 # witness verify -f dist/witness_demo-1.0.0-py3-none-any.whl -a witness-demo-att.json -p policy-signed.json -k witness-demo-pub.pem INFO Using config file: .witness.yaml INFO Verification succeeded INFO Evidence: INFO 0: witness-demo-att.json 最后 # 以上就是使用 witness 针对 Non-GitHub 项目的演示。\n如果你的项目代码是放在 GitHub 上的，目前最容易、最流行的方式就是使用 slsa-github-generator 一个由 SLSA Framework 官方提供的工具，然后使用 slsa-verifier 来验证 Provenance。具体可以参考我的上一篇文章 Python 和 SLSA 💃\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-11-30","externalUrl":null,"permalink":"/posts/witness-and-slsa/","section":"Posts","summary":"本文介绍了 Witness 的概念、工作原理以及如何使用 Witness 来生成和验证软件工件的来源，强调了其在提高软件供应链安全性方面的重要性。","title":"Witness 和 SLSA 💃","type":"posts"},{"content":"由于近些年针对软件的供应链的攻击越来越频繁，据 SonaType 的统计从 2019 年到 2022 年针对开源软件的攻击增长了 742%，因此 2021 年 Google 提出的解决方案是软件工件供应链级别（Supply chain Levels for Software Artifacts，\u0026ldquo;SLSA\u0026rdquo;）\n本篇将介绍在 Python 生态系统中，我们如何使用 SLSA 框架来生成和验证 Python 工件的来源，从而让你的 SLSA Level 从 L0/L1 到 L3。\n注意：本文介绍的是针对托管在 GitHub 上的 Python 项目。SLSA 框架可通过 GitHub Actions 来实现开箱即用，只需较少的配置即可完成。\n对于托管在非 GitHub 上的项目（例如 Bitbucket）可以尝试 Witness，下一篇我将更新关于如何使用 Witness。\n内容 # 构建纯净的Python包 生成出处证明 上传到PyPI 验证Python包的来源 文中用到的项目 下面是从维护人员到用户的端到端工作流程：从构建 Wheel package -\u0026gt; 生成出处 -\u0026gt; 验证出处 -\u0026gt; 发布到 PyPI -\u0026gt; 以及用户验证出处 -\u0026gt; 安装 wheel。接下来让我们一起来完成这其中的每一步。\n如果你想了解 Python 打包的流程或是术语可以参见Python 打包用户指南。\n构建纯净的Python包 # 构建纯 Python 包通常只有两个工件：即纯 Python Wheel Package 和源代码 distribution。可以使用命令 python3 -m build 从源代码构建。\n下面是 GitHub Actions job 定义来构建 Wheel Package 和源代码 distribution，并为每个工件创建 SHA-256 哈希值：\njobs: build: steps: - uses: actions/checkout@... - uses: actions/setup-python@... with: python-version: 3.x - run: | # 安装 build，创建 sdist 和 wheel python -m pip install build python -m build # 收集所有文件的哈希值 cd dist \u0026amp;\u0026amp; echo \u0026#34;hashes=$(sha256sum * | base64 -w0)\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT - uses: actions/upload-artifacts@... with: path: ./dist 这里将 build 完的 wheel package 上传到 GitHub Artifacts 存起来，用作后续在 “上传到PyPI” job 中使用。另外还将 dist 下的所有文件的哈希值存储在 hashes 用作后续的 provenance job 的输入。\n注意： SLSA 使用 sha265sum 的输出作为出处证明中 subject-base64 字段的输入。sha256sum 的输出是一个或多个对散列 + 名称。\n生成出处证明 # 现在我们已经构建了 sdist 和 wheel，我们可以从文件哈希生成来出处证明。\n因为我们需要将 Build 阶段的的输出作为这里生成出处的输入，因此这里使用了 needs 选项来作为 provenance job 执行的前提条件。可以看到上面生成的哈希值在这里被 subject-base64 所使用。\njobs: provenance: needs: [build] uses: slsa-framework/slsa-github-builder/.github/workflows/generator_generic_slsa3.yml@v1.9.0 permissions: # 需要检测 GitHub 操作环境 actions: read # 需要通过 GitHub OIDC 创建出处 id-token: write # 需要创建并上传到 GitHub Releases contents: write with: # 生成的 package SHA-256 哈希值 subject-base64: ${{ provenance.needs.build.output.hashes }} # 将出处文件上传到 GitHub Release upload-assets: true 你会注意到 SLSA builders 使用可重用工作流功能来证明给定的 builders 行为不能被用户或其他进程修改。\n出处证明文件是 JSON lines，以 .intoto.jsonl 结尾。*.intoto.jsonl 文件可以包含多个工件的证明，也可以在同一文件中包含多个出处证明。该 .jsonl 格式意味着该文件是一个 “JSON lines” 文件，即每行一个 JSON 文档。\n注意：这里有一点令人困惑的是 GitHub job 中的 id-token 需要 write 权限才能读取 GitHub OIDC 令牌。read 不允许你读取 OIDC\u0026hellip;🤷。有关 id-token 权限的更多信息，请参阅 GitHub 文档。\n上传到PyPI # 我们使用官方 pypa/gh-action-pypi-publish GitHub Action 将 wheel 包上传到 PyPI。\n注意：publish job 需要在 build 和 provenance 都完成后开始执行，这意味着我们可以假设 provenance job 已经为我们起草了 GitHub Release（因为 upload-assets: true 的设置），并且我们可以假设该 job 已成功。如果不先创建来 provenance 文件，我们不想将这些 wheel 包上传到 PyPI，因此我们最后上传到 PyPI。\npublish: needs: [\u0026#34;build\u0026#34;, \u0026#34;provenance\u0026#34;] permissions: contents: write runs-on: \u0026#34;ubuntu-latest\u0026#34; steps: # 下载已构建的 distributions - uses: \u0026#34;actions/download-artifact@...\u0026#34; with: name: \u0026#34;dist\u0026#34; path: \u0026#34;dist/\u0026#34; # 上传 distributions 到 GitHub Release - env: GITHUB_TOKEN: \u0026#34;${{ secrets.GITHUB_TOKEN }}\u0026#34; run: gh release upload ${{ github.ref_name }} dist/* --repo ${{ github.repository }} # 发布 distributions 到 PyPI - uses: \u0026#34;pypa/gh-action-pypi-publish@...\u0026#34; with: user: __token__ password: ${{ secrets.PYPI_TOKEN }} 验证Python包的来源 # 让我们使用一个真正的 Python 项目来验证它的出处。以 urllib3 项目为例，它在 GitHub Releases 发布了版本中包含出处证明，这里演示的是使用它的最新版本 2.1.0 。\n首先我们需要下载 slsa-verifier 用来验证出处。下载完 slsa-verifier 工具后，让我们从 PyPI 获取 urllib3 wheel 包，而不使用 pip download. 我们使用该 --only-binary 选项强制 pip 下载 wheel。\npython3 -m pip download --only-binary=:all: urllib3 Collecting urllib3 Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB) Downloading urllib3-2.1.0-py3-none-any.whl (104 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.6/104.6 kB 761.0 kB/s eta 0:00:00 Saved ./urllib3-2.1.0-py3-none-any.whl Successfully downloaded urllib3 下载软件包后，我们需要从 GitHub 版本下载出处证明。我们需要使用与包版本相同的 GitHub Release 来确保获得正确的出处证明，因此 tag 也是 2.1.0。\ncurl --location -O https://github.com/urllib3/urllib3/releases/download/2.1.0/multiple.intoto.jsonl 该出处文件的名称为 multiple.intoto.jsonl，这是一个包含多个工件证明的出处证明的标准名称。\n此时，我们当前的工作目录中应该有两个文件：wheel 和出处证明，ls 浏览一下确保已经准备好了：\nls multiple.intoto.jsonl urllib3-2.1.0-py3-none-any.whl 从这里我们可以使用 slsa-verifier 来验证出处。我们可以验证最重要的事情，即哪个 GitHub 仓库实际构建了 wheel，以及其他信息，例如 git 标签、分支和建造者 ID：\n源存储库 (--source-uri) 建造者 ID (--builder-id) Git 分支 (--source-branch) git 标签 (--source-tag)\n# 这里仅验证 wheel package 的 GitHub 仓库 slsa-verifier verify-artifact --provenance-path multiple.intoto.jsonl --source-uri github.com/urllib3/urllib3 urllib3-2.1.0-py3-none-any.whl Verified signature against tlog entry index 49513169 at URL: https://rekor.sigstore.dev/api/v1/log/entries/24296fb24b8ad77a08c2f012d69948ed5d12e8e020852bb7936ea9208d684688e5108cca859a3302 Verified build using builder \u0026#34;https://github.com/slsa-framework/slsa-github-generator/.github/workflows/generator_generic_slsa3.yml@refs/tags/v1.9.0\u0026#34; at commit 69be2992f8a25a1f27e49f339e4d5b98dec07462 Verifying artifact urllib3-2.1.0-py3-none-any.whl: PASSED PASSED: Verified SLSA provenance 成功了！🥳 我们已经验证了这个 wheel 的出处，所以现在我们可以放心的安装它，因为我们知道它是按照我们的预期构建的：\npython3 -m pip install urllib3-2.1.0-py3-none-any.whl Defaulting to user installation because normal site-packages is not writeable Processing ./urllib3-2.1.0-py3-none-any.whl Installing collected packages: urllib3 Attempting uninstall: urllib3 Found existing installation: urllib3 2.0.5 Uninstalling urllib3-2.0.5: Successfully uninstalled urllib3-2.0.5 Successfully installed urllib3-2.1.0 文中用到的项目 # 以下这些是本文使用的所有项目和工具：\nSLSA GitHub Builder slsa-framework/slsa-verifier pypa/gha-action-pypi-publish pypa/build urllib3/urllib3 英文原文：https://sethmlarson.dev/python-and-slsa\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-11-15","externalUrl":null,"permalink":"/posts/python-and-slsa/","section":"Posts","summary":"本文介绍了如何在 Python 生态系统中使用 SLSA 框架来生成和验证 Python 工件的来源，从而提升软件供应链的安全性。","title":"Python 和 SLSA 💃","type":"posts"},{"content":"","date":"2023-10-08","externalUrl":null,"permalink":"/tags/aix/","section":"标签","summary":"","title":"AIX","type":"tags"},{"content":"","date":"2023-10-08","externalUrl":null,"permalink":"/tags/xlc/","section":"标签","summary":"","title":"XLC","type":"tags"},{"content":"本文记录了从 IBM XLC 10.1 升级到 XLC 17.1（IBM Open XL C/C++ for AIX 17.1.0） 过程中遇到的 12 个问题及修复方法。\n如果你遇到了本文未涵盖的错误，欢迎在评论区分享，无论是否有解决方案。\n1. 将 cc 替换为 ibm-clang # 首先需要在全局 Makefile 中，将所有相关的 cc 替换为 ibm-clang，例如：\n- CC=cc - CXX=xlC_r - XCC=xlC_r - MAKE_SHARED=xlC_r + CC=ibm-clang + CXX=ibm-clang_r + XCC=ibm-clang_r + MAKE_SHARED=ibm-clang_r 同时，可参考 选项映射文档 进行新 Clang 选项的映射。\n2. error: unknown argument: \u0026lsquo;-qmakedep=gcc\u0026rsquo; # - GEN_DEPENDENTS_OPTIONS=-qmakedep=gcc -E -MF $@.1 \u0026gt; /dev/null + GEN_DEPENDENTS_OPTIONS= -E -MF $@.1 \u0026gt; /dev/null 3. should not return a value [-Wreturn-type] # - return -1; + return; 4. error: non-void function \u0026lsquo;main\u0026rsquo; should return a value [-Wreturn-type] # - return; + return 0; 5. error: unsupported option \u0026lsquo;-G\u0026rsquo; for target \u0026lsquo;powerpc64-ibm-aix7.3.0.0\u0026rsquo; # - LIB_101_FLAGS := -G + LIB_101_FLAGS := -shared -Wl,-G 6. Undefined symbol (libxxxx.so) # - LIB_10_FLAGS := -bexport:$(SRC)/makefiles/xxxx.def + LIB_10_FLAGS := -lstdc++ -lm -bexport:$(SRC)/makefiles/xxxx.def 7. unsupported option -qlongdouble # - drv_connect.c.CC_OPTIONS=$(CFLAGS) -qlongdouble -brtl + drv_loadfunc.c.CC_OPTIONS=$(CFLAGS) $(IDIR) -brtl 8. Undefined symbol: ._Z8u9_closei # - extern int u9_close(int fd) ; + extern \u0026#34;C\u0026#34; int u9_close(int fd) ; 9. ERROR: Undefined symbol: .pow # - CXXLIBES = -lpthread -lC -lstdc++ + CXXLIBES = -lpthread -lC -lstdc++ -lm 10. \u0026lsquo;main\u0026rsquo; (argument array) must be of type \u0026lsquo;char **\u0026rsquo; # - d_char *argv[]; + char *argv[]; 11. first parameter of \u0026lsquo;main\u0026rsquo; (argument count) must be of type \u0026lsquo;int\u0026rsquo; # - int main(char *argc, char *argv[]) + int main(int argc, char *argv[]) 12. ERROR: Undefined symbol: ._ZdaPv # - LIB_3_LIBS\t:= -lverse -llog_nosig + LIB_3_LIBS\t:= -lverse -llog_nosig -lstdc++ 转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2023-10-08","externalUrl":null,"permalink":"/posts/upgrade-xlc-10-to-xlc-17.1/","section":"Posts","summary":"本文记录了从 IBM XLC 10.1 升级到 XLC 17.1（IBM Open XL C/C++ for AIX 17.1.0）过程中遇到的问题及解决方法，共涵盖 12 个错误的修复方案。","title":"从 XLC 10.1 升级到 IBM Open XL C/C++ for AIX 17.1.0 的问题与解决方案","type":"posts"},{"content":"","date":"2023-09-11","externalUrl":null,"permalink":"/tags/artifactory/","section":"标签","summary":"","title":"Artifactory","type":"tags"},{"content":"","date":"2023-09-11","externalUrl":null,"permalink":"/tags/java/","section":"标签","summary":"","title":"Java","type":"tags"},{"content":"最近遇到了通过 Jenkins agent 无法上传 artifacts 到 Artifactory 的情况，具体错误如下：\n[2023-09-11T08:21:53.385Z] Executing command: /bin/sh -c git log --pretty=format:%s -1 [2023-09-11T08:21:54.250Z] [consumer_0] Deploying artifact: https://artifactory.mycompany.com/artifactory/generic-int-den/my-project/hotfix/1.2.0.HF5/3/pj120_bin_opt_SunOS_3792bcf.tar.Z [2023-09-11T08:21:54.269Z] Error occurred for request GET /artifactory/api/system/version HTTP/1.1: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target. [2023-09-11T08:21:54.282Z] Error occurred for request PUT /artifactory/generic-int-den/my-project/hotfix/1.2.0.HF5/3/pj120_bin_opt_SunOS_3792bcf.tar.Z;build.timestamp=1694418199972;build.name=hotfix%2F1.2.0.HF5;build.number=3 HTTP/1.1: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target. [2023-09-11T08:21:54.284Z] [consumer_0] An exception occurred during execution: [2023-09-11T08:21:54.284Z] java.lang.RuntimeException: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target [2023-09-11T08:21:54.284Z] at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:44) [2023-09-11T08:21:54.284Z] at org.jfrog.build.extractor.producerConsumer.ConsumerRunnableBase.run(ConsumerRunnableBase.java:11) [2023-09-11T08:21:54.284Z] at java.lang.Thread.run(Thread.java:745) [2023-09-11T08:21:54.285Z] Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target 这个问题产生的原因是通过 HTTPS 来上传文件的时候没有通过 Java 的安全认证。解决这个问题的办法就是重新生成认证文件，然后导入即可，具体的步骤如下。\n生成安全认证（Security Certificate）文件 # 步骤如下：\n首先生成通过浏览器打开的你的 Artifactory 网址 在网址的左侧应该有一个锁的图标，点击 Connection is secure -》Certificate is valid -》Details -》 Export 选择 DER-encoded binary, single certificate (*.der) 生成认证文件 比如我生成的安全认证文件的名字叫：artifactory.mycompany.der（名字可以任意起，只要后缀名不变即可）\n通过命令行导入安全认证 # 登录到那台有问题的 Solaris agent，上传 artifactory.mycompany.der 到指定目录下，然后找到 cacerts 的路径，执行如下命令：\nroot@mysolaris:/# keytool -import -alias example -keystore /usr/java/jre/lib/security/cacerts -file /tmp/artifactory.mycompany.der # 然后选择 yes 这时候会提示你输入密码，默认密码为 changeit，输入即可。然后重启你的 JVM 或是 VM。等再次通过该 Agent 上传 artifacts，一切恢复正常。\n可参考： https://stackoverflow.com/questions/21076179/pkix-path-building-failed-and-unable-to-find-valid-certification-path-to-requ\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-09-11","externalUrl":null,"permalink":"/posts/unable-to-find-valid-certification-path/","section":"Posts","summary":"本文介绍了如何解决 Jenkins agent 上传 artifacts 到 Artifactory 时遇到的 SSL 证书验证问题，包括生成安全认证文件和导入到 Java 的 cacerts 中。","title":"解决通过 Jenkins Artifactory plugin 上传 artifacts 失败的问题 “unable to find valid certification path to requested target”","type":"posts"},{"content":"DevOps 运动仍然是一个不断发展的领域，受到技术进步、行业趋势和组织需求等多种因素的影响。这使得很难对 DevOps 工程的未来做出具体预测。然而我认为有一些趋势可能会在来年继续影响 DevOps 领域的发展。\n云原生技术的持续采用 # 容器化、微服务和无服务器计算等云原生技术会继续在 DevOps 环境中广泛采用，这些技术为各种项目提供了许多好处，包括：\n提高可扩展性和可靠性：云原生技术可以让 DevOps 团队更轻松地构建、部署和管理可扩展且有弹性的应用程序。 更快的部署和迭代：云原生技术可以使团队更快地部署和迭代应用程序，帮助组织更快地行动并响应不断变化的业务需求。 更大的灵活性和敏捷性：云原生技术可以为 DevOps 团队提供更大的灵活性和敏捷性，使他们能够使用各种工具和平台构建和部署应用程序。 总体而言，随着组织寻求提高数字时代的效率和竞争力，采用云原生技术可能会继续成为未来几年 DevOps 的趋势。 Kubernetes 和类似的编排平台仍将是这一过程的重要组成部分，因为它们为构建、部署和管理容器化提供了一致的、基于云的环境、应用程序和基础设施。\n更加关注安全性和合规性 # 随着安全性和合规性的重要性不断增长，组织会更加重视将安全性和合规性最佳实践纳入其流程和工具中。\n未来几年，安全性和合规性在一些特定领域尤为重要：\n云安全：随着越来越多的组织采用基于云的基础设施和服务，DevOps 团队将更加需要关注云安全性和合规性，包括确保云环境配置安全、数据在传输和静态时加密以及对云资源的访问进行控制和监控。 应用程序安全性：DevOps 团队需要专注于构建安全的应用程序并确保它们以安全的方式进行测试和部署，包括实施安全编码实践、将安全测试纳入开发过程以及使用安全配置部署应用程序。 遵守法规和标准：团队需要确保他们的流程和工具符合相关法规和行业标准，包括实施控制措施来保护敏感数据，确保系统配置为以合规的方式，通过审计和评估证明合规性，以及在适用的情况下将部分责任推给云提供商。 总而言之，随着组织寻求保护其系统、数据和客户免受网络威胁并确保遵守相关法规和标准，DevOps 中对安全性和合规性的关注肯定会增加，它还将使 DevSecOps 专家 在 DevOps 团队中发挥更大的作用。\n开发和运营团队之间加强协作 # DevOps 运动的理念是将开发和运营团队聚集在一起，以便更紧密、更有效地工作。未来几年，开发和运营团队之间的协作在一些关键领域可能会特别重要：\n持续集成和交付 (CI/CD)：开发和运营团队需要密切合作，以确保有效地测试、部署和监控代码更改，并快速识别和解决任何问题。 事件管理：开发和运营团队需要合作识别和解决生产环境中出现的问题，并制定策略以防止将来发生类似问题。 容量规划和资源管理：开发和运营团队需要共同努力，以确保系统拥有必要的资源来满足需求，并规划未来的增长。 DevOps 的成功取决于开发和运营团队之间的强有力合作，这可能仍然是 2023 年的重点。\n自动化和人工智能的持续发展 # 自动化和人工智能 (AI) 可能在 DevOps 的发展中发挥重要作用。自动化工具可以帮助 DevOps 团队对重复性任务进行编程、提高效率并降低人为错误的风险，而人工智能可用于分析数据、识别模式并协助做出更明智的决策。\n人工智能可能对 DevOps 产生重大影响的一个潜在领域是预测分析领域。通过分析过去部署和性能指标的数据，人工智能算法可以识别模式并预测未来的结果，从而使团队能够更好地优化其流程并提高整体性能。 人工智能可能产生影响的另一个领域是事件管理领域。人工智能算法可用于分析日志数据并在潜在问题发生之前识别它们，从而使团队能够在出现重大事件之前主动解决出现的问题。 总体而言，DevOps 中自动化和人工智能的发展可能会带来更高效、更有效的流程，并提高应用程序和系统的性能和可靠性。然而，组织必须仔细考虑这些技术的潜在影响，并确保它们的使用方式符合其业务目标和价值观。 自动化和人工智能的实施应该成为战略的一部分：包括从一开始就需要集成到业务流程中，调整期望和目标，估计成本以及相关的风险和挑战。仅仅为了实现两者而实施并不一定会从中获益，相反，从长远来看，它可能会因维护它们而导致其他问题。\n基础设施即代码的多云支持 # 基础设施即代码 (IaC) 正在成为一种越来越流行的实践，涉及使用与管理代码相同的版本控制和协作工具来管理基础设施。这使得组织能够将其基础设施视为一等公民，并且更容易自动化基础设施的配置和管理。\n多云基础设施是指在单个组织内使用多个云计算平台，例如 AWS、Azure 和 Google Cloud。这种方法可以为组织提供更大的灵活性和弹性，因为它们不依赖于单个提供商。\n结合这两个概念，对 IaC 的多云支持是指使用 IaC 实践和工具来管理和自动化跨多个云平台的资源调配和配置的能力。这可以包括使用 IaC 定义和部署基础设施资源，例如虚拟机、网络和存储，以及管理这些资源的配置。\n使用 IaC 管理多云基础设施可以带来多种好处。它可以帮助组织在其环境中实现更高的一致性和标准化，并降低在多个云平台中管理资源的复杂性。它还可以使组织能够更轻松地在云平台之间迁移工作负载，并利用每个平台的独特功能。\n总体而言，对于寻求优化多个云平台的使用并简化多云基础设施管理的组织来说，对 IaC 的多云支持可能仍然是一个重要因素。\n更加强调多样性和包容性 # 随着技术行业继续关注多样性和包容性，DevOps 团队可能会更加重视建立多元化和包容性的团队并创造更具包容性的工作环境 - 包括：\n为团队提供具有新的非技术技能的人员，以填补深刻的技能差距。根据 《提高 IT 技能 2022》报告，IT 需要七种关键技能：流程和框架技能、人员技能、技术技能、自动化技能、领导技能、数字技能、业务技能和高级认知技能。不幸的是，大多数 IT 经理首先追求的是技术技能，而忽视了“软技能” —— 这可能是阻碍 DevOps 在组织中进一步发展的最大因素。 整个团队以及单个 DevOps 成员的技能发展。近年来，每个人都面临着考验（新冠等流行病、不确定的经济形势），并使越来越多的 IT 专业人员不确定自己的技能，并感到需要进一步的培训和发展。与此同时，根据《2022 年 IT 技能提升》报告，接受调查的 IT 组织中只有 52% 制定了正式的技能提升计划。这实际上导致全球 IT 团队面临的最大挑战是缺乏技能。 向外部团队开放，保证能力和预期质量的快速提高。无论是外包以及其他形式的合作。这还可以在优化生产成本方面带来切实的好处（这无疑是当今开展业务最重要的因素之一）。 概括 # 上述 6 个方面是我们认为 2023 年 DevOps 的主要趋势。当然，每个人都可以从自己的角度添加其他要点，例如：无服务器计算、低代码和无代码解决方案、GitOps 等。\n过往 DevOps 趋势文章 # 2022 年最值得关注的 DevOps 趋势和问答 参考 # Trends for DevOps engineering in 2023 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-09-01","externalUrl":null,"permalink":"/posts/devops-trends-2023/","section":"Posts","summary":"本文介绍了2023年DevOps领域的主要趋势，包括云原生技术的持续采用、加强安全性和合规性、开发与运营团队协作、自动化和人工智能的发展等。","title":"2023 年最值得关注的 DevOps 趋势","type":"posts"},{"content":"","date":"2023-09-01","externalUrl":null,"permalink":"/tags/kubernetes/","section":"标签","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"2023-08-29","externalUrl":null,"permalink":"/tags/troubleshooting/","section":"标签","summary":"","title":"Troubleshooting","type":"tags"},{"content":"最近，我的 CI 流水线在 AIX 7.1 上突然无法运行，出现如下错误：\nCaused by: javax.net.ssl.SSLHandshakeException: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath. 点击查看详细的失败日志 22:13:30 Executing command: /bin/sh -c git log --pretty=format:%s -1 22:13:36 [consumer_0] Deploying artifact: https://artifactory.company.com/artifactory/generic-int-den/myproject/PRs/PR-880/1/myproject_bin_rel_AIX_5797b20.tar.Z 22:13:36 Error occurred for request GET /artifactory/api/system/version HTTP/1.1: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error. 22:13:36 Error occurred for request PUT /artifactory/generic-int-den/myproject/PRs/PR-880/1/cpplinter_bin_rel_AIX_5797b20.tar.Z;build.timestamp=1693273923987;build.name=PR-880;build.number=1 HTTP/1.1: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error. 22:13:36 [consumer_0] An exception occurred during execution: 22:13:36 java.lang.RuntimeException: javax.net.ssl.SSLHandshakeException: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:44) 22:13:36 at org.jfrog.build.extractor.producerConsumer.ConsumerRunnableBase.run(ConsumerRunnableBase.java:11) 22:13:36 at java.lang.Thread.run(Thread.java:785) 22:13:36 Caused by: javax.net.ssl.SSLHandshakeException: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.jsse2.j.a(j.java:3) 22:13:36 at com.ibm.jsse2.as.a(as.java:213) 22:13:36 at com.ibm.jsse2.C.a(C.java:339) 22:13:36 at com.ibm.jsse2.C.a(C.java:248) 22:13:36 at com.ibm.jsse2.D.a(D.java:291) 22:13:36 at com.ibm.jsse2.D.a(D.java:217) 22:13:36 at com.ibm.jsse2.C.r(C.java:373) 22:13:36 at com.ibm.jsse2.C.a(C.java:352) 22:13:36 at com.ibm.jsse2.as.a(as.java:752) 22:13:36 at com.ibm.jsse2.as.i(as.java:338) 22:13:36 at com.ibm.jsse2.as.a(as.java:711) 22:13:36 at com.ibm.jsse2.as.startHandshake(as.java:454) 22:13:36 at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:436) 22:13:36 at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384) 22:13:36 at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) 22:13:36 at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376) 22:13:36 at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) 22:13:36 at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) 22:13:36 at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) 22:13:36 at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) 22:13:36 at org.apache.http.impl.execchain.ServiceUnavailableRetryExec.execute(ServiceUnavailableRetryExec.java:85) 22:13:36 at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) 22:13:36 at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) 22:13:36 at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) 22:13:36 at org.jfrog.build.client.PreemptiveHttpClient.execute(PreemptiveHttpClient.java:76) 22:13:36 at org.jfrog.build.client.PreemptiveHttpClient.execute(PreemptiveHttpClient.java:64) 22:13:36 at org.jfrog.build.client.JFrogHttpClient.sendRequest(JFrogHttpClient.java:133) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.JFrogService.execute(JFrogService.java:112) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.artifactory.services.Upload.execute(Upload.java:77) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.artifactory.ArtifactoryManager.upload(ArtifactoryManager.java:267) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.client.artifactory.ArtifactoryManager.upload(ArtifactoryManager.java:262) 22:13:36 at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:39) 22:13:36 ... 2 more 22:13:36 Caused by: com.ibm.jsse2.util.h: PKIX path building failed: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.jsse2.util.f.a(f.java:107) 22:13:36 at com.ibm.jsse2.util.f.b(f.java:143) 22:13:36 at com.ibm.jsse2.util.e.a(e.java:6) 22:13:36 at com.ibm.jsse2.aA.a(aA.java:99) 22:13:36 at com.ibm.jsse2.aA.a(aA.java:112) 22:13:36 at com.ibm.jsse2.aA.checkServerTrusted(aA.java:28) 22:13:36 at com.ibm.jsse2.D.a(D.java:588) 22:13:36 ... 29 more 22:13:36 Caused by: java.security.cert.CertPathBuilderException: PKIXCertPathBuilderImpl could not build a valid CertPath.; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.engineBuild(PKIXCertPathBuilderImpl.java:422) 22:13:36 at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:268) 22:13:36 at com.ibm.jsse2.util.f.a(f.java:120) 22:13:36 ... 35 more 22:13:36 Caused by: java.security.cert.CertPathValidatorException: The certificate issued by CN=DigiCert Global Root G2, OU=www.digicert.com, O=DigiCert Inc, C=US is not trusted; internal cause is: 22:13:36 java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.security.cert.BasicChecker.\u0026lt;init\u0026gt;(BasicChecker.java:111) 22:13:36 at com.ibm.security.cert.PKIXCertPathValidatorImpl.engineValidate(PKIXCertPathValidatorImpl.java:199) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.myValidator(PKIXCertPathBuilderImpl.java:749) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.buildCertPath(PKIXCertPathBuilderImpl.java:661) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.buildCertPath(PKIXCertPathBuilderImpl.java:607) 22:13:36 at com.ibm.security.cert.PKIXCertPathBuilderImpl.engineBuild(PKIXCertPathBuilderImpl.java:368) 22:13:36 ... 37 more 22:13:36 Caused by: java.security.cert.CertPathValidatorException: Certificate chaining error 22:13:36 at com.ibm.security.cert.CertPathUtil.findIssuer(CertPathUtil.java:316) 22:13:36 at com.ibm.security.cert.BasicChecker.\u0026lt;init\u0026gt;(BasicChecker.java:108) 22:13:36 ... 42 more 22:13:36 我尝试从 Artifactory 下载 certificate.pem 并运行以下命令导入证书，但在 AIX 7.1 上问题依旧：\n/usr/java8_64/jre/bin/keytool -importcert \\ -alias cacertalias \\ -keystore /usr/java8_64/jre/lib/security/cacerts \\ -file /path/to/your/certificate.pem 奇怪的是，在 Windows、Linux 和 AIX 7.3 构建机上无法复现该问题。\n差异分析 # 唯一的区别是 Java 运行时版本。\n在有问题的 AIX 7.1 构建机上，我使用的是共享 Java 运行时：\n/tools/AIX-7.1/Java8_64-8.0.0.401/usr/java8_64/bin/java -version java version \u0026#34;1.8.0\u0026#34; Java(TM) SE Runtime Environment (build pap6480sr4fp1-20170215_01(SR4 FP1)) ... JCL - 20170215_01 based on Oracle jdk8u121-b13 而该机器上其实还有另一个本地安装的 Java 运行时：\n/usr/java8_64/bin/java -version java version \u0026#34;1.8.0_241\u0026#34; Java(TM) SE Runtime Environment (build 8.0.6.5 - pap6480sr6fp5-20200111_02(SR6 FP5)) ... JCL - 20200110_01 based on Oracle jdk8u241-b07 解决方法 # 我将 Jenkins 节点的 JavaPath 从：\n/tools/AIX-7.1/Java8_64-8.0.0.401/usr/java8_64/bin/java 改为：\n/usr/java8_64/bin/java 然后断开并重新启动 Jenkins Agent，问题就解决了。\n我不太清楚具体原因，可能与 Java 运行时自带的证书存储（cacerts）版本或证书链支持有关。 如果你对此有更多见解，欢迎在评论区留言告诉我。\n转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2023-08-29","externalUrl":null,"permalink":"/posts/upload-artifacts-failed-on-aix/","section":"Posts","summary":"本文介绍在 AIX 上通过 Jenkins 上传构件到 Artifactory 时遇到的 SSL 证书验证问题，包括更新 Java 的 cacerts 文件来解决问题。","title":"从 AIX 上传构件到 Artifactory 失败","type":"posts"},{"content":"","date":"2023-08-25","externalUrl":null,"permalink":"/tags/nuget/","section":"标签","summary":"","title":"NuGet","type":"tags"},{"content":"其实创建包管理平台账户没什么可说的，但最近准备在 https://www.nuget.org 上面发布产品前创建 Organization 的时候确遇到了问题。\n事情是这样的 # 作为一名公司员工在首次打开 NuGet 网站 (www.nuget.org) 的时候，点击【Sign in】，映入眼帘的就是【Sign in with Microsoft】，点击，下一步、下一步，我就顺利的就用公司邮箱注册了我的第一个 NuGet 的账户。\n此时我准备创建一个 Organization 的时候，输入自己的公司邮箱提示这个邮箱地址已经被使用了，What ？？？\nOK。那我就填写同事的公司邮箱地址吧。\n同事在收到邮件通知后也是一步步操作，先是打开 NuGet.org，点击【Sign in with Microsoft】，然后也是需要填写自己的账户名，结果完成这一系列的操作之后，再输入他的邮件地址去注册 Organization 的时候也同样提示这个邮箱已经被使用了？？？什么操作！！！醉了\u0026hellip;\n如何解决的 # 就在这千钧一发焦急得等待发布之际，我突然灵机一动，以死马当活马医的心态，将我注册的 NuGet 的个人账户绑定的公司邮箱修改为了自己的 Gmail 邮箱，然后此时再去创建 Organization 的时候输入的是自己的公司邮箱，创建 Organization 成功了！！\n好了，谨以此记录一下在注册 NuGet 时候遇到的问题。不知道对你是否有用，如果真的有帮助到你，举手示意一下哦。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-08-25","externalUrl":null,"permalink":"/posts/nuget/","section":"Posts","summary":"本文记录了在创建 NuGet Organization 时遇到的问题和解决方法，特别是关于公司邮箱地址的使用。","title":"创建 NuGet Organization 的遇到的坑","type":"posts"},{"content":"随着容器化技术的普及和应用场景的增多，构建和管理多平台镜像变得越来越重要。Docker Buildx 是 Docker 官方对于 Docker CLI 的一个扩展，为 Docker 用户提供了更强大和灵活的构建功能。包括：\n多平台构建：Docker Buildx 允许用户在一个构建命令中为多个不同的平台构建容器镜像。这样，你可以一次性构建适用于多种 CPU 架构的镜像，比如 x86、ARM 等，从而在不同的硬件设备上运行相同的镜像。 构建缓存优化：Docker Buildx 改进了构建过程中的缓存机制，通过自动识别 Dockerfile 中哪些部分是可缓存的，从而减少重复构建和加快构建速度。 并行构建：Buildx 允许并行构建多个镜像，提高了构建的效率。 多种输出格式：Buildx 支持不同的输出格式，包括 Docker 镜像、OCI 镜像、以及 rootfs 等。 构建策略：通过支持多种构建策略，用户可以更好地控制构建过程，例如，可以在多个节点上构建、使用远程构建等。 使用 docker buildx 需要 Docker Engine 版本不低于 19.03。\n其中，Docker Buildx Bake 是 Buildx 的一个子命令，也是本篇文章要重点介绍包括概念、优势、使用场景以及如何使用该功能来加速构建和管理多平台镜像。\n什么是 Docker Buildx Bake？ # Docker Buildx Bake 是 Docker Buildx 的一项功能，它旨在简化和加速镜像构建过程。Bake 是一种声明式的构建定义方式，它允许用户在一个命令中定义多个构建配置和目标平台，实现自动化批量构建和发布跨平台镜像。\n为什么使用 Docker Buildx Bake？ # 1. 提高构建效率 # Bake 通过并行构建和缓存机制来提高构建效率。使用 Bake 可以一次性定义和构建多个镜像，而无需为每个镜像分别执行构建过程，这样可以大大节省构建时间，提高工作效率。\n2. 支持多个平台和架构 # Docker Buildx Bake 的另一个优势是它能够构建多个平台和架构的镜像。通过在 Bake 配置中指定不同的平台参数就可以轻松构建适用于不同操作系统和架构的镜像。这对于跨平台应用程序的开发和部署非常有用。\n3. 一致的构建环境 # 通过 docker-bake.hcl （除了 HCL 配置文件之外还可以是 JSON 或是 YAML 文件）文件描述构建过程确保一致的构建环境，使不同的构建配置和目标平台之间具有相同的构建过程和结果。这种一致性有助于减少构建过程中的错误，而且构建配置更易于维护和管理。\n如何使用 Docker Buildx Bake？ # 以下是使用 Docker Buildx Bake 进行高效构建的基本步骤，首先确保你已经安装了 Docker Engine 或 Docker Desktop 版本 19.03 以及以上。\n然后你的 docker 命令将变成这样 docker buildx bake。以下 docker buildx bake --help 的帮助输出：\ndocker buildx bake --help Usage: docker buildx bake [OPTIONS] [TARGET...] Build from a file Aliases: docker buildx bake, docker buildx f Options: --builder string Override the configured builder instance -f, --file stringArray Build definition file --load Shorthand for \u0026#34;--set=*.output=type=docker\u0026#34; --metadata-file string Write build result metadata to the file --no-cache Do not use cache when building the image --print Print the options without building --progress string Set type of progress output (\u0026#34;auto\u0026#34;, \u0026#34;plain\u0026#34;, \u0026#34;tty\u0026#34;). Use plain to show container output (default \u0026#34;auto\u0026#34;) --provenance string Shorthand for \u0026#34;--set=*.attest=type=provenance\u0026#34; --pull Always attempt to pull all referenced images --push Shorthand for \u0026#34;--set=*.output=type=registry\u0026#34; --sbom string Shorthand for \u0026#34;--set=*.attest=type=sbom\u0026#34; --set stringArray Override target value (e.g., \u0026#34;targetpattern.key=value\u0026#34;) 接下来尝试一下如何使用 docker buildx bake\n1. 创建 Bake 配置文件 # 比如创建一个名为 docker-bake.dev.hcl 的 Bake 配置文件，并在其中定义构建上下文、目标平台和其他构建选项。以下是一个简单的示例：\n# docker-bake.dev.hcl group \u0026#34;default\u0026#34; { targets = [\u0026#34;db\u0026#34;, \u0026#34;webapp-dev\u0026#34;] } target \u0026#34;db\u0026#34; { dockerfile = \u0026#34;Dockerfile.db\u0026#34; tags = [\u0026#34;xianpengshen/docker-buildx-bake-demo:db\u0026#34;] } target \u0026#34;webapp-dev\u0026#34; { dockerfile = \u0026#34;Dockerfile.webapp\u0026#34; tags = [\u0026#34;xianpengshen/docker-buildx-bake-demo:webapp\u0026#34;] } target \u0026#34;webapp-release\u0026#34; { inherits = [\u0026#34;webapp-dev\u0026#34;] platforms = [\u0026#34;linux/amd64\u0026#34;, \u0026#34;linux/arm64\u0026#34;] } 2. 运行 Bake 构建 # 运行以下命令开始使用 Bake 构建镜像：\n$ docker buildx bake -f docker-bake.dev.hcl db webapp-release\n3. 打印构建选项 # 你还可以无需构建打印构建选项，使用用 --print 来查看某个目标构建是否符合预期。例如：\n$ docker buildx bake -f docker-bake.dev.hcl --print db [+] Building 0.0s (0/0) { \u0026#34;group\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;targets\u0026#34;: [ \u0026#34;db\u0026#34; ] } }, \u0026#34;target\u0026#34;: { \u0026#34;db\u0026#34;: { \u0026#34;context\u0026#34;: \u0026#34;.\u0026#34;, \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile.db\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;xianpengshen/docker-buildx-bake-demo:db\u0026#34; ] } } } 4. 发布构建镜像 # 通过添加 --push 选项可以将构建完成的镜像一键发布的镜像仓库，例如 $ docker buildx bake -f docker-bake.dev.hcl --push db webapp-release\n以上示例中的 demo 放在这里了：https://github.com/shenxianpeng/docker-buildx-bake-demo\n5. Buildx Bake 高级用法 # Buildx Bake 还有其他更多的使用技巧，比如 variable, function , matrix 等这里就不一一介绍了，详情请参见官方 Buildx Bake reference 文档。\n总结 # Docker Buildx Bake 是一个功能强大的构建工具，它提供了一种简化和加速构建过程的方法。通过使用 Bake 你可以高效地构建和测试多个镜像，并且可以跨多个平台和架构进行构建。所以说 Bake 是开发人员和构建工程师的重要利器，掌握 Docker Buildx Bake 的使用方法将帮助你更好地应对多镜像构建的带来的挑战、加快应用程序的交付速度。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-07-17","externalUrl":null,"permalink":"/posts/buildx-bake/","section":"Posts","summary":"本文介绍 Docker Buildx Bake 的概念、优势、使用场景以及如何使用该功能来加速构建和管理多平台镜像。","title":"Docker Buildx Bake：加速构建和管理多平台镜像的利器","type":"posts"},{"content":"","date":"2023-06-10","externalUrl":null,"permalink":"/tags/sbom/","section":"标签","summary":"","title":"SBOM","type":"tags"},{"content":" 什么是 SBOM # SBOM 是软件材料清单（Software Bill of Materials）的缩写。它是一份详细记录软件构建过程中使用的所有组件、库和依赖项的清单。\nSBOM 类似于产品的配方清单，它列出了构成软件应用程序的各种元素，包括开源软件组件、第三方库、框架、工具等。每个元素在 SBOM 中都会有详细的信息，如名称、版本号、许可证信息、依赖关系等。\nSBOM 的目的是增加软件供应链的可见性和透明度，并提供更好的风险管理和安全性。它可以帮助软件开发者、供应商和用户了解其软件中使用的组件和依赖项，以便更好地管理潜在的漏洞、安全风险和合规性问题。通过 SBOM 用户可以识别和跟踪软件中存在的任何潜在漏洞或已知的安全问题，并及时采取相应的补救措施。\nSBOM 还可以用于软件审计、合规性要求和法规遵从性等方面。一些行业标准和法规（如软件供应链安全框架（SSCF）和欧盟网络和信息安全指令（NIS指令））已经要求软件供应商提供 SBOM，以提高软件供应链的安全性和可信度。\n总之，SBOM 是一份记录软件构建过程中使用的所有组件和依赖项的清单，它提供了对软件供应链的可见性，有助于管理风险、提高安全性，并满足合规性要求。\nSBOM 和 SLSA 之间有什么关系，两者的区别是什么 # SBOM（软件材料清单）和 SLSA（Supply Chain Levels for Software Artifacts）是两个不同但相关的概念。\nSBOM 是软件材料清单，它提供了对软件供应链的可见性，包括组件的版本、许可证信息、漏洞等。SBOM 旨在帮助组织更好地管理和控制软件供应链，识别和处理潜在的漏洞、合规性问题和安全风险。 SLSA 是一种供应链安全框架，它定义了不同级别的安全要求和实践，以确保软件供应链的安全性。SLSA 旨在加强软件的可信度和安全性，防止恶意代码、供应链攻击和漏洞的传播。SLSA 关注整个软件供应链的安全，包括组件的来源、验证、构建过程和发布机制。 关于两者的区别：\n视角不同：SBOM 关注软件构建物料的清单和可见性，提供对组件和依赖项的详细信息。SLSA 关注供应链的安全性，定义了安全级别和实践，强调确保软件供应链的可信度和安全性。 用途不同：SBOM 用于识别和管理软件构建中的组件、漏洞和合规性问题。它提供了一种管理软件供应链风险的工具。SLSA 则提供了一种安全框架，通过定义安全级别和要求，帮助组织确保软件供应链的安全性。 关联性：SLSA 可以利用 SBOM 作为其实施的一部分。SBOM 提供了 SLSA 所需的组件和依赖项的详细信息，有助于验证和审计供应链的安全性。SLSA 的实践可以包括要求使用 SBOM 生成和验证，以确保软件供应链的可见性和完整性。 SBOM 和 SLSA 都是软件供应链安全中的两个关键概念。它们可以相互关联和互补使用，以加强软件供应链的安全性和管理。\nSBOM 和 Black Duck 之间有什么区别 # SBOM（Software Bill of Materials）和 Synopsys BlackDuck 是两个相关但不同的概念。下面是它们之间的区别：\nSBOM：\n定义：SBOM 是一种文档或清单，用于记录软件构建过程中使用的所有组件和依赖项。它提供了对软件供应链的可见性和透明度。 内容：SBOM 列出了每个组件的名称、版本号、作者、许可证信息等详细信息。它有助于追踪和管理软件的组件、依赖关系、漏洞和许可证合规性等。 用途：SBOM 用于软件供应链管理、安全审计、合规性验证和风险管理。它帮助组织了解软件构建中使用的组件，识别潜在的漏洞和风险，并确保合规性。 Synopsys Black Duck：\n功能：Synopsys Black Duck 是一种供应链风险管理工具。它可以扫描软件项目，识别其中使用的开源组件和第三方库，并分析其许可证合规性、安全漏洞和其他潜在风险。 特点：Black Duck 具有广泛的漏洞数据库和许可证知识库，可与开发流程和 CI/CD 工具集成，提供漏洞警报、许可证合规报告和风险分析等功能。 目的：Black Duck 帮助组织管理和控制软件供应链风险，提供实时的开源组件和第三方库的安全和合规性信息，以支持决策和采取适当的措施。 综上所述，SBOM 是记录软件构建中使用的组件和依赖项，提供对软件供应链的可见性和管理。而 Black Duck 是一种供应链风险管理工具，通过扫描和分析软件项目中的开源组件和第三方库，提供许可证合规性、安全漏洞和风险分析等功能。Black Duck 可以用于生成 SBOM，并提供更全面的风险和合规性分析。因此，Black Duck 是一个具体的工具，而 SBOM 是一种记录和管理软件供应链信息的概念。\nSBOM 的最佳实践 # 自动化生成：使用自动化工具生成 SBOM，避免手动创建和维护，确保准确性和一致性。 包含详细信息：在 SBOM 中包含尽可能详细的信息，如组件名称、版本号、作者、许可证信息、依赖关系、漏洞信息等。 定期更新：定期更新 SBOM 以反映最新的构建物料清单，确保其准确性和完整性。 版本控制：对于每个软件版本，建立和管理相应的 SBOM 版本，以便跟踪软件版本和其对应的构建物料清单。 集成到软件生命周期：将 SBOM 集成到整个软件生命周期中，包括开发、构建、测试、部署和维护阶段。 漏洞管理和风险评估：利用 SBOM 中的漏洞信息，与漏洞数据库集成，进行漏洞管理和风险评估。 供应商合作：与供应商和合作伙伴共享和获取 SBOM 信息，确保他们也提供准确的 SBOM，并持续关注他们的漏洞管理和合规性措施。 SBOM 的生成工具 # CycloneDX：CycloneDX 是一种开放的软件组件描述标准，用于生成和共享 SBOM。它支持多种语言和构建工具，具有广泛的生态系统和工具集成。 SPDX：SPDX（Software Package Data Exchange）是一种开放的标准，用于描述软件组件和相关许可证信息。它提供了一种统一的方式来生成和交换SBOM。 OWASP Dependency-Track：Dependency-Track 是一个开源的供应链安全平台，可生成和分析 SBOM，提供漏洞管理、许可证合规性和供应链可视化等功能。 WhiteSource: WhiteSource 是一种供应链管理工具，提供了自动化的开源组件识别、许可证管理和漏洞分析，可以生成SBOM并进行风险评估。 JFrog Xray：JFrog Xray 是一种软件供应链分析工具，可以扫描和分析构建物料清单，提供漏洞警报、许可证合规性和安全性分析。 Microsoft sbom-tool：是一种高度可扩展、适用于企业的工具，可为各种工件创建 SPDX 2.2 兼容的 SBOM。 trivy：支持在容器、Kubernetes、代码存储库、Cloud 等中查找漏洞、错误配置、密钥 和生成 SBOM。 除了以上这些还有一些其他工具也提供了 SBOM 生成、管理和分析的功能，你可以根据具体需求选择适合的工具来实施 SBOM 的最佳实践。\n总结 # 希望通过这篇文章，让你了解了 SBOM 的概念、与 SLSA 和 Black Duck 的关系和区别、最佳实践以及可用的生成工具来帮助更好地管理软件供应链安全。\n","date":"2023-06-10","externalUrl":null,"permalink":"/posts/sbom/","section":"Posts","summary":"本文介绍了SBOM的定义、与SLSA和Black Duck的关系和区别、最佳实践以及可用的生成工具，帮助读者更好地理解和应用SBOM。","title":"详解SBOM：定义、关系、区别、最佳实践和生成工具","type":"posts"},{"content":"","date":"2023-06-09","externalUrl":null,"permalink":"/tags/fork/","section":"标签","summary":"","title":"Fork","type":"tags"},{"content":"想必你也见到过很多开源项目中的 CONTRIBUTION.md 文档中通常都会让贡献者 Fork 仓库，然后做修改。\n那么如果你是该开源项目中的成员是否需要 Fork 仓库进行修改呢？\n以前我没有认真去想过这个问题，对于项目成员感觉 Fork 或不 Fork 好像差不多，但仔细想想 Fork 仓库与不 Fork 仓库其实是有以下几个主要的差别的：\n修改权限 # 在原始仓库中，你可能没有直接修改代码的权限，当你 Fork 一个仓库时，会创建一个属于你自己的副本，你可以在这个副本中拥有完全的修改权限，你可以自由地进行更改、添加新功能、解决bug等，而不会对原始仓库产生直接影响。\n做实验性的工作 # 如果你计划进行较大的修改或实验性工作，并且不希望直接影响原始仓库，那么 fork 仓库并在 fork 的中进行修改更为合适。\n比如你需要实验性的去大量清理现有仓库里的一些垃圾文件或是代码，如果你需要需要多次尝试，并将多次修改直接 git push 到推送原始仓库进行保存或是测试，这大大增加原始仓库的存储空间，如果你的修改是大型文件，那么对原始仓库的存储空间影响则会更大；如果你是 Fork 仓库则不会造成原始仓库的影响，直到你完成修改通过 Pull Request 合并到原始仓库时才会产生新的存储空间。\n代码审查和协作 # 当你 Fork 一个仓库并在自己的副本中进行修改后，你必须通过 Pull Request（PR）向原始仓库合并修改，有助于确保代码质量和功能正确性。（当然不 Fork 也可以这样做或不做，但 Fork 了就必须这样做了）\n版本控制和历史记录 # Fork 一个仓库后，你可以在自己的副本中维护独立的版本控制历史。你可以跟踪自己的更改、回溯历史、管理代码版本，而不会影响到原始仓库的版本控制。同时，你可以从原始仓库同步最新的更改，保持你的副本与原始仓库的同步。\n总结 # Fork 仓库与不 Fork 仓库的主要差别在于修改权限、做实验性的工作、代码审查和协作，以及版本控制和历史记录。\n个人认为只要一个仓库的贡献者超过 3 人，都建议所有人都 Fork 原始仓库，通过 Pull Request 方式合并代码。\n但也有例外情况可能不适合 Fork：项目在 Fork 之后 CI/CD 无法独立工作，但是你需要它们。比如 Fork 后的仓库因为环境等原因不支持独立的运行 CI/CD 而你需要在提交 Pull Request 之前通过自动化对分支进行测试。\n另外还要为原始仓库需要做适当的分支权限设置，以防止就算两个人的团队另外一个人不熟悉 Git 使用了非常危险的操作，比如强制推送（Force Push），变基（Rebasing），强制检出（Force Checkout）可能导致代码丢失、数据损坏或版本控制问题。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-06-09","externalUrl":null,"permalink":"/posts/fork-or-unfork/","section":"Posts","summary":"本文讨论了在开源项目中，作为项目成员是 Fork 原始仓库还是直接在原始仓库中修改代码的利弊，帮助开发者做出更合适的选择。","title":"如果你是项目成员，是 Fork 原始仓库还是直接原始仓库中修改代码？","type":"posts"},{"content":"Git 提交信息和 Git 分支命名规范是团队协作中非常重要的一部分，它们能够使代码库更加规范、易于维护和理解。\n我们需要通过工具来帮助实现Git提交信息和分支创建规范，本篇将介绍如何使用 Commit Check 这个工具来验证提交信息、分支命名、提交用户名字、提交用户邮箱等是否符合规范。\n更多关于Git提交信息和分支创建规范可以参看我之前发布的文章《程序员自我修养之Git提交信息和分支创建规范》，这里不再赘述。\nCommit Check 简介 # Commit Check 是一个可以检查 Git 提交信息，分支命名、提交者用户名、提交者邮箱等等。它是 Yet Another Commit Checker 的开源平替版。\nCommit Check 的配置 # 使用默认设置 # 如果没有进行自定义设置，Commit Check 会使用默认设置。具体设置在此\n默认设置中，提交信息遵循约定式提交，分支命名见分支模型详细信息\n使用自定义配置 # 你可以在你的 Git 仓库下创建一个配置文件 .commit-check.yml 来自定义设置，例如：\nchecks: - check: message regex: \u0026#39;^(build|chore|ci|docs|feat|fix|perf|refactor|revert|style|test){1}(\\([\\w\\-\\.]+\\))?(!)?: ([\\w ])+([\\s\\S]*)|(Merge).*|(fixup!.*)\u0026#39; error: \u0026#34;The commit message should be structured as follows:\\n\\n \u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt;\\n [optional body]\\n [optional footer(s)]\\n\\n More details please refer to https://www.conventionalcommits.org\u0026#34; suggest: please check your commit message whether matches above regex - check: branch regex: ^(bugfix|feature|release|hotfix|task)\\/.+|(master)|(main)|(HEAD)|(PR-.+) error: \u0026#34;Branches must begin with these types: bugfix/ feature/ release/ hotfix/ task/\u0026#34; suggest: run command `git checkout -b type/branch_name` - check: author_name regex: ^[A-Za-z ,.\\\u0026#39;-]+$|.*(\\[bot]) error: The committer name seems invalid suggest: run command `git config user.name \u0026#34;Your Name\u0026#34;` - check: author_email regex: ^\\S+@\\S+\\.\\S+$ error: The committer email seems invalid suggest: run command `git config user.email yourname@example.com` 你可以根据自己的需要来修改 regex, error, suggest 的值。\nCommit Check 使用 # Commit Check 支持多种使用方式\n以 GitHub Action 来运行 # 例如创建一个 GitHub Action workflow 文件 .github/workflows/commit-check.yml\nname: Commit Check on: push: pull_request: branches: \u0026#39;main\u0026#39; jobs: commit-check: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: commit-check-action@v1 with: message: true branch: true author-name: true author-email: true dry-run: true job-summary: true 详细请参考 https://github.com/commit-check-action\n以 pre-commit hook 运行 # 首选需要安装了 pre-commit\n然后将如下设置添加到你的 .pre-commit-config.yaml 文件中。\n- repo: https://github.com/commit-check rev: the tag or revision hooks: # support hooks - id: check-message - id: check-branch - id: check-author-name - id: check-author-email 以命令行来运行 # 可以通过 pip 先安装\npip install commit-check 然后运行 commit-check --help 命令就可以查看如何使用了，具体可以参见文档\n以 Git Hooks 来运行 # 要配置 Git Hooks ，你需要在 Git 存储库的 .git/hooks/ 目录中创建一个新的脚本文件。\n例如 .git/hooks/pre-push，文件内容如下：\n#!/bin/sh commit-check --message --branch --author-name --author-email 并修改为可执行权限 chmod +x .git/hooks/pre-push，然后当你运行 git push 命令时，这个 push hook 会自动执行。\nCommit Check 执行失败示例 # 检查提交信息失败\nCommit rejected by Commit-Check. (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) / ._. \\ / ._. \\ / ._. \\ / ._. \\ / ._. \\ __\\( C )/__ __\\( H )/__ __\\( E )/__ __\\( C )/__ __\\( K )/__ (_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._) || E || || R || || R || || O || || R || _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ (.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.) `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ Invalid commit message =\u0026gt; test It doesn\u0026#39;t match regex: ^(build|chore|ci|docs|feat|fix|perf|refactor|revert|style|test){1}(\\([\\w\\-\\.]+\\))?(!)?: ([\\w ])+([\\s\\S]*) The commit message should be structured as follows: \u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt; [optional body] [optional footer(s)] More details please refer to https://www.conventionalcommits.org Suggest: please check your commit message whether matches above regex 检查分支命名失败\nCommit rejected by Commit-Check. (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) (c).-.(c) / ._. \\ / ._. \\ / ._. \\ / ._. \\ / ._. \\ __\\( C )/__ __\\( H )/__ __\\( E )/__ __\\( C )/__ __\\( K )/__ (_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._)(_.-/\u0026#39;-\u0026#39;\\-._) || E || || R || || R || || O || || R || _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ _.\u0026#39; \u0026#39;-\u0026#39; \u0026#39;._ (.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.)(.-./`-´\\.-.) `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ `-´ Commit rejected. Invalid branch name =\u0026gt; test It doesn\u0026#39;t match regex: ^(bugfix|feature|release|hotfix|task)\\/.+|(master)|(main)|(HEAD)|(PR-.+) Branches must begin with these types: bugfix/ feature/ release/ hotfix/ task/ Suggest: run command `git checkout -b type/branch_name` 以上就是 Commit Check 的使用介绍了，更多新信息请参考 https://github.com/commit-check\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-05-09","externalUrl":null,"permalink":"/posts/commit-check/","section":"Posts","summary":"本文介绍如何使用 Commit Check 工具来验证 Git 提交信息、分支命名、提交用户名字、提交用户邮箱等是否符合规范。","title":"程序员自我修养之Git提交信息和分支创建规范（工具篇）","type":"posts"},{"content":"随着近些年针对软件供应链发起的攻击次数越来越多，Google 发布了一系列指南来确保软件包的完整性，目的是为了防止未经授权的代码修改影响软件供应链。\nGoogle 的 SLSA 框架（Supply-chain Levels for Software Artifacts 软件制品的供应链级别）是通过识别 CI/CD 流水线中的问题并减小影响，为实现更安全的软件开发和部署流程提供建议。\n目录 # 什么是SLSA 软件供应链中的问题 2.1 供应链攻击包括哪些 2.2 真实世界的例子 SLSA等级 3.1 详细解释 3.2 限制 SLSA落地 其他工具 什么是SLSA # SLSA 全名是 Supply chain Levels for Software Artifacts, or SLSA (发音“salsa”).\nSLSA 是一个端到端框架，一个标准和控制的清单确保软件构建和部署过程的安全性，防止篡改源代码、构建平台以及构件仓库而产生的威胁。\n软件供应链中的问题 # 任何软件供应链都可能引入漏洞，随着系统变得越来越复杂，做好最佳实践从而保证交付工件的完整性变得非常重要。如果没有一定的规范和系统发展计划，就很难应对下一次黑客攻击。\n供应链攻击包括哪些 # A 提交未经认证的修改 B 泄露源码仓库 C 从被修改源代码构建 D 泄露构建过程 E 使用已泄露的依赖 F 上传被修改的包 G 泄露了包仓库 H 使用已泄露的包\n真实世界的例子 # 完整性威胁 已知例子 SLSA 如何提供帮助 A 提交未经认证的修改 研究人员试图通过邮件列表上的\n补丁程序故意将漏洞引入 Linux 内核。 两人审查发现了大部分（但不是全部）漏洞。 B 泄露源码仓库 PHP：攻击者破坏了 PHP 的自托管\ngit 服务器并注入了两个恶意提交。 一个受到更好保护的源代码平台\n将成为攻击者更难攻击的目标。 C 从被修改源代码构建 Webmin：攻击者修改了构建基础设施\n以使用与源代码控制不匹配的源文件。 符合 SLSA 标准的构建服务器会生成出处，\n以识别实际使用的来源，从而使消费者能够检测到此类篡改。 D 泄露构建过程 SolarWinds：攻击者破坏了构建平台\n并安装了在每次构建期间注入恶意行为的植入程序。 更高的 SLSA 级别需要对构建平台进行更强大的安全控制，\n这使得妥协和获得持久性变得更加困难。 E 使用已泄露的依赖 event-stream：攻击者添加了一个无害的依赖项，然后更新了该依赖项\n以添加恶意行为。更新与提交到 GitHub 的代码不匹配（即攻击 F）。 递归地将 SLSA 应用于所有依赖项会阻止这个特定的向量，因为\n出处会表明它不是由适当的构建器构建的，或者源不是来自 GitHub。 F 上传被修改的包 CodeCov：攻击者使用泄露的凭据将恶意工件上传到\nGoogle Cloud Storage(GCS)，用户可以从中直接下载。 GCS 中工件的出处表明工件不是以\n预期的方式从预期的源代码库中构建的。 G 泄露了包仓库 对包镜像的攻击：研究人员为几个流行的\n包存储库运行镜像，这些镜像可能被用来提供恶意包。 与上面的 (F) 类似，恶意工件的来源表明它们不是\n按预期构建的，也不是来自预期的源代码库。 H 使用已泄露的包 Browserify typosquatting：攻击者\n上传了一个与原始名称相似的恶意包。 SLSA 不直接解决这种威胁，但将出处链接回源代码控制\n可以启用和增强其他解决方案。 SLSA等级 # 等级 描述 示例 1 构建过程的文档 无署名的出处 2 构建服务的防篡改 托管源/构建，署名出处 3 对特定威胁的额外抵抗力 对主机的安全控制，不可伪造的来源 4 最高级别的信心和信任 两方审查+密封构建 详细解释 # 等级 要求 0 没有保证。 SLSA 0 表示缺少任何 SLSA 级别。 1 构建过程必须完全脚本化/自动化并生成出处。 出处是关于工件构建方式的元数据，包括构建过程、顶级源和依赖项。\n了解出处允许软件消费者做出基于风险的安全决策。\nSLSA 1 的 Provenance 不能防止篡改，但它提供了基本级别的代码源识别并有助于漏洞管理。 2 需要使用版本控制和生成经过身份验证的来源的托管构建服务。 这些附加要求使软件消费者对软件的来源更有信心。\n在此级别，出处可防止篡改到构建服务受信任的程度。\nSLSA 2 还提供了一个轻松升级到 SLSA 3 的途径。 3 源和构建平台符合特定标准，以分别保证源的可审计性和出处的完整性。 我们设想了一个认证流程，审计员可以通过该流程证明平台符合要求，然后消费者就可以信赖了。 SLSA 3 通过防止特定类别的威胁（例如交叉构建污染），提供比早期级别更强的防篡改保护。 4 需要两人审查所有更改和密封、可重现的构建过程。 两人审查是发现错误和阻止不良行为的行业最佳实践。 密封构建保证来源的依赖项列表是完整的。 可重现的构建虽然不是严格要求的，但提供了许多可审计性和可靠性的好处。 总的来说，SLSA 4 让消费者对软件未被篡改具有高度的信心。 限制 # SLSA 可以帮助减少软件工件中的供应链威胁，但也有局限性。\n许多工件在供应链中存在大量依赖关系，完整的依赖关系图可能非常大。 实际上从事安全工作的团队需要确定并关注供应链中的重要组成部分，可以手动执行，但工作量可能很大。 工件的 SLSA 级别不可传递并且依赖项有自己的 SLSA 评级，这意味着可以从 SLSA 0 依赖项构建 SLSA 4 工件。因此，虽然主要的工件具有很强的安全性，但其他地方可能仍然存在风险。这些风险的总和将帮助软件消费者了解如何以及在何处使用 SLSA 4 工件。 虽然这些任务的自动化会有所帮助，但对于每个软件消费者来说，全面审查每个工件的整个图表并不切实际。为了缩小这一差距，审计员和认证机构可以验证并断言某些东西符合 SLSA 要求。这对于闭源软件可能特别有价值。 作为 SLSA 路线图的一部分，SLSA 团队还会继续探讨如何识别重要组成部分、如何确定整个供应链的总体风险以及认证的作用。\nSLSA落地 # SLSA 是一个标准，但如何落地呢？\n我们可以通过 SLSA 的 Requirements 的汇总表来一一对照进行自检，查看当前的 CI/CD 工作流处在哪个安全等级。\n有没有工具能够更好的帮助我们检查并指导我们如何提高安全等级呢？\n目前只有少数可以实现此目的的工具，并且绝大多数只限于 GitHub。\nOpenSSF Scorecard 就是一个来自于开源安全基金会（OpenSSF）针对开源软件安全指标检查的一个自动化工具，它可以帮助开源维护者改进他们的安全最佳实践，并帮助开源消费者判断他们的依赖项是否安全。\n它是通过评估软件安全相关的许多重要项目，并为每个检查分配 0-10 的分数。你可以使用这些分数来了解需要改进的特定领域，以加强项目的安全状况。还可以评估依赖项引入的风险，并就接受这些风险、评估替代解决方案或与维护人员合作进行改进做出明智的决定。\n其他工具 # slsa-verifier - 验证符合 SLSA 标准的构建出处 Sigstore - 用于签名、验证和保护软件的新标准 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-03-23","externalUrl":null,"permalink":"/posts/slsa/","section":"Posts","summary":"本文介绍了 SLSA 框架的概念、目的、等级划分以及如何在软件供应链中应用 SLSA 来提升安全性，帮助读者理解 SLSA 在软件开发和部署中的重要性。","title":"SLSA 框架与软件供应链安全防护","type":"posts"},{"content":"","date":"2023-02-26","externalUrl":null,"permalink":"/tags/chatgpt/","section":"标签","summary":"","title":"ChatGPT","type":"tags"},{"content":"随着 DevOps 的流行，越来越多的开发团队正在寻找一些工具来帮助他们更好地完成任务。ChatGPT 是一款基于人工智能的自然语言处理工具，它可以用来帮助开发团队在 DevOps 任务中更加高效地工作。\n本文将探讨如何在 DevOps 任务中使用 ChatGPT。\n一、ChatGPT 简介 # ChatGPT 是一款由 OpenAI 开发的人工智能自然语言处理工具。它可以用于许多不同的应用程序，例如语音识别、自然语言处理、文本生成等。 ChatGPT 使用深度学习技术，可以生成与输入内容相关的文本。它是一款非常强大的工具，可以帮助开发团队更加高效地工作。\n二、ChatGPT 在 DevOps 中的应用 # 在 DevOps 中，开发团队通常需要快速解决问题，并与团队成员和客户进行有效沟通。ChatGPT 可以用来帮助解决这些问题。\n自动化代码审查 开发团队通常需要花费大量时间来进行代码审查。ChatGPT 可以用来自动化这个过程。它可以根据代码库中的样本代码，生成与样本代码风格相似的代码，并对新代码进行审查。这可以帮助开发团队更快地进行代码审查，并减少人为错误的可能性。\n自动化测试 测试是 DevOps 中不可或缺的一部分。ChatGPT 可以用来自动化测试。它可以根据测试用例生成相应的测试代码，并对测试结果进行评估。这可以帮助开发团队更快地进行测试，并减少人为错误的可能性。\n自动化部署 部署是 DevOps 中不可或缺的一部分。ChatGPT 可以用来自动化部署。它可以根据部署规则生成相应的部署代码，并对部署结果进行评估。这可以帮助开发团队更快地进行部署，并减少人为错误的可能性。\n自动化文档生成 文档是 DevOps 中不可或缺的一部分。ChatGPT 可以用来自动化文档生成。它可以根据项目的代码库和测试用例生成相应的文档，并对文档的质量进行评估。这可以帮助开发团队更快地生成文档，并减少人为错误的可能性。\n三、如何使用 ChatGPT # 要使用 ChatGPT，开发团队需要进行以下步骤：\n收集数据 收集数据是使用 ChatGPT 的第一步。开发团队需要收集开发团队需要收集与其任务相关的数据，例如代码库、测试用例、部署规则和文档。这些数据将用于训练 ChatGPT 模型，以生成与任务相关的文本。\n训练 ChatGPT 模型 训练 ChatGPT 模型是使用 ChatGPT 的第二步。开发团队可以使用已有的数据来训练模型，也可以使用迁移学习技术，将已有的 ChatGPT 模型进行微调，以适应其任务的需求。训练好的 ChatGPT 模型将用于生成与任务相关的文本。\n集成 ChatGPT 模型 集成 ChatGPT 模型是使用 ChatGPT 的第三步。开发团队可以将 ChatGPT 模型集成到其 DevOps 工具链中。例如，可以将 ChatGPT 模型集成到自动化代码审查工具、自动化测试工具、自动化部署工具和自动化文档生成工具中。这将使这些工具更加智能化，并帮助开发团队更加高效地工作。\n优化 ChatGPT 模型 优化 ChatGPT 模型是使用 ChatGPT 的第四步。开发团队需要定期监控 ChatGPT 模型的性能，并对其进行优化。例如，可以增加更多的训练数据、调整模型的超参数、增加正则化约束等。这将有助于提高 ChatGPT 模型的准确性和性能，并使其更加适应任务需求。\n四、结论 # 在 DevOps 任务中使用 ChatGPT 可以帮助开发团队更加高效地工作。ChatGPT 可以用来自动化代码审查、自动化测试、自动化部署和自动化文档生成等任务。开发团队需要收集与其任务相关的数据，训练 ChatGPT 模型，并将其集成到其 DevOps 工具链中。开发团队还需要定期监控 ChatGPT 模型的性能，并对其进行优化。这将有助于提高 ChatGPT 模型的准确性和性能，并使其更加适应任务需求。\n对了，本篇文章是由 ChatGPT 生成的，你觉得它写的怎么样，像个油腻的中年人？\n","date":"2023-02-26","externalUrl":null,"permalink":"/posts/chatgpt-for-devops/","section":"Posts","summary":"本文探讨如何在 DevOps 任务中使用 ChatGPT，包括自动化代码审查、测试、部署和文档生成等方面的应用。","title":"如何在 DevOps 任务中使用 ChatGPT?","type":"posts"},{"content":"就像标题所说的，为什么我的 Jenkins Controller 越来越慢，可能是因为没有遵循 Jenkins pipeline 编写的一些最佳实践。\n所以主要介绍 Jenkins pipeline 的一些最佳实践，目的是为了向 pipeline 作者和维护者展示一些他们过去可能并没有意识到的“反模式”。\n我会尽量列出所有可能的 Pipeline 最佳实践，并提供一些实践中常见的具体示例。\n一般问题 # 确保在 pipeline 中使用 Groovy 代码作为粘帖剂 # 使用 Groovy 代码连接一组操作而不是作为 pipeline 的主要功能。\n换句话说，与其依赖 pipeline 功能（Groovy 或 pipeline 步骤）来推动构建过程向前发展，不如使用单个步骤（例如 sh）来完成构建的多个部分。\npipeline 随着其复杂性的增加（Groovy 代码量、使用的步骤数等），需要 controller 上的更多资源（CPU、内存、存储）。将 Pipeline 视为完成构建的工具，而不是构建的核心。\n示例：使用单个 Maven 构建步骤通过其构建/测试/部署过程来驱动构建。\n在 Jenkins pipeline 中运行 shell 脚本 # 在 Jenkins Pipeline 中使用 shell 脚本可以通过将多个步骤合并到一个阶段来帮助简化构建。shell 脚本还允许用户添加或更新命令，而无需单独修改每个步骤或阶段。\nJenkins Pipeline 中使用 shell 脚本及其提供的好处：\n避免 pipeline 中的复杂 Groovy 代码 # 对于 pipeline，Groovy 代码始终在 controller 上执行，这意味着使用 controller 资源（内存和 CPU）。\n因此，减少 Pipeline 执行的 Groovy 代码量至关重要（这包括在 Pipeline 中导入的类上调用的任何方法）。以下是要避免使用的最常见 Groovy 方法示例：\nJsonSlurper：此函数（以及其他一些类似函数，如 XmlSlurper 或 readFile）可用于从磁盘上的文件中读取数据，将该文件中的数据解析为 JSON 对象，然后使用 JsonSlurper().parseText(readFile(\u0026quot;$LOCAL_FILE\u0026quot;))。该命令两次将本地文件加载到 controller 的内存中，如果文件很大或命令执行频繁，将需要大量内存。\n解决方案：不使用 JsonSlurper，而是使用 shell 步骤并返回标准输出。这个 shell 看起来像这样：def JsonReturn = sh label: '', returnStdout: true, script: 'echo \u0026quot;$LOCAL_FILE\u0026quot;| jq \u0026quot;$PARSING_QUERY\u0026quot;'。这将使用代理资源来读取文件，$PARSING_QUERY 将帮助将文件解析成更小的尺寸。\nHttpRequest：此命令经常用于从外部源获取数据并将其存储在变量中。这种做法并不理想，因为不仅该请求直接来自 controller （如果 controller 没有加载证书，这可能会为 HTTPS 请求之类的事情提供错误的结果），而且对该请求的响应被存储了两次。\n解决方案：使用 shell 步骤执行来自代理的 HTTP 请求，例如使用 curl 或 wget 等工具，视情况而定。如果结果必须在 Pipeline 的后面，尽量在 agent 端过滤结果，这样只有最少的需要的信息必须传回 Jenkins controller。\n减少类似 pipeline 步骤的重复 # 尽可能多地将 pipeline 步骤组合成单个步骤，以减少 pipeline 执行引擎本身造成的开销。\n例如，如果你连续运行三个 shell 步骤，则每个步骤都必须启动和停止，需要创建和清理 agent 和 controller 上的连接和资源。\n但是，如果将所有命令放入单个 shell 步骤，则只需启动和停止一个步骤。\n示例：与其创建一系列 echo 或 sh 步骤，不如将它们组合成一个步骤或脚本。\n避免调用 Jenkins.getInstance # 在 Pipeline 或共享库中使用 Jenkins.instance 或其访问器方法表示该 Pipeline/共享库中的代码滥用。\n从非沙盒共享库使用 Jenkins API 意味着共享库既是共享库又是一种 Jenkins 插件。\n从 pipeline 与 Jenkins API 交互时需要非常小心，以避免严重的安全和性能问题。如果你必须在你的构建中使用 Jenkins API，推荐的方法是在 Java 中创建一个最小的插件，它在你想要使用 Pipeline 的 Step API 访问的 Jenkins API 周围实现一个安全的包装器。\n直接从沙盒 Jenkinsfile 使用 Jenkins API 意味着你可能不得不将允许沙盒保护的方法列入白名单，任何可以修改 pipeline 的人都可以绕过它，这是一个重大的安全风险。列入白名单的方法以系统用户身份运行，具有整体管理员权限，这可能导致开发人员拥有比预期更高的权限。\n解决方案：最好的解决方案是解决正在进行的调用，但如果必须完成这些调用，那么最好实施一个能够收集所需数据的 Jenkins 插件。\n清理旧的 Jenkins 构建 # 作为 Jenkins 管理员，删除旧的或不需要的构建可以使 Jenkins controller 高效运行。\n当你不删除旧版本时，用于更新和相关版本的资源就会减少。可以在每个 pipeline 作业中使用 buildDiscarder 来保留特定历史构建数量。\n使用共享库 # 不要覆盖内置的 pipeline 步骤 # 尽可能远离自定义/覆盖的 pipeline 步骤。覆盖内置 pipeline 步骤是使用共享库覆盖标准 pipeline API（如 sh 或 timeout）的过程。此过程很危险，因为 pipeline API 可能随时更改，导致自定义代码中断或给出与预期不同的结果。\n当自定义代码因 Pipeline API 更改而中断时，故障排除很困难，因为即使自定义代码没有更改，在 API 更新后它也可能无法正常工作。\n因此，即使自定义代码没有更改，也不意味着在 API 更新后它会保持不变。\n最后，由于在整个 pipeline 中普遍使用这些步骤，如果某些内容编码不正确/效率低下，结果对 Jenkins 来说可能是灾难性的。\n避免大型全局变量声明文件 # 拥有较大的变量声明文件可能需要大量内存而几乎没有任何好处，因为无论是否需要变量，都会为每个 pipeline 加载该文件。\n建议创建仅包含与当前执行相关的变量的小变量文件。\n避免非常大的共享库 # 在 Pipelines 中使用大型共享库需要在 Pipeline 启动之前检出一个非常大的文件，并为当前正在执行的每个作业加载相同的共享库，这会导致内存开销增加和执行时间变慢。\n回答其他常见问题 # 处理 pipeline 中的并发 # 尽量不要跨多个 pipeline 执行或多个不同的 pipeline 共享工作区。这种做法可能会导致每个 pipeline 中的意外文件修改或工作区重命名。\n理想情况下，共享卷/磁盘安装在单独的位置，文件从该位置复制到当前工作区，然后当构建完成时，如果有更新完成，文件可以被复制回来。\n构建不同的容器，从头开始创建所需的资源（云类型代理非常适合此）。构建这些容器将确保构建过程每次都从头开始，并且很容易重复。如果构建容器不起作用，请禁用 pipeline 上的并发性或使用可锁定资源插件在运行时锁定工作区，以便其他构建在锁定时无法使用它。\n警告：如果这些资源被任意锁定，则在运行时禁用并发或锁定工作区可能会导致 pipeline 在等待资源时被阻塞。\n另外，请注意，与为每个作业使用唯一资源相比，这两种方法获得构建结果的时间都比较慢。\n避免 NotSerializableException # Pipeline 代码经过 CPS 转换，以便 pipeline 能够在 Jenkins 重启后恢复。也就是说，当 pipeline 正在运行你的脚本时，你可以关闭 Jenkins 或失去与代理的连接。当它返回时，Jenkins 会记住它在做什么，并且你的 pipeline 脚本会恢复执行，就好像它从未被中断过一样。一种称为“连续传递样式 (CPS)”的执行技术在恢复 pipeline 中起着关键作用。但是，由于 CPS 转换，某些 Groovy 表达式无法正常工作。\n在幕后，CPS 依赖于能够序列化 pipeline 的当前状态以及要执行的 pipeline 的其余部分。 这意味着在 pipeline 中使用不可序列化的对象将触发 NotSerializableException 在 pipeline 尝试保留其状态时抛出。\n有关更多详细信息和一些可能有问题的示例，请参阅 Pipeline CPS 方法不匹配。\n下面将介绍确保 pipeline 能够按预期运行的技术。\n确保持久变量可序列化 # 在序列化期间，局部变量作为 pipeline 状态的一部分被捕获。这意味着在 pipeline 执行期间将不可序列化的对象存储在变量中将导致抛出 NotSerializableException。\n不要将不可序列化的对象分配给变量 # 一种策略是利用不可序列化的对象始终“及时”推断它们的值，而不是计算它们的值并将该值存储在变量中。\n使用 @NonCPS # 如果有必要，你可以使用 @NonCPS 注释为特定方法禁用 CPS 转换，如果它经过 CPS 转换，该方法的主体将无法正确执行。请注意，这也意味着 Groovy 函数将不得不完全重新启动，因为它没有被转换。\n异步 pipeline 步骤（例如 sh 和 sleep）始终是 CPS 转换的，并且不能在使用 @NonCPS 注释的方法内部使用。通常，你应该避免在使用 @NonCPS 注释的方法内部使用 pipeline 步骤。\nPipeline 耐久性 # 值得注意的是，更改 pipeline 的持久性可能会导致 NotSerializableException 不会被抛出，否则它们会被抛出。这是因为通过 PERFORMANCE_OPTIMIZED 降低 pipeline 的持久性意味着 pipeline 当前状态的持久化频率大大降低。因此，pipeline 从不尝试序列化不可序列化的值，因此不会抛出异常。\n此注释的存在是为了告知用户此行为的根本原因。不建议纯粹为了避免可串行化问题而将 pipeline 的持久性设置设置为性能优化。\nhttps://www.jenkins.io/doc/book/pipeline/pipeline-best-practices/ https://www.cloudbees.com/blog/top-10-best-practices-jenkins-pipeline-plugin https://github.com/jenkinsci/pipeline-examples/blob/master/docs/BEST_PRACTICES.md https://devopscook.com/jenkinsfile-best-practices/ 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2023-02-06","externalUrl":null,"permalink":"/posts/jenkins-pipeline-best-practices/","section":"Posts","summary":"本文介绍了 Jenkins pipeline 的一些最佳实践，旨在帮助开发者和运维人员优化 Jenkins 的性能和可维护性。","title":"为什么我的 Jenkins Controller 越来越慢？可能犯了这些错误...","type":"posts"},{"content":"时间过得好快，又过完了一年。\n今年想写一些总结回顾一下过去的一年发生在自己身上的重要事件。\n由于 2021 年没有写年终总结，2021 年在我的脑海里已经变化模糊，我只能凭着一些照片和日记才想起来的一些事情。看来以后的年终总结不能落下。\n回顾 2021 # 2021 年我的个人关键词是“最后的潇洒”。\n年初我搬家了，新家离公司开车只要十几分钟。这节省了很多花在路上的时间，周末我也更愿意往公司跑。\n四月，第一次 Fork 并开始维护 cpp-linter-action 这个开源项目，并且吸引了另外一个开发者与我共同维护。\n七月，广鹿岛旅行\n八月，老婆怀孕了。现在回看 2021 的照片，两个人的生活真的是太自在和潇洒了，两人吃饱全家不饿。\n十一月，被告知为次密接，要求去酒店隔离。就这样我们被拉去酒店隔离了一周，然后回家后又要求隔离一周。现在想想呵呵不可思议！\n回顾 2022 # 2022 的我个人的关键词就是“责任”。\n五月，随着女儿的出生的，除了工作之外，几乎所有的时间都花在了照顾家庭方面，留给我学习和输出时间寥寥无几了。\n孩子的出生最直接的感受就是身上责任的重大，养育一个孩子不但需要付出时间还有金钱，我真正地成为了上有老下有小的中年人了，一刻都不能倒下。\n以前觉得自己无所不能，未来可期，现在觉得自己肩膀上的责任重大，从此再也无法躺平了。\n六月至九月，陪产假。当全职奶爸，在孩子的睡觉的时候放弃了一些休息时间用来读书和开源。\n十月至十二月，回归岗位，因为疫情关系大部时间都在家办公。上班工作，下班看娃，再也没有大块时间来学习和输出了。\n因此我今年输出的文章很少，博客上一共发布了 19 篇，其中公众号只输出了 11 篇文章。这些输出绝大多数发生在上半年孩子还没出生的时候。\n在业余时间相较于输出文章，第一优先级还是在开源项目，这能让我学到更多。我在 cpp-linter 这个项目上花了比较多的业余时间，目前 cpp-linter-action 已经其他被超过 100 个其他项目所使用（依赖），我希望能把 cpp-linter 做成所有 C/C++ 项目的首选的代码格式化和静态检查工具。\n展望 2023 # 工作和家庭的平衡，希望女儿健康成长，早点睡整觉，这样爸爸可以晚上工作和学习了 英语和技术能够有进步，比如通过 TOEIC Tests 和加入知名的 Python Org 和学习 Cloud 方面的技术 超过 2022 年在博客和公众号的输出文章数，完成 24（博客）+ 12（公众号）要求不高吧 保持身体健康，恢复游泳、足球、运动，让体重回到 160 斤以下 过去的年终总结 # 2020 年终总结 2019 年终总结 2018 从测试到开发的五个月\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-12-28","externalUrl":null,"permalink":"/misc/2022-summary/","section":"Miscs","summary":"\u003cp\u003e时间过得好快，又过完了一年。\u003c/p\u003e\n\u003cp\u003e今年想写一些总结回顾一下过去的一年发生在自己身上的重要事件。\u003c/p\u003e","title":"2022 年终总结","type":"misc"},{"content":"在 Jenkins 多分支流水线中实现 [skip ci] 或 [ci skip] 时，现有插件似乎已不可用。\nJENKINS-35509 JENKINS-34130 建议：如果可能，尽量不要依赖 Jenkins 插件。\n于是，我决定自己实现一个 [skip ci] 功能。\n在 Jenkins Shared Library 中创建函数 # 如果你和我一样使用 Jenkins Shared Library，可以在 src/org/cicd/utils.groovy 中创建一个 SkipCI 函数，这样其他任务也可以复用。\n// src/org/cicd/utils.groovy def SkipCI(number = \u0026#34;all\u0026#34;){ def statusCodeList = [] String[] keyWords = [\u0026#39;ci skip\u0026#39;, \u0026#39;skip ci\u0026#39;] // 可根据需要添加更多关键词 keyWords.each { keyWord -\u0026gt; def statusCode = null if (number == \u0026#34;all\u0026#34;) { statusCode = sh script: \u0026#34;git log --oneline --all | grep \\\u0026#39;${keyWord}\\\u0026#39;\u0026#34;, returnStatus: true } else { statusCode = sh script: \u0026#34;git log --oneline -n ${number} | grep \\\u0026#39;${keyWord}\\\u0026#39;\u0026#34;, returnStatus: true } statusCodeList.add(statusCode) } return statusCodeList.contains(0) } 在其他任务中调用 # // 以下为示例代码，并非完整可运行的版本 import org.cicd.utils def call(){ pipeline { agent { node { label \u0026#39;linux\u0026#39; } } parameters { booleanParam defaultValue: true, name: \u0026#39;Build\u0026#39;, summary: \u0026#39;取消勾选以跳过构建。\u0026#39; } def utils = new org.cicd.utils() stage(\u0026#34;Checkout\u0026#34;) { checkout scm // 仅检查最新一次提交信息 SkipCI = utils.SkipCI(\u0026#39;1\u0026#39;) } stage(\u0026#34;Build\u0026#34;){ when { beforeAgent true expression { return params.Build \u0026amp;\u0026amp; !SkipCI } } steps { script { sh \u0026#34;make build\u0026#34; } } } } } 总结 # 通过这种方式，你可以在不依赖插件的情况下，在 Jenkins 多分支流水线中灵活地实现 [skip ci] 功能。 如果有任何问题或建议，欢迎留言交流。\n转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2022-10-09","externalUrl":null,"permalink":"/posts/jenkins-skip-ci/","section":"Posts","summary":"本文介绍如何在 Jenkins 多分支流水线中实现 [skip ci] 功能，根据提交信息跳过构建。","title":"如何在 Jenkins 多分支流水线中实现 [skip ci]","type":"posts"},{"content":"","date":"2022-09-27","externalUrl":null,"permalink":"/tags/wsl/","section":"标签","summary":"","title":"WSL","type":"tags"},{"content":" 问题描述 # 我在 WSL 中执行 ping google.com 时失败，并提示：\nTemporary failure in name resolution 解决方法 # 在 WSL2 中创建或修改 /etc/wsl.conf 文件\n添加以下内容，以防止 DNS 修改被系统覆盖：\nsudo tee /etc/wsl.conf \u0026lt;\u0026lt; EOF [network] generateResolvConf = false EOF 在 Windows CMD 中运行：\nwsl --shutdown 重启 WSL2\n在 WSL2 中执行以下命令（search 行可选）：\nsudo rm -rf /etc/resolv.conf sudo tee /etc/resolv.conf \u0026lt;\u0026lt; EOF search yourbase.domain.local nameserver 8.8.8.8 nameserver 1.1.1.1 EOF 如果删除 /etc/resolv.conf 时出现：\nrm: cannot remove \u0026#39;/etc/resolv.conf\u0026#39;: Operation not permitted 则先运行以下命令解锁文件：\nsudo chattr -a -i /etc/resolv.conf 参考链接 # https://askubuntu.com/questions/1192347/temporary-failure-in-name-resolution-on-wsl\nhttps://askubuntu.com/questions/125847/un-removable-etc-resolv-conf\n转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2022-09-27","externalUrl":null,"permalink":"/posts/fix-wsl-networking-issue/","section":"Posts","summary":"本文介绍如何通过配置 DNS 并确保修改持久化，来解决 WSL 中的 \u0026ldquo;Temporary failure in name resolution\u0026rdquo; 问题。","title":"如何修复 WSL 中的 \"Temporary Failure in name resolution\" 错误","type":"posts"},{"content":"","date":"2022-09-16","externalUrl":null,"permalink":"/tags/linux/","section":"标签","summary":"","title":"Linux","type":"tags"},{"content":"如果你有一台关键机器，例如团队的 CI 服务器，你可能不希望组内所有成员都能访问它。\n在 Linux 中修改 /etc/security/access.conf 配置文件即可实现该限制。\n配置方法 # 我将 TEAM A 的访问配置注释掉，并添加了允许访问的用户账号：\n#+ : (SRV_WW_TEAM_A_CompAdmin) : ALL + : shenx, map, xiar : ALL ⚠ 注意不要把自己也锁在外面\n最好允许多个账号有权限登录，以防因账号问题或离开组织而无法访问服务器。\n测试效果 # 当我使用不在名单中的账号尝试访问该机器时，连接会直接被关闭：\n$ ssh test@devciserver.organization.com test@devciserver.organization.com\u0026#39;s password: Connection closed by 10.84.17.119 port 22 转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2022-09-16","externalUrl":null,"permalink":"/posts/restrict-connect-server/","section":"Posts","summary":"本文介绍如何通过修改 \u003ccode\u003e/etc/security/access.conf\u003c/code\u003e 文件，限制只有特定用户可以登录关键的 Linux 服务器。","title":"限制他人登录你的重要 Linux 服务器","type":"posts"},{"content":"本篇是关于 C/C++ 代码格式化和静态分析检查的实践分享。\n目前 C/C++ 语言的代码格式化和检查工具使用的最为广泛的是 LLVM 项目中的 Clang-Format 和 Clang-Tidy。\nLLVM 项目是模块化和可重用的编译器和工具链技术的集合。\n对于 C/C++ 代码格式化和静态分析检查用到是 LLVM 项目中 clang-format 和 clang-tidy，放在一起我们称它为 clang-tools。\n虽然我们有了工具，但如何把工具更好的集成到我们的工作流中才是本篇重点要讨论的。\ncpp-linter 组织的诞生就是为 C/C++ 代码格式化和静态分析检查提供一站式的工作流，包括：\n方便下载 clang-tools，提供了 Docker images 和 binaries 两种使用方式； 方便与工作流进行集成，包括与 CI 以及 git hooks 的集成。 下面介绍如何使用 clang-tools 下载工具，以及集成到工作流中。\nclang-tools Docker images # 如果你想通过 Docker 来使用 clang-format 和 clang-tidy，clang-tools 项目是专门用来提供 Docker 镜像的。\n只要下载 clang-tools Docker 镜像，然后就可以使用 clang-format 和 clang-tidy 了。例如：\n# 检查 clang-format 版本 $ docker run xianpengshen/clang-tools:12 clang-format --version Ubuntu clang-format version 12.0.0-3ubuntu1~20.04.4 # 格式化代码 (helloworld.c 在仓库的 demo 目录下) $ docker run -v $PWD:/src xianpengshen/clang-tools:12 clang-format --dry-run -i helloworld.c # 查看 clang-tidy 版本 $ docker run xianpengshen/clang-tools:12 clang-tidy --version LLVM (http://llvm.org/): LLVM version 12.0.0 Optimized build. Default target: x86_64-pc-linux-gnu Host CPU: cascadelake # 诊断代码 (helloworld.c 在仓库的 demo 目录下) $ docker run -v $PWD:/src xianpengshen/clang-tools:12 clang-tidy helloworld.c \\ -checks=boost-*,bugprone-*,performance-*,readability-*,portability-*,modernize-*,clang-analyzer-cplusplus-*,clang-analyzer-*,cppcoreguidelines-* clang-tools binaries # 如果你需要使用 clang-tools binaries，以 Windows 为例，通常下载指定版本的 clang-tools 需要先安装 LLVM 这个大的安装包才能获得 clang-format \u0026amp; clang-tidy 这些工具；在 Linux 上会方便很多，可以使用命令来下载，但如果想下载指定版本的 clang-format \u0026amp; clang-tidy 可能要面临手动下载和安装。\nclang-tools-pip 提供并支持在 Windows，Linux，MacOs 上通过命令行下载任何指定版本的 clang-tools 可执行文件。\n只需要使用 pip 安装 clang-tools （即 pip install clang-tools）后，然后通过 clang-tools 命令就可以安装任何版本的可执行文件了。\n例如，安装 clang-tools 版本 13：\n$ clang-tools --install 13\n也可以将它安装到指定目录下面：\n$ clang-tools --install 13 --directory .\n安装成功后，可以查看安装版本：\n$ clang-format-13 --version clang-format version 13.0.0 $ clang-tidy-13 --version LLVM (http://llvm.org/): LLVM version 13.0.0 Optimized build. Default target: x86_64-unknown-linux-gnu Host CPU: skylake clang-tools CLI 还提供了其他选项，比如自动帮你创建链接等，可以查看它的 CLI 文档来获得帮助。\n把 clang-tools 集成到工作流 # 上面介绍了方便下载 clang-tools 的 Docker images 和 binaries 这两种方式，如何把它们集成到工作流中是我们最终所关心的。\n当前的主流 IDE 可以通过插件的方式来使用 clang-format 和 clang-tidy，但这样的问题是：\n不同的开发可能会使用不同的 IDE，这样在不同的 IDE 上安装插件需要比较高的学习成本； 没法保证所有开发人员在提交代码的时候都会去执行 Clang-Format 或 Clang-Tidy。 那么怎样确保每次提交代码都做了 Clang-Format 或 Clang-Tidy 检查呢？\ncpp-linter-action 提供了通过 CI 进行检查，当发现没有格式化或有诊断错误的代码时 CI 会失败，来防止合并到主分的代码没有通过代码检查； cpp-linter-hooks 通过 git hook 在提交代码的时候自动运行 clang-format 和 clang-tidy，如果不符合规范则提交失败，并提示并自动格式化。 cpp-linter-action 在代码合并前做自动检查 # 如果你使用的是 GitHub，那么非常推荐你使用 cpp-linter-action 这个 GitHub Action。\n目前 cpp-linter 还没有跟其他除 GitHub 以外的 SCM 做 API 集成。\n以下是它的一些重要特性：\n运行结果支持 Annotations 和 Thread Comment 两种方式展示 支持 GitHub 的 public 和 private 仓库 支持绝大多数 Clang 版本 还有很多其他的 optional-inputs 使用这个 Action 只需要在 .github/workflows/ 下面创建一个 cpp-linter.yml，内容如下：\n当然也可以把下面的配置加到一个已经存在的 Workflow，例如 build。\nname: cpp-linter on: pull_request: types: [opened, reopened] push: jobs: cpp-linter: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: cpp-linter-action@v1 id: linter env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: style: file - name: Fail fast?! if: steps.linter.outputs.checks-failed \u0026gt; 0 run: | echo \u0026#34;Some files failed the linting checks!\u0026#34; exit 1 如果发现存在没有进行格式化或是静代码态检查，CI workflow 会失败，并且会有如下注释说明，annotations 默认是开启的。\n如果开启了 Thread Comment 选项（即 thread-comments: true）就会将在 Pull Request 中自动添加如下错误评论。\n目前已经有很多知名项目依赖这个 Action，它在 GitHub Marketplace 上面搜索它的排名也非常靠前，可以放心使用。\n注：annotations 和 comment 这两个功能目前只支持 GitHub，该项目未来考虑支持其他 SCM，像 Bitbucket，GitLab。\ncpp-linter-hooks 在提交代码时自动检查 # cpp-linter-hooks 是通过 git hook 在提交代码时做自动检查，这种方式不限制使用任何 SCM。\n只需要在项目仓库中添加一个 .pre-commit-config.yaml 配置文件，然后将 cpp-linter-hooks 这个 hook 添加到 .pre-commit-config.yaml 中，具体设置如下：\n.pre-commit-config.yaml 是 pre-commit framework 的默认配置文件。\n安装 pre-commit\npip install pre-commit 创建配置文件 .pre-commit-config.yaml，设置如下：\nrepos: - repo: https://github.com/cpp-linter-hooks rev: v0.2.1 hooks: - id: clang-format args: [--style=file] # to load .clang-format - id: clang-tidy args: [--checks=.clang-tidy] # path/to/.clang-tidy 这里的 file 是指 .clang-format, clang-format 默认支持的编码格式包括 LLVM, GNU, Google, Chromium, Microsoft, Mozilla, WebKit，如果需要特殊设置可以在仓库的根目录下面创建配置文件 .clang-format。同理，如果默认的静态分析设置不满足要求，可以在仓库的根目录下创建 .clang-tidy 配置文件。\n更多配置可以参考 README\n安装 git hook 脚本\n$ pre-commit install pre-commit installed at .git/hooks/pre-commit 之后每次 git commit 都会自动执行 clang-format 和 chang-tidy。\n如果检查到没有格式化或有静态分析错误，会提示如下错误信息：\nchang-format 的输出\nclang-format.............................................................Failed - hook id: clang-format - files were modified by this hook 并自动帮你进行格式化\n--- a/testing/main.c +++ b/testing/main.c @@ -1,3 +1,6 @@ #include \u0026lt;stdio.h\u0026gt; -int main() {for (;;) break; printf(\u0026#34;Hello world!\\n\u0026#34;);return 0;} - +int main() { + for (;;) break; + printf(\u0026#34;Hello world!\\n\u0026#34;); + return 0; +} chang-tidy 的输出\nclang-tidy...............................................................Failed - hook id: clang-tidy - exit code: 1 418 warnings and 1 error generated. Error while processing /home/ubuntu/cpp-linter-hooks/testing/main.c. Suppressed 417 warnings (417 in non-user code). Use -header-filter=.* to display errors from all non-system headers. Use -system-headers to display errors from system headers as well. Found compiler error(s). /home/ubuntu/cpp-linter-hooks/testing/main.c:3:11: warning: statement should be inside braces [readability-braces-around-statements] for (;;) break; ^ { /usr/include/stdio.h:33:10: error: \u0026#39;stddef.h\u0026#39; file not found [clang-diagnostic-error] #include \u0026lt;stddef.h\u0026gt; ^~~~~~~~~~ 最后 # 选择 CI 还是 git hook？\n如果你的团队已经在使用 pre-commit，那么推荐使用 git hook 这种方式，只需要添加 cpp-linter-hooks 即可 如果不希望引入 pre-commit 则可以通过添加 CI 来进行检查。当然也可以两个都选。 cpp-linter organization 是我创建的，由 Brendan Doherty 和我为主要贡献者共同维护的开源项目，我们都是追求代码质量、力求构建最好的软件的开发者，为此我花费了很多业余时间在上面，但也学到了很多，后面我会分享其中一些有意思的实现方式。\n目前 cpp-linter 在 GitHub 上提供了最好用的 C/C++ Linter Action 和 clang-tools，欢迎大家使用，有什么意见或问题都可以通过 Issue 反馈。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-08-23","externalUrl":null,"permalink":"/posts/cpp-linter/","section":"Posts","summary":"本文介绍了 C/C++ 代码格式化和静态分析检查的工具和工作流，重点介绍了 clang-tools 的使用和集成方式。","title":"C/C++ 代码格式化和静态分析检查的一站式工作流 Cpp Linter","type":"posts"},{"content":"","date":"2022-07-28","externalUrl":null,"permalink":"/tags/gpg/","section":"标签","summary":"","title":"GPG","type":"tags"},{"content":" 1. 查看现有 GPG 密钥 # # 如果文件夹不存在会自动创建 $ gpg --list-keys gpg: directory \u0026#39;/home/ubuntu/.gnupg\u0026#39; created gpg: keybox \u0026#39;/home/ubuntu/.gnupg/pubring.kbx\u0026#39; created gpg: /home/ubuntu/.gnupg/trustdb.gpg: trustdb created $ gpg --list-key 2. 生成 GPG 密钥 # $ gpg --gen-key gpg (GnuPG) 2.2.19; Copyright (C) 2019 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Note: Use \u0026#34;gpg --full-generate-key\u0026#34; for a full featured key generation dialog. GnuPG needs to construct a user ID to identify your key. Real name: shenxianpeng Email address: xianpeng.shen@gmail.com You selected this USER-ID: \u0026#34;shenxianpeng \u0026lt;xianpeng.shen@gmail.com\u0026gt;\u0026#34; Change (N)ame, (E)mail, or (O)kay/(Q)uit? O ... gpg: key 5F72A7D009FC935A marked as ultimately trusted ... public and secret key created and signed. pub rsa3072 2022-07-28 [SC] [expires: 2024-07-27] F0F32CB8C65536ECE0187EAD5F72A7D009FC935A uid shenxianpeng \u0026lt;xianpeng.shen@gmail.com\u0026gt; sub rsa3072 2022-07-28 [E] [expires: 2024-07-27] 3. 导出公钥内容 # # 使用邮箱导出 gpg --armor --export xianpeng.shen@gmail.com # 或使用公钥 ID 导出 gpg --armor --export F0F32CB8C65536ECE0187EAD5F72A7D009FC935A # 输出为公钥内容 4. 将公钥添加到 GitHub # 打开 GitHub，进入： Settings → SSH and GPG keys → New GPG key 粘贴刚才导出的公钥内容。\n完成后，当你使用命令提交：\ngit commit -S -m \u0026#34;Your commit message\u0026#34; GitHub 会显示 Verified 签名标识：\n转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2022-07-28","externalUrl":null,"permalink":"/posts/git-create-gpg-keys/","section":"Posts","summary":"本文介绍如何生成 GPG 密钥、导出公钥并将其添加到 GitHub，用于提交签名验证。","title":"如何创建 GPG 密钥并添加到 GitHub","type":"posts"},{"content":"有幸赶上了公司的政策变化，我有 12 周的陪产假来做全职奶爸，照顾家人的同时希望挤出时间来学习，毕竟在职期间很有有机会能有近 3 个月的假期。\n照顾孩子兼顾学习真不是一件轻松的事情，我尽力兼顾了两者，做了如下的流水账记录。\n计划 # 我知道 12 周会很快过去，就在已经快要过去了 2 周时我决定有计划的来完成一些任务，比如：\n完成《代码整洁之道》、《重构》以及《动手学习深度学习这三本书》的阅读和豆瓣评论 为 pre-commit 写一个 clang-format 和 clang-tidy 的 cpp-linter-hooks 完成每个月 15 节英语课以及 3~4 的体育锻炼（游泳和足球） 找一个可以作为长期业余参与的开源项目，例如 pytest，tox，pypa。 也就是从休假的第 2 周开始，我开始记录每周的完成的小任务。\n周报 # 第 11~12 周（8.15 - 8.28）- 最后两周\n时间过得太快了，不知不觉就是假期的最后两周了。\nValidate VMs 更新关于 setuptools_scm 的使用。 在 cpp-linter Org 中做一些项目的修改和代码评审 第 10 周（8.8 - 8.14）- 最后三周\nValidate VMs 总结关于 setuptools_scm 的使用。 第 9 周（8.1 - 8.7）- 时间过得真快，转眼就到了陪产假的最后 4 周了\nDraft 一篇关于参与开源的文章 做了一些工作，以及 Troubleshooting support；参与开源项目和 Code Review。 游泳以及完成《重构》书评 第 8 周（7.25 - 7.31）\n做 cpp-linter 里的一些项目的更新，做 Artifactory 的迁移和测试。 这周计划做的事情世纪没完成几样，比如写文章和看书。 游泳 第 7 周（7.18 - 7.24）\n发现 Python 真是入门容易学好难\u0026hellip; 初步学了一下 tox 和 mypy，会之后的 project 中尝试使用。 重构了代码，本打算并把 Code Coverage 写到 100 %，但没实现，pytest 还需要继续学。 在琢磨一个有意思的可以作为长期业余时间来做的项目，目前有个模糊的雏形，先试试看 《重构》书没怎么读，周六游泳可以继续 第 6 周（7.11 - 7.17）\n完成了 cpp-linter-hooks 功能的开发并把它迁移到 cpp-linter org 下面。 创建了 .github 仓库，对于 org 这是一个很神奇的残酷，玩法很多，还在陆续探索中。 终于把读 Code Clean 的书评交了，还需继续读完成任务。 周五发了一篇公众号文章，是之前写的，整理终于发出了，这是 3 个月以来的第一次更新。 周日去游一次泳。 第 5 周（7.4 - 7.10）\n上周主要是抽空写 clang-tools-pip 和 cpp-linter-hooks 这两个功能，目前完成大概 70%，预计本周可以基本结束。 工作上也花了点时间，修复了之前写的 pipeline 的几个问题 上周开始读《重构》了，但没多少时间花在读书上，没读几页。一致想更新公号文章，可惜挺花时间的 上周日游泳也没游，因为脖子坏了，可能是喂奶低头造成的 :( 第 3 - 4 周：\n《代码整洁之道》 P56 - P130 在实现 cpp-linter-hooks 之前需要实现 install clang-tools with pip, 因此我创建了 clang-tools-pip 去市游泳馆游了一次泳，第二次本已约好但临时有事取消了 第 2 周：\n《代码整洁之道》 P26 - P56 创建了 cpp-linter-hooks 仓库，学习别人的代码 计划本周末和乔教练去游泳 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-06-13","externalUrl":null,"permalink":"/posts/vacation-weekly-report/","section":"Posts","summary":"本文记录了我在陪产假期间的学习和工作安排，包括阅读书籍、参与开源项目、体育锻炼等，分享了如何在照顾家庭的同时保持学习和成长。","title":"12 周的陪产假我一刻都没闲着","type":"posts"},{"content":"","date":"2022-06-13","externalUrl":null,"permalink":"/tags/report/","section":"标签","summary":"","title":"Report","type":"tags"},{"content":"","date":"2022-04-19","externalUrl":null,"permalink":"/tags/vscode/","section":"标签","summary":"","title":"VSCode","type":"tags"},{"content":"","date":"2022-04-19","externalUrl":null,"permalink":"/tags/windows/","section":"标签","summary":"","title":"Windows","type":"tags"},{"content":"很久以来很多程序员都遇到过在 Windows 做开发的各种不便：\n比如设置开发环境不能像 Linux 和 Mac 那样只需要输入一行命令就能安装各种 command 和 package，因此有的程序员从此转到了 Mac 上开发，也有的干脆就使用 Linux 作为开发机，只有那些不得不使用 Windows 作为开发环境程序员还一直留在了 Windows 上继续凑合中。\n直到 WSL 的到来，准确来说是 WSL2。\nWSL + VS Code + Docker Desktop 这三剑客的组合，开始让我觉得在 Windows 上做开发是一件非常爽的事情。\n什么是 WSL # WSL 是 Windows Subsystem for Linux 的缩写，它是 Windows 10 操作系统的一项功能，使你能够在 Windows 上直接运行 Linux 文件系统，以及 Linux 命令行工具和 GUI 应用程序，并与传统的 Windows 桌面和应用程序一起运行。\nWSL 的最低版本要求是 Windows 10 version 1903 及更高。\nWSL 是专为那些需要使用 Linux 的开发人员所开发的，例如从事网络开发人员、开源项目、以及需要部署到 Linux 服务器环境的开发者。\nWSL 适用于喜欢使用 Bash、常用 Linux 工具（sed、awk等）和 Linux 优先框架（Ruby、Python 等），同时也喜欢使用 Windows 作为生产力工具的人。\n下面来看看 WSL 和虚拟机相比有哪些优势。\n使用 WSL 的几点优势 # 与完整的虚拟机相比，WSL 需要的资源（CPU、内存和存储）更少 你可以同时使用 Windows 和 Linux，并从 Linux 中访问你的 Windows 文件，有更好的交互体验。 最最重要的是，使用 WSL 结合 VS Code + Docker 既有 Linux 的完美体验感，也同时拥有在 Windows 上的办公生产力。这是虚拟机或是 Linux 操作系统所办不到。Mac 可以，但并不是所有人都适合 Mac。 下面就来说说如何安装 WSL，以及和 VS Code + Docker 进行搭配使用。\n安装 WSL # wsl --install 这个命令将启用所需的可选组件，下载最新的 Linux 内核，将 WSL 2 设置为你的默认值，并为你安装一个 Linux 发行版（默认为 Ubuntu）。\n# 查看可用的发行版列表 C:\\Users\\xshen\u0026gt;wsl --list --online The following is a list of valid distributions that can be installed. Install using \u0026#39;wsl --install -d \u0026lt;Distro\u0026gt;\u0026#39;. NAME FRIENDLY NAME Ubuntu Ubuntu Debian Debian GNU/Linux kali-linux Kali Linux Rolling openSUSE-42 openSUSE Leap 42 SLES-12 SUSE Linux Enterprise Server v12 Ubuntu-16.04 Ubuntu 16.04 LTS Ubuntu-18.04 Ubuntu 18.04 LTS Ubuntu-20.04 Ubuntu 20.04 LTS 安装其他发行版，比如 Debian\nwsl --install -d Debian 更详细的请参考官方文档\nWSL + VS Code 演示 # 以下以 Ubuntu 为例，演示从下载代码，并通过 VS Code 打开代码目录进行 Coding。\n此时我已经通过 WSL 打开了已经安装好的 Ubuntu 操作系统了。\n首选下载代码\nubuntu@CN-L-2680:~$ git clone https://github.com/cue-lang/cue.git --depth 1 Cloning into \u0026#39;cue\u0026#39;... remote: Enumerating objects: 1833, done. remote: Counting objects: 100% (1833/1833), done. remote: Compressing objects: 100% (1502/1502), done. remote: Total 1833 (delta 238), reused 1161 (delta 148), pack-reused 0 Receiving objects: 100% (1833/1833), 1.53 MiB | 5.39 MiB/s, done. Resolving deltas: 100% (238/238), done. 然后到下载好的代码目录下面，输入 code .\nubuntu@CN-L-2680:~$ cd cue/ ubuntu@CN-L-2680:~/cue$ code . # 只有第一次才会安装 VS Code Server Installing VS Code Server for x64 (dfd34e8260c270da74b5c2d86d61aee4b6d56977) Downloading: 100% Unpacking: 100% Unpacked 2341 files and folders to /home/ubuntu/.vscode-server/bin/dfd34e8260c270da74b5c2d86d61aee4b6d56977. 第一次会自动下载并安装 VS Code Server，安装完成后会自动启动你本机上的 VS Code，并打开了 Ubuntu 上的代码目录，整个过程非常丝滑。\n之后你就可以在 VS Code 上通过命令行 apt-get 命令安装你需要的任何软件了，真爽~\n本机 VS Code 上需要安装微软出的 Remote - WSL 插件；\n另外，如果需要在 WSL 中使用 Docker，需要在 Windows 上预先安装 Docker Desktop。\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-04-19","externalUrl":null,"permalink":"/posts/wsl/","section":"Posts","summary":"本文介绍了如何在 Windows 上使用 WSL、VS Code 和 Docker Desktop 进行开发，提供了安装和配置的详细步骤，以及使用这些工具的优势和体验。","title":"在 Windows 做开发还能这么爽？WSL + VS Code + Docker Desktop 你值得有用","type":"posts"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/tags/containerd/","section":"标签","summary":"","title":"Containerd","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/tags/cri/","section":"标签","summary":"","title":"CRI","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/tags/cri-o/","section":"标签","summary":"","title":"CRI-O","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/tags/oci/","section":"标签","summary":"","title":"OCI","type":"tags"},{"content":"","date":"2022-03-29","externalUrl":null,"permalink":"/tags/runc/","section":"标签","summary":"","title":"Runc","type":"tags"},{"content":"自 Docker 开启了使用容器的爆发式增长，有越来越多的工具和标准来帮助管理和使用这项容器化技术，与此同时也造成了有很多术语让人感到困惑。\n比如 Docker, containerd, CRI, CRI-O, OCI, runc，本篇将介绍这些你听过但并不了解的术语，并解释容器生态系统是如何在一起工作的。\n容器生态系统 # 容器生态系统是由许多令人兴奋的技术、大量的专业术语和大公司相互争斗组成的。\n幸运的是，这些公司偶尔会在休战中走到一起合作，商定一些标准，这些标准有助于使这个生态系统在不同的平台和操作系统之间更具互操作性，并减少对单一公司或项目的依赖。\n这张图显示了 Docker、Kubernetes、CRI、OCI、containerd 和 runc 在这个生态系统中是如何结合的。\n其工作流程简单来说是这样的：\nDocker，Kubernetes 等工具来运行一个容器时会调用容器运行时（CRI）比如 containerd，CRI-O 通过容器运行时来完成容器的创建、运行、销毁等实际工作 Docker 使用的是 containerd 作为其运行时；Kubernetes 支持 containerd，CRI-O 等多种容器运行时 这些容器运行时都遵循了 OCI 规范，并通过 runc 来实现与操作系统内核交互来完成容器的创建和运行 下面就分别介绍图中所提到的术语和规范。\nDocker # 首先我们从大家都很熟悉的 Docker 开始，因为它是管理容器的最流行的工具。对很多人来说\u0026quot;Docker\u0026quot;这个名字本身就是\u0026quot;容器\u0026quot;的代名词。\nDocker 启动了整个容器的革命，它创造了一个很好用的工具来处理容器也叫 Docker，这里最主要的要明白：\nDocker 并不是这个唯一的容器竞争者 容器也不再与 Docker 这个名字紧密联系在一起 目前的容器工具中，Docker 只是其中之一，其他著名的容器工具还包括：Podman，LXC，containerd，Buildah 等。\n因此，如果你认为容器只是关于 Docker 的，那是片面的不对的。\nDocker 组成 # Docker 可以轻松地构建容器镜像，从 Docker Hub 中拉取镜像，创建、启动和管理容器。实际上，当你用 Docker 运行一个容器时实际上是通过 Docker 守护程序、containerd 和 runc 来运行它。\n为了实现这一切，Docker 是由这些项目组成（还有其他项目，但这些是主要的）。\ndocker-cli：这是一个命令行工具，它是用来完成 docker pull, build, run, exec 等命令进行交互。 containerd：这是一个管理和运行容器的守护进程。它推送和拉动镜像，管理存储和网络，并监督容器的运行。 runc：这是低级别的容器运行时间（实际创建和运行容器的东西）。它包括 libcontainer，一个用于创建容器的基于 Go 的本地实现。 Docker 镜像 # 许多人所说的 Docker 镜像，实际上是以 Open Container Initiative（OCI）格式打包的镜像。\n因此，如果你从 Docker Hub 或其他注册中心拉出一个镜像，你应该能够用 docker 命令使用它，或在 Kubernetes 集群上使用，或用 podman 工具以及任何其他支持 OCI 镜像格式规范的工具。\nDockershim # 在 Kubernetes 包括一个名为 dockershim 的组件，使它能够支持 Docker。但 Docker 由于比 Kubernetes 更早，没有实现 CRI，所以这就是 dockershim 存在的原因，它支持将 Docker 被硬编码到 Kubernetes 中。随着容器化成为行业标准，Kubernetes 项目增加了对额外运行时的支持，比如通过 Container Runtime Interface (CRI) 容器运行时接口来支持运行容器。因此 dockershim 成为了 Kubernetes 项目中的一个异类，对 Docker 和 dockershim 的依赖已经渗透到云原生计算基金会（CNCF）生态系统中的各种工具和项目中，导致代码脆弱。\n2022 年 4 月 dockershim 将会从 Kubernetes 1.24 中完全移除。今后 Kubernetes 将取消对 Docker 的直接支持，而倾向于只使用实现其容器运行时接口的容器运行时，这可能意味着使用 containerd 或 CRI-O。这并不意味着 Kubernetes 将不能运行 Docker 格式的容器。containerd 和 CRI-O 都可以运行 Docker 格式（实际上是 OCI 格式）的镜像，它们只是无需使用 docker 命令或 Docker 守护程序。\nContainer Runtime Interface (CRI) # CRI（容器运行时接口）是 Kubernetes 用来控制创建和管理容器的不同运行时的 API，它使 Kubernetes 更容易使用不同的容器运行时。它一个插件接口，这意味着任何符合该标准实现的容器运行时都可以被 Kubernetes 所使用。\nKubernetes 项目不必手动添加对每个运行时的支持，CRI API 描述了 Kubernetes 如何与每个运行时进行交互，由运行时决定如何实际管理容器，因此只要它遵守 CRI 的 API 即可。\n你可以使用你喜欢的 containerd 来运行你的容器，也可以使用 CRI-O 来运行你的容器，因为这两个运行时都实现了 CRI 规范。\ncontainerd # containerd 是一个来自 Docker 的高级容器运行时，并实现了 CRI 规范。它是从 Docker 项目中分离出来，之后 containerd 被捐赠给云原生计算基金会（CNCF）为容器社区提供创建新容器解决方案的基础。\n所以 Docker 自己在内部使用 containerd，当你安装 Docker 时也会安装 containerd。\ncontainerd 通过其 CRI 插件实现了 Kubernetes 容器运行时接口（CRI），它可以管理容器的整个生命周期，包括从镜像的传输、存储到容器的执行、监控再到网络。\nCRI-O # CRI-O 是另一个实现了容器运行时接口（CRI）的高级别容器运行时，可以使用 OCI（开放容器倡议）兼容的运行时，它是 containerd 的一个替代品。\nCRI-O 诞生于 RedHat、IBM、英特尔、SUSE、Hyper 等公司。它是专门从头开始创建的，作为 Kubernetes 的一个容器运行时，它提供了启动、停止和重启容器的能力，就像 containerd 一样。\nOpen Container Initiative (OCI) # OCI 开放容器倡议，是一个由科技公司组成的团体，其目的是围绕容器镜像和运行时创建开放的行业标准。他们维护容器镜像格式的规范，以及容器应该如何运行。\nOCI 背后的想法是，你可以选择符合规范的不同运行时，这些运行时都有不同的底层实现。\n例如，你可能有一个符合 OCI 的运行时用于你的 Linux 主机，另一个用于你的 Windows 主机。这就是拥有一个可以由许多不同项目实施的标准的好处。这种同样的 \u0026ldquo;一个标准，多种实现\u0026rdquo; 的方法其实还有很多都在使用，从蓝牙设备到 Java APIs。\nrunc # runc 是轻量级的通用运行时容器，它遵守 OCI 规范，是实现 OCI 接口的最低级别的组件，它与内核交互创建并运行容器。\nrunc 为容器提供了所有的低级功能，与现有的低级 Linux 功能交互，如命名空间和控制组，它使用这些功能来创建和运行容器进程。\nrunc 的几个替代品：\ncrun 一个用 C 语言编写的容器运行时（相比之下，runc 是用Go编写的。） 来自 Katacontainers 项目的 kata-runtime，它将 OCI 规范实现为单独的轻量级虚拟机（硬件虚拟化）。 Google 的 gVisor，它创建了拥有自己内核的容器。它在其运行时中实现了 OCI，称为 runsc。 runc 是一个在 Linux 上运行容器的工具，所以这意味着它可以在 Linux 上、裸机上或虚拟机内运行。\n在 Windows 上，它略有不同，与 runc 相当的是微软的主机计算服务（HCS），它包括一个叫 runhcs 的工具，它本身是 runc 的一个分叉，也实现了开放容器倡议的规范。\n总结 # 在本篇中，我们看到 Docker 只是容器生态系统中的一个小部分。另外还有一堆开放的标准，这就使得不同的实现互相之间是可替换的。\n这就是为什么有 CRI 和 OCI 标准，以及 containerd、runc 和 CRI-O 等项目存在的原因了。\n现在你知道了关于容器这个有趣而又略显复杂的世界的一切，下次和别人讨论时，不要说你在使用 \u0026ldquo;Docker 容器\u0026rdquo; :)\n参考 # The differences between Docker, containerd, CRI-O and runc\n转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-03-29","externalUrl":null,"permalink":"/posts/container-ecosystem/","section":"Posts","summary":"本文介绍了 Docker、containerd、CRI、CRI-O、OCI 和 runc 等容器生态系统中的关键组件和标准，解释它们之间的关系以及如何协同工作。","title":"关于 Docker，containerd，CRI，CRI-O，OCI，runc 的解释以及它们是如何工作在一起的","type":"posts"},{"content":"","date":"2022-03-15","externalUrl":null,"permalink":"/tags/ldap/","section":"标签","summary":"","title":"LDAP","type":"tags"},{"content":" 简介 # 在很多组织中，使用 LDAP 登录 是用户凭证认证的常见方式。\n配置 LDAP # 前提：已安装 Jenkins LDAP 插件\n关于详细配置方法，可以参考 Jenkins LDAP 插件官方文档：\nhttps://plugins.jenkins.io/ldap/\n以下是我用于测试的 LDAP 配置示例：\n无法使用 LDAP 登录？ # 有时由于组织的 LDAP 服务器出现问题，导致无法通过 LDAP 登录 Jenkins，但你仍需要继续使用 Jenkins。\n此时可以通过修改 config.xml 临时禁用 LDAP 身份验证。\n# 登录 Jenkins 服务器并进入 Jenkins 主目录 cd /var/lib/jenkins/ # 强烈建议在修改前备份 config.xml !!! cp config.xml config.xml.bak # 修改 config.xml，将 \u0026lt;useSecurity\u0026gt;true\u0026lt;/useSecurity\u0026gt; # 改为 \u0026lt;useSecurity\u0026gt;false\u0026lt;/useSecurity\u0026gt; # 重启 Jenkins 服务 sudo service jenkins restart 这样你就可以再次登录 Jenkins 了。\n当组织的 LDAP 恢复正常后，你可以将 config.xml 替换为之前的备份文件，用户即可继续通过 LDAP 登录。\n转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2022-03-15","externalUrl":null,"permalink":"/posts/jenkins-ldap-configuration/","section":"Posts","summary":"本文介绍如何在 Jenkins 中启用和配置 LDAP 身份验证，以及在需要时临时禁用它的方法。","title":"如何启用、配置和禁用 Jenkins LDAP","type":"posts"},{"content":" 背景 # 有开发者、甚至公司可能会遇到过以下几个问题：\n最开始 Fork 了一个仓库，之后做了大量的修改，从功能到开发语言，已经与父仓库各自发展了 由于是 Fork 的仓库，在每次提 Pull Request 的默认目标分支是父仓库，一不注意就会提 PR 到父仓库里去了 Fork 的仓库有人贡献并使用了，但不能显示贡献者，以及该项目被哪些其他的项目所使用，这不利于项目的发展 基于这些问题，开发者会考虑与父仓库进行分离，但目前 GitHub 没有提供 Unfork/Detach 的功能。\n如果直接删除项目并重建可以达到分离的目的，但这样会丢失一些重要的信息，比如项目中的 Issues，Wikis 以及 Pull Requests 等。\nUnfork 跟某节旗下某引擎白嫖 Apache SkyWalking 有本质区别，它更像是 Hudson 和 Jenkins 的分道扬镳。\n解决办法 # 在经过一番调查和测试，目前最可行的办法就是通过 GitHub Support 来处理，具体操作如下：\n打开这个链接：https://support.github.com/contact?tags=rr-forks 选择你的账户或是组织，然后在 Subject 中输入 \u0026ldquo;unfork\u0026rdquo; 会自动弹出虚拟助手，选择虚拟机助手 然后根据虚拟助手的问题然后选择答案（如下是部分截图） 最后这些对话会自动转换成文字脚本，然后 Send request，等着 Support 处理就可以了（不会太久） 这里要注意一下，如果你的仓库被其他人 Fork 了，你想跟父仓库分离之后继续保留你的子仓库的 Fork 记录，你应该选择 \u0026ldquo;Bring the child forks with the repository\u0026rdquo;。\n另外，通过其他方式，比如命令 git clone --bare 和 git push --mirror，可以保留完成的 Git 历史，但不能保留 Issues，Wikis 以及 Pull Requests 等信息。\n希望对有需要的你有所帮助。\n参考 # Delete fork dependency of a GitHub repository Unfork a Github fork without deleting 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-03-09","externalUrl":null,"permalink":"/posts/unfork-github-repo/","section":"Posts","summary":"本文介绍了如何通过 GitHub Support 实现与父仓库的分离，避免删除和重建仓库带来的数据丢失问题，帮助开发者更好地管理 Fork 的仓库。","title":"靠谱：在不删除和重建 GitHub 仓库的情况下与父(Fork)仓库分离(Unfork)","type":"posts"},{"content":"","date":"2022-03-06","externalUrl":null,"permalink":"/tags/groovy/","section":"标签","summary":"","title":"Groovy","type":"tags"},{"content":"在使用 Jenkins 和 Groovy 越久，我就这样的疑问：\nGroovy 到底是什么语言？ Groovy 有哪些特性？ Groovy 和 Java 有什么区别？ Groovy 和 Java 如何选择？ Groovy 在整个开发语言中占什么位置？要不要学？\n本篇我的学习结果的分享，希望也能帮助你解答以上的这些问题。\n什么是 Groovy # Apache Groovy 是一种强大的、可选类型的动态语言，具有静态类型和静态编译功能，适用于 Java 平台，旨在通过简洁、熟悉且易于学习的语法提高开发人员的工作效率。 它与任何 Java 程序顺利集成，并立即为你的应用程序提供强大的功能，包括脚本功能、特定领域语言创作、运行时和编译时元编程和函数式编程。\nGroovy 的特性 # 翻译官方的说法，Groovy 有以下六大特性。\n平坦的学习曲线 - 简洁、易读且富有表现力的语法，Java 开发人员易于学习 强大的功能 - 闭包、构建器、运行时和编译时元编程、函数式编程、类型推断和静态编译 流畅的 Java 集成 - 与 Java 和任何第三方库无缝、透明地集成和互操作 领域特定语言 - 灵活可延展的语法，先进的集成和定制机制，在你的应用程序中集成可读的业务规则 充满活力和丰富的生态系统 - Web 开发、响应式应用程序、并发/异步/并行库、测试框架、构建工具、代码分析、GUI 构建 脚本和测试胶水 - 非常适合编写简洁和可维护的测试，以及所有构建和自动化任务 Groovy 和 Java 的区别 # Groovy 是一种编程语言，也支持脚本语言；Java 是一种面向对象的编程语言。 Groovy 支持多方法，运行方法的选择将在运行时选择；Java 提供多方法的声明，在编译时而不是运行时选择。 Groovy 中，自动资源管理机制是不存在的，静态的、匿名的内部类；Java 从 Java7 版本开始就提供了自动资源管理，在内部静态类或匿名类方面占上风。 Groovy 中，有一些函数式编程特性，如 Lambda 函数，函数式接口；而 Java 从 JDK 8 版本开始就有 Lambda 函数、函数式接口和许多其他的流和并行操作功能。 Groovy 可以用单引号或双引号格式定义和声明字符串和字符字面；Java 只有双引号格式来声明和定义字符串字面或字符字面。 Groovy 中所有东西都是一个对象，并且只使用对象。因此，不存在自动装箱或拆箱的概念，也不存在基元的转换；相反，Java 有基元数据类型和 Wrapper 类，可以显式或隐式地进行自动装箱和自动拆箱。 Groovy 中，数据类型的自动拓宽和缩小有很多宽广的范围，有很多转换；而Java在数据类型的缩小或拓宽方面有限制。 Groovy 对其所有类型的类成员或数据都有一个默认的访问修饰符；而Java的默认访问级别是包级，取决于类成员的类型。 Groovy 在其类中自动生成 getters 和 setter 来访问和修改类的成员；而在 Java 中，它们必须在类中明确提到访问修饰符。 Groovy 有 Groovy beans；而Java有Java beans。 Groovy 也被称为 Java 的超集，因为 Java 程序可以在 Groovy 环境中运行。反过来并不一定。 Groovy 在定义类型时有更简单的语法，只需使用 def 来声明一个变量；Java有不同类型的类型名称来声明变量或类的任何方法或成员。 Groovy 不要求任何主方法或方法的入口点来运行类或任何程序；而 Java 则要求类中的 main 方法来运行程序。 Groovy 和 Java 如何选择 # 如果可扩展性和性能至关重要并且公司开发 Web 应用程序，当然是 Java。比如电商、银行、金融、国防、医疗保健等领域的大公司都会选择 Java，因为它经过时间证明，通常用于开发复杂的企业项目。 Groovy 可以用来编排一些 Pipeline，自动化，测试任务，因为它既是编程语言也是一种出色的脚本语言，功能强大且易于学习。 Groovy 目前流行排名 # 我们从这张图看到 2022 年 Groovy 语言的排行有一个非常大的下滑，从之前的排名 12 直接跌倒了 20。\n从数据上 2022 年 2 月 Groovy 有小幅减少（从 0.76，%0.74%），但这不是主要原因，主要是很可能是因为 TIOBE index 从 Alexa 更换为 Similarweb 网络流量引擎导致的波动。\n这些语言的排名上升包括：Assembly language，Go，Swift, MATLAB, Delphi/Object Pascal, Classic Visual Basic, Objective-C 的增长抢占了 Groovy 原有的位置。\n另外从 Groovy 的流行历史来看，它目前还是有很多人在使用的。开发者或许不会把它当作第一语言，但作为脚本语言学习一下还是可以的。\n对于使用 Jenkins Shared Libraries 的 DevOps 工程师，需要学习 Groovy。\n参考 # Groovy vs Java: https://flyoutsourcing.com/blog/groovy-vs.-java-what-suits-your-needs.html Differences with Java：https://groovy-lang.org/differences.html TIOBE index：https://www.tiobe.com/tiobe-index/ ","date":"2022-03-06","externalUrl":null,"permalink":"/posts/groovy/","section":"Posts","summary":"Groovy 是一种强大的动态语言，适用于 Java 平台，本文介绍了 Groovy 的特性、与 Java 的区别以及在 Jenkins 中的应用场景。","title":"在 Jenkins 上用了这么久的 Groovy，是时候认识一下它了","type":"posts"},{"content":"","date":"2022-03-02","externalUrl":null,"permalink":"/tags/blackduck/","section":"标签","summary":"","title":"BlackDuck","type":"tags"},{"content":" 问题详情 # Failure: PIP - Pip Inspector The Pip Inspector tree parse failed to produce output. Overall Status: FAILURE_DETECTOR - Detect had one or more detector failures while extracting dependencies. 点击展开完整输出 [main] --- ======== Detect Issues ======== [main] --- [main] --- DETECTORS: [main] --- Detector Issue [main] --- /workdir/test [main] --- Failure: PIP - Pip Inspector [main] --- The Pip Inspector tree parse failed to produce output. [main] --- [main] --- ======== Detect Result ======== [main] --- [main] --- Black Duck Project BOM: https://org.blackducksoftware.com/api/projects/246c8952-7cb8-40e9-9987-35f7d4602ae1/versions/e1cb4204-42d0-4445-8675-978df62b150d/components [main] --- [main] --- ======== Detect Status ======== [main] --- [main] --- GIT: SUCCESS [main] --- PIP: FAILURE [main] --- [main] --- Signature scan / Snippet scan on /workdir/test: SUCCESS [main] --- Overall Status: FAILURE_DETECTOR - Detect had one or more detector failures while extracting dependencies. Check that all projects build and your environment is configured correctly. [main] --- [main] --- If you need help troubleshooting this problem, generate a diagnostic zip file by adding \u0026#39;-d\u0026#39; to the command line, and provide it to Synopsys Technical Support. See \u0026#39;Diagnostic Mode\u0026#39; in the Detect documentation for more information. [main] --- [main] --- =============================== [main] --- [main] --- Detect duration: 00h 00m 54s 951ms [main] --- Exiting with code 5 - FAILURE_DETECTOR 运行环境：\nProduct: synopsys-detect-7.11.1.jar OpenJDK 11 Python 3.6 和 Python 2.7.5 根本原因 # 在调试输出中可以看到，Detect 调用的是 python（指向 python2），而不是 python3，导致运行 pip-inspector.py 失败：\nDEBUG [main-Executable_Stream_Thread] --- Python 2.7.5 ... [main] --- Running executable \u0026gt;/usr/bin/python .../pip-inspector.py --projectname=test 解决方法 # 将 python 链接到 python3 即可。\n# 备份原 python sudo mv /usr/bin/python /usr/bin/python.old # 建立指向 python3 的软链接 sudo ln -s /usr/bin/python3 /usr/bin/python 然后重新运行 Detect，例如：\nbash \u0026lt;(curl -s -L https://detect.synopsys.com/detect7.sh) \\ --blackduck.url=https://org.blackducksoftware.com \\ --blackduck.api.token=MmMwMjdlOTctMT \\ --detect.project.name=HUB \\ --detect.project.version.name=TEST_v1.1.1 \\ --detect.source.path=/workdir/test \\ --logging.level.com.synopsys.integration=DEBUG \\ --blackduck.trust.cert=TRUE \\ --detect.tools.excluded=POLARIS \\ --detect.blackduck.signature.scanner.snippet.matching=SNIPPET_MATCHING Docker 方式 # 如果想用 Docker 来进行 Black Duck 扫描，可以构建镜像，例如：\nFROM openjdk:11 # 可指定需要的 DETECT 版本，留空则下载最新 ENV DETECT_LATEST_RELEASE_VERSION=\u0026#34;\u0026#34; RUN apt-get update \\ \u0026amp;\u0026amp; apt-get upgrade -y \\ \u0026amp;\u0026amp; apt-get install -y \\ git \\ python \\ pip \\ \u0026amp;\u0026amp; apt-get autoremove \\ \u0026amp;\u0026amp; apt-get clean RUN curl -sSOL https://detect.synopsys.com/detect7.sh \u0026amp;\u0026amp; bash detect7.sh --help \\ \u0026amp;\u0026amp; rm -rf /usr/bin/python \\ \u0026amp;\u0026amp; ln -s /usr/bin/python3 /usr/bin/python WORKDIR /src 希望能帮到你。\n转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2022-03-02","externalUrl":null,"permalink":"/posts/blackduck-troubleshooting/","section":"Posts","summary":"本文介绍在 Black Duck Detect 中出现 \u0026ldquo;The Pip Inspector tree parse failed to produce output\u0026rdquo; 错误的原因分析及解决方法。","title":"已解决 - The Pip Inspector tree parse failed to produce output","type":"posts"},{"content":"DevOps 是 IT 界最近几年的一个热门话题，而且还会越来越热。\n最近有幸和一位做传播咨询的读者朋友交流关于 2022 年最值得关注的 DevOps 趋势以及一些问题和回答，分享给大家。\n行业趋势 # 趋势一：转向无服务器计算 # 无服务器计算是一种新兴趋势，实际上已经存在了十多年。企业购买无服务器框架需要一段时间，主要是因为对行业支持和对投资回报的担忧。\n无服务器具有许多越来越难以忽视的优势，主要的两个最大好处是效率和可靠性。没有基础设施管理的负担，企业可以将资源集中在正重要的事项上。此外，无服务器还降低了传统框架可能出现的潜在维护问题的风险。\n无服务器提供固有的可扩展性和可靠性并自动化开发人员不喜欢的日常操作任务，2022 年无服务器计算会经历下一次发展。\n趋势二：微服务架构增长 # 随着无服务器计算在 2022 年的发展，微服务也将如此。\n微服务架构是将单体应用分化为小的独立单元，或服务，从而为大型团队提供了更大的灵活性。它有以下优势：\n为企业提供比单体应用程序更好的可扩展性和敏捷性 开发人员可以使用他们熟悉的编程语言和工具，消除传统应用程序开发的局限 开发人员能够在不破坏整个代码库的情况下部署小的特性或功能 DevOps 团队可以根据业务需求来扩展每个应用部分，而不是一次性扩展整个应用 出现问题微服务可以轻松控制问题，而不会中断整个应用程序 当然也必须认识到微服务的一个弊端，如果实施不佳可能导致严重问题，包括数据丢失、可靠性差和安全风险。\n趋势三：Kubernetes 成为基础架构 # Kubernetes，也称 K8s，是容器编排开源平台，它能够与容器组交互，同时管理更多集群。除了容器管理，还提供安全、网络和存储服务，自我监控，节点和容器的健康状况检查。它可以处理从虚拟机集群管理到负载平衡等所有方方面面，提高生产力，简化 DevOps 开发、测试和部署流程。\n根据 Flexera 的 2021 年云计算状况报告，48% 的企业使用 Kubernetes，另有 25% 的企业计划使用它。另外 53% 的组织使用 Docker，21% 的组织计划使用。\n趋势四：DevSecOps 成为重要组成部分 # 安全性正在成为 DevOps 领域的另一个日益关注的问题。\n为了避免网络攻击，许多大型企业正在将安全性集成到他们的 DevOps 流程中。从 DevOps 到 DevSecOps 的转变预计在 2022 会有更多公司在软件开发生命周期的早期加入安全控制。 这使 DevOps 团队能够在开发阶段持续监控和修复安全缺陷，从而提高交付速度和质量。DevSecOps 正在成为许多公司组织结构图的重要组成部分。\n行业问答 # 问题一: DevOps 整个目前行业头部本土和国际玩家有哪些（GitLab)？ # 以我所在的外企而言通常是选择国际玩家，以最常用的代码管理和项目管理的工具为例：\n上了年头的外企大公司通常在使用 Atlassian 家的 Jira 和 Bitbucket。船大难掉头，选择 GitLab，GitHub 这样一站式的 DevOps 迁移成本很高，需要有足够的理由才可能换工具。 对于年轻的公司，GitLab 和 GitHub 都是很好的选择。GitLab 在企业内部建立私服居多；GitHub 也提供企业版私服，但对于开源项目而言 GitHub 依然是代码托管的首选。 其他用到的付费级 DevOps 工具还包括 Synopsys (Polaris, Blackduck)，Jfrog (Artifactory)，SonarQube 等。\n问题二: 行业目前有哪些重点趋势？比如安全这块，是不是目前行业关注度比较高？有哪些工具？ # 安全领域的关注度在逐年升高，尤其在外企很注重安全这块，他们愿意花钱来购买安全扫描工具来扫描代码，甚至还会要求所有的发布的产品代码中不能有高危漏洞。\n一些常用的工具包括：静态代码扫描，比如 Polaris, Veracode, Snyk, SonarQube, PVS-Studio；代码组成分析，比如 Blackduck，X-Ray 等等。\n问题三：企业在选择 DevOps 平台时主要考虑的因素有哪些？比如数据库安全，公司成熟度，海外知名度，等等 # 我认为主要考虑公司的知名度，其次产品的知名度，如果是开源产品会着重关注 GitHub 上的 Contributors 数量，它更能代表社区的活跃度，其次是 Fork 和 Star 数量。\n问题四：目前 DevOps 处于哪个阶段? 未来的发展机会是在哪里？ # DevOps 市场目前处在相对成熟的阶段，每个细分领域都有很多工具可以选择。未来基础设施会更多的向容器云方向发展。\n具有创新的 DevOps 产品依然会很有市场，像是 GitLab，HashiCorp 等公司的产品，他们在短短十年内成为世界级的软件公司。\n问题五：有哪些主流或平时重点关注的行业媒体号或自媒体公众号？ # 会经常看一些 DevOps 相关的以及 InfoQ，阿里、腾讯、美团等技术公众号。\n还会看 YouTube 上一些 DevOps 的个人及公司频道：TechWorld with Nana, CloudBeesTV，CNCF，DevOps Paradox，DevOps Toolkit 等。\n问题六：除了公众号外，你平时会上哪些行业社区？比如说 GitHub 或 CSDN？ # 最常看的是 GitHub 以及 GitHub Trending 来看最近受关注的项目。还会\n社区会定期去看 DEV Community, Medium, InfoQ 。会看知乎上一些话题下的精华，很少看 CSDN，懂得都懂。\n参考 # Top DevOps Trends to Watch in 2022 DevOps Trends To Look Out for in 2022 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-02-24","externalUrl":null,"permalink":"/posts/devops-trends-2022/","section":"Posts","summary":"本文介绍了2022年DevOps领域的主要趋势，包括无服务器计算、微服务架构、Kubernetes的普及以及DevSecOps的兴起，并回答了一些关于DevOps行业的问题。","title":"2022 年最值得关注的 DevOps 趋势和问答","type":"posts"},{"content":"在写博客和公众号这件事上，不知不觉已经是我的第五个年头了，没想过能这么久。\n借此分享一下这些年我的职业线路的变化，以及写博客\u0026amp;公众号有什么收获，算是自己过去的一个总结，如果能有点共鸣和帮助就更好了。\n从QA到DEV到DEVOPS # 最早关注我公众号读者朋友大概都是因为软件测试而结缘的。是的，我做了近 10 的软件测试工作，先后在 SIMcom、东软、京东商城、外企从事过功能\u0026amp;自动化\u0026amp;性能测试工作。\n从功能测试入行开始，我慢慢地感受到编程不是开发的独门武功，它也是测试工程师的必备技能，只有具备良好的编码能力，才能去做自动化、Unittest、以及测试开发等工作。\n当我做了自动化测试工程师，我又发现相对于“发现”问题，“解决”问题更令我愉悦。我开始梦想有机会能去做开发，这样不但可以提高自己的编程能力，另外开发、测试都懂也能为自己今后的职业发展找到更多可能性。\n最终是因为有这样的机会+自己的主动+编码过得去，我从测试转到了开发。起初的艰难和压力都是我工作近 10 年来前所未有的，白天看代码、晚上看代码、周末看代码\u0026hellip; 天天如此。经过了半年多的努力，才终于上岸，可以做 C/C++ 项目的 Bugfix 了。\n也正是因为有开发、自动化、持续集成的经验，在团队需要一名 Build/Release 工程师的时候，我知道这就是我最适合的岗位，负责产品的自动化构建、发布、基础设施建设、CI/CD 以及提高研发效能的相关开发工作。\n就这样我从 QA 到 DEV 到 DEVOPS。公众号的更名记录也记录了我的职业路线变化：\n2019年07月28日 “软件测试与开发”改名“DevOps攻城狮” 2018年12月29日 “DevQA”改名“软件测试与开发” 2018年12月26日 “软件测试QA”改名“DevQA” 2017年08月01日 注册“软件测试QA” 写作五年有哪些收获 # 写作是一项长期收益远超短期收益的事情。\n对于绝大多数人在短期内几乎不会有什么实质性的收益，还会花费大量的业余时间，妥妥的是用爱在发电。从金钱角度来衡量这件事，这是一件投入和产出完全不成比例的事情，很难坚持。\n如果从长期来看，坚持写作一定会带来价值的，我总结有以五个方面的好处：\n好记性不如烂笔头 - 当我们弄明白了一个技术难题，虽然当时明白了，但如果没记录下来，很有可能以后遇到同样的问题又不知道该如何解决。 让别人听懂才是真的懂 - 有时候对于一个问题我们认为自己明白了，当分享给别人的时候，才发现其中有的逻辑说不通，因此不得不继续思考并彻底搞清楚。 打造学习飞轮 - 当你坚持分享并有人关注到你并与你互动的时候，你就会有动力继续分享，学习新的知识然后再分享，一旦学习的飞轮造好了，坚持下去就变得容易。 间接收益 - 但凡坚持写点东西，对于以后找工作都或多或少会有些帮助，至少说明你是一个爱学习的人。如果你的分享让同行、未来你的面试官觉得很不错，很可能会给你带来一次新的工作机会。 直接收益 - 直接利益包括平台流量和广告收益、以及卖专栏、做咨询等。这要求就很高了，不但需要会自媒体运营，还有要超强的输出功力，这背后就是比别人更多的付出。 2017 年的时候我没想那么多，只是觉得自己也可以写点东西，就在 2017 年 7 月 6 日通过 GitHub Page 建立了自己的个人博客\n内容有了，复制过来也不费电，还能了解下公众号怎么玩的，然后就在同年 8 月开通了微信公众号；后来想看看小程序是怎么玩的，然后在 2020 年五一假期为我的博客创建了微信小程序(DevOps攻城狮)\n对于我来说花了这么多的业余时间来写作，说说有哪些具体的收获。\n知道如何在 GitHub 上建站、发布博客，把 GitHub 变成最常访问的网站之一 知道 Hexo 博客如何集成 Disqus, Google Analytics, Google Adsense, etc，并做了很多个改进 知道如何使用和集成 Github Actions、Travis、SonarQube 等工具 知道如何运营一个公众号；知道如何创建、发布一个微信小程序 参与开源项目，在开源项目中学习编码、开阔眼界和最佳实践 有同事说 ta 读到了我的文章，找到并关注了我的微信公众号，这让我很荣幸 收到过咨询，还有咨询者的感谢红包，能够帮到别人并收到正反馈让我非常开心 不敢想还能收到大出版社的邀请写一本技术书籍，因工作忙以及还有更重要的知识要学，主动放弃了 \u0026hellip; \u0026hellip; 这些收获中，我觉得最大的收获是打造学习飞轮，养成分享习惯。\n最好的时间是十年前，其次是现在。日拱一卒，功不唐捐，持续做对的事情，其他的就交给时间。\n—— 2022 年 2 月 20 日，凌晨更新。\n相关推荐阅读 # 做了9年测试，我为何转开发？ 从测试到开发的五个月 转载本站文章请注明作者和出处，请勿用于任何商业用途。欢迎关注公众号「DevOps攻城狮」\n","date":"2022-02-21","externalUrl":null,"permalink":"/misc/from-qa-to-dev-to-devops/","section":"Miscs","summary":"从软件测试到开发再到DevOps，码字五年有哪些收获？分享我的职业发展和写作经验。","title":"从QA到DEV到DEVOPS，码字五年有哪些收获","type":"misc"},{"content":"","date":"2022-02-14","externalUrl":null,"permalink":"/tags/vagrant/","section":"标签","summary":"","title":"Vagrant","type":"tags"},{"content":"关于 Vagrant 的介绍，可以参看前一篇文章：什么是 Vagrant? Vagrant 和 VirtualBox 的区别\n什么是 Vagrant # 关于 Vagrant 的介绍，可以参看前一篇文章：什么是 Vagrant? Vagrant 和 VirtualBox 的区别\nVagrant 和 Docker 区别 # 关于 Vagrant 被问到最多的问题：Vagrant 和 Docker 之间有什么区别。\n如果不分场景的直接比对 Vagrant 和 Docker 是不恰当的。在一些简单场景中，它们的作用是重复的，但在更多场景中，它们是无法相互替代的。\n那么什么情况下应该用 Vagrant，什么情况下用 Docker 呢？\n所以如果你仅仅是想管理虚拟机，那么你应该使用 Vagrant；如果你想快速开发和部署应用，那么应该使用 Docker。\n下面具体来说说为什么。\nVagrant 是 VM 的管理工具，或是说编排工具；Docker 是用来构建、运行、管理容器的工具。那么这个问题其实落在了虚拟机（VM）和 容器（Container）的区别。\n引用网络上一组照片来感受一下物理机（Host），虚拟机（VM）和 容器（Container）之间的区别。\n物理机（Host）\n虚拟机（VM）\n容器（Container）\n从图上我们更容易理解虚拟机（VM）和容器（Container）的这些不同：\n特性 虚拟机 容器 隔离级别 操作系统级 进程级别 隔离策略 Hypervisor CGROUPS 系统资源 5 - 15% 0 - 5% 启动时间 分钟级 秒级 镜像存储 GB MB 总结：Vagrant 和 Docker 的使用场景区别\nVagrant 设计是用来管理虚拟机的，Docker 设计是用来管理应用环境。\nVagrant 更适合用来做开发、测试，解决环境一致性的问题；Docker 更适合做快速开发和部署，CI/CD。\n最后，Vagrant 和 Docker 都有大量社区贡献的 “Box” 和 “Image” 可供选择。\n欢迎扫码关注公众号「DevOps攻城狮」- 专注于DevOps领域知识分享。\n","date":"2022-02-14","externalUrl":null,"permalink":"/posts/vagrant-vs-docker/","section":"Posts","summary":"本文介绍了 Vagrant 和 Docker 的区别，分析了它们各自的使用场景和优势，帮助读者选择合适的工具来管理虚拟机或容器。","title":"Vagrant 和 Docker 的区别，该如何选？","type":"posts"},{"content":"","date":"2022-02-14","externalUrl":null,"permalink":"/tags/virtualbox/","section":"标签","summary":"","title":"VirtualBox","type":"tags"},{"content":" 什么是 Vagrant # Vagrant 是一种开源软件产品，用来方便构建和维护虚拟软件开发环境。\n例如，它可以基于 VirtualBox、VMware、KVM、Hyper-V 和 AWS 甚至是 Docker 等提供商来构建开发环境。它通过简化虚拟化的软件配置管理，来提高开发效率。\nVagrant 是用 Ruby 语言开发的，但它的生态系统支持使用其他几种语言进行开发。\n简单来说 Vagrant 是对传统虚拟机的一层封装，能够让你更方便的使用虚拟开发环境。\nVagrant 的发展史 # Vagrant 最初是由 Mitchell Hashimoto 于 2010 年 1 月作为个人项目启动的。\nVagrant 的第一个版本于 2010 年 3 月发布。2010 年 10 月，Engine Yard 宣布他们将赞助 Vagrant 项目。\nVagrant 的第一个稳定版本 Vagrant 1.0 于 2012 年 3 月发布，正好是原始版本发布两年后。\n同年 11 月，Mitchell 成立了 HashiCorp 公司，以支持 Vagrant 的全职开发。Vagrant 仍然是开源软件，HashiCorp 公司致力于创建商业版本，并为 Vagrant 提供专业支持和培训。\n现在 HashiCorp 已经成为世界顶级开源公司，它通过一系列的产品，包括 Vagrant，Packer（打包），Momad（部署），Terraform（配置云环境），Vault（权限管理） 以及 Consul（监控），从端到端重新定义了整个 DevOps。\nVagrant 最初支持 VirtualBox，在 1.1 版增加了对其他虚拟化软件（如 VMware 和 KVM）的支持，以及对 Amazon EC2 等服务器环境的支持。从 1.6 版开始，Vagrant 原生支持 Docker 容器，在某些情况下可以替代完全虚拟化的操作系统。\n如何使用 Vagrant # 使用 Vagrant 的前提条件：\n安装 Vagrant。下载 Vagrant 安装 VirtualBox 当以上两个都准备好了，你就可以通过命令行创建并使用你的虚拟机了。\n比如你需要一个 Ubuntu 18.04 LTS 64-bit的虚拟机。更多其他的虚拟机可以到 Box 网站上去搜索查找，它类似于 Docker Hub，用户可以在上面下载和上传各种 Vagrant Box。\n你只需执行一些简单的命令就可以完成启动、登录、退出、及销毁。\n初始化 Vagrant\nvagrant init hashicorp/bionic64 启动虚拟机。大概几十秒钟就可以完成了（第一次需要下载镜像，时间会长一点，取决于网速）。\nvagrant up 登录你的虚拟机，然后可以使用你创建的 Ubuntu 虚拟机了\nvagrant ssh 当你不想用的时候，执行 logout 就可以退出登录了。\nVagrant 和传统虚拟机软件的区别 # Vagrant 相比传统使用虚拟机的方式要方便得多，我们来看看传统方式是怎样创建一台虚拟机的。\n还是以 VirtualBox 为例，假设你已经安装好了 VirtualBox，使用传统方式要创建一个虚拟机的动作是这样的：\n首先，下载对应的 ISO 文件 然后，用 VirtualBox 或 VMware 来加载 ISO 最后，通过一步步的配置 CPU、内存、磁盘，网络、用户等设置，等待安装完成安装\n这种方式配置起来就非常繁琐，需要一步步地进行。这些配置的步骤往往还会写一个文档来记录下来才能保证以后能够创建出来“一模一样”的虚拟开发环境。\n相信通过对比你已经大概了解 Vagrant 是怎么使用的，以及它和传统使用虚拟机之间的一些区别了。\n总结 # Vagrant 相比于传统使用虚拟机的优势：提供易于配置、可重现和便携的工作环境，从而提高生产力和灵活性。\nVagrant 可以说是创建、管理虚拟化环境的最简单、最快捷的方式！\n它之所以可以这么方便是站在了这些巨人（VirtualBox、VMware、AWS、OpenStack 或其他提供商）的肩膀上，然后通过 Shell 脚本、Ansbile、Chef、Puppet 等工具实现自动在虚拟机上安装和配置软件。\n下一篇将介绍 Vagrant 和 Docker 之间的区别。\n欢迎扫码关注公众号「DevOps攻城狮」- 专注于DevOps领域知识分享。\n","date":"2022-02-11","externalUrl":null,"permalink":"/posts/vagrant/","section":"Posts","summary":"本文介绍了 Vagrant 的概念、发展历史以及如何使用 Vagrant 创建和管理虚拟机，强调了 Vagrant 相比传统虚拟机的优势。","title":"什么是 Vagrant? Vagrant 和 VirtualBox 的区别","type":"posts"},{"content":"","date":"2022-01-18","externalUrl":null,"permalink":"/tags/go/","section":"标签","summary":"","title":"Go","type":"tags"},{"content":"Go 是一种开源编程语言，可以轻松构建简单、可靠和高效的软件。\nGo 还是 Golang # 先问一个大多数人可能会忽略的问题：Google 的这门开源编程语言叫 Go 还是 Golang？还是两个都行？给你三秒钟想一下 \u0026hellip;\nGoogle 说：它叫 Go。之所以有人称它为 Golang 是由于之前的 Go 语言官网是 golang.org（因为 go.org 已经被别人用了），因此有人将 Golang 和 Go 混用了。\n现在输入 golang.org 会直接跳转到 go.dev 这个网址，这也算是彻底给自家孩子正个名。\nGo 语言有哪些优势 # 官网是这样介绍 Go 语言的：\nGo 适合大规模快速构建，可靠、高效的软件 Go 是 Google 在背后支持的一门开源编程语言 易于学习和入门 内置并发和强大的标准库 不断发展的合作伙伴、社区和工具生态系统 今天，Go 被用于各种应用程序：\nGo 在基于云或服务器端的应用程序中很受欢迎 云基础设施方面。当今最流行的基础设施工具是用 Go 编写的，例如 Kubernetes、Docker 和 Prometheus 许多命令行工具都是用 Go 编写的 DevOps 和 Web 可靠性自动化也常常用 Go 来写 Go 也被用于人工智能和数据科学领域 微控制器编程、机器人技术和游戏中使用也会使用 Go 这也就是为什么 Go 越来越流行。\n正是因为这些优势以及在工作上的需要写一个 CLI，我就入门 Go 语言了。\nGo 语言的排名 # Go 语言在国内热度可谓是非常高了，我们来看看 Go 语言目前最新的排名怎么样。\n这是 TIOBE 2022 年一月排名前 20 编程语言，可以看到 Go 语言位于这个排行榜的第 13 位，相比于去年上升了一位。\n对比排在榜单前五的 Python，C，Java，C++，C#，你觉得 Go 能否追上它们呢？\n从我身边非云厂商的公司和同事来看，目前大多数都是 C/C++，Java，C#，Python 的开发人员，所以这个排名我认为还是挺符合预期的。\n初学者应该学习 Python 还是 Go ？ # Python 已有 30 多年的历史，但它的受欢迎程度仍在继续增长。Python 是一门出色的面向对象语言，你也可以使用函数式编程风格来编写代码。在所有编程语言中，你可能找不到一种比 Python 被更多非程序员使用的语言。\n它的灵活性是 Python 如此受欢迎的原因之一 它经常用于编写脚本，Web 开发、数据科学、以及面向孩子们教授编程、制作动画等等。 那么 Go 与 Python 相比如何呢？\nPython 和 Go 都具有简单的语法 Python 和 Go 对于初学者来说都很容易上手，且相对容易学习（Python 相对更容易） Python 往往在数据科学领域占据主导地位；Go 非常适合系统编程 程序的执行速度 Go 比 Python 快多了 作为高级语言，Python 拥有更广泛的库和围绕它建立的社区 Go 是处理大型并发应用程序的理想选择、支持并发，同时运行多个程序/任务的能力。Python 没有。 今天 Python 和 Go 都是目前最流行和最方便使用的两种编程语言。对于初学者应该是学习 Python 还是 Go ？\n如果你是零基础，建议先学习 Python。相比于 Go，Python 还是更容易学习。 如果你是测试工程师，想学习一门编程语言，建议学习 Python。因此绝大多数的自动化测试岗位要求是掌握 Python。 如果你是软件开发、DevOps 工程师，最好两门都要会。\u0026ldquo;小孩子才做选择，大人全都要。\u0026rdquo; 如何学习 Go 语言 # 看文档或视频，最最重要的是要动手！！\n我最早是在 2010~2020 期间看过 Go 语言的视频教程，但由于没怎么动手写过，一直处在只知其一不知其二的阶段。\n对于新手学习任何一门编程语言，看教程大概只能学会 30%，想要真正的学会必须亲自上手实践，否则一定会是：“一看就会，一写就废”。\n确定要一个方向，立刻开始 Coding。\n我的方向就是写一个 CLI 工具。尽管 Go 语言内置的 Flag 这个 package 可以用来编写 CLI 命令，我也看了很多使用 Go 开发的 CLI 项目后，注意到这些项目都没有使用内置的 Flag 包，而是绝大多数使用了 spf13/cobra 或 urfave/cli。\n这是使用 cobra 的项目列表，其中包括了著名的项目比如 Kubernetes, Hugo, Docker，Github CLI 等都使用的 cobra。 至于 urfave/cli，我看到 Jfrog CLI 在使用它，其他正在使用 urfave/cli 的知名项目我并没有看到像 cobra 那样的列表。 对于我这样的初学者，最重要的是马上开始，因此在选择的框架的时候不需要花费太多时间，cobra 有那么多优秀的项目背书，跟着用就行，最重要的是尽快动手。在编码的过程中，选择同样使用该框架的顶级项目做参考，这能帮助我们通过阅读别人的代码也让我们自己写出更优秀的代码。千万不要去 Ctrl + C 然后 Ctrl + V。\n最后，再分享几个在开发 CLI 时一切其他的优秀项目。比如：\ngithub.com/AlecAivazis/survey/v2 - 支持终端上构建交互式命令行 github.com/enescakir/emoji - 表情符号库，支持在终端输出表情符号 github.com/mgutz/ansi - 可以创建 ANSI 彩色字符串 欢迎扫码关注公众号「DevOps攻城狮」- 专注于DevOps领域知识分享。\n","date":"2022-01-18","externalUrl":null,"permalink":"/posts/what-is-go/","section":"Posts","summary":"本文介绍了 Go 语言的基本概念、优势、排名以及初学者应该如何选择学习 Python 还是 Go，提供了实用的学习建议和资源。","title":"什么是 Go ？Go 的优势和现状。初学者应该学习 Python 还是 Go？","type":"posts"},{"content":"","date":"2022-01-12","externalUrl":null,"permalink":"/tags/dokerfile/","section":"标签","summary":"","title":"Dokerfile","type":"tags"},{"content":"本篇分享在编写 Dockerfiles 和使用 Docker 时应遵循的一些最佳实践。篇幅较长，建议先收藏慢慢看，保证看完会很有收获。\n文章目录 # Dockerfile 最佳实践\n使用多阶段的构建 调整 Dockerfile 命令的顺序 使用小型 Docker 基础镜像 尽量减少层的数量 使用无特权的容器 优先选择 COPY 而不是 ADD 将 Python 包缓存到 Docker 主机上 每个容器只运行一个进程 优先选择数组而不是字符串语法 理解 ENTRYPOINT 和 CMD 之间的区别 添加健康检查 HEALTHCHECK Docker 镜像最佳实践\nDocker 镜像的版本 不要在镜像中存储密钥 使用 .dockerignore 文件 检查和扫描你的 Docker 文件和镜像 签署和验证镜像 Dockerfile 最佳实践 # 1. 使用多阶段的构建 # 利用多阶段构建的优势来创建更精简、更安全的Docker镜像。多阶段 Docker 构建(multi-stage builds)允许你将你的 Dockerfile 分成几个阶段。\n例如，你可以有一个阶段用于编译和构建你的应用程序，然后可以复制到后续阶段。由于只有最后一个阶段被用来创建镜像，与构建应用程序相关的依赖关系和工具就会被丢弃，因此可以留下一个精简的、模块化的、可用于生产的镜像。\nWeb 开发示例：\n# 临时阶段 FROM python:3.9-slim as builder WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y --no-install-recommends gcc COPY requirements.txt . RUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt # 最终阶段 FROM python:3.9-slim WORKDIR /app COPY --from=builder /app/wheels /wheels COPY --from=builder /app/requirements.txt . RUN pip install --no-cache /wheels/* 在这个例子中，GCC 编译器在安装某些 Python 包时是必需的，所以我们添加了一个临时的、构建时的阶段来处理构建阶段。\n由于最终的运行时映像不包含 GCC，所以它更轻，也更安全。镜像大小比较：\nREPOSITORY TAG IMAGE ID CREATED SIZE docker-single latest 8d6b6a4d7fb6 16 seconds ago 259MB docker-multi latest 813c2fa9b114 3 minutes ago 156MB 再来看一个例子：\n# 临时阶段 FROM python:3.9 as builder RUN pip wheel --no-cache-dir --no-deps --wheel-dir /wheels jupyter pandas # 最终阶段 FROM python:3.9-slim WORKDIR /notebooks COPY --from=builder /wheels /wheels RUN pip install --no-cache /wheels/* 镜像大小比较：\nREPOSITORY TAG IMAGE ID CREATED SIZE ds-multi latest b4195deac742 2 minutes ago 357MB ds-single latest 7c23c43aeda6 6 minutes ago 969MB 总之，多阶段构建可以减少你的生产镜像的大小，帮助你节省时间和金钱。此外，这将简化你的生产容器。由于尺寸较小和简单，相对会有较小的攻击面。\n2. 调整 Dockerfile 命令的顺序 # 密切注意你的 Dockerfile 命令的顺序，以利用层缓存。\nDocker 在一个特定的 Docker 文件中缓存每个步骤（或层），以加快后续的构建。当一个步骤发生变化时，不仅该步骤，而且所有后续步骤的缓存都将被废止。\n例如：\nFROM python:3.9-slim WORKDIR /app COPY sample.py . COPY requirements.txt . RUN pip install -r /requirements.txt 在这个 Dockerfile 中，我们在安装需求之前复制了应用程序的代码。现在，每次我们改变 sample.py 时，构建都会重新安装软件包。这是非常低效的，特别是在使用 Docker 容器作为开发环境时。因此，把经常变化的文件放在 Dockerfile 的末尾是很关键的。\n你也可以通过使用 .dockerignore 文件来排除不必要的文件，使其不被添加到 Docker 构建环境和最终镜像中，从而帮助防止不必要的缓存失效。更多信息后面会提到。\n因此，在上面的 Dockerfile 中，你应该把 COPY sample.py . 命令移到底部，如下所示：\nFROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r /requirements.txt COPY sample.py . 注意。\n总是把可能发生变化的层放在 Dockerfile 中尽可能的低。 将多个 RUN apt-get update，RUN apt-get install 等命令结合到一起执行。(这也有助于减少镜像的大小，后面会很快就会提到这一点)。 如果你想关闭某个 Docker 构建的缓存，可以添加 --no-cache=True 标志。 3. 使用小型 Docker 基础镜像 # 较小的 Docker 镜像更具有模块化和安全性。较小的 Docker 基础镜像在构建、推送和拉动镜像的速度较小，它们也往往更安全，因为它们只包括运行应用程序所需的必要库和系统依赖。\n你应该使用哪个 Docker 基础镜像？这个没有一个固定的答案，它这取决于你要做什么。下面是 Python 的各种 Docker 基础镜像的大小比较。\nREPOSITORY TAG IMAGE ID CREATED SIZE python 3.9.6-alpine3.14 f773016f760e 3 days ago 45.1MB python 3.9.6-slim 907fc13ca8e7 3 days ago 115MB python 3.9.6-slim-buster 907fc13ca8e7 3 days ago 115MB python 3.9.6 cba42c28d9b8 3 days ago 886MB python 3.9.6-buster cba42c28d9b8 3 days ago 886MB 虽然基于 Alpine Linux 的 Alpine flavor 是最小的，但如果你找不到可以与之配合的编译二进制文件，往往会导致构建时间的增加。因此，你最终可能不得不自己构建二进制文件，这可能会增加镜像的大小（取决于所需的系统级依赖）和构建时间（由于必须从源头编译）。\n关于为什么最好不要使用基于 Alpine 的基础镜像，请参考适用于 Python 应用程序的最佳 Docker 基础映像 和 使用 Alpine 可以使 Python Docker 构建速度慢 50 倍 了解更多关于为什么最好避免使用基于 Alpine 的基础镜像。\n归根结底，这都是关于平衡的问题。如果有疑问，从 *-slim flavor 开始，特别是在开发模式下，因为你正在构建你的应用程序。你想避免在添加新的 Python 包时不得不不断地更新 Dockerfile 以安装必要的系统级依赖。当你为生产强化你的应用程序和 Dockerfile 时，你可能想探索使用 Alpine 来完成多阶段构建的最终镜像。\n另外，别忘了定期更新你的基础镜像，以提高安全性和性能。当一个基础镜像的新版本发布时，例如：3.9.6-slim \u0026ndash;\u0026gt; 3.9.7-slim，你应该拉出新的镜像并更新你正在运行的容器以获得所有最新的安全补丁。\n4. 尽量减少层的数量 # 尽量把 RUN、COPY 和 ADD 命令结合起来使用，因为它们会创建层。每一层都会增加镜像的大小，因为它们是被缓存的。因此，随着层数的增加，镜像大小也会增加。\n你可以用 docker history 命令来测试一下。\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE dockerfile latest 180f98132d02 51 seconds ago 259MB docker history 180f98132d02 IMAGE CREATED CREATED BY SIZE COMMENT 180f98132d02 58 seconds ago COPY . . # buildkit 6.71kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 58 seconds ago RUN /bin/sh -c pip install -r requirements.t… 35.5MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago COPY requirements.txt . # buildkit 58B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; About a minute ago WORKDIR /app ... 请注意尺寸。只有 RUN、COPY 和 ADD 命令增加了镜像的尺寸，你可以尽可能地通过合并命令来减少镜像的大小。比如：\nRUN apt-get update RUN apt-get install -y gcc 可以合并成一个 RUN 命令：\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y gcc 因此，创建一个单层而不是两个，这就减少了最终镜像的大小。虽然减少层数是个好主意，但更重要的是，这本身不是一个目标，而是减少镜像大小和构建时间的一个副作用。换句话说呢，与其试图优化每一条命令，你更应该关注前面的三种做法！！！\n多阶段构建 Dockerfile命令的顺序 以及使用一个小的基础镜像。 注意 # RUN、COPY 和 ADD 都会创建图层 每个图层都包含与前一个图层的差异 图层会增加最终镜像的大小 提示 # 合并相关命令 在创建过程中执行 RUN 步骤中删除不必要的文件 尽量减少运行 apt-get upgrade 的次数，因为它将所有软件包升级到最新版本。 对于多阶段的构建，不要太担心过度优化临时阶段的命令 最后，为了便于阅读，建议将多行参数按字母数字排序。\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ git \\ gcc \\ matplotlib \\ pillow \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* 5. 使用无特权的容器 # 默认情况下，Docker 在容器内以 root 身份运行容器进程。然而，这是一个糟糕的做法，因为在容器内以 root 身份运行的进程在 Docker 主机中也是以 root 身份运行。\n因此，如果攻击者获得了对容器的访问权，他们就可以获得所有的 root 权限，并可以对 Docker 主机进行一些攻击，例如：\n将敏感信息从主机的文件系统复制到容器中 执行远程命令 为了防止这种情况，确保以非 root 用户运行容器进程。\nRUN addgroup --system app \u0026amp;\u0026amp; adduser --system --group app USER app 你可以更进一步，删除 shell 权限，确保没有主目录。\nRUN addgroup --gid 1001 --system app \u0026amp;\u0026amp; \\ adduser --no-create-home --shell /bin/false --disabled-password --uid 1001 --system --group app USER app 验证\ndocker run -i sample id uid=1001(app) gid=1001(app) groups=1001(app) 在这里，容器内的应用程序在一个非 root 用户下运行。然而，请记住，Docker 守护进程和容器本身仍然是以 root 权限运行的。\n请务必查看以非根用户身份运行 Docker 守护进程，以获得以非根用户身份运行守护进程和容器的帮助。\n6. 优先选择 COPY 而不是 ADD # 除非你确定你需要 ADD 所带来的额外功能，否则请使用 COPY。\n那么 COPY 和 ADD 的区别是什么？\n首先，这两个命令都允许你从一个特定的位置复制文件到 Docker 镜像中。\nADD \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; COPY \u0026lt;src\u0026gt; \u0026lt;dest\u0026gt; 虽然它们看起来作用相同，但 ADD 有一些额外的功能。\nCOPY 用于将本地文件或目录从 Docker 主机复制到镜像上。 ADD 可以用于同样的事情，也可以用于下载外部文件。另外，如果你使用压缩文件（tar、gzip、bzip2等）作为 参数，ADD 会自动将内容解压到指定位置。 # 将主机上的本地文件复制到目的地 COPY /source/path /destination/path ADD /source/path /destination/path # 下载外部文件并复制到目的地 ADD http://external.file/url /destination/path # 复制和提取本地压缩文件 ADD source.file.tar.gz /destination/path 最后 COPY 在语义上比 ADD 更加明确和更容易理解。\n7. 缓存安装包到 Docker 主机上 # 当一个需求文件被改变时，镜像需要被重建以安装新的包。先前的步骤将被缓存，正如在最小化层数中提到的。在重建镜像时下载所有的包会导致大量的网络活动，并需要大量的时间。每次重建都要占用同等的时间来下载不同构建中的通用包。\n以 Python 为例，你可以通过将 pip 缓存目录映射到主机上的一个目录来避免这种情况。所以对于每次重建，缓存的版本会持续存在，这可以提高构建速度。\n在 Docker 运行中添加一个卷，作为 -v $HOME/.cache/pip-docker/:/root/.cache/pip 或者作为 Docker Compose 文件中的映射。\n上面介绍的目录只供参考，要确保你映射的是 cache 目录，而不是 site-packages（内置包所在的位置）。\n将缓存从 docker 镜像中移到主机上可以为你节省最终镜像的空间。\n# 忽略 ... COPY requirements.txt . RUN --mount=type=cache,target=/root/.cache/pip \\ pip install -r requirements.txt # 忽略 ... 8. 每个容器只运行一个进程 # 为什么建议每个容器只运行一个进程？\n让我们假设你的应用程序栈由两个 Web 服务器和一个数据库组成。虽然你可以很容易地从一个容器中运行所有三个，但你应该在一个单独的容器中运行每个服务，以便更容易重复使用和扩展每个单独的服务。\n扩展性 - 由于每个服务都在一个单独的容器中，你可以根据需要水平地扩展你的一个网络服务器来处理更多的流量。 可重用性 - 也许你有另一个服务需要一个容器化的数据库，你可以简单地重复使用同一个数据库容器，而不需要带着两个不必要的服务。 日志 - 耦合容器会让日志变得更加复杂。（我们将在本文后面进一步详细讨论这个问题） 可移植性和可预测性 - 当容器有较少的部分在工作时，制作安全补丁或调试问题就会容易得多。 9. 优先选择数组而不是字符串语法 # 你可以在你的 Dockerfiles 中以数组（exec）或字符串（shell）格式\n在 Dockerfile 中，你可以以数组（exec）或字符串（shell）格式来使用 CMD 和 ENTRYPOINT 命令\n# 数组（exec） CMD [\u0026#34;gunicorn\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;-k\u0026#34;, \u0026#34;uvicorn.workers.UvicornWorker\u0026#34;, \u0026#34;main:app\u0026#34;] # 字符串（shell） CMD \u0026#34;gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app\u0026#34; 两者都是正确的，并且实现了几乎相同的事情；但是，你应该尽可能地使用 exec 格式。\n以下来自 Docker的官方文档内容：\n确保你在 Dockerfile 中使用 CMD 和 ENTRYPOINT 的 exec 形式。 例如，使用 [\u0026quot;program\u0026quot;, \u0026quot;arg1\u0026quot;, \u0026quot;arg2\u0026quot;] 而不是 \u0026quot;program arg1 arg2\u0026quot;。使用字符串形式会导致 Docker 使用 bash 运行你的进程，而 bash 并不能正确处理信号。Compose 总是使用 JSON 形式，所以不用担心如果你在你的 Compose 文件中覆盖了命令或入口。 因此，由于大多数 shell 不处理对子进程的信号，如果你使用 shell 格式，CTRL-C（产生 SIGTERM）可能不会停止一个子进程。\n例子:\nFROM ubuntu:18.04 # BAD: 字符串（shell）格式 ENTRYPOINT top -d # GOOD: 数组（exec）格式 ENTRYPOINT [\u0026#34;top\u0026#34;, \u0026#34;-d\u0026#34;] 这两种情况执行效果一样。但请注意，在字符串（shell）格式的情况下，CTRL-C 不会杀死这个进程。相反，你会看到 ^C^C^C^C^C^C^C^C^C^C。\n另一个注意事项是，字符串（shell）格式携带的是 shell 的 PID，而不是进程本身。\n# 数组格式 root@18d8fd3fd4d2:/app# ps ax PID TTY STAT TIME COMMAND 1 ? Ss 0:00 python manage.py runserver 0.0.0.0:8000 7 ? Sl 0:02 /usr/local/bin/python manage.py runserver 0.0.0.0:8000 25 pts/0 Ss 0:00 bash 356 pts/0 R+ 0:00 ps ax # 字符串格式 root@ede24a5ef536:/app# ps ax PID TTY STAT TIME COMMAND 1 ? Ss 0:00 /bin/sh -c python manage.py runserver 0.0.0.0:8000 8 ? S 0:00 python manage.py runserver 0.0.0.0:8000 9 ? Sl 0:01 /usr/local/bin/python manage.py runserver 0.0.0.0:8000 13 pts/0 Ss 0:00 bash 342 pts/0 R+ 0:00 ps ax 10. 了解 ENTRYPOINT 和 CMD 之间的区别 # 我应该使用 ENTRYPOINT 还是 CMD 来运行容器进程？有两种方法可以在容器中运行命令。\nCMD [\u0026#34;gunicorn\u0026#34;, \u0026#34;config.wsgi\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:8000\u0026#34;] # 和 ENTRYPOINT [\u0026#34;gunicorn\u0026#34;, \u0026#34;config.wsgi\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:8000\u0026#34;] 两者本质上做的是同一件事：用 Gunicorn 服务器在 config.wsgi 启动应用程序，并将其绑定到 0.0.0.0:8000。\nCMD 很容易被重写。如果你运行 docker run \u0026lt;image_name\u0026gt; uvicorn config.asgi，上述 CMD 就会被新的参数所取代。\n例如，uvicorn config.asgi。而要覆盖 ENTRYPOINT 命令，必须指定 --entrypoint 选项。\ndocker run --entrypoint uvicorn config.asgi \u0026lt;image_name\u0026gt; 在这里，很明显，我们正在覆盖入口点。所以，建议使用 ENTRYPOINT 而不是 CMD，以防止意外地覆盖命令。\n它们也可以一起使用。比如说\nENTRYPOINT [\u0026#34;gunicorn\u0026#34;, \u0026#34;config.wsgi\u0026#34;, \u0026#34;-w\u0026#34;] CMD [\u0026#34;4\u0026#34;] 当像这样一起使用时，为启动容器所运行的命令就变成了：\ngunicorn config.wsgi -w 4 如上所述，CMD 很容易被重写。因此，CMD 可以被用来向 ENTRYPOINT 命令传递参数。比如很容易更改 workers 的数量，就像这样：\ndocker run \u0026lt;image_name\u0026gt; 6 这样就将有 6 个 Gunicorn workers 启动容器，而不是默认的 4 个。\n11. 添加健康检查 HEALTHCHECK # 使用 HEALTHCHECK 来确定容器中运行的进程是否不仅已启动并正在运行，而且是“健康”的。\nDocker 公开了一个 API 来检查容器中运行的进程的状态，它提供的信息不仅仅是进程是否“正在运行”，因为“运行”涵盖了“它正在运行”、“仍在启动”、甚至“陷入某种无限循环错误状态”。你可以通过 HEALTHCHECK 指令与此 API 交互。\n例如，如果你正在提供 Web 应用程序，则可以使用以下内容来确定 / 端点是否已启动并可以处理服务请求：\nHEALTHCHECK CMD curl --fail http://localhost:8000 || exit 1 如果你运行 docker ps，你可以看到 HEALTHCHECK 的状态。\n健康的示例\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 09c2eb4970d4 healthcheck \u0026#34;python manage.py ru…\u0026#34; 10 seconds ago Up 8 seconds (health: starting) 0.0.0.0:8000-\u0026gt;8000/tcp, :::8000-\u0026gt;8000/tcp xenodochial_clarke 不健康的示例\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 09c2eb4970d4 healthcheck \u0026#34;python manage.py ru…\u0026#34; About a minute ago Up About a minute (unhealthy) 0.0.0.0:8000-\u0026gt;8000/tcp, :::8000-\u0026gt;8000/tcp xenodochial_clarke 你可以更进一步，设置一个仅用于健康检查的自定义端点，然后配置 HEALTHCHECK 以针对返回的数据进行测试。\n例如，如果端点返回 {\u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot;} 的 JSON 响应，你可以指示 HEALTHCHECK 验证响应正文。\n以下是使用 docker inspect 查看运行状况检查状态的方法：\n这里省略了部分输出。\n❯ docker inspect --format \u0026#34;{{json .State.Health }}\u0026#34; ab94f2ac7889 { \u0026#34;Status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;FailingStreak\u0026#34;: 0, \u0026#34;Log\u0026#34;: [ { \u0026#34;Start\u0026#34;: \u0026#34;2021-09-28T15:22:57.5764644Z\u0026#34;, \u0026#34;End\u0026#34;: \u0026#34;2021-09-28T15:22:57.7825527Z\u0026#34;, \u0026#34;ExitCode\u0026#34;: 0, \u0026#34;Output\u0026#34;: \u0026#34;...\u0026#34; 你还可以向 Docker Compose 文件添加运行状况检查：\nversion: \u0026#34;3.8\u0026#34; services: web: build: . ports: - \u0026#39;8000:8000\u0026#39; healthcheck: test: curl --fail http://localhost:8000 || exit 1 interval: 10s timeout: 10s start_period: 10s retries: 3 选项：\ntest：要测试的命令。 interval：要测试的间隔 - 即，测试每 x 时间单位。 timeout：等待响应的最长时间。 start_period：何时开始健康检查。它可以在容器准备就绪之前执行其他任务时使用，例如运行迁移。 retries：在将测试指定为失败之前的最大重试次数。 如果你使用的是 Docker Swarm 以外的编排工具（比如 Kubernetes 或 AWS ECS），它们很可能有自己的内部系统来处理健康检查。在添加 HEALTHCHECK 指令之前，请参阅特定工具的文档。\nDocker 镜像最佳实践 # 1. Docker 镜像版本 # 只要有可能，就要避免使用 latest 标签的镜像。\n如果你依赖 latest 标签（这并不是一个真正的 \u0026ldquo;标签\u0026rdquo;，因为当镜像没有明确的标签时，它是默认应用的），你无法根据镜像标签来判断你的代码正在运行哪个版本。\n如果你想回滚就变得很困难，并且很容易被覆盖（无论是意外还是恶意的）。标签，就像你的基础设施和部署，应该是不可改变的。\n所以无论你如何对待你的内部镜像，都不应该对基本镜像使用 latest 标签，因为你可能会无意中把一个带有破坏性变化的新版本部署到生产中。\n对于内部镜像，应使用描述性的标签，以便更容易分辨哪个版本的代码正在运行，处理回滚，并避免命名冲突。例如，你可以使用以下描述符来组成一个标签。\n时间戳 Docker 镜像 ID Git 提交哈希值 语义版本 (Semantic version) 关于更多的选择，也可以参考 Stack Overflow 问题 \u0026ldquo;Properly Versioning Docker Images\u0026rdquo; 中的这个答案。\n比如说\ndocker build -t web-prod-b25a262-1.0.0 . 在这里，我们用下面的内容来形成标签\n项目名称：web 环境名称: prod Git commit short hash: b25a262 (通过命令 git rev-parse --short HEAD 来获得) 语义学版本：1.0.0 选择一个标签方案并与之保持一致是至关重要的。由于提交哈希值（commit hashes）可以很容易地将镜像标签与代码联系起来，建议将它们纳入你的标签方案。\n2. 不要在镜像中存储机密信息 # Secrets 是敏感的信息，如密码、数据库凭证、SSH密钥、令牌和 TLS 证书等。这些信息不应该在没有加密的情况下被放入你的镜像中，因为未经授权的用户如果获得了镜像的访问权，只需要检查这些层就可以提取密钥。\n因此不要在 Docker 文件中添加明文的密钥，尤其是当你把镜像推送到像 Docker Hub 这样的公共仓库！！\nFROM python:3.9-slim ENV DATABASE_PASSWORD \u0026#34;SuperSecretSauce\u0026#34; 相反，它们应该通过以下方式注入\n环境变量（在运行时) 构建时参数（在构建时) 协调工具，如 Docker Swarm（通过 Docker secrets）或 Kubernetes（通过 Kubernetes secrets）。 此外，你还可以通过在你的 .dockerignore 文件中添加常见的密钥文件和文件夹来帮助防止密钥的泄露。\n**/.env **/.aws **/.ssh 最后，要明确哪些文件会被复制到镜像中，而不是递归地复制所有文件。\n# 不好的做法 COPY . . # 好的做法 COPY ./app.py . 明确的做法也有助于限制缓存的破坏。\n环境变量 # 你可以通过环境变量来传递密钥，但它们会在所有子进程、链接的容器和日志以及 docker inspect 中可见。要更新它们也很困难。\ndocker run --detach --env \u0026#34;DATABASE_PASSWORD=SuperSecretSauce\u0026#34; python：3.9-slim b25a262f870eb0fdbf03c666e7fcf18f9664314b79ad58bc7618ea3445e39239 docker inspect --format=\u0026#39;{{range .Config.Env}}{{println .}}{{end}}\u0026#39; b25a262f870eb0fdbf03c666e7fcf18f9664314b79ad58bc7618ea3445e39239 DATABASE_PASSWORD=SuperSecretSauce PATH=/usr/local/bin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin LANG=C.UTF-8 GPG_KEY=E3FF2839C048B25C084DEBE9B26995E310250568 python_version=3.9.7 python_pip_version=21.2.4 python_setuptools_version=57.5.0 python_get_pip_url=https://github.com/pypa/get-pip/raw/c20b0cfd643cd4a19246ccf204e2997af70f6b21/public/get-pip.py PYTHON_GET_PIP_SHA256=fa6f3fb93cce234cd4e8dd2beb54a51ab9c247653b52855a48dd44e6b21ff28b 这是最直接的密钥管理方法。虽然它不是最安全的，但它会让诚实的人保持诚实，因为它提供了一个薄薄的保护层，有助于使密钥不被好奇的游荡的眼睛发现。\n使用共享卷传递密钥是一个更好的解决方案，但它们应该被加密，通过 Vault 或 AWS密钥管理服务（KMS），因为它们被保存到磁盘。\n构建时参数 # 你可以在构建时使用构建时参数来传递密钥，但这些密钥对于那些可以通过 docker 历史访问镜像的人来说是可见的。\n例子\nFROM python:3.9-slim ARG DATABASE_PASSWORD 构建\ndocker build --build-arg \u0026#34;DATABASE_PASSWORD=SuperSecretSauce\u0026#34; . 如果你只需要临时使用密钥作为构建的一部分。例如，用于克隆私有 repo 或下载私有软件包的 SSH 密钥。你应该使用多阶段构建，因为构建者的历史会被临时阶段忽略。\n# 临时阶段 FROM python:3.9-slim as builder # 密钥参数 arg ssh_private_key # 安装 git RUN apt-get update \u0026amp;\u0026amp; （运行 apt-get update）。 apt-get install -y --no-install-recommends git # 使用 ssh 密钥来克隆 repo RUN mkdir -p /root/.ssh/ \u0026amp;\u0026amp; \\\\ echo \u0026#34;${PRIVATE_SSH_KEY}\u0026#34; \u0026gt; /root/.ssh/id_rsa RUN touch /root/.ssh/known_hosts \u0026amp; \u0026amp; ssh-keyscan bitbucket.org \u0026gt;\u0026gt; /root/.ssh/known_hosts RUN git clone git@github.com:testdrivenio/not-real.git # 最后阶段 FROM python:3.9-slim 工作目录 /app # 从临时镜像中复制版本库 COPY --from=builder /your-repo /app/your-repo 多阶段构建只保留了最终镜像的历史。你可以把这个功能用于你的应用程序需要的永久密钥，比如数据库凭证。\n你也可以使用 docker build 中新的 --secret 选项来向 Docker 镜像传递密钥，这些密钥不会被存储在镜像中。\n# \u0026#34;docker_is_awesome\u0026#34; \u0026gt; secrets.txt FROM alpine # 从默认的密钥位置显示密钥。 RUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret 这将装载 secrets.txt 文件中的密钥。\n构建镜像\ndocker build --no-cache --progress=plain --secret id=mysecret,src=secrets.txt . # 输出 ... #4 [1/2] FROM docker.io/library/alpine #4 sha256:665ba8b2cdc0cb0200e2a42a6b3c0f8f684089f4cd1b81494fbb9805879120f7 #4 缓存的 #5 [2/2] RUN --mount=type=secret,id=mysecret cat /run/secrets/myecret #5 sha256:75601a522ebe80ada66dedd9dd86772ca932d30d7e1b11bba94c04aa55c237de #5 0.635 docker_is_awesome#5 DONE 0.7s #6 导出到镜像 最后，检查历史记录，看看密钥是否泄露了。\n❯ docker history 49574a19241c IMAGE CREATED CREATED BY SIZE COMMENT 49574a19241c 5 minutes ago CMD [\u0026#34;/bin/sh\u0026#34;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 5 minutes ago RUN /bin/sh -c cat /run/secrets/mysecret # b… 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 4 weeks ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/sh\u0026#34;] 0B \u0026lt;missing\u0026gt; 4 weeks ago /bin/sh -c #(nop) ADD file:aad4290d27580cc1a… 5.6MB Docker 密钥 # 如果你正在使用 Docker Swarm，你可以用 Docker secrets 来管理密钥。\n例如，启动 Docker Swarm 模式。\ndocker swarm init 创建一个 docker 密钥。\necho \u0026#34;supersecretpassword\u0026#34; | docker secret create postgres_password - qdqmbpizeef0lfhyttxqfbty0 docker secret ls ID NAME DRIVER CREATED UPDATED qdqmbpizeef0lfhyttxqfbty0 postgres_password 4 seconds ago 4 seconds ago 当一个容器被赋予上述密钥的访问权时，它将挂载在 /run/secrets/postgres_password。这个文件将包含明文的密钥的实际值。\n使用其他的编排工具？\n使用 AWS Secrets Manager 的密钥与 Kubernetes 的密钥 DigitalOcean Kubernetes - 保护 DigitalOcean Kubernetes 集群的推荐步骤 Google Kubernetes引擎 - 与其他产品一起使用密钥管理器 Nomad - Vault 集成和检索动态密钥 3. 使用 .dockerignore 文件 # 之前已经提到过几次使用 .dockerignore 文件。这个文件用来指定你不希望被添加到发送给 Docker 守护进程的初始构建上下文中的文件和文件夹，后者将构建你的镜像。换句话说，你可以用它来定义你需要的构建环境。\n当一个 Docker 镜像被构建时，整个 Docker 上下文 - 即你的项目的根在 COPY 或 ADD 命令执行之前就被发送给了 Docker 守护进程。\n这可能是相当费资源，尤其是当你的项目中有许多依赖关系、大量的数据文件或构建工件时。\n另外，当 Docker CLI 和守护程序不在同一台机器上。比如守护进程是在远程机器上执行的，你就更应该注意构建环境的大小了。\n你应该在 .dockerignore 文件中添加什么？\n临时文件和文件夹 构建日志 本地 secrets 本地开发文件，如 docker-compose.yml 版本控制文件夹，如 \u0026ldquo;.git\u0026rdquo;、\u0026quot;.hg\u0026quot; 和 \u0026ldquo;.vscode\u0026rdquo; 等 例子：\n**/.git **/.gitignore **/.vscode **/coverage **/.env **/.aws **/.ssh Dockerfile README.md docker-compose.yml **/.DS_Store **/venv **/env 总之，结构合理的 .dockerignore 可以帮助\n减少 Docker 镜像的大小 加快构建过程 防止不必要的缓存失效 防止泄密 4. 检查并扫描你的 Dockerfile 和镜像 # Linting 是检查源代码中是否存在可能导致潜在缺陷的编程和风格错误以及不良做法的过程。就像编程语言一样，静态文件也可以被 lint。特别是对于你的 Dockerfile，linter 可以帮助确保它们的可维护性、避免弃用语法并遵守最佳实践。整理镜像应该是 CI 管道的标准部分。\nHadolint 是最流行的 Dockerfile linter：\nhadolint Dockerfile Dockerfile:1 DL3006 warning: Always tag the version of an image explicitly Dockerfile:7 DL3042 warning: Avoid the use of cache directory with pip. Use `pip install --no-cache-dir \u0026lt;package\u0026gt;` Dockerfile:9 DL3059 info: Multiple consecutive `RUN` instructions. Consider consolidation. Dockerfile:17 DL3025 warning: Use arguments JSON notation for CMD and ENTRYPOINT arguments 这是 Hadolint 一个在线的链接 https://hadolint.github.io/hadolint/ 也可以安装 VS Code 插件\n你可以将 Dockerfile 与扫描镜像和容器的漏洞结合使用。\n以下是一些有影响力的镜像扫描工具：\nSnyk 是 Docker 本地漏洞扫描的独家提供商。你可以使用 docker scan CLI 命令来扫描镜像。 Trivy 可用于扫描容器镜像、文件系统、git 存储库和其他配置文件。 Clair 是一个开源项目，用于对应用程序容器中的漏洞进行静态分析。 Anchore 是一个开源项目，为容器镜像的检查、分析和认证提供集中式服务。 总而言之，对你的 Dockerfile 和镜像进行 lint 和扫描，来发现任何偏离最佳实践的潜在问题。\n5. 签名和验证镜像 # 你怎么知道用于运行生产代码的镜像没有被篡改？\n篡改可以通过中间人（MITM）攻击或注册表被完全破坏来实现。Docker 内容信任（DCT）可以对来自远程注册中心的 Docker 镜像进行签名和验证。\n为了验证镜像的完整性和真实性，请设置以下环境变量。\nDOCKER_CONTENT_TRUST=1 现在，如果你试图拉一个没有被签名的镜像，你会收到以下错误。\nError: remote trust data does not exist for docker.io/namespace/unsigned-image: notary.docker.io does not have trust data for docker.io/namespace/unsigned-image 你可以从使用 Docker 内容信任签署镜像文档中了解签署镜像的情况。\n当从 Docker Hub下 载镜像时，确保使用官方镜像或来自可信来源的经过验证的镜像。较大的团队应该使用他们自己的内部私有容器仓库\n6. 设置内存和 CPU 的限制 # 限制 Docker 容器的内存使用是一个好主意，特别是当你在一台机器上运行多个容器时。这可以防止任何一个容器使用所有可用的内存，从而削弱其他容器的功能。\n限制内存使用的最简单方法是在 Docker cli 中使用 --memory 和 --cpu 选项。\ndocker run --cpus=2 -m 512m nginx 上述命令将容器的使用限制在 2 个 CPU 和 512 兆的内存。\n你可以在 Docker Compose 文件中做同样的事情，像这样。\nversion: \u0026#34;3.9\u0026#34; services: redis: image: redis:alpine deploy: resources: limits: cpus: 2 memory: 512M reservations: cpus: 1 memory: 256M 请注意 reservations 字段。它是用来设置软限制的，当主机的内存或CPU资源不足时，它就会优先考虑。\n其他相关资源\n带有内存、CPU和GPU的运行时选项：https://docs.docker.com/config/containers/resource_constraints/ Docker Compose 的资源限制：https://docs.docker.com/compose/compose-file/compose-file-v3/#resources 总结 # 以上就是本文介绍的 17 条最佳实践，掌握这些最佳实践一定会让你的 Dockerfile 和 Docker Image 变得精简，干净，和安全。\n本文出自 Docker Best Practices for Python Developers。\n欢迎扫码关注公众号「DevOps攻城狮」- 专注于DevOps领域知识分享。\n","date":"2022-01-12","externalUrl":null,"permalink":"/posts/docker-best-practice/","section":"Posts","summary":"本文分享了在编写 Dockerfiles 和使用 Docker 时应遵循的一些最佳实践，包括多阶段构建、镜像优化、安全性等方面的建议。","title":"你一定要了解这 17 条 Docker 最佳实践！","type":"posts"},{"content":"工作十几年用过了不少显示器，从最初的 17寸，到后来的 23寸、27寸、32寸、再到现在的 34 寸，根据我自己的使用体验，来个主观推荐：\n第一名，一个34寸曲面显示器 第二名，一个27寸 + 一个23寸的双屏组合 第三名，一个32寸 + 一个23寸的双屏组合 第三名，两个 23 寸的双屏组合（并列第三名）\n以上这些屏幕推荐购买 2K 及以上的分辨率，1080p 的分辨率不推荐。\n下面我就按照时间轴说说我用过的那些显示器。\n我用过的显示器 # 双屏 23寸 1080p # 五年前公司配置的是两个 23 寸的 1080p 显示器，还有显示器支架，这在当时是非常好用的组合了，对工作效率的提升确实很有帮助。\n（真不知道那些不肯在显示器上花钱的公司是怎么想的，换个大屏显示器是花最少的钱提高程序员效率的最有效办法了）\n放在现在 1080p 的分辨率已经不推荐了，建议购买 2K 以及以上分辨率的显示屏。\n单屏 27寸 2K # 在家办公和学习苦于家里的办公桌大小有限，放置两个 23 寸显示器没那么多空间，因此我打算购买一个 27 寸显示器。 但分辨率是继续选择 1080p 还是 2K 呢？\n当时我看好的 Dell 一款 2K 分辨率的显示器，就是价格有点贵了（两千多），于是我就在网上买了另外一款 1080p 的 27 寸显示器。\n当我拿到这款 1080p 的 27 寸显示器，能明显的看到它的颗粒感，一旦注意到了就没法再忽略它了。最后退而求其次，花了不到 1500 选择了优派的一款 27 寸 2K 显示器。\n这一用就是三年，直到今年搬家了，我有了更大的办公桌，27 寸的 2K 显示器在大小上已经不太能满足我了，我就开始搜罗新的升级目标。\n双屏 27寸 2K # 我开始考虑是购买一个 34 寸显示器还是两个 27 寸 2K 显示器？\n最后处于成本的考虑，我打算在闲鱼上收一台同款的 27 寸 2K 显示器组成双屏。但因为货源确实很少，加上一本不出外地，迟迟没有买到，最后不得不考虑购买两个新的 27 寸 2K 显示器。我再次在购买的两块 27 寸的 2K 显示器，还在闲鱼上买的显示器支架全都到货了，就等快乐的组装了。\n可当我把两个显示器都摆在桌子上，我后悔了，比我想象中的要长，一米六的桌子都快盛不下它们了，另外可能是我支架没调好的原因，这两个显示器摆放在一起的时候中间有一条上宽下窄的缝 \u0026hellip; \u0026hellip;\n曲面单屏 34寸 2K # 没有尝试过真的不知道到底什么才是适合自己的。现在我终于知道了\u0026hellip; 目前最适合我的就是 34寸 显示曲面屏。\n为什么是曲面的呢？\n我目前公司的显示器组合是 32寸2K + 23寸1080p，这个32寸显示器就是直面屏幕的，在看屏幕边缘的内容时没有曲面屏幕来的舒服。\n回到34寸曲面屏，市场中种类繁多，如果不想踩坑（折腾），建议直接买小米34寸曲面显示器。双十一钱购买的好像不足 1900。\n最后 # 以上就是我用过的显示器的简单分享，比较主流但也比较有限。比如我没用过超过 2K 分辨率以上的显示器，也没用过更大尺寸和三屏以上的组合。\n最后还是希望这个分享对你选购显示器有一点点帮助。\n","date":"2021-12-21","externalUrl":null,"permalink":"/posts/choose-monitor/","section":"Posts","summary":"本文分享了个人在选择显示器时的经验和建议，包括不同尺寸、分辨率和屏幕组合的优缺点，以及如何根据工作需求选择最合适的显示器。","title":"2022年序员如何选择显示器？1080p还是2K? 单屏还是多屏？","type":"posts"},{"content":"","date":"2021-12-21","externalUrl":null,"permalink":"/tags/monitor/","section":"标签","summary":"","title":"Monitor","type":"tags"},{"content":"","date":"2021-12-07","externalUrl":null,"permalink":"/tags/badge/","section":"标签","summary":"","title":"Badge","type":"tags"},{"content":"","date":"2021-12-07","externalUrl":null,"permalink":"/tags/cd/","section":"标签","summary":"","title":"CD","type":"tags"},{"content":" 问题 # 在一个组织内，不同的团队之间可能会有不同的维度来评估 CI/CD 的成熟度。这使得对衡量每个团队的 CI/CD 的表现变得困难。\n如何快速评估哪些项目遵循最佳实践？如何更容易地构建高质量的安全软件？组织内需要建立一个由团队成员一起讨论出来的最佳实践来帮助团队建立明确的努力方向。\n如何评估 # 这里我参考了开源项目 CII 最佳实践徽章计划，这是 Linux 基金会 (LF) 发起的一个开源项目。它提供一系列自由/开源软件 (FLOSS) 项目的最佳实践方法。参照这些最佳实践标准的项目可以进行自认证, 以获得核心基础设施促进会(CII)徽章。要做到这点无需任何费用，你的项目可以使用 Web 应用（BadgeApp) 来证明是如何符合这些实践标准的以及其详细状况。\n这些最佳实践标准可以用来：\n鼓励项目遵循最佳实践。 帮助新的项目找到那些它们要遵循的最佳实践 帮助用户了解哪些项目遵循了最佳实践（这样用户可以更倾向于选择此类项目）。 最佳实践包含以下五个标准：基本，变更控制，报告，质量，安全，分析。\n更多关于标准的细分可以参考 CII 中文文档 或 CII 英文文档。\n已经很多知名的项目比如 Kubernetes, Node.js 等在使用这个最佳实践徽章计划\n如果你的项目在 GitHub 上或是你可以按照上述的徽章计划进行评估，就可以使用它来评估你项目的最佳实践，并可以在项目主页的 README 上显示徽章结果。\n定制最佳实践标准 # 如果上述项目不能满足你的评估要求，结合我的实践，制定了如下“最佳实践标准”并分配了相应的成熟度徽章，供参考。\n计算规则 # 每个最佳实践标准都有分数，通常一般的标准是10分，重要的标准是20分 带有🔰的最佳实践标准表示“一定要有” 带有👍的最佳实践标准表示“应当有” 每个项目的最佳实践标准分数之和落在的区间获得对应的徽章 徽章分数对照表 # 徽章 分数 描述 🚩WIP \u0026lt; 100 小于100分获得 🚩Work In Progress 徽章 ✔️PASSING = 100 等于100分获得 ✔️PASSING 徽章 🥈SILVER \u0026gt; 100 \u0026amp;\u0026amp; \u0026lt;= 150 大于100，小于等于150分获得🥈银牌徽章 🥇GOLD \u0026gt; 150 大于等于150分获得🥇金牌徽章 注：这个分数区间可调整。\n最佳实践标准和分数 # 类别 最佳实践标准 分数 描述 基本 🔰构建任何分支 20 Jenkins：支持任何分支构建 🔰构建任何PR 20 Jenkins：支持对任何 Pull Request 在 Merge 之前进行构建 🔰上传制品 10 Jenkins：构建产物上传到制品仓库保存 👍容器化构建 10 推荐使用容器化技术实现Pipeline 质量 🔰自动化测试 20 Jenkins：支持触发冒烟/单元/回归测试 👍性能测试 10 Jenkins：支持触发性能测试 👍代码覆盖率收集 10 Jenkins：支持获得代码覆盖率 安全 🔰漏洞扫描 10 Jenkins：支持触发漏洞扫描 🔰License扫描 10 Jenkins：支持触发证书扫描 分析 👍Code Lint 10 Jenkins：支持对PR进行代码格式检查 👍静态代码分析 10 Jenkins：支持对PR进行静态代码分析 👍动态代码分析 10 Jenkins：支持对PR进行动态代码分析 报告 🔰Email或Slack通知 10 支持通过Email或Slack等方式通知 注：以Jenkins为例。\n最终的结果 # No Repository Name 实现的最佳实践标准 徽章 1 project-a 🔰构建任何分支🔰构建任何PR🔰上传制品🔰自动化测试🔰Email或Slack通知 🚩WIP 2 project-b 🔰构建任何分支🔰构建任何PR🔰上传制品🔰自动化测试🔰漏洞扫描🔰License扫描🔰Email或Slack通知 ✔️PASSING 3 project-c 🔰构建任何分支🔰构建任何PR🔰上传制品👍容器化构建🔰自动化测试🔰漏洞扫描🔰License扫描🔰Email或Slack通知 🥈SILVER 4 project-d 🔰构建任何分支🔰构建任何PR🔰上传制品👍容器化构建🔰自动化测试👍性能测试👍代码覆盖率收集🔰漏洞扫描🔰License扫描👍Code Lint👍静态代码分析👍动态代码分析🔰Email或Slack通知 🥇GOLD Q\u0026amp;A # Q: 为什么使用徽章而不是分数？\nA: 使用徽章能更好的帮助团队朝着目标而不是分数努力。\nQ: 建立最佳实践标准还有哪些帮助？\nA: 团队之间容易进行技术共享，更容易地构建高质量的安全软件，保持团队之间在统一的高水准。\n","date":"2021-12-07","externalUrl":null,"permalink":"/posts/cicd-assessment/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 的 generic-webhook-trigger 插件来实时获取 Bitbucket 仓库的事件信息，如 Pull Request ID 等。","title":"组织内如何评估 CI/CD 成熟度","type":"posts"},{"content":"","date":"2021-11-09","externalUrl":null,"permalink":"/tags/actions/","section":"标签","summary":"","title":"Actions","type":"tags"},{"content":"最近实现了一个很有意思的 Workflow，就是通过 GitHub Actions 自动将每次最新发布的文章自动同步到我的 GitHub 首页。\n就像这样在首页显示最近发布的博客文章。\n要实现这样的工作流需要了解以下这几点：\n需要创建一个与 GitHub 同名的个人仓库，这个仓库的 README.md 信息会显示在首页 通过 GitHub Actions 自动获取博客的最新文章并更新 README.md 只有当有新的文章发布的时候才触发自动获取、更新文章 GitHub Action GitHub 同名的个人仓库是一个特殊仓库，即创建一个与你的 GitHub 账号同名的仓库，添加的 README.md 会在 GitHub 个人主页显示。\n举个例子：如果你的 GitHub 名叫 GeBiLaoWang，那么当你创建一个叫 GeBiLaoWang 的 Git 仓库，添加 README.md 后就会在主页显示。\n针对这个功能 GitHub 上有很多丰富多彩的个人介绍（如下）。更多灵感可以参看这个链接：https://awesomegithubprofile.tech/\n自动获取文章并更新 README.md # 在 GitHub 上有很多开发者为 GitHub Actions 开发新的小功能。我这里用到一个开源项目叫 blog-post-workflow，它可以通过 RSS（订阅源）来获取到博客的最新文章。\n它不但支持 RSS 还支持获取 StackOverflow 以及 Youtube 视频等资源。\n我只需要在 GitHub 同名的仓库下添加一个这样的 Workflow YML .github/workflows/blog-post-workflow.yml 即可。\nname: Latest blog post workflow on: schedule: - cron: \u0026#39;* 2 * * *\u0026#39; workflow_dispatch: jobs: update-readme-with-blog: name: Update this repo\u0026#39;s README with latest blog posts runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: gautamkrishnar/blog-post-workflow@master with: # 我的博客 RSS 链接 feed_list: \u0026#34;https://shenxianpeng.github.io/atom.xml\u0026#34; # 获取最新 10 篇文章 max_post_count: 10 刚开始我需要让这个 Workflow 能工作即可。因此用的定时触发，即就是每天早上两点就自动获取一次最新文章并更新这个特殊仓库 README.md。\n这个做法还可以，但不够节省资源也不够完美。最好的做法是：只有当有新文章发布时才触发上面的 Workflow 更新 README.md。这就需要有一个 Webhook 当检测到有文章更新时自动触发这里的 Workflow。\n触发另一个 GitHub Action # GitHub Actions 提供了一个 Webhook 事件叫做 repository_dispatch 可以来做这件事。\n它的原理：使用 GitHub API 来触发一个 Webhook 事件，这个事件叫做 repository_dispatch，这个事件里的类型是可以自定义的，并且在要被触发的 workflow 里需要使用 repository_dispatch 事件。\n即：在存放博客文章的仓库里要有一个 Workflow 通过发送 repository_dispatch 事件触发特殊仓库中的 Workflow 来更新 README.md。\n这里我定义事件类型名叫 special_repository，它只接受来自 GitHub API repository_dispatch 事件。\n再次调整上面的 .github/workflows/blog-post-workflow.yml 文件如下：\n# special_repository.yml name: Latest blog post workflow on: repository_dispatch: # 这里的类型是可以自定义的，我将它起名为：special_repository types: [special_repository] workflow_dispatch: jobs: update-readme-with-blog: name: Update this repo\u0026#39;s README with latest blog posts runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: gautamkrishnar/blog-post-workflow@master with: feed_list: \u0026#34;https://shenxianpeng.github.io/atom.xml\u0026#34; max_post_count: 10 接受事件的 Workflow 修改好了。如何发送类型为 special_repository 的 repository_dispatch 事件呢？我这里通过 curl 直接调用 API 来完成。\ncurl -XPOST -u \u0026#34;${{ secrets.PAT_USERNAME}}:${{secrets.PAT_TOKEN}}\u0026#34; \\ -H \u0026#34;Accept: application/vnd.github.everest-preview+json\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; https://api.github.com/repos/shenxianpeng/shenxianpeng/dispatches \\ --data \u0026#39;{\u0026#34;event_type\u0026#34;: \u0026#34;special_repository\u0026#34;}\u0026#39; 最后，发送事件 Workflow YML .github/workflows/send-dispatch.yml 如下:\nname: Tigger special repository on: push: # 当 master 分支有变更的时候触发 workflow branches: - master workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: Send repository dispatch event run: | curl -XPOST -u \u0026#34;${{ secrets.PAT_USERNAME}}:${{secrets.PAT_TOKEN}}\u0026#34; \\ -H \u0026#34;Accept: application/vnd.github.everest-preview+json\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; https://api.github.com/repos/shenxianpeng/shenxianpeng/dispatches \\ --data \u0026#39;{\u0026#34;event_type\u0026#34;: \u0026#34;special_repository\u0026#34;}\u0026#39; 注：PAT_USERNAME 和 PAT_TOKEN 需要在当前的仓库【设置 -\u0026gt; Secrets】里进行添加，这里就不具体介绍了，需要可以自行搜索。\n以上就是通过 GitHub Actions 实现当博客有新发布的文章后自动更新 GitHub 首页的所有内容了。\n如果还有什么有意思的玩法欢迎评论区里分享一下吧。\n","date":"2021-11-09","externalUrl":null,"permalink":"/posts/github-special-repository/","section":"Posts","summary":"本文介绍了如何使用 GitHub Actions 自动将发布的博客文章更新到 GitHub 个人主页，提升个人主页的动态性和可读性。","title":"GitHub Actions 还能这么玩？自动将发布的博客文章更新到 GitHub 个人主页","type":"posts"},{"content":" 前言 # 2021-22 世界质量报告（World Quality Report 简称 WQR）是由 Micro Focus，Capgemini 和 Sogeti 三家公司合作的来分析软件质量和测试趋势在全球范围内唯一的报告。\n这份报告共采访了 1750 名高管和专业人士。从最高管理层到 QA 测试经理和质量工程师，涵盖了来自全球 32 个国家的 10 个行业。\n世界质量报告（WQR）是一项独一无二的全球研究，今年的调查强调了新部署方法中不断变化的受大流行影响的应用程序需求的影响，以及 QA 对敏捷和 DevOps 实践的采用，AI 的持续增长。\n作为测试关注这类软件质量报告可以帮助我们快速了解软件测试行业的现状和趋势。\n五大主题 # WQR 的一个关键信息：在新冠疫情依旧的今天，我们看到了数字化转型的融合以及敏捷和 DevOps 实践的实时采用。此外，QA 正在成为采用敏捷和 DevOps 实践的领导者，为团队提供工具和流程以促进整个软件生命周期（SDLC）的质量。\nWQR 围绕关键发现和趋势突出了五个特定主题：\n新冠疫情对 QA 组织和软件测试的影响 数字化转型与 DevOps 和敏捷采用的实时融合以及 QA 在其中的日益重要的作用 地理上分散的团队在跨环境部署应用程序时专注于业务成果 人工智能 (AI) 增强了敏捷和 DevOps 在所有团队中培养不断增长的质量责任文化 使用 AI 驱动的持续测试和质量管理工具来解决客户体验优先事项和快速变化的受疫情影响的要求 主要发现和趋势 # 1. 新冠疫情对 QA 组织和软件测试的影响 # 新冠疫情对 QA 在内的几乎所有业务方面都产生了直接而真实的影响。然而许多 QA 组织能够适应新的混合工作环境的现实，并过渡到在分布式团队中工作的新现实。这可能发生了，因为混合分布式团队的趋势已经在增长，而疫情只是加速了这种趋势。\n客户体验为王 # 新冠疫情将重点重新放在了客户及其体验上。今年最受好评的方面是：\n增强客户体验，63% 的受访者选择了这一点 其次是增强安全性（62%） 再次是对业务需求的更高响应能力（61%） 以及更高质量的软件解决方案 (61%) 从保管人到质量冠军 # 在过去的一年中，测试和 QA 目标也发生了重新排序。去年业务成果和质量的守护者是明显的领导者，而今年，这些数据之间的支持率缩小了。\n团队中的质量保管人、质量速度和质量赋能以 62% 领先 业务保证、数字化幸福和自动化以 61% 排在了第二位 QA 团队正在从质量的监护人演变为质量的拥护者。QA 团队正在成为组织质量计划中充满活力的领导者，支持团队中的每个人实现质量，同时为业务成果和增长做出贡献。\n2. 数字化转型与 DevOps 和敏捷采用的实时融合以及 QA 在其中的日益重要的作用 # 推动数字化转型 # 今年，数字化转型举措符合疫情的要求。在疫情开始之前，敏捷和 DevOps 是一种增长趋势。在疫情期间，我们开始看到 QA 现在在组织采用敏捷和 DevOps 方面发挥着关键作用，模糊了开发和测试之间的界限，同时创建了一种混合的质量方法。\n来看看数字化转型的驱动因素有哪些：提高生产力和效率以 47% 的比例领先；其次是提高服务/产品质量的比例为 46%；第三是速度、更好的敏捷性和灵活性；客户体验直接落后于速度；紧随其后的是降低成本和创造创新机会；竞争差异化是最后。\n因为竞争差异似乎更多是数字化转型的附带好处，而数字化转型本身有助于提高效率、质量、速度和整体更好的客户体验。\nQA 在 DevOps 和敏捷采用中的作用越来越大 —— 以业务优先级为指导 # 今年，我们见证了业务需求的重大调整，变得比技术堆栈的需求更重要。与去年相比，对技术栈给予权重的参与者数量下降了 16 个百分点，取而代之的是上升最大的：\n业务优先级，现在排名第一，比去年增加了 11 个百分点 此外，与去年相比显着增加的是文化/敏捷性，增加了 21 个百分点 3. 地理上分散的团队在跨环境部署应用程序时专注于业务成果 # 去年的调查是在全球疫情开始时进行的，该调查显示了满足业务目标所需的变革迹象，以及远程工作和数字化转型的新要求。通过今年的调查，我们看到数字化转型仍在继续，即使与受疫情影响的新工作要求保持同步。这加速了公司将工作负载迁移到云的计划，部分原因是计划中的数字化转型计划，以及向远程工作的快速转变，刺激了提高安全性的需求。\n由于受疫情影响的工作场所，评价最高的焦点是对测试系统和测试环境的远程访问（使用 SaaS 和云）。支持这种远程访问是基于远程的次要因素，包括更好的团队协作工具。为了支持现代应用程序的质量，测试环境本身也必须现代化。今年，我们看到：\n组织对使用云和容器实现测试环境现代化的越来越满意（最高满意度） 其次是改进预订和管理测试环境（+16） 然后提供可见性（+22） 最后成本效率（+18） 4. 人工智能 (AI) 增强了敏捷和 DevOps 在所有团队中培养不断增长的质量责任文化 # 人工智能继续改变测试自动化的构建方式，以及测试的执行方式。我们看到组织内部对基于 AI 的测试水平的信心越来越高，几乎一半的受访者表示他们已经拥有 AI 和 ML 所需的测试执行数据的仓库，并表示他们愿意根据他们的 AI 和 ML 平台提供的情报采取行动。\n今年，我们还要求受访者预测他们利用一系列方法来加速和优化敏捷和 DevOps 环境中的测试的可能性。与去年同期相比：\n将测试集成到 CI/CD 流水线的自动质量门（+5） 通过实施智能和自动化仪表板以实现持续质量监控增长最多（+9） 今年新增加的使用 AI 来优化测试用例在总体上并列第二，仅落后于测试左移。 5. 使用 AI 驱动的持续测试和质量管理工具来解决客户体验优先事项和快速变化的受疫情影响的要求 # 今年，我们向受访者询问了测试自动化有哪些好处：\n首先与去年相比所有项目同比趋势均呈下降趋势，这显示了在混合和分布式团队中工作所面临的挑战 像是更好的发现缺陷、缩短测试周期、降低整体安全风险、更好的测试覆盖率、降低测试成本，以及控制测试的透明度都是显而易见的好处 AI/ML 是第四高的好处，这也证明了它的潜力和价值 主要建议 # 敏捷和 DevOps 中的 QA 编排 # 关注最重要的事情：客户体验和业务目标，以效率和速度满足这两者的需求。同时，在你的团队中采用工程思维，并接受多技能和技能提升。一个正在迅速成为新常态的新趋势是 SDET（测试中的软件开发工程师）。投资洞察力，尤其是跨越整个 QA 和测试功能的实时洞察力。从短期战术计划到长期规划和战略方向，专注于开发具有实时 KPI 的智能仪表板。\n智能测试自动化 # 通过采用自动化优先的软件质量交付方法，在 QA 中标准化测试自动化的使用。在 E2E 生命周期中扩展自动化，将自动化纳入所有 QA 活动。\n人工智能和机器学习 # 推动人工智能的使用 —— 不要被它所驱动。AI 和 ML 有望提供指数级的改进，但将 AI 用作工具，而不是替代你正在制定的业务决策。例如，使用 AI 来阐明该做什么以及何时该做。它不仅有助于识别故障，而且有助于识别这些故障发生的原因。此外，将 AI 重点放在最重要的事情上，确定软件交付中最具挑战性的质量领域。如果你尚未将 AI 纳入质量，那么现在是开始的最佳时机。\n测试环境管理 (TEM) 和测试数据管理 (TDM) # 云计算的采用正在继续肯定和稳定的增长，但要注意确保未来不会掩盖现在的需求。成功采用云计算的关键是要确保与传统应用程序的完整性。另外，数据分析现在是测试数据管理框架的一个关键方面。\n安防与智能产业 # 远程连接要求对测试和QA组织的安全性和弹性进行适当的考虑。投资于创新，在你的实验室和你的团队中。无论你是否从 POC 开始来证明可行性，确保管理层的支持是实施变革的关键。\n总结 # 看完整个 WQR 报告之后我从中学习到的变化：\n以业务优先级为指导。与去年相比，技术栈的权重下降了16%，取而代之的是业务优先级。然后是文化、敏捷增加了21% 对云和容器实现测试环境现代化满意度评价最高 将测试集成到 CI/CD 流水线的自动质量门（+5%） 实施智能和自动化 Dashboard 以实现持续质量监控的增长最多（+9%） 使用 AI 来优化测试用例在总体上并列第二，仅落后与测试左移 人工智能继续改变自动化的构建方式和测试的执行方式。几乎一半的受访者表示他们拥有 AI/ML 所需要的测试执行仓库，并且愿意根据 AI/ML 提供的情况采取行动 关注微信公众号《DevOps攻城狮》回复：\u0026ldquo;WQR\u0026rdquo; 可以下载完整版的《2021-22 世界质量报告(WQR) 》。\n","date":"2021-11-06","externalUrl":null,"permalink":"/posts/world-quality-report/","section":"Posts","summary":"本文介绍了 2021-22 世界质量报告（WQR）的主要发现和趋势，强调了新冠疫情对软件质量和测试的影响，以及 QA 在敏捷和 DevOps 中的重要作用。","title":"2021-22 世界质量报告（World Quality Report）","type":"posts"},{"content":"","date":"2021-11-06","externalUrl":null,"permalink":"/tags/quality/","section":"标签","summary":"","title":"Quality","type":"tags"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/tags/coverity/","section":"标签","summary":"","title":"Coverity","type":"tags"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/tags/polaris/","section":"标签","summary":"","title":"Polaris","type":"tags"},{"content":" 这可能是中文网里介绍Polaris最详细的文章了\n什么是 Polaris？ # Polaris - 托管静态应用程序软件测试(SAST)工具的 SaaS 平台，它是用于分类和修复漏洞并运行报告的 Web 站点。 SAST - 一种对源代码分析或构建过程中去寻找安全漏洞的工具，是一种在软件开发的生命周期(SDLC)中确保安全的重要步骤。 Coverity - Coverity 是 Synopsys 公司提供的原始静态应用软件测试 (SAST) 工具。Polaris 是 Coverity 的 SaaS 版本。 Synopsys - 是开发 Polaris 和其他软件扫描工具的公司，比如 BlackDuck 也是他们的产品。\nPolaris 支持哪些语言？ # C/C++ C# Java JavaScript TypeScript PHP Python Fortran Swift ...and more Polaris SaaS 平台 # 通常如果你的组织引入了 Polaris 的 SaaS 服务，你将会有如下网址可供访问 URL: https://organization.polaris.synopsys.com\n然后登录，你就可以给自己的 Git Repository 创建对应的项目了。\n建议：创建的项目名称与 Git Repository 的名称一致。\nPolaris 如何进行漏洞扫描？ # Polaris 安装 # 在进行 Polaris 扫描之前，你需要先下载并安装 polaris。\n如果你的 Polaris server URL 为：POLARIS_SERVER_URL=https://organization.polaris.synopsys.com\n下载连接为：$POLARIS_SERVER_URL/api/tools/polaris_cli-linux64.zip\n然后将下载到本地的 polaris_cli-linux64.zip 进行解压，将其 bin 目录添加到 PATH 中。\nPolaris YAML 文件配置 # 在进行扫描之前，你需要为你的项目创建 YAML 文件。默认配置文件名为 polaris.yml，位于项目根目录。如果你希望指定不同的配置文件名，你可以在 polaris 命令中使用 -c 选项。\n在项目根目录运行 polaris setup 以生成通用的 polaris.yml 文件。\n运行 polaris configure 以确认你的文件在语法上是正确的并且 polaris 没有任何问题。\nCapture - 捕获 # YAML 配置文件可以包含三种类型的 Capture：\nBuild(构建) - 运行构建命令，然后分析结果 Filesystem(文件系统) - 对于解释型语言，提供项目类型和要分析的扩展列表 Buildless - 对于一些可以使用依赖管理器的语言，比如 maven Languages Build Options C, C++, ObjectiveC, Objective C++,Go, Scala, Swift 使用 Build 捕获 PHP, Python, Ruby 使用 Buildless 或 Filesystem 捕获 C#, Visual Basic. 如果想获得更准确的结果使用 Build 捕获；如果寻求简单使用 Buildless 捕获 Java 如果想获得更准确的结果使用 Build 捕获；如果寻求简单使用 Buildless 捕获 JavaScript,TypeScript 使用 Filesystem 捕获；如果寻求简单使用 Buildless 捕获 Analyze - 分析 # 如果你正在扫描 C/C++ 代码，则应包括此分析部分以充分利用 Polaris 的扫描功能：\nanalyze: mode: central coverity: cov-analyze: [\u0026#34;--security\u0026#34;,\u0026#34;--concurrency\u0026#34;] Polaris YAML 示例文件 # 示例1：一个C/C++ 项目\nversion: \u0026#34;1\u0026#34; project: name: test-cplus-demo branch: ${scm.git.branch} revision: name: ${scm.git.commit} date: ${scm.git.commit.date} capture: build: cleanCommands: - shell: [make, -f, GNUmakefile, clean] buildCommands: - shell: [make, -f, GNUmakefile] analyze: mode: central install: coverity: version: default serverUrl: https://organization.polaris.synopsys.com 示例2：一个 Java 项目\nversion: \u0026#34;1\u0026#34; project: name: test-java-demo branch: ${scm.git.branch} revision: name: ${scm.git.commit} date: ${scm.git.commit.date} capture: build: cleanCommands: - shell: [gradle, -b, build.gradle, --no-daemon, clean] buildCommands: - shell: [gradle, -b, build.gradle, --no-daemon, shadowJar] fileSystem: ears: extensions: [ear] files: - directory: ${project.projectDir} java: files: - directory: ${project.projectDir} javascript: files: - directory: client-vscode - excludeRegex: node_modules|bower_components|vendor python: files: - directory: ${project.projectDir} wars: extensions: [war] files: - directory: ${project.projectDir} analyze: mode: central install: coverity: version: default serverUrl: https://organization.polaris.synopsys.com 示例3：一个 CSharp 项目\nversion: \u0026#34;1\u0026#34; project: name: test-ssharp-demo branch: ${scm.git.branch} revision: name: ${scm.git.commit} date: ${scm.git.commit.date} capture: build: buildCommands: # 如果构建过程很复杂，你可以写一个脚本，然后调用它 - shell: [\u0026#39;script\\polaris.bat\u0026#39;] # 跳过一些你不想扫描的文件 skipFiles: - \u0026#34;*.java\u0026#34; - \u0026#34;*.text\u0026#34; - \u0026#34;*.js\u0026#34; analyze: mode: central install: coverity: version: default serverUrl: https://organization.polaris.synopsys.com 更多关于如何编写 polaris.yml 就不一一罗列了，详细请参考 Polaris 的官方文档：https://sig-docs.synopsys.com/polaris/topics/c_conf-overview.html\n执行分析 # 可以使用如下命令进行 Polaris 分析：\npolaris -c polaris.yml analyze -w --coverity-ignore-capture-failure --coverity-ignore-capture-failure - 忽略 Coverity 捕获失败。运行 polaris help analyze 可以查看更多分析命令的介绍。\nPolaris 分析结果 # 如果 Polaris 分析成功，将会在控制台看到一条成功信息如下：\n[INFO] [1zb99xsu] Coverity job completed successfully! [INFO] [1zb99xsu] Coverity - analyze phase took 4m 36.526s. Analysis Completed. Coverity analysis { \u0026#34;JobId\u0026#34;: \u0026#34;mlkik4esb961p0dtq8i6m7pm14\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Success\u0026#34; } Job issue summary { \u0026#34;IssuesBySeverity\u0026#34;: { \u0026#34;Critical\u0026#34;: 0, \u0026#34;High\u0026#34;: 250, \u0026#34;Medium\u0026#34;: 359, \u0026#34;Low\u0026#34;: 81 }, \u0026#34;Total\u0026#34;: 690, \u0026#34;NewIssues\u0026#34;: 0, \u0026#34;ClosedIssues\u0026#34;: 0, \u0026#34;SummaryUrl\u0026#34;: \u0026#34;https://organization.polaris.synopsys.com/projects/bb079756-194e-4645-9121-5131493a0c93/branches/d567c376-4d5d-4941-8733-aa27bb2f5f5b\u0026#34; } 这里显示了一共发现了多少 690 个漏洞，以及每种不同严重程度各占多少个。具体的漏洞信息需要登录到 Polaris SaaS 平台进行查看。\n点击 SummaryUrl 中的链接将会直接跳转到该项目的 Polaris 扫描结果。\n","date":"2021-10-24","externalUrl":null,"permalink":"/posts/what-is-polaris/","section":"Posts","summary":"这篇文章介绍了 Polaris 的基本概念、支持的编程语言、SaaS 平台的使用方法，以及如何配置和运行 Polaris 进行静态代码分析。它还提供了示例 YAML 配置文件和分析结果的查看方式。","title":"Polaris - 静态代码分析","type":"posts"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/tags/security/","section":"标签","summary":"","title":"Security","type":"tags"},{"content":"","date":"2021-10-24","externalUrl":null,"permalink":"/tags/static/","section":"标签","summary":"","title":"Static","type":"tags"},{"content":"不管是对于 Git 的初学者还是经常使用 Git 的码农们，在日常工作中难免会有遇到有的命令一时想不起来。不妨将下面总结的一些 Git 常用命令及技巧收藏或打印出来，以备需要的时候可以很快找到。\ngit config # # 检查 git 配置 git config -l # 设置你的 git 提交 username 和 email # 例如：对于公司里项目 git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your_email@organization.com\u0026#34; # 例如：对于个人的 GitHub 项目 git config user.name \u0026#34;Your Name\u0026#34; git config user.email \u0026#34;your_emailn@gmail.com\u0026#34; # 使用 HTTP/HTTPS 避免每次输入密码 git config --global credential.helper cache git init # # 初始化一个仓库 git init git add # # 将文件添加到暂存区 git add file_name # 将所有文件添加到暂存区 git add . # 仅将某些文件添加到暂存区, 例如:仅添加所有以 \u0026#39;test*\u0026#39; 开头的文件 git add test* git status # # 检查仓库状态 git status git commit # # 提交更改 git commit # 提交带有消息的更改 git commit -m \u0026#34;This is a commit message\u0026#34; git log # # 查看提交历史 git log # 查看提交历史和显示相应的修改 git log -p # 显示提交历史统计 git log --stat # 显示特定的提交 git show commit_id # 以图形方式显示当前分支的提交信息 git log --graph --oneline # 以图形方式显示所有分支的提交信息 git log --graph --oneline --all # 获取远程仓库的当前提交日志 git log origin/master git diff # # 在使用 diff 提交之前所做的更改 git diff git diff some_file.js git diff --staged git rm # # 删除跟踪文件 git rm file_name git mv # # 重命名文件 git mv old_file_name new_file_name git checkout # # 切换分支 git checkout branch_name # 还原未暂存的更改 git checkout file_name git reset # # 还原暂存区的更改 git reset HEAD file_name git reset HEAD -p git commit --amend # # 修改最近的提交信息 git commit --amend # 修改最近的提交信息为：New commit message git commit --amend -m \u0026#34;New commit message\u0026#34; git revert # # 回滚最后一次提交 git revert HEAD # 回滚指定一次提交 git revert commit_id git branch # # 创建分支 git branch branch_name # 创建分支并切到该分支 git checkout -b branch_name # 显示当前分支 git branch # 显示所有分支 git branch -a # 检查当前正在跟踪的远程分支 git branch -r # 删除分支 git branch -d branch_name git merge # # 将 branch_name 合并到当分支 git merge branch_name # 中止合并 git merge --abort git pull # # 从远程仓库拉取更改 git pull git fetch # # 获取远程仓库更改 git fetch git push # # 推送更改到远程仓库 git push # 推送一个新分支到远程仓库 git push -u origin branch_name # 删除远程仓库分支 git push --delete origin branch_name git remote # # 添加远程仓库 git add remote https://repository_name.com # 查看远程仓库 git remote -v # 查看远程仓库的更多信息 git remote show origin Git技巧和窍门 # 清理已合并分支 # 清理已经合并的本地分支\ngit branch --merged master | grep -v \u0026#34;master\u0026#34; | xargs -n 1 git branch -d .gitignore # 指明 Git 应该忽略的故意不跟踪的文件的文件，比如 .gitignore 如下\n# 忽略 .vscode 目录 .vscode/ # 忽略 build 目录 build/ # 忽略文件 output.log .gitattributes # 关于 .gitattributes 请参考\nhttps://www.git-scm.com/docs/gitattributes https://docs.github.com/en/get-started/getting-started-with-git/configuring-git-to-handle-line-endings ","date":"2021-10-23","externalUrl":null,"permalink":"/posts/git-cheatsheet/","section":"Posts","summary":"本文总结了 Git 的常用命令和技巧，帮助开发者快速查找和使用 Git 命令，提高工作效率。","title":"Git 常用命令备忘录","type":"posts"},{"content":"","date":"2021-09-18","externalUrl":null,"permalink":"/tags/gradle/","section":"标签","summary":"","title":"Gradle","type":"tags"},{"content":"","date":"2021-09-18","externalUrl":null,"permalink":"/tags/sonarqube/","section":"标签","summary":"","title":"SonarQube","type":"tags"},{"content":"在你搭建好 SonarQube 实例后，需要将其与项目集成。\n由于我使用的是 Community Edition，它不支持 C/C++ 项目，所以这里只演示 Maven、Gradle 及其他类型项目的集成方法。\n假设在 SonarQube 中的项目名称和 ID 都为 test-demo，并通过 Jenkins 进行构建。\nMaven 项目集成 # 在 pom.xml 中添加：\n\u0026lt;properties\u0026gt; \u0026lt;sonar.projectKey\u0026gt;test-demo\u0026lt;/sonar.projectKey\u0026gt; \u0026lt;/properties\u0026gt; 在 Jenkinsfile 中添加：\nstage(\u0026#39;SonarQube Analysis\u0026#39;) { def mvn = tool \u0026#39;Default Maven\u0026#39; withSonarQubeEnv() { sh \u0026#34;${mvn}/bin/mvn sonar:sonar\u0026#34; } } Gradle 项目集成 # 在 build.gradle 中添加：\nplugins { id \u0026#34;org.sonarqube\u0026#34; version \u0026#34;3.3\u0026#34; } sonarqube { properties { property \u0026#34;sonar.projectKey\u0026#34;, \u0026#34;test-demo\u0026#34; } } 在 Jenkinsfile 中添加：\nstage(\u0026#39;SonarQube Analysis\u0026#39;) { withSonarQubeEnv() { sh \u0026#34;./gradlew sonarqube\u0026#34; } } 其他类型项目（JS、TS、Python 等） # 在代码仓库根目录创建 sonar-project.properties 文件：\nsonar.projectKey=test-demo 在 Jenkinsfile 中添加：\nstage(\u0026#39;SonarQube Analysis\u0026#39;) { def scannerHome = tool \u0026#39;SonarScanner\u0026#39; withSonarQubeEnv() { sh \u0026#34;${scannerHome}/bin/sonar-scanner\u0026#34; } } 更多关于 SonarQube 集成的方法，可以访问你本地的 SonarQube 文档页面：\nhttp://localhost:9000/documentation\n转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2021-09-18","externalUrl":null,"permalink":"/posts/sonarqube-integration/","section":"Posts","summary":"本文介绍如何将 SonarQube Community Edition 集成到 Maven、Gradle 及其他类型项目中，包括必要的配置和 Jenkins 流水线示例。","title":"SonarQube Community Edition 如何集成到项目中","type":"posts"},{"content":"","date":"2021-09-07","externalUrl":null,"permalink":"/tags/lcov/","section":"标签","summary":"","title":"Lcov","type":"tags"},{"content":"","date":"2021-09-07","externalUrl":null,"permalink":"/tags/perl/","section":"标签","summary":"","title":"Perl","type":"tags"},{"content":"在执行以下命令生成代码覆盖率报告时：\nlcov --capture --directory . --no-external --output-file coverage.info 我遇到了如下错误：\nCapturing coverage data from . Can\u0026#39;t locate JSON/PP.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /usr/local/bin/geninfo line 63. BEGIN failed--compilation aborted at /usr/local/bin/geninfo line 63. 1. Can\u0026rsquo;t locate CPAN.pm # 当运行 perl -MCPAN -e 'install JSON' 时出现：\nCan\u0026#39;t locate CPAN.pm in @INC ... 解决方法：安装 perl-CPAN\nsudo yum install perl-CPAN 2. Can\u0026rsquo;t locate JSON/PP.pm # 安装好 CPAN 后，再运行安装 JSON：\nsudo perl -MCPAN -e \u0026#39;install JSON\u0026#39; 如果依然报找不到 JSON/PP.pm，可通过复制 backportPP.pm 解决：\ncd /usr/local/share/perl5/JSON cp backportPP.pm PP.pm 3. Can\u0026rsquo;t locate Module/Load.pm # geninfo --version Can\u0026#39;t locate Module/Load.pm in @INC ... 解决方法：安装 perl-Module-Load-Conditional\nsudo yum install perl-Module-Load-Conditional 4. Can\u0026rsquo;t locate Capture/Tiny.pm # lcov --version Can\u0026#39;t locate Capture/Tiny.pm in @INC ... 解决方法：用 CPAN 安装 Capture::Tiny\nperl -MCPAN -e \u0026#39;install Capture::Tiny\u0026#39; 5. Can\u0026rsquo;t locate DateTime.pm # genhtml --help Can\u0026#39;t locate DateTime.pm in @INC ... 在 CentOS 7 下安装：\nsudo yum install \u0026#39;perl(DateTime)\u0026#39; （但我测试中该方法无效，仅作参考。）\n6. 运行 geninfo 报 Zlib 版本错误 # Compress::Raw::Zlib version 2.201 required--this is only version 2.061 ... 解决方法：安装最新 Compress::Raw::Zlib\nperl -MCPAN -e \u0026#39;install Compress::Raw::Zlib\u0026#39; 通过逐一安装缺失的 Perl 模块，lcov 最终恢复正常工作：\nlcov --version lcov: LCOV version v1.16-16-g038c2ca 转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2021-09-07","externalUrl":null,"permalink":"/posts/lcov-error/","section":"Posts","summary":"本文介绍在运行 lcov 生成代码覆盖率报告时遇到 \u0026ldquo;Can\u0026rsquo;t locate JSON/PP.pm in @INC \u0026hellip;\u0026rdquo; 等 Perl 模块缺失错误的解决方法，包括安装缺失的 Perl 模块。","title":"运行 lcov 报错 \"Can't locate JSON/PP.pm in @INC ...\"","type":"posts"},{"content":"","date":"2021-08-17","externalUrl":null,"permalink":"/tags/coverage/","section":"标签","summary":"","title":"Coverage","type":"tags"},{"content":"","date":"2021-08-17","externalUrl":null,"permalink":"/tags/gcov/","section":"标签","summary":"","title":"Gcov","type":"tags"},{"content":"本篇分享如何使用 Gcov 和 LCOV 对 C/C++ 项目进行代码覆盖率的度量。\n如果你想了解代码覆盖率工具 Gcov 是如何工作的，或是以后需要做 C/C++ 项目的代码覆盖率，希望本篇对你有所帮助。\n问题 # 不知道你没有遇到过和我一样的问题：几十年前的 C/C++ 项目没有单元测试，只有回归测试，但是想知道回归测试测了哪些代码？还有哪些代码没测到？代码覆盖率是多少？今后哪些地方需要提高自动化测试用例？\n可能对于接触过 Java 的 Junit 和 JaCoCo 的人来说，没有单元测试应该测不了代码覆盖率吧 \u0026hellip; 其实不然，如果不行就没有下文了 :)\n现状 # 市场上有一些工具可以针对黑盒测试来衡量代码覆盖率 Squish Coco，Bullseye 等，它们的原理就是在编译的时候插入 instrumentation，中文叫插桩，在运行测试的时候用来跟踪和记录运行结果。\n其中我比较深入的了解过 Squish Coco 它如何使用，但对于大型项目，引入这类工具都或多或少的需要解决编译上的问题。也正是因为有一些编译问题没有解决，就一直没有购买这款价格不菲的工具 License。\n当我再次重新调查代码覆盖率的时候，我很惭愧的发现原来正在使用的 GCC 其实有内置的代码覆盖率的工具的，叫 Gcov\n前提条件 # 对于想使用 Gcov 的人，为了说明它是如何工作的，我准备了一段示例程序，运行这个程序之前需要先安装 GCC 和 LCOV。\n如果没有环境或不想安装，可以直接查看示例仓库的 GitHub 仓库：https://github.com/shenxianpeng/gcov-example\n注：主分支 master 下面放的是源码，分支 coverage 下的 out 目录是最终的结果报告。\n# 这是我的测试环境上的 GCC 和 lcov 的版本 sh-4.2$ gcc --version gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39) Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. sh-4.2$ lcov -v lcov: LCOV version 1.14 Gcov 是如何工作的 # Gcov 工作流程图\n主要分三步：\n在 GCC 编译的时加入特殊的编译选项，生成可执行文件，和 *.gcno； 运行（测试）生成的可执行文件，生成了 *.gcda 数据文件； 有了 *.gcno 和 *.gcda，通过源码生成 gcov 文件，最后生成代码覆盖率报告。 下面就开始介绍其中每一步具体是怎么做的。\n1. 编译 # 第一步编译，这里已经将编译用到的参数和文件都写在了 makefile 里了，只要执行 make 就可以编译了。\nmake 点击查看 make 命令的输出 sh-4.2$ make gcc -fPIC -fprofile-arcs -ftest-coverage -c -Wall -Werror main.c gcc -fPIC -fprofile-arcs -ftest-coverage -c -Wall -Werror foo.c gcc -fPIC -fprofile-arcs -ftest-coverage -o main main.o foo.o 通过输出可以看到，这个程序在编译的时候添加了两个编译选项 -fprofile-arcs and -ftest-coverage。在编译成功后，不仅生成了 main and .o 文件，同时还生成了两个 .gcno 文件.\n.gcno 记录文件是在加入 GCC 编译选项 -ftest-coverage 后生成的，在编译过程中它包含用于重建基本块图和为块分配源行号的信息。\n2. 运行可执行文件 # 在编译完成后，生成了 main 这个可执行文件，运行（测试）它：\n./main 点击查看运行 main 时输出 sh-4.2$ ./main Start calling foo() ... when num is equal to 1... when num is equal to 2... 当运行 main 后，执行结果被记录在了 .gcda 这个数据文件里，查看当前目录下可以看到一共有生成了两个 .gcda 文件，每个源文件都对应一个 .gcda 文件。\n$ ls foo.c foo.gcda foo.gcno foo.h foo.o img main main.c main.gcda main.gcno main.o makefile README.md .gcda 记录数据文件的生成是因为程序在编译的时候引入了 -fprofile-arcs 选项。它包含弧过渡计数、值分布计数和一些摘要信息。\n3. 生成报告 # make report 点击查看生成报告的输出 sh-4.2$ make report gcov main.c foo.c File \u0026#39;main.c\u0026#39; Lines executed:100.00% of 5 Creating \u0026#39;main.c.gcov\u0026#39; File \u0026#39;foo.c\u0026#39; Lines executed:85.71% of 7 Creating \u0026#39;foo.c.gcov\u0026#39; Lines executed:91.67% of 12 lcov --capture --directory . --output-file coverage.info Capturing coverage data from . Found gcov version: 4.8.5 Scanning . for .gcda files ... Found 2 data files in . Processing foo.gcda geninfo: WARNING: cannot find an entry for main.c.gcov in .gcno file, skipping file! Processing main.gcda Finished .info-file creation genhtml coverage.info --output-directory out Reading data file coverage.info Found 2 entries. Found common filename prefix \u0026#34;/workspace/coco\u0026#34; Writing .css and .png files. Generating output. Processing file gcov-example/main.c Processing file gcov-example/foo.c Writing directory view page. Overall coverage rate: lines......: 91.7% (11 of 12 lines) functions..: 100.0% (2 of 2 functions) 执行 make report 来生成 HTML 报告，这条命令的背后实际上主要执行了以下两个步骤：\n在有了编译和运行时候生成的 .gcno 和 .gcda 文件后，执行命令 gcov main.c foo.c 即可生成 .gcov 代码覆盖率文件。\n有了代码覆盖率 .gcov 文件，通过 LCOV 生成可视化代码覆盖率报告。\n生成 HTML 结果报告的步骤如下：\n# 1. 生成 coverage.info 数据文件 lcov --capture --directory . --output-file coverage.info # 2. 根据这个数据文件生成报告 genhtml coverage.info --output-directory out 删除所有生成的文件 # 上传过程中所有生成的文件可通过执行 make clean 命令来彻底删除掉。\n点击查看 make clean 命令的输出 sh-4.2$ make clean rm -rf main *.o *.so *.gcno *.gcda *.gcov coverage.info out 代码覆盖率报告 # 首页以目录结构显示\n进入目录后，显示该目录下的源文件\n蓝色表示这些语句被覆盖\n红色表示没有被覆盖的语句\nLCOV 支持语句、函数和分支覆盖度量。\n旁注：\n还有另外一个生成 HTML 报告的工具叫 gcovr，使用 Python 开发的，它的报告在显示方式上与 LCOV 略有不同。比如 LCOV 以目录结构显示， gcovr 以文件路径来显示，前者与代码结构一直因此我更倾向于使用前者。 相关阅读 # 关于代码覆盖率(About Code Coverage)：https://shenxianpeng.github.io/2021/07/code-coverage/ 在 Linux 内核中使用 Gcov 的示例：https://01.org/linuxgraphics/gfx-docs/drm/dev-tools/gcov.html 当构建环境与测试环境不同时设置环境变量：https://gcc.gnu.org/onlinedocs/gcc/Cross-profiling.html#Cross-profiling ","date":"2021-08-17","externalUrl":null,"permalink":"/posts/gcov-example/","section":"Posts","summary":"本文介绍了如何使用 Gcov 和 LCOV 对 C/C++ 项目进行代码覆盖率的度量，包括编译、运行和生成报告的步骤。","title":"使用 Gcov 和 LCOV 做 C/C++ 项目的代码覆盖率","type":"posts"},{"content":"","date":"2021-08-05","externalUrl":null,"permalink":"/tags/postgresql/","section":"标签","summary":"","title":"PostgreSQL","type":"tags"},{"content":" 背景 # 相较于 Jenkins、Artifactory 等 DevOps 工具，SonarQube 的安装与配置并不算简单。除了启动脚本外，还需要提前准备数据库、在配置文件中设置 LDAP 等。\n这里记录我在安装 SonarQube 9.0.1 版本时的关键步骤（LDAP、PostgreSQL 等），以便后续参考，也希望能帮到其他人。\n前置条件与下载 # JRE/JDK 11 必须已安装\n官方要求参考：https://docs.sonarqube.org/latest/requirements/requirements/\n下载 SonarQube\nwget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-9.0.1.46107.zip unzip sonarqube-9.0.1.46107.zip cd sonarqube-9.0.1.46107/bin/linux-x86-64 sh sonar.sh console 切换 Java 版本 # CentOS 7 默认 Java 版本为 1.8.0，但 SonarQube 要求 JDK 11。若机器已安装 JDK 11，可使用 alternatives 切换：\nalternatives --config java # 选择 java-11-openjdk java -version 安装数据库 # SonarQube 需依赖外部数据库，支持 Oracle、PostgreSQL、SQL Server 等。推荐使用开源、轻量的 PostgreSQL。 安装方法参考：https://www.postgresql.org/download/linux/redhat/\n常见问题 # 1. 配置 PostgreSQL 连接 # 在 sonar.properties 中设置，例如：\nsonar.jdbc.username=sonarqube sonar.jdbc.password=mypassword sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube 2. 配置 LDAP 登录 # sonar.security.realm=LDAP ldap.url=ldap://den.example-org:389 ldap.bindDn=user@example-org.com ldap.bindPassword=mypassword ldap.authentication=simple ldap.user.baseDn=DC=example-org,DC=com ldap.user.request=(\u0026amp;(objectClass=user)(sAMAccountName={login})) ldap.user.realNameAttribute=cn ldap.user.emailAttribute=email 3. LDAP 登录缓慢 # 注释掉以下配置可提升首次登录速度：\n# ldap.followReferrals=false 参考：https://community.sonarsource.com/t/ldap-login-takes-2-minutes-the-first-time/1573/7\n4. 修复 \u0026ldquo;Could not resolve file paths in lcov.info\u0026rdquo; # 当使用 sonar.javascript.lcov.reportPaths=coverage/lcov.info 时，如果路径包含 ..\\，SonarQube 会无法解析。 解决方法：运行 sed 移除前缀。\nsed -i \u0026#39;s/\\..\\\\//g\u0026#39; coverage/lcov.info 5. 输出更多日志 # 将：\nsonar.log.level=INFO 改为：\nsonar.log.level=DEBUG 重启 SonarQube 生效。\n完整 sonar.properties 示例 # 可参考：Gist 链接 或下方配置：\nsonar.jdbc.username=sonarqube sonar.jdbc.password=mypassword sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=10.118.245.19 sonar.web.port=9000 sonar.security.realm=LDAP ldap.url=ldap://den.example-org:389 ldap.bindDn=user@example-org.com ldap.bindPassword=mypassword ldap.authentication=simple ldap.user.baseDn=DC=example-org,DC=com ldap.user.request=(\u0026amp;(objectClass=user)(sAMAccountName={login})) ldap.user.realNameAttribute=cn ldap.user.emailAttribute=email sonar.search.javaAdditionalOpts=-Dbootstrap.system_call_filter=false sonar.log.level=INFO sonar.path.data=/var/sonarqube/data sonar.path.temp=/var/sonarqube/temp 转载本文请注明作者与出处，禁止商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2021-08-05","externalUrl":null,"permalink":"/posts/sonarqube-setup/","section":"Posts","summary":"本文记录了 SonarQube 的安装步骤，包括 LDAP 配置与 PostgreSQL 数据库设置，并附带一些常见问题的排查方法。","title":"SonarQube 安装与常见问题排查","type":"posts"},{"content":" 问题描述 # 在项目中引入 Gcov 做代码覆盖率构建时，可能遇到以下报错：\n错误 1 # hidden symbol `__gcov_init\u0026#39; in /usr/lib/gcc/x86_64-redhat-linux/4.8.5/libgcov.a(_gcov.o) is referenced by DSO 错误 2 # undefined reference to `__gcov_init\u0026#39; undefined reference to `__gcov_merge_add\u0026#39; 问题定位 # 以 错误 1 为例，从报错中可看到涉及多个 .so 动态库，例如：\n-lundata -lutcallc_nfasvr 使用 nm 命令查看库文件符号，发现 __gcov_init 被标记为 U（未定义符号）：\nfind -name *utcallc_nfasvr* nm ./bin/libutcallc_nfasvr.so | grep __gcov_init # 输出： # U __gcov_init 解决方法 # 在我的项目中，只需在 Makefile 中为对应的库添加 -lgcov 链接选项即可：\nLIB_1 := utcallc_nfasvr LIB_1_LIBS := -lgcov 重新构建后，查看符号时标记变为 t（符号已定义）：\nnm ./bin/libutcallc_nfasvr.so | grep __gcov_init # 输出： # t __gcov_init 对于直接构建 .so 库的情况，也可以在编译命令中直接添加 -lgcov：\ng++ -shared -o libMyLib.so src_a.o src_b.o src_c.o -lgcov 总结 # 我遇到过以下多种形式的报错：\nundefined reference to `__gcov_init` undefined reference to `__gcov_merge_add` hidden symbol `__gcov_init\u0026#39; in libgcov.a(_gcov.o) is referenced by DSO 通用修复方式：在链接阶段添加 -lgcov 选项，然后重新编译，并用 nm 检查符号是否已正确引入。\n转载本文请注明作者与出处，禁止用于商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2021-07-27","externalUrl":null,"permalink":"/posts/how-to-fix-gcov-hidden-symbol/","section":"Posts","summary":"本文介绍在使用 Gcov 编译项目进行代码覆盖率统计时，出现 \u0026ldquo;hidden symbol `__gcov_init\u0026rsquo;\u0026hellip;\u0026rdquo; 等错误的原因及解决方法，包括如何在构建时确保符号不被隐藏。","title":"修复 \"hidden symbol `__gcov_init' in ../libgcov.a(_gcov.o) is referenced by DSO\" 错误","type":"posts"},{"content":"","date":"2021-07-25","externalUrl":null,"permalink":"/tags/shell/","section":"标签","summary":"","title":"Shell","type":"tags"},{"content":" 使用场景 # 添加构建状态\n当你从某个分支启动构建时，希望为该分支的特定提交添加构建状态。\n更新构建状态\n当构建状态错误时（如错误地标记为 FAILED），你可能需要将其手动更新为 SUCCESSFUL 等正确状态。\n此时可通过 Bitbucket REST API 实现。\n示例脚本 # 以下 Shell 脚本演示了如何使用 REST API 更新 Bitbucket 构建状态。\n代码 GitHub Gist 链接：gist.github.com/shenxianpeng/bd5eddc5fb39e54110afb8e2e7a6c4fb\n#!/bin/sh username=your-bitbucket-user password=your-bitbucket-password commit_id=\u0026#39;57587d7d4892bc4ef2c4375028c19b27921e2485\u0026#39; # 构建状态可选值：SUCCESSFUL, FAILED, INPROGRESS build_result=\u0026#39;SUCCESSFUL\u0026#39; description=\u0026#39;Manually update bitbucket status\u0026#39; build_name=\u0026#39;test #1\u0026#39; build_url=http://localhost:8080/job/test/ bitbucket_rest_api=\u0026#39;https://myorg.bitbucket.com/rest/build-status/latest/commits\u0026#39; gen_post_data() { cat \u0026lt;\u0026lt;EOF { \u0026#34;state\u0026#34;: \u0026#34;$build_result\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;$commit_id\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;$build_name\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;$build_url\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;$description\u0026#34; } EOF } echo \u0026#34;$(gen_post_data)\u0026#34; curl -u $username:$password \\ -H \u0026#34;Accept: application/json\u0026#34; \\ -H \u0026#34;Content-Type:application/json\u0026#34; \\ -X POST $bitbucket_rest_api/$commit_id --data \u0026#34;$(gen_post_data)\u0026#34; if [ $? -ne 0 ]; then echo \u0026#34;$0: Update bitbucket build status failed.\u0026#34; exit 1 else echo \u0026#34;$0: Update bitbucket build status success.\u0026#34; exit 0 fi 最终效果 # 执行成功后，可以在 Bitbucket 中看到构建状态的更新结果：\n转载本文请注明作者与出处，禁止用于商业用途。欢迎关注公众号「DevOps攻城狮」。\n","date":"2021-07-25","externalUrl":null,"permalink":"/posts/bitbucket-update-build-status/","section":"Posts","summary":"本文介绍如何通过 Bitbucket REST API 为指定提交添加或更新构建状态，并给出 Shell 脚本示例，适用于手动修复或补充构建状态的场景。","title":"使用 REST API 添加或更新 Bitbucket 构建状态","type":"posts"},{"content":"本篇简要介绍：什么是代码覆盖率？为什么要做代码覆盖率？代码覆盖率的指标、工作原理，主流的代码覆盖率工具以及不要高估代码覆盖率指标。\n什么是代码覆盖率？ # 代码覆盖率是对整个测试过程中被执行的代码的衡量，它能测量源代码中的哪些语句在测试中被执行，哪些语句尚未被执行。\n为什么要测量代码覆盖率？ # 众所周知，测试可以提高软件版本的质量和可预测性。但是，你知道你的单元测试甚至是你的功能测试实际测试代码的效果如何吗？是否还需要更多的测试？\n这些是代码覆盖率可以试图回答的问题。总之，出于以下原因我们需要测量代码覆盖率：\n了解我们的测试用例对源代码的测试效果 了解我们是否进行了足够的测试 在软件的整个生命周期内保持测试质量 注：代码覆盖率不是灵丹妙药，覆盖率测量不能替代良好的代码审查和优秀的编程实践。\n通常，我们应该采用合理的覆盖目标，力求在代码覆盖率在所有模块中实现均匀覆盖，而不是只看最终数字的是否高到令人满意。\n举例：假设代码覆盖率只在某一些模块代码覆盖率很高，但在一些关键模块并没有足够的测试用例覆盖，那样虽然代码覆盖率很高，但并不能说明产品质量就很高。\n代码覆盖率的指标种类 # 代码覆盖率工具通常使用一个或多个标准来确定你的代码在被自动化测试后是否得到了执行，常见的覆盖率报告中看到的指标包括：\n函数覆盖率：定义的函数中有多少被调用 语句覆盖率：程序中的语句有多少被执行 分支覆盖率：有多少控制结构的分支（例如if语句）被执行 条件覆盖率：有多少布尔子表达式被测试为真值和假值 行覆盖率：有多少行的源代码被测试过 代码覆盖率是如何工作的？ # 代码覆盖率测量主要有以下三种方式：\n1. Source code instrumentation - 源代码检测 # 将检测语句添加到源代码中，并使用正常的编译工具链编译代码以生成检测的程序集。这是我们常说的插桩，Gcov 是属于这一类的代码覆盖率工具。\n2. Runtime instrumentation - 运行时收集 # 这种方法在代码执行时从运行时环境收集信息以确定覆盖率信息。以我的理解 JaCoCo 和 Coverage 这两个工具的原理属于这一类别。\n3. Intermediate code instrumentation - 中间代码检测 # 通过添加新的字节码来检测编译后的类文件，并生成一个新的检测类。说实话，我 Google 了很多文章并找到确定的说明哪个工具是属于这一类的。\n了解这些工具的基本原理，结合现有的测试用例，有助于正确的选择代码覆盖率工具。比如：\n产品的源代码只有 E2E（端到端）测试用例，通常只能选择第一类工具，即通过插桩编译出的可执行文件，然后进行测试和结果收集。 产品的源代码有单元测试用例，通常选择第二类工具，即运行时收集。这类工具的执行效率高，易于做持续集成。 当前主流代码覆盖率工具 # 代码覆盖率的工具有很多，以下是我用过的不同编程语言的代码覆盖率工具。在选择工具时，力求去选择那些开源、流行（活跃）、好用的工具。\n编程语言 代码覆盖率工具 C/C++ Gcov Java JaCoCo JavaScript Istanbul Python Coverage.py Golang cover 不要高估代码覆盖率指标 # 代码覆盖率不是灵丹妙药，它只是告诉我们有哪些代码没有被测试用例“执行到”而已，高百分比的代码覆盖率不等于高质量的有效测试。\n首先，高代码覆盖率不足以衡量有效测试。相反，代码覆盖率更准确地给出了代码未被测试程度的度量。这意味着，如果我们的代码覆盖率指标较低，那么我们可以确定代码的重要部分没有经过测试，然而反过来不一定正确。具有高代码覆盖率并不能充分表明我们的代码已经过充分测试。\n其次，100% 的代码覆盖率不应该是我们明确努力的目标之一。这是因为在实现 100% 的代码覆盖率与实际测试重要的代码之间总是需要权衡。虽然可以测试所有代码，但考虑到为了满足覆盖率要求而编写更多无意义测试的趋势，当你接近此限制时，测试的价值也很可能会减少。\n借 Martin Fowler 在这篇测试覆盖率的文章说的一句话：\n代码覆盖率是查找代码库中未测试部分的有用工具，然而它作为一个数字说明你的测试有多好用处不大。\n参考 # https://www.lambdatest.com/blog/code-coverage-vs-test-coverage/ https://www.atlassian.com/continuous-delivery/software-testing/code-coverage https://www.thoughtworks.com/insights/blog/are-test-coverage-metrics-overrated\n","date":"2021-07-14","externalUrl":null,"permalink":"/posts/code-coverage/","section":"Posts","summary":"本文简要介绍了代码覆盖率的概念、重要性、常见指标、工作原理以及主流工具，强调了不要过度依赖代码覆盖率指标。","title":"关于代码覆盖率 (About Code Coverage)","type":"posts"},{"content":"在一些情况下，即使设置了超时，Jenkins 作业依然不会因为超时自动失败，比如某些进程没有正常结束。\n为了解决这个问题，可以在流水线中结合 try .. catch 与 error，在超时后显式让构建失败。\n示例代码 # pipeline { agent none stages { stage(\u0026#39;Hello\u0026#39;) { steps { script { try { timeout(time: 1, unit: \u0026#39;SECONDS\u0026#39;) { echo \u0026#34;timeout step\u0026#34; sleep 2 } } catch (err) { // 捕获超时异常 println err echo \u0026#39;Time out reached.\u0026#39; error \u0026#39;build timeout failed\u0026#39; } } } } } } 日志输出 # [Pipeline] Start of Pipeline [Pipeline] stage [Pipeline] { (Hello) [Pipeline] script [Pipeline] { [Pipeline] timeout Timeout set to expire in 1 sec [Pipeline] { [Pipeline] echo timeout step [Pipeline] sleep Sleeping for 2 sec Cancelling nested steps due to timeout [Pipeline] } [Pipeline] // timeout [Pipeline] echo org.jenkinsci.plugins.workflow.steps.FlowInterruptedException [Pipeline] echo Time out reached. [Pipeline] error [Pipeline] } [Pipeline] // script [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline ERROR: build timeout failed Finished: FAILURE 总结 # timeout 步骤会在超时后抛出 FlowInterruptedException 通过 try/catch 捕获异常后调用 error，可以强制让 Jenkins 构建标记为 FAILURE 这种方式适用于任何需要在超时后终止并失败的场景 转载请注明作者与出处，欢迎关注公众号「DevOps攻城狮」获取更多 Jenkins 实战技巧。\n","date":"2021-06-24","externalUrl":null,"permalink":"/posts/jenkins-timeout/","section":"Posts","summary":"本文介绍如何在 Jenkins 流水线中正确处理超时场景，通过 \u003ccode\u003etry\u003c/code\u003e 和 \u003ccode\u003ecatch\u003c/code\u003e 结合 \u003ccode\u003eerror\u003c/code\u003e 确保超时后作业会失败。","title":"Jenkins 作业超时后让构建失败的方法（已解决）","type":"posts"},{"content":" 前言 # 本篇记录两个在做 Jenkins 与 AIX 做持续集成得时候遇到的 Git clone 代码失败的问题，并已解决，分享出来或许能有所帮助。\nDependent module /usr/lib/libldap.a(libldap-2.4.so.2) could not be loaded. 通过 SSH 进行 git clone 出现 Authentication failed 问题1：Dependent module /usr/lib/libldap.a(libldap-2.4.so.2) could not be loaded # Jenkins 通过 HTTPS 来 checkout 代码的时候，出现了如下错误：\n[2021-06-20T14:50:25.166Z] ERROR: Error cloning remote repo \u0026#39;origin\u0026#39; [2021-06-20T14:50:25.166Z] hudson.plugins.git.GitException: Command \u0026#34;git fetch --tags --force --progress --depth=1 -- https://git.company.com/scm/vas/db.git +refs/heads/*:refs/remotes/origin/*\u0026#34; returned status code 128: [2021-06-20T14:50:25.166Z] stdout: [2021-06-20T14:50:25.166Z] stderr: exec(): 0509-036 Cannot load program /opt/freeware/libexec64/git-core/git-remote-https because of the following errors: [2021-06-20T14:50:25.166Z] 0509-150 Dependent module /usr/lib/libldap.a(libldap-2.4.so.2) could not be loaded. [2021-06-20T14:50:25.166Z] 0509-153 File /usr/lib/libldap.a is not an archive or [2021-06-20T14:50:25.166Z] the file could not be read properly. [2021-06-20T14:50:25.166Z] 0509-026 System error: Cannot run a file that does not have a valid format. [2021-06-20T14:50:25.166Z] [2021-06-20T14:50:25.166Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:2450) [2021-06-20T14:50:25.166Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandWithCredentials(CliGitAPIImpl.java:2051) [2021-06-20T14:50:25.166Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.access$500(CliGitAPIImpl.java:84) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$1.execute(CliGitAPIImpl.java:573) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$2.execute(CliGitAPIImpl.java:802) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:161) [2021-06-20T14:50:25.167Z] at org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$GitCommandMasterToSlaveCallable.call(RemoteGitImpl.java:154) .......................... [2021-06-20T14:50:25.167Z] Suppressed: hudson.remoting.Channel$CallSiteStackTrace: Remote call to aix-devasbld-01 [2021-06-20T14:50:25.167Z] at hudson.remoting.Channel.attachCallSiteStackTrace(Channel.java:1800) .......................... [2021-06-20T14:50:25.168Z] at java.lang.Thread.run(Thread.java:748) [2021-06-20T15:21:20.525Z] Cloning repository https://git.company.com/scm/vas/db.git 但是直接在虚拟机上通过命令 git clone https://git.company.com/scm/vas/db.git ，可以成功下载，没有出现任何问题。\n如果将 LIBPATH 设置为 LIBPATH=/usr/lib 就能重现上面的错误，这说明通过 Jenkins 下载代码的时候它是去 /usr/lib/ 下面找 libldap.a 如果将变量 LIBPATH 设置为空 export LIBPATH= 或 unset LIBPATH，执行 git clone https://... 就正常了。 尝试在 Jenkins 启动 agent 的时候修改 LIBPATH 变量设置为空，但都不能解决这个问题，不明白为什么不行！？\n那就看看 /usr/lib/libldap.a 是什么问题了。\n# ldd 的时候发现这个静态库有问题 $ ldd /usr/lib/libldap.a /usr/lib/libldap.a needs: /opt/IBM/ldap/V6.4/lib/libibmldapdbg.a /usr/lib/threads/libc.a(shr.o) Cannot find libpthreads.a(shr_xpg5.o) /opt/IBM/ldap/V6.4/lib/libidsldapiconv.a Cannot find libpthreads.a(shr_xpg5.o) Cannot find libc_r.a(shr.o) /unix /usr/lib/libcrypt.a(shr.o) Cannot find libpthreads.a(shr_xpg5.o) Cannot find libc_r.a(shr.o) # 可以看到它链接到是 IBM LDAP $ ls -l /usr/lib/libldap.a lrwxrwxrwx 1 root system 35 Jun 10 2020 /usr/lib/libldap.a -\u0026gt; /opt/IBM/ldap/V6.4/lib/libidsldap.a # 再看看同样的 libldap.a 在 /opt/freeware/lib/ 是没问题的 $ ldd /opt/freeware/lib/libldap.a ldd: /opt/freeware/lib/libldap.a: File is an archive. $ ls -l /opt/freeware/lib/libldap.a lrwxrwxrwx 1 root system 13 May 27 2020 /opt/freeware/lib/libldap.a -\u0026gt; libldap-2.4.a 问题1：解决办法 # # 尝试替换 # 先将 libldap.a 重名为 libldap.a.old（不删除以防需要恢复） $ sudo mv /usr/lib/libldap.a /usr/lib/libldap.a.old # 重新链接 $ sudo ln -s /opt/freeware/lib/libldap.a /usr/lib/libldap.a $ ls -l /usr/lib/libldap.a lrwxrwxrwx 1 root system 27 Oct 31 23:27 /usr/lib/libldap.a -\u0026gt; /opt/freeware/lib/libldap.a 重新链接完成后，重新连接 AIX agent，再次执行 Jenkins job 来 clone 代码，成功了！\n问题2：通过 SSH 进行 git clone 出现 Authentication failed # 由于 AIX 7.1-TL4-SP1 即将 End of Service Pack Support，因此需要升级。但是升级到 AIX 7.1-TL5-SP6 后无法通过 SSH 下载代码。\n$ git clone ssh://git@git.company.com:7999/vas/db.git Cloning into \u0026#39;db\u0026#39;... Authentication failed. fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 像这样的错误，在使用 Git SSH 方式来 clone 代码经常会遇到，通常都是没有设置 public key。只要执行 ssh-keygen -t rsa -C your@email.com 生成 id_rsa keys，然后将 id_rsa.pub 的值添加到 GitHub/Bitbucket/GitLab 的 public key 中一般就能解决。\n但这次不一样，尽管已经设置了 public key，但错误依旧存在。奇快的是之前 AIX 7.1-TL4-SP1 是好用的，升级到 AIX 7.1-TL5-SP6 就不好用了呢？\n使用命令 ssh -vvv \u0026lt;git-url\u0026gt; 来看看他们在请求 git 服务器时候 debug 信息。\n# AIX 7.1-TL4-SP1 bash-4.3$ oslevel -s 7100-04-01-1543 bash-4.3$ ssh -vvv git.company.com OpenSSH_6.0p1, OpenSSL 1.0.1e 11 Feb 2013 debug1: Reading configuration data /etc/ssh/ssh_config debug1: Failed dlopen: /usr/krb5/lib/libkrb5.a(libkrb5.a.so): 0509-022 Cannot load module /usr/krb5/lib/libkrb5.a(libkrb5.a.so). 0509-026 System error: A file or directory in the path name does not exist. debug1: Error loading Kerberos, disabling Kerberos auth. ....... ....... ssh_exchange_identification: read: Connection reset by peer # New machine AIX 7.1-TL5-SP6 $ oslevel -s 7100-05-06-2015 $ ssh -vvv git.company.com OpenSSH_7.5p1, OpenSSL 1.0.2t 10 Sep 2019 debug1: Reading configuration data /etc/ssh/ssh_config debug1: Failed dlopen: /usr/krb5/lib/libkrb5.a(libkrb5.a.so): 0509-022 Cannot load module /usr/krb5/lib/libkrb5.a(libkrb5.a.so). 0509-026 System error: A file or directory in the path name does not exist. debug1: Error loading Kerberos, disabling Kerberos auth. ....... ....... ssh_exchange_identification: read: Connection reset by peer 可以看到的差别是 OpenSSH 的版本不同，可能是因此导致的，根据这个推测很快就找到了类似的问题和答案（Stackoverflow 链接)\n问题2：解决办法 # 在 ~/.ssh/config 文件里添加选项 AllowPKCS12keystoreAutoOpen no\n但问题又来了，这个选项是 AIX 上的一个定制选项，在 Linux 上是没有的。\n这会导致同一个域账户在 AIX 通过 SSH 可以 git clone 成功，但在 Linux 上 git clone 会失败。\n# Linux 上不识别改选项 stderr: /home/****/.ssh/config: line 1: Bad configuration option: allowpkcs12keystoreautoopen /home/****/.ssh/config: terminating, 1 bad configuration options fatal: Could not read from remote repository. 如果 config 文件可以支持条件选项就好了，即当为 AIX 是添加选项 AllowPKCS12keystoreAutoOpen no，其他系统则没有该选项。可惜 config 并不支持。 如果能单独的设置当前 AIX 的 ssh config 文件就好了。尝试将 /etc/ssh/ssh_config 文件修改如下，重启服务，再次通过 SSH clone，成功~！ Host * AllowPKCS12keystoreAutoOpen no # ForwardAgent no # ForwardX11 no # RhostsRSAAuthentication no # RSAAuthentication yes # PasswordAuthentication yes # HostbasedAuthentication no # GSSAPIAuthentication no # GSSAPIDelegateCredentials no # GSSAPIKeyExchange no # GSSAPITrustDNS no # ....省略 ","date":"2021-06-20","externalUrl":null,"permalink":"/posts/git-clone-failed-on-aix/","section":"Posts","summary":"本文记录了在 AIX 上使用 Jenkins 进行 Git Clone 时遇到的两个问题及其解决方法，包括依赖库加载失败和 SSH 认证失败。","title":"解决在 AIX 上 Git Clone 失败的两个问题","type":"posts"},{"content":"","date":"2021-06-17","externalUrl":null,"permalink":"/tags/ulimit/","section":"标签","summary":"","title":"Ulimit","type":"tags"},{"content":"最近使用 AIX 7.1 从 Bitbucket 下载代码的时候遇到了这个错误：\nfatal: write error: A file cannot be larger than the value set by ulimit.\n$ git clone -b dev https://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@git.company.com/scm/vmcc/opensrc.git --depth 1 Cloning into \u0026#39;opensrc\u0026#39;... remote: Counting objects: 2390, done. remote: Compressing objects: 100% (1546/1546), done. fatal: write error: A file cannot be larger than the value set by ulimit. fatal: index-pack failed 在 AIX 7.3 我遇到的是这个错误：\nfatal: fetch-pack: invalid index-pack output\n$ git clone -b dev https://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@git.company.com/scm/vmcc/opensrc.git --depth 1 Cloning into \u0026#39;opensrc\u0026#39;... remote: Counting objects: 2390, done. remote: Compressing objects: 100% (1546/1546), done. fatal: write error: File too large68), 1012.13 MiB | 15.38 MiB/s fatal: fetch-pack: invalid index-pack output 这是由于这个仓库里的文件太大，超过了 AIX 对于用户文件资源使用的上限。\n通过 ulimit -a 可以来查看。更多关于 ulimit 命令的使用 ulimit Command\n$ ulimit -a time(seconds) unlimited file(blocks) 2097151 data(kbytes) unlimited stack(kbytes) 32768 memory(kbytes) 32768 coredump(blocks) 2097151 nofiles(descriptors) 2000 threads(per process) unlimited processes(per user) unlimited 可以看到 file 有一个上限值 2097151。如果将它也改成 unlimited 应该就好了。\n通过 root 用户可以访问到 limits 文件 /etc/security/limits(普通用户没权限访问)。\n# 以下是这个文件里的部分内容 default: fsize = 2097151 core = 2097151 cpu = -1 data = -1 rss = 65536 stack = 65536 nofiles = 2000 将上述的值 fsize = 2097151 改成 fsize = -1 就将解除了文件块大小的限制了。修改完成后，重新登录来让这次修改生效。\n再次执行 ulimit -a，已经生效了。\n$ ulimit -a time(seconds) unlimited file(blocks) unlimited data(kbytes) unlimited stack(kbytes) 32768 memory(kbytes) 32768 coredump(blocks) 2097151 nofiles(descriptors) 2000 threads(per process) unlimited processes(per user) unlimited 此时 file(blocks) 已经变成 unlimited 了。再次尝试 git clone\ngit clone -b dev https://\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;@git.company.com/scm/vmcc/opensrc.git --depth 1 Cloning into \u0026#39;opensrc\u0026#39;... remote: Counting objects: 2390, done. remote: Compressing objects: 100% (1546/1546), done. remote: Total 2390 (delta 763), reused 2369 (delta 763) Receiving objects: 100% (2390/2390), 3.80 GiB | 3.92 MiB/s, done. Resolving deltas: 100% (763/763), done. Checking out files: 100% (3065/3065), done. 这次就成功了！\n","date":"2021-06-17","externalUrl":null,"permalink":"/posts/aix-ulimit/","section":"Posts","summary":"在 AIX 系统中遇到 Git 下载大容量仓库时因文件大小限制导致失败，通过修改 ulimit 设置解决问题。","title":"通过解除文件资源限制：解决在 AIX 使用 Git 下载大容量仓库失败问题","type":"posts"},{"content":"最近在我使用 Artifactory Enterprise 遇到了上传制品非常缓慢的问题，在经过与 IT，Artifactory 管理员一起合作终于解决这个问题，在此分享一下这个问题的解决过程。\n如果你也遇到类似或许有所帮助。\n问题描述 # 最近发现通过 Jenkins 往 Artifactory 里上传制品的时候偶尔出现上传非常缓慢的情况，尤其是当一个 Jenkins stage 里有多次上传，往往会在第二次上传的时候出现传输速度极为缓慢（KB/s ）。\n问题排查和解决 # 我的构建环境和 Jenkins 都没有任何改动，所有的构建任务都出现了上传缓慢的情况，为了排除可能是使用 Artifactory plugin 的导致原因，通过 curl 命令来进行上传测试，也同样上传速度经常很慢。\n那么问题就在 Artifactory 上面。\n是 Artifactory 最近升级了？ 还是 Artifactory 最近修改了什么设置？ 也许是 Artifactory 服务器的问题？ 在跟 Artifactory 管理员进行了沟通之后，排除了以上 1，2 的可能。为了彻底排除是 Artifactory 的问题，通过 scp 进行拷贝的时候同样也出现了传输速度非常慢的情况，这问题就出现在网络上了。\n这样需要 IT 来帮助排查网络问题了，最终 IT 建议更换网卡进行尝试（因为他们之前有遇到类似的情况），但这种情况会有短暂的网络中断，不过最终还是得到了管理者的同意。\n幸运的是在更换网卡之后，Jenkins 往 Artifactory 传输制品的速度恢复了正常。\n总结 # 处理次事件的一点点小小的总结：\n由于这个问题涉及到几个团队，为了能够快速推进，此时明确说明问题，推测要有理有据，以及该问题导致了什么样的严重后果（比如影响发布）才能让相关人重视起来，否则大家都等着，没人回来解决问题。\n当 Artifactory 管理推荐使用其他数据中心 instance，建议他们先尝试更换网卡；如果问题没有得到解决，在同一个数据中心创建另外一台服务器。如果问题还在，此时再考虑迁移到其他数据中心instance。这大大减少了作为用户去试错所带来的额外工作量。\n","date":"2021-06-16","externalUrl":null,"permalink":"/posts/artifactory-slow-upload/","section":"Posts","summary":"在使用 JFrog Artifactory 上传制品时遇到速度缓慢和上传失败的问题，经过排查和解决，分享经验和教训。","title":"关于 Artifactory 上传制品变得非常缓慢，偶尔失败的问题分享","type":"posts"},{"content":"","date":"2021-06-07","externalUrl":null,"permalink":"/tags/eslint/","section":"标签","summary":"","title":"ESlint","type":"tags"},{"content":"在将 ESlint 报告集成到 Jenkins 时，我遇到了一个问题：\n本地打开 eslint-report.html 显示正常 上传到 Jenkins 并通过 HTML Publisher 插件展示时，报告样式和脚本无法加载 其他 HTML 报告在 Jenkins 中显示正常，只有 ESlint 报告有问题 将 Jenkins 中的 eslint-report.html 下载到本地后又能正常显示 这让我怀疑是 Jenkins 的安全策略导致的，最终在 Stack Overflow 找到了答案。\n解决步骤 # 打开 Jenkins 首页 进入 Manage Jenkins 点击 Script Console 在控制台中粘贴以下代码并执行 System.setProperty(\u0026#34;hudson.model.DirectoryBrowserSupport.CSP\u0026#34;, \u0026#34;\u0026#34;) 执行后刷新页面，CSS 和 JS 就能正常加载 原因说明 # Jenkins 引入了新的 Content Security Policy (CSP) 在 Chrome 控制台中查看 Elements 时会看到类似 No frames allowed 的提示，这正是导致 ESlint 报告无法加载样式和脚本的原因。\n通过清空 hudson.model.DirectoryBrowserSupport.CSP，我们允许 Jenkins 在 HTML 报告中加载 CSS 和 JS，从而解决了问题。\n📌 提示：此方法会放宽安全策略，请在内网环境或可信项目中使用。\n","date":"2021-06-07","externalUrl":null,"permalink":"/posts/jenkins-eslint/","section":"Posts","summary":"本文记录了 ESlint HTML 报告在 Jenkins 中因内容安全策略（CSP）限制而无法正确显示的问题，并介绍了如何通过修改 Jenkins 配置使报告正常加载。","title":"解决 ESlint HTML 报告在 Jenkins 作业中无法正常显示的问题","type":"posts"},{"content":"在使用 Git 提交代码之前，建议做以下这些设置。\n叫指南有点夸张，因为它在有些情况下下不适用，比如你已经有了 .gitattributes 或 .editorconfig 等文件，那么有些设置就不用做了。\n因此暂且叫他指北吧，它通常情况下还是很有用的。\n废话不多说，看看都需要哪些设置吧。\n1. 配置 name 和 email # # 注意，你需要将下面示例中我的 name 和 email 换成你自己的 $ git config --global user.name \u0026#34;shenxianpeng\u0026#34; $ git config --global user.email \u0026#34;xianpeng.shen@gmail.com\u0026#34; 对于，我还推荐你设置头像，这样方便同事间的快速识别。\n当你不设置头像的时候，只有把鼠标放到头像上才知道 Pull Request 的 Reviewers 是谁（来自于Bitubkcet）。\n2. 设置 core.autocrlf=false # 为了防止 CRLF(windows) 和 LF(UNIX/Linux/Mac) 的转换问题。为了避免在使用 Git 提交代码时出现历史被掩盖的问题，强烈建议每个使用 Git 的人执行以下命令\n$ git config --global core.autocrlf false # 检查并查看是否输出 \u0026#34;core.autocrlf=false\u0026#34;，这意味着命令设置成功。 $ git config --list 如果你的项目底下已经有了 .gitattributes 或 .editorconfig 文件，通常这些文件里面都有放置 CRLF 和 LF 的转换问题的设置项。\n这时候你就不必特意执行命令 git config --global core.autocrlf false\n3. 编写有规范的提交 # 我在之前的文章里分享过关于如何设置提交信息规范，请参看《Git提交信息和分支创建规范》。\n4. 提交历史的压缩 # 比如你修改一个 bug，假设你通过 3 次提交到你的个人分支才把它改好。这时候你提 Pull Request 就会显示有三个提交。\n如果提交历史不进行压缩，这个 PR 被合并到主分支后，以后别人看到你这个 bug 的修复就是去这三个 commits 里去一一查看，进行对比，才能知道到底修改了什么。\n压缩提交历史就是将三次提交压缩成一次提交。\n可以通过 git rebase 命令进行 commit 的压缩，比如将最近三次提交压缩成一次可以执行\ngit rebase -i HEAD~3 5. 删除已经 merge 的分支 # 有些 SCM，比如 Bitbucket 不支持默认勾选 Delete source branch after merging，这个问题终于在 Bitbucket 7.3 版本修复了。详见 BSERV-9254 和 BSERV-3272 （2013年创建的）。\n注意在合并代码时勾选删除源分支这一选项，否则会造成大量的开发分支留在 Git 仓库下。\n如果还需要哪些设置这里没有提到的，欢迎补充。\n","date":"2021-05-14","externalUrl":null,"permalink":"/posts/git-guidelines/","section":"Posts","summary":"本文介绍了在使用 Git 提交代码之前需要进行的一些常见设置，包括配置用户名和邮箱、处理换行符、编写规范的提交信息等，帮助开发者更好地管理代码版本。","title":"Git 常见设置指北","type":"posts"},{"content":" 问题背景 # 在 CI 过程中，如果需要从 JFrog Artifactory 下载整个文件夹的制品，而 IT 出于某些原因关闭了 Download Folder 功能，就会遇到以下报错：\n访问类似 API：\n[https://den-artifactory.company.com/artifactory/api/archive/download/team-generic-release-den/project/abc/main/?archiveType=zip](https://den-artifactory.company.com/artifactory/api/archive/download/team-generic-release-den/project/abc/main/?archiveType=zip) 返回：\n{ \u0026#34;errors\u0026#34;: [ { \u0026#34;status\u0026#34;: 403, \u0026#34;message\u0026#34;: \u0026#34;Download Folder functionality is disabled.\u0026#34; } ] } 官方文档：Retrieve Folder or Repository Archive\n解决思路 # 虽然直接下载文件夹功能被禁用，但 Artifactory 还有其他 API 可以间接实现批量下载。\n下面介绍两种常用方式。\n方法 1：按创建时间范围获取制品列表 # API 文档：Artifacts Created in Date Range\n示例 Shell 脚本 download.sh：\n#!/bin/sh USERNAME=$1 PASSWORD=$2 REPO=$3 N_DAY_AGO=$4 # 时间范围（毫秒） START_TIME=$(($(date --date=\u0026#34;$N_DAY_AGO days ago\u0026#34; +%s%N)/1000000)) END_TIME=$(($(date +%s%N)/1000000)) ARTIFACTORY=https://den-artifactory.company.com/artifactory if [ ! -x \u0026#34;`which sha1sum`\u0026#34; ]; then echo \u0026#34;You need to have the \u0026#39;sha1sum\u0026#39; command in your path.\u0026#34; exit 1 fi # 获取制品 URI 列表 RESULTS=$(curl -s -X GET -u $USERNAME:$PASSWORD \\ \u0026#34;$ARTIFACTORY/api/search/creation?from=$START_TIME\u0026amp;to=$END_TIME\u0026amp;repos=$REPO\u0026#34; | grep uri | awk \u0026#39;{print $3}\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed -r \u0026#39;s/^.{1}//\u0026#39;) # 循环下载 for RESULT in $RESULTS; do echo \u0026#34;Fetching path from $RESULT\u0026#34; PATH_TO_FILE=$(curl -s -X GET -u $USERNAME:$PASSWORD $RESULT | grep downloadUri | awk \u0026#39;{print $3}\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed s\u0026#39;/.$//\u0026#39; | sed -r \u0026#39;s/^.{1}//\u0026#39;) echo \u0026#34;Downloading $PATH_TO_FILE\u0026#34; curl -u $USERNAME:$PASSWORD -O $PATH_TO_FILE done 使用方式：\nsh download.sh \u0026lt;USERNAME\u0026gt; \u0026lt;PASSWORD\u0026gt; \u0026lt;REPO_NAME\u0026gt; \u0026lt;N_DAY_AGO\u0026gt; 方法 2：使用模式匹配搜索制品 # API 文档：Pattern Search\n通过 Ant 风格的路径模式搜索制品，例如：\nrepo-key/path/to/files/*.jar 解析 API 响应中的 downloadUri，再用 curl 批量下载：\ncurl -u $USERNAME:$PASSWORD -O $PATH_TO_FILE 💡 如果你有更好的方法，可以在评论中分享交流。\n","date":"2021-05-04","externalUrl":null,"permalink":"/posts/artifactory-api-search/","section":"Posts","summary":"本文介绍了在 JFrog Artifactory 禁用“Download Folder”功能时，如何使用 Artifactory REST API 来批量下载整个文件夹的制品，并提供了基于 Shell 脚本的示例实现。","title":"当 Artifactory “Download Folder 功能被禁用”时如何下载整个文件夹的制品","type":"posts"},{"content":" 问题描述 # 今天我遇到一个问题：自己打包的 Windows 安装程序无法安装，并弹出如下安装窗口：\n奇怪的是，上一个版本一切正常，而且我并没有修改任何代码。\n最终发现问题很简单，但定位过程却不轻松。\n解决方法 # 我的情况是 构建目录名称里包含了空格。\n例如：\n正常的构建名称：v2.2.2.3500-da121sa-Developer 出问题的构建名称：v2.2.2.3500-32jkjdk - Developer（多了空格） 将文件夹名中的空格去掉后，安装程序恢复正常工作。\n如何找到原因 # 我在 Google 搜索时看到了这篇文章，得到启发。\n按照文章思路，如果用命令行执行：\nmsiexec.exe other-commands ... 比较正常版本和出问题版本的行为，很快就能发现问题出在路径命名上。\n✅ 结论：Windows Installer 在某些情况下无法正确处理路径中的空格，尤其是在文件夹命名中。 如果你遇到类似问题，不妨先检查一下构建路径和文件名是否包含空格。\n如果这个方法对你也有效，欢迎在评论区留言。\n","date":"2021-04-22","externalUrl":null,"permalink":"/posts/why-windows-installer-pop-up/","section":"Posts","summary":"本文解释了一个常见的 Windows 安装程序问题：安装时意外弹出窗口的原因，以及通过修正构建文件夹命名规则来解决该问题的方法。","title":"为什么 Windows Installer 会弹出窗口？（已解决）","type":"posts"},{"content":"","date":"2021-04-06","externalUrl":null,"permalink":"/tags/jacoco/","section":"标签","summary":"","title":"JaCoCo","type":"tags"},{"content":"本文适用的是 Gradle 来构建和适用 JaCoCo。\n分别介绍了 build.gradle 的文件配置，执行测试和生成报告，报告参数说明，以及如何忽略指定的包或类从而影响测试覆盖率的结果。\nbuild.gradle 文件配置 # 比如使用 gradle 来管理的项目可以在 build.gradle 里添加如下代码\nplugins { id \u0026#39;jacoco\u0026#39; } jacoco { toolVersion = \u0026#34;0.8.5\u0026#34; } test { useJUnitPlatform() exclude \u0026#39;**/**IgnoreTest.class\u0026#39; // 如果有 test case 不通过，如有必要可以通过这样忽略掉 finalizedBy jacocoTestReport // report is always generated after tests run } jacocoTestReport { dependsOn test // tests are required to run before generating the report reports { xml.enabled true csv.enabled false html.destination file(\u0026#34;${buildDir}/reports/jacoco\u0026#34;) } } 执行测试，生成代码覆盖率报告 # 然后执行 gradle test 就可以了。之后可以可以在 build\\reports\\jacoco 目录下找到报告了。\n重点是如何分析报告。打开 index.html，报告显示如下：\n报告参数说明 # Coverage Counters（覆盖计数器） # JaCoCo 使用一组不同的计数器来计算覆盖率指标，所有这些计数器都来自于 Java 类文件中包含的信息，这些信息基本上是 Java 字节码指令和嵌入类文件中的调试信息。这种方法可以在没有源代码的情况下，对应用程序进行有效的即时检测和分析。在大多数情况下，收集到的信息可以映射到源代码，并可视化到行级粒度。\n这种方法也有一定的局限性，就是类文件必须与调试信息一起编译，以计算行级覆盖率并提供源码高亮。但不是所有的 Java 语言结构都可以直接编译成相应的字节码。在这种情况下，Java 编译器会创建所谓的合成代码，有时会导致意外的代码覆盖率结果。\nInstructions (C0 Coverage) - 指令（C0覆盖率） # 最小的单位 JaCoCo 计数是单个 Java 字节码指令，指令覆盖率提供了关于被执行或遗漏的代码量的信息，这个指标完全独立于源码格式化，即使在类文件中没有调试信息的情况下也始终可用。\nBranches (C1 Coverage) - 分支（C1覆盖率） # JaCoCo 还计算所有 if 和 switch 语句的分支覆盖率，这个指标计算一个方法中此类分支的总数，并确定执行或遗漏的分支数量。即使在类文件中没有调试信息的情况下，分支覆盖率总是可用的。但请注意在这个计数器定义的上下文中异常处理不被认为是分支。\n如果类文件没有编译调试信息，决策点可以被映射到源行并相应地高亮显示。\n没有覆盖。行中没有分支被执行（红菱形 部分覆盖。仅执行了该线的部分分支（黄钻 全覆盖。线路中的所有分支都已执行（绿色菱形） Cyclomatic Complexity - 环形复杂度 # JaCoCo 还计算了每个非抽象方法的循环复杂度并总结了类、包和组的复杂度。循环复杂度是指在（线性）组合中，能够产生通过一个方法的所有可能路径的最小路径数。 因此复杂度值可以作为完全覆盖某个软件的单元测试用例数量的指示，即使在类文件中没有调试信息的情况下，也可以计算出复杂度数字。\n循环复杂度v(G)的正式定义是基于将方法的控制流图表示为一个有向图。\nv(G) = E - N + 2\n其中E为边数，N为节点数。JaCoCo根据分支数(B)和决策点数(D)计算方法的循环复杂度，其等价公式如下。\nv(G) = B - D + 1\n根据每个分支的覆盖状态，JaCoCo还计算每个方法的覆盖和遗漏复杂度。遗漏的复杂度再次表明了完全覆盖一个模块所缺少的测试用例数量。请注意，由于JaCoCo不考虑异常处理作为分支，尝试/捕获块也不会增加复杂性。\nLines - 行 # 对于所有已经编译过调试信息的类文件，可以计算出各个行的覆盖率信息。当至少有一条分配给该行的指令被执行时，就认为该源行已被执行。\n由于单行通常会编译成多条字节码指令，源码高亮显示每行包含源码的三种不同状态。\nNo coverage: 该行没有指令被执行（红色背景）。 部分覆盖。该行中只有部分指令被执行（黄色背景）。 全覆盖。该行的所有指令都已执行（绿色背景）。 根据源码的格式，一个源码的一行可能涉及多个方法或多个类。因此，方法的行数不能简单地相加来获得包含类的总行数。同样的道理也适用于一个源文件中多个类的行数。JaCoCo根据实际的源代码行数来计算类和源代码文件的行数。\nMethod - 方法 # 每个非抽象方法至少包含一条指令。当至少有一条指令被执行时，一个方法就被认为是被执行的。由于 JaCoCo 工作在字节代码层面，构造函数和静态初始化器也被算作方法，其中一些方法在 Java 源代码中可能没有直接的对应关系，比如隐式的，因此生成了默认的构造函数或常量的初始化器。\nClasses - 类 # 当一个类中至少有一个方法被执行时，该类被认为是被执行的。请注意，JaCoCo 认为构造函数和静态初始化器都是方法。由于 Java 接口类型可能包含静态初始化器，这种接口也被视为可执行类。\n覆盖率的计算原文\n从代码覆盖率报告中忽略指定的包或代码 # 对于有些包和代码可能不属于你的项目，但也被统计在内，可以通修改在 build.gradle 将指定的代码或是包从 JaCoCo 报告中忽略掉。如下：\n// 省略部分代码 jacocoTestReport { dependsOn test // tests are required to run before generating the report reports { xml.enabled true csv.enabled false html.destination file(\u0026#34;${buildDir}/reports/jacoco\u0026#34;) } afterEvaluate { classDirectories.setFrom(files(classDirectories.files.collect { fileTree(dir: it, exclude: [ \u0026#39;com/vmware/antlr4c3/**\u0026#39;]) \u0026#39;com/vmware/antlr4c3/**\u0026#39;, \u0026#39;com/basic/parser/BasicParser*\u0026#39; ]) })) } } ","date":"2021-04-06","externalUrl":null,"permalink":"/posts/jacoco-imp/","section":"Posts","summary":"本文介绍了 JaCoCo 的使用方法，包括 Gradle 配置、执行测试生成报告、报告参数说明以及如何忽略指定的包或类影响测试覆盖率结果。","title":"JaCoCo 代码覆盖率实践分享","type":"posts"},{"content":"","date":"2021-03-28","externalUrl":null,"permalink":"/tags/pythonic/","section":"标签","summary":"","title":"Pythonic","type":"tags"},{"content":"Python 不必多说，它是众多编程语言中最容易学习的动态类型语言。它的跨平台、易读、易写、丰富的 Packages 等众多特性，也是众多DevOps/测试/开发工程师是最常用的语言之一。\n相信不少人用它完成了很多工作，但你是不是仅仅止步于功能的实现而忽略了去写出更加简洁，优美的 Pythonic 代码呢？\n在我最开始用 Python 时，我还不知道 Pythonic 这个词，直到多年前一位资深的程序员在给我培训的时候提到了项目中有一些代码不够 Pythonic，需要重构。根据语境，我理解他的意思：就是 Python 的代码没有按照 Python 的方式来写。\n什么是 Pythonic # 充分利用 Python 语言的特性来产生清晰、简洁和可维护的代码。Pythonic 的意思是指代码不仅仅是语法正确，而是遵循 Python 社区的惯例，并以其预期的方式使用该语言。\n举例 # 以下是 C/C++ 程序员的一段代码:\nint a = 1; int b = 100; int total_sum = 0; while (b \u0026gt;= a) { total_sum += a; a++; } 如果没有学习 Python 编程模式，那么将上面的代码改用 Python 来写可能会是这样：\na = 1 b = 100 total_sum = 0 while b \u0026gt;= a: total_sum += a a += 1 如果用 Pythonic 的方式来写，应该是这样的：\ntotal_sum = sum(range(1, 101)) 再举个常见的例子，如果用 Java 可能是这样写出来一个 For 循环\nfor(int index=0; index \u0026lt; items.length; index++) { items[index].performAction(); } 在 Python中，使用以下方法会更干净一些：\nfor item in items: item.perform_action() 甚至是一个生成器表达式：\n(item.some_attribute for item in items) 因此，从本质上讲，当有人说某件事情不符合 pythonic 时，他们是在说这段代码可以用一种更适合 Python 编码风格的方式来重新编写。 另外，去了解 Python Built-in Functions，而不是重新造轮子。\n关于 Pythonic 的“官方介绍” # 其实，Python 命令行里已经秘密“隐藏”了关于 Pythonic 的介绍。只要打开 Python 控制台，输入 import this，你就能看到：\nC:\\Users\\xshen\u0026gt;python Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32 Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren\u0026#39;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you\u0026#39;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it\u0026#39;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let\u0026#39;s do more of those! \u0026gt;\u0026gt;\u0026gt; 直译过来是：Tim Peters 的《Python的禅意》\n美丽的比丑陋的好。 明确的比含蓄的好。 简单的比复杂的好 复杂的比复杂的好 扁平的比嵌套的好。 稀疏比密集好。 可读性很重要。 特殊情况不特殊，不足以打破规则。 虽然实用性胜过纯粹性。 错误永远不应该默默地通过。 除非明确沉默。 在面对模棱两可的情况下，拒绝猜测的诱惑。 应该有一个--最好只有一个--明显的方法。 虽然这种方式一开始可能并不明显，除非你是荷兰人。 现在总比不做要好。 虽然从不比现在*好。 如果实现很难解释，那就是个坏主意。 如果实现很容易解释，它可能是个好主意。 命名空间是一个非常棒的想法--让我们做更多的命名空间! 关于 Pythonic 你 get 到了吗？\n","date":"2021-03-28","externalUrl":null,"permalink":"/posts/pythonic/","section":"Posts","summary":"本文介绍了 Pythonic 的概念，并通过示例展示如何编写更简洁、优美的 Python 代码，帮助开发者提升代码质量和可读性。","title":"你的 Python 代码够不够 Pythonic？","type":"posts"},{"content":" 问题背景 # 在使用 Jenkins 多分支流水线（Multibranch Pipeline）时，有时需要针对不同分支设置不同的默认参数。\n例如：\ndevelop / hotfix / release 分支\n除了常规构建外，还需要进行代码扫描等额外分析（如安全扫描、质量检测）。 feature / bugfix 分支或 Pull Request\n只需要进行常规构建，不执行额外扫描。 因此，我们希望参数能够根据分支自动设置默认值。\n解决方案 # 在 Jenkinsfile 中，可以通过 env.BRANCH_NAME 判断当前构建分支，并设置默认参数值。\n示例代码：\ndef polarisValue = false def blackduckValue = false if (env.BRANCH_NAME.startsWith(\u0026#34;develop\u0026#34;) || env.BRANCH_NAME.startsWith(\u0026#34;hotfix\u0026#34;) || env.BRANCH_NAME.startsWith(\u0026#34;release\u0026#34;)) { polarisValue = true blackduckValue = true } pipeline { agent { node { label \u0026#39;gradle\u0026#39; } } parameters { booleanParam defaultValue: polarisValue, name: \u0026#39;Polaris\u0026#39;, description: \u0026#39;取消勾选可禁用 Polaris 扫描\u0026#39; booleanParam defaultValue: blackduckValue, name: \u0026#39;BlackDuck\u0026#39;, description: \u0026#39;取消勾选可禁用 BlackDuck 扫描\u0026#39; } stages { // 构建、测试、扫描等步骤 } } ✅ 优点\n不需要为每个分支单独维护一套 Jenkinsfile。 参数可灵活控制，方便扩展更多分支规则。 支持在多分支流水线中实现“按需扫描”，节省构建时间。 这样配置后，Jenkins 在不同分支构建时会自动使用对应的参数默认值，避免人工干预。\n","date":"2021-03-24","externalUrl":null,"permalink":"/posts/jenkins-dynamic-default-parameters/","section":"Posts","summary":"本文介绍如何在 Jenkins 多分支流水线中，根据构建分支动态设置不同的默认参数，从而实现分支差异化配置。","title":"在 Jenkins 中为不同分支设置不同的默认参数","type":"posts"},{"content":"","date":"2021-03-20","externalUrl":null,"permalink":"/tags/codereview/","section":"标签","summary":"","title":"CodeReview","type":"tags"},{"content":" 背景 # 代码审查（Code Review），就是让别人来审查你的代码，其目的就是确保代码库的整体代码运行状况随着时间推移而不断改善。\n中国有句古话：三人行必有我师。\n代码审查同样如此：\n他人的审查或许会有不一样的思考和建议； 人都会犯错，多一个人检查就减少犯错的机率。 因此代码审查是你编写的代码在合并到主分支前最重要的一项检查工作，也是一项最直接、最低成本的发现软件中的错误绝佳方式。\n既然代码审查这么重要，而且有这样显而易见的收益，但总能听到代码审查在团队里执行起来不容易、效果不理想的问题。问题出在哪呢？\n据我观察有两点原因：\n第一，读别人代码需要花时间，往往还需要代码提交者带着业务为审查者讲一遍，同时占用双方时间； 其次，如果代码审查者工作繁重、压力大而没有时间，也很容易造成执行不到位，走过场；\n如何才能比较好的开展代码审查？让我们先来看看大公司是怎么做的，Google 的这篇关于代码审查的文章里给出了具体法则。\nGoogle 的代码审查法则 # 在进行代码审查时，应确保：\n代码经过精心设计 该功能对代码用户很有帮助 任何 UI 更改都是明智的，并且看起来不错 任何并行编程都是安全完成的 代码没有比需要的复杂 开发人员没有实现他们将来可能需要的东西 代码具有适当的单元测试 测试经过精心设计 开发人员对所有内容使用了清晰的名称 注释清晰实用，并且主要说明Why而不是What 代码已正确文档化 该代码符合我们的样式指南 确保检查要求你检查的每一行代码，查看上下文，确保你在改善代码运行状况，并称赞开发人员所做的出色工作。\n原文：https://google.github.io/eng-practices/review/reviewer/looking-for.html\n代码审查法的落地 # 可见，想要更好的落地代码审查，需要先要确立法则，你可以根据实际情况对上述法则进行借鉴、删减或补充；\n第二，作为技术领导应当积极布道，让开发者了解统一的代码审查法则；\n第三，应当把法则里的具体规则尽可能地通过流程控制、自动化检查则纳入到 Pull Request 中。\n另外提醒作为 Reviewer 要 Peaceful ！！！在代码审查时注意不要带有“教育”性质的去给别人提出修改建议，那样很容易适得其反。\n以下是一些不完全实践，供参考。\n流程控制 # 规避任何不经 Review 的代码进入到主分支 # 以 Bitucket 为例。GitHub，GitLab 在设置上大同小异。\n打开分支权限设置里的选项 Prevent changes without a pull request 打开它。当然如果有需要可以在这个选项里添加 Exception，被添加的人可以不通过 Pull Reuqest 来提交代码。\n在 Merge Check 里开启 Minimum approvals 这个选项。比如设置 Number of approvals = 1，这样需要至少有一个 Reviewers 点击 Approve 按钮才允许 Merge。\n自动化检查 # 通过CI流水线验证编译和测试 # 建立自动化构建和测试 Pipeline，这样在创建 Pull Request 的时候可以自动构建、测试以及检查。Jenkins 的 Multi-branch pipeline 可以满足这个需求。\n开启 Bitucket 的 Merge Check 里 Minimum successful builds 选项，验证构建/测试结果，以防止任何没有通过构建和测试的代码可以 Merge 到主分支。\n另外，可以通过自行编写工具来实现，或可以集成其他 CI 工具来做检查，例如：\n针对 Pull Request 的修改历史来分析提交历史并推荐 Reiewer； 通过 Lint 工具来检查编码规范； 通过 REST API 检查是否需要压缩 Commits 来保证清晰的提交历史； 通过 SonarQube 检查 Quality Gate 等。 实现自动化检查，可以帮助 Reviewers 将审查的工作精力放在代码的具体实现上，其他的交给工具。\n最后 # 代码审查做的好不好，跟一个团队有没有良好的技术氛围，或者是否存在有技术领导力，有“品位”的技术大牛也是正相关的。\n如果团队里大多数都是有“品位”的工程师，他们会以写出优秀的代码（或挑刺）乐此不疲。 相反如果团队不重视规范，只追求短期的绩效达成，只会让技术债越欠越多，产品越做越烂。 欢迎留言分享你的意见或建议。\n","date":"2021-03-20","externalUrl":null,"permalink":"/posts/code-review/","section":"Posts","summary":"本文介绍了谷歌的代码审查法则，并分享了如何在团队中有效实施代码审查的实践经验，包括流程控制和自动化检查等方面。","title":"基于谷歌代码审查（Code Review）法则的思考与实践","type":"posts"},{"content":" 问题描述 # 在将团队的 Jenkins 从 2.235.1 升级到 2.263.3 后，发现 Windows Agent 无法启动，日志报错如下：\n[windows-agents] Installing the Jenkins agent service ERROR: Unexpected error in launching an agent. This is probably a bug in Jenkins java.lang.NullPointerException at hudson.os.windows.ManagedWindowsServiceLauncher.launch(ManagedWindowsServiceLauncher.java:298) 该问题已在 Jenkins Jira 上有反馈：\nJENKINS-63198 Windows Support Updates 解决步骤 # 1. 更新 Windows Slaves 插件 # 将 windows-slaves-plugin 升级到 1.7 版本（支持 Jenkins 2.248+）。\n更新后，如果仍出现如下错误：\nERROR: Unexpected error in launching an agent. This is probably a bug in Jenkins org.jinterop.dcom.common.JIException: Unknown Failure at org.jvnet.hudson.wmi.Win32Service$Implementation.start(Win32Service.java:149) 请继续执行下一步。\n2. 修改 jenkins-agent.exe.config # 找到 Jenkins agent 安装目录下的 jenkins-agent.exe.config 文件，注释或删除以下行：\n\u0026lt;!-- \u0026lt;supportedRuntime version=\u0026#34;v2.0.50727\u0026#34; /\u0026gt; --\u0026gt; 确保配置如下：\n\u0026lt;configuration\u0026gt; \u0026lt;runtime\u0026gt; \u0026lt;generatePublisherEvidence enabled=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;/runtime\u0026gt; \u0026lt;startup\u0026gt; \u0026lt;supportedRuntime version=\u0026#34;v4.0\u0026#34; /\u0026gt; \u0026lt;/startup\u0026gt; \u0026lt;/configuration\u0026gt; 如果存在 jenkins-slave.exe.config 文件，也需同样修改。\n3. 检查 .NET Framework 版本 # 如果启动时提示：\n.NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service 则需要升级本机的 .NET Framework。 可参考：更新 .NET Framework\n总结 # 通过 更新插件 + 修改配置文件 + 升级 .NET Framework，即可解决 Jenkins 升级后 Windows Agent 无法启动的问题。\n","date":"2021-02-11","externalUrl":null,"permalink":"/posts/jenkins-windows-agent-cannot-start/","section":"Posts","summary":"本文介绍 Jenkins 升级后 Windows Agent 无法启动的问题，以及通过更新 Windows Slaves 插件和修改配置文件来解决的步骤。","title":"Jenkins 升级后 Windows Agent 无法启动的解决方法","type":"posts"},{"content":"DevOps 实际上是什么意思？🤔\nDevOps 是一种软件开发方法，涉及持续开发，持续测试，持续集成，部署和监视。这一系列过程跨越了传统上孤立的开发和运营团队，DevOps 试图消除它们之间的障碍。\n因此，DevOps 工程师基本上与 Development 和 Operations 团队合作。它是这两个主要部分之间的链接。\n概念与工具 # DevOps 包括诸如构建自动化、CI/CD、基础架构即代码等概念，并且有许多工具可以实现这些概念。由于这些工具数量众多，因此可能会造成混乱和压倒性的结果。\n最重要的是要了解概念，并为每个类别的学习找一种特定的工具。例如，当你已经知道什么是 CI/CD 并知道如何使用 Jenkins 时，也将很容易学习同类型的其他替代工具。\n接下来让就来看看学习 DevOps 需要掌握哪些技能。\n1）软件开发的概念 # 作为一名 DevOps 工程师，你不会直接对应用程序进行编程，但是当你与开发团队紧密合作以改善和自动化他们的任务时，你需要了解以下概念：\n开发人员的工作方式 他们正在使用哪个 git 工作流程 如何配置应用程序 自动化测试 2）操作系统 # 作为 DevOps 工程师，你负责准备在操作系统上部署应用程序的所需要的基础结构环境。并且由于大多数服务器是 Linux 服务器，因此你需要了解 Linux 操作系统，并善于使用命令行，所以你需要知道：\n基本的 Shell 命令 Linux 文件系统 管理服务器的基础知识 SSH 密钥管理 在服务器上安装不同的工具 3）网络与安全 # 你还需要了解网络和安全性的基础知识才能配置基础架构，例如：\n配置防火墙以保护应用程序 了解 IP 地址，端口和 DNS 的工作方式 负载均衡器 代理服务器 HTTP/HTTPS 但是，要在 DevOps 和 IT Operations 之间划清界线，你不是系统管理员。因此，在这里不需要高级知识，理解和了解基本知识就够了。IT 方面是这些 SysAdmins，Networking 或 Security Engineers 人的专长。\n4）容器化 # 随着容器成为新标准，你可能会将应用程序作为容器运行，这意味着你需要大致了解：\n虚拟化的概念 容器的概念 学习哪个工具？ Docker - 当今最受欢迎的容器技术 5）持续集成和部署 # 在 DevOps 中，所有代码更改（例如开发人员的新功能和错误修复）都应集成到现有应用程序中，并以自动化方式连续地部署到最终用户。因此，建立完整的 CI/CD 管道是 DevOps 工程师的主要任务和职责。\n在完成功能或错误修正后，应自动触发在 CI 服务器（例如 Jenkins ）上运行的管道，该管道：\n运行测试 打包应用程序 构建 Docker 镜像 将 Docker Image 推送到工件存储库，最后 将新版本部署到服务器（可以是开发，测试或生产服务器） 因此，你需要在此处学习技能：\n设置 CI/CD 服务器 构建工具和程序包管理器工具以执行测试并打包应用程序 配置工件存储库（例如 Nexus，Artifactory） 当然，可以集成更多的步骤，但是此流程代表 CI/CD 管道的核心，并且是 DevOps 任务和职责的核心。\n学习哪个工具？Jenkins 是最受欢迎的人之一。其他：Bamboo，Gitlab，TeamCity，CircleCI，TravisCI。\n6）云提供商 # 如今，许多公司正在使用云上的虚拟基础架构，而不是管理自己的基础架构。这些是基础架构即服务（IaaS）平台，可提供一系列服务，例如备份，安全性，负载平衡等。\n因此，你需要学习云平台的服务。例如。对于 AWS，你应该了解以下基本知识：\nIAM 服务-管理用户和权限 VPC 服务-你的专用网络 EC2 服务-虚拟服务器 AWS 提供了更多的服务，但是你只需要了解你实际需要的服务即可。例如，当 K8s 集群在 AWS 上运行时，你还需要学习 EKS 服务。 AWS 是功能最强大，使用最广泛的一种，但也是最困难的一种。\n学习哪个工具？AWS 是最受欢迎的一种。其他热门：Azure，Google Cloud，阿里云，腾讯云。\n7）容器编排 # 如前所述，容器已被广泛使用，在大公司中，成百上千个容器正在多台服务器上运行，这意味着需要以某种方式管理这些容器。\n为此目的，有一些容器编排工具，而最受欢迎的是 Kubernetes。因此，你需要学习：\nKubernetes 如何工作 管理和管理 Kubernetes 集群 并在其中部署应用程序 学习哪个工具？Kubernetes - 最受欢迎，没有之一。\n8）监视和日志管理 # 软件投入生产后，对其进行监视以跟踪性能，发现基础结构以及应用程序中的问题非常重要。因此，作为 DevOps 工程师的职责之一是：\n设置软件监控 设置基础架构监控，例如用于你的 Kubernetes 集群和底层服务器。 学习哪个工具？Prometheus, Grafana\u0026hellip;\n9）基础设施即代码 # 手动创建和维护基础架构非常耗时且容易出错，尤其是当你需要复制基础架构时，例如用于开发，测试和生产环境。\n在 DevOps 中，希望尽可能地自动化，那就是将“基础结构即代码（Infrastructure as Configuration）”引入其中。因此使用 IaC ，我们将使用代码来创建和配置基础结构，你需要了解两种 IaC 方式：\n基础设施配置 配置管理 使用这些工具，可以轻松地复制和恢复基础结构。因此，你应该在每个类别中都知道一种工具，以使自己的工作更有效率，并改善与同事的协作。\n学习哪个工具？\n基础架构设置：Terraform 是最受欢迎的一种。 配置管理：Ansible，Puppet，Chef。\n10）脚本语言 # 作为 DevOps 工程师就常见的工作就是编写脚本和小型的应用程序以自动化任务。为了能够做到这一点，你需要了解一种脚本或编程语言。\n这可能是特定于操作系统的脚本语言，例如 bash 或 Powershell。\n还需要掌握一种独立于操作系统的语言，例如 Python 或 Go。这些语言功能更强大，更灵活。如果你善于使用其中之一，它将使你在就业市场上更具价值。\n学习哪个工具？Python：目前是最需要的一个，它易于学习，易于阅读并且具有许多可用的库。其他：Go，NodeJS，Ruby。\n11）版本控制 # 上述所有这些自动化逻辑都作为代码编写，使用版本控制工具（例如Git）来管理这些代码和配置文件。\n学习哪个工具？Git - 最受欢迎和广泛使用，没有之一。\nDevOps Roadmap [2021] - How to become a DevOps Engineer\n","date":"2021-01-21","externalUrl":null,"permalink":"/posts/devops-roadmap-2021/","section":"Posts","summary":"本文介绍了成为DevOps工程师所需的技能和工具，涵盖软件开发、操作系统、网络安全、容器化、持续集成与部署等方面的知识。","title":"2021年DevOps工程师的学习路线","type":"posts"},{"content":"DevOps 已经走了很长一段路，毫无疑问，它将在今年继续发光。由于许多公司都在寻求有关数字化转型的最佳实践，因此重要的是要了解领导者认为行业发展的方向。从这个意义上讲，以下文章是 DevOps 领导者对 DevOps 趋势的回应的集合，需要在 2021 年关注。\n让我们看看他们每个人对来年的 DevOps 有何评价。\n迁移到微服务将成为必须 —— Wipro Limited 首席 DevOps 工程师 “从整体迁移到微服务和容器化架构对于所有公司的数字化转型之旅都是必不可少的，它不再是一个选择或选项。这就是Kubernetes 的采用率将上升的地方，当组织迁移到多重云时，Terraform 将成为实现基础架构自动化的最终选择。”\nHybrid将成为部署规范 —— JFrog 开发人员关系 VP “2020年将加速远程工作，加快向云的迁移，并将 DevOps 从最佳实践转变为每个业务的重要组成部分。随着我们进入2021年，该行业将在多个方面拥抱Hybrid。首先，企业将完全采用混合型劳动力，将远程工作和现场团队协作的优势相结合。 其次，商业模式将变得混合，例如将虚拟规模与本地网络合并的会议。最终，随着公司对堆栈进行现代化以利用云原生技术的优势，混合将成为部署规范，但要意识到并非所有事物都可以迁移到外部。2021年的赢家将是拥抱业务，模型和产品混合的公司。”\nDataOps将蓬勃发展 - 乐天高级 DevOps 工程师 “DataOps 肯定会在 2021 年蓬勃发展，COVID 可能会在其中发挥作用。由于 COVID 和 WFH 的情况，数字内容的消费量猛增，这要求自动缩放和自我修复系统的自动化达到新水平，以满足增长和需求。\n到目前为止，DevOps 设置的系统仅用于日志记录，监视和警报（ELK/EFK 堆栈，Prometheus/Grafana/Alertmanager等）。现在，DevOps 现在应该加强并使用可用数据和指标来 产生有价值的见解，学习并应用机器学习模型来预测事件或中断，开发从数据中学习自身并预测能力的自动化以改进预算计划。许多人已经开始对此部分调用 MLOps/AIOps。”\n弹性测试将成为主流 —— Neotys 产品负责人 从我的角度来看，可观察性，性能测试和弹性测试之间的交叉点将成为主流。 随着 AWS 和 Google 等 WW 领导者最近发布的 Ops 问题，以及各个领域的数字化转型都在加速发展，市场将逐渐意识到，公共或私有云形式提供的无限可扩展性是不够的。” - Neotys 产品负责人 Patrick Wolf\nGitOps 将成为常态 —— 梅西百货的首席架构师 “一个“构建，拥有，拥有”的开发过程需要开发人员知道和理解的工具。GitOps 是 DevOps 如何使用开发人员工具来驱动操作的名称。\nGitOps 是一种进行持续交付的方法。 更具体地说，它是用于构建统一部署，监视和管理的 Cloud Native 应用程序的操作模型。 它通过将 Git 用作声明性基础结构和应用程序的真实来源来工作。 当在 Git 中推送和批准提交时，自动化的 CI/CD 管道将对你的基础架构进行更改。它还利用差异工具将实际生产状态与源代码控制下的生产状态进行比较，并在出现差异时提醒你。GitOps 的最终目标是加快开发速度，以便你的团队可以安全可靠地对 Kubernetes 中运行的复杂应用程序进行更改和更新。” -梅西百货（Macy\u0026rsquo;s）首席建筑师 Soumen Sarkar\n将会有更多的迁移到无服务器 —— ADP Lifion 的站点 SRE 经理 “2021 年将是注视更多迁移到无服务器的一年。如果容器和业务流程是 Z 代.. 那么无服务器的实时负载将是 Gen+ .. 仅在你使用时使用和付款可能看起来是一样的.. 但请考虑运行基于 k8s pod 的微服务，以便在需要时在无服务器上运行相同的服务。” - ADP Lifion 网站可靠性工程经理 Shivaramakrishnan G\nNoOps 出现 —— ClickIT Smart Technologies 的 CEO “我希望出现更多托管服务，并减少我们的 DevOps 运营并减少客户的运营支出。更多无服务器应用程序，更多无服务器服务，例如 Aurora 无服务器，Fargate，Amazon S3 和无服务器静态网站。数据中心中的 Amazon ECS/EKS（新版本 re：invent 2020）以及云管理服务，可让你减少数据中心的维护和开发。同样，将更多云原生的原理和功能移植到数据中心，例如。亲戚。” - ClickIT Smart Technologies 首席执行官 Alfonso Valdes\nBizDevOps 将大放异彩 —— Petco 的 DevOps 经理 “在架构和公司层次结构方面朝着成本优化的方向发展-随着业务的发展，DevOps 的价值不断提高。\n专注于灵活的，云原生的架构和工具，这些功能一旦具备了“大佬”的能力，就可以打包成小型企业使用的包装（Snowflake 或 Hazelcast 与 Oracle/Teradata）\nFaaS 才刚刚起步（无服务器，Lambda 等）- 运营问题正在得到解决，人们正在意识到潜力。”\n基础设施即代码（IaC）的地位将更高 —— 沃尔沃高级解决方案架构师 “基础架构即代码（IaC）：云中 DevOps 的核心原则。你的基础架构本地或云中的服务器，网络和存储设备（定义为代码）。这使公司可以自动化并简化其基础架构。 IaC 还提供了一个简单的基础结构版本控制系统，该系统可让团队在发生灾难性故障时回退到“有效的最后配置”。这意味着可以快速恢复并减少停机时间。”\n自动化和混乱工程变得非常重要 —— 直布罗陀印度开发中心的集团开发经理 “一切都是自动化的-构建，部署，测试，基础架构和发布。\n具有所需质量门的生产线。更快，可重复，可自定义和可靠的自动化是任何项目成功的关键。混沌工程-在当今混合基础设施世界中非常关键的方面。系统行为和客户体验紧密结合在一起，你越早对其进行测试，就越能为客户提供更好的体验。”\n云原生方法将被标准化 —— Ben Sapp “由于云空间已经真正地发展起来（最近十年左右），并且容器化已成为规范，所以一切都非常标准化，几乎就像大型机时代一样。\n当然，会有趋势和赚钱的机会。但是我不知道下一个大破坏者是什么。现在的一切基本上都与五年前的最佳做法相同，但更加可靠。我想越来越多的人将继续从宠物转移到牛身上，剩下诸如 Ansible 和 puppet 之类的工具仅用于打包程序和云 init 来构建容器主机。\nimo 是软件开发的黄金时代。 DevOps 和云原生方法已经实现了许多目标。管道，托管，存储，负载平衡……这些都在 5 分钟之内解决了。”\n安全将成为重中之重 —— CloudSkiff “从 DevSecOps 角度绝对跟踪基础设施中不受控制的变化。作为代码的基础架构很棒，但是有太多可移动的部分：代码库，状态文件，实际云状态。事情倾向于漂移。这些更改可能有多种原因：从开发人员通过 Web 控制台创建或更新基础架构而不告知任何人，到云提供商方面不受控制的更新。处理基础架构漂移与代码库之间的挑战可能会充满挑战。” - CloudSkiff\nChaos Engineering 将变得越来越重要 —— International Technology Ventures 的 CTO “在更多组织中的 DevOps 规划讨论中，混沌工程将变得越来越重要（且更常见）。大多数组织通常不执行混沌工程学（Chaos Engineering），即在生产中对软件系统进行实验以建立对系统抵御动荡和意外情况能力的信心。\n如果我们在传统的五个成熟度模型框架内考虑 DevOps，那么 Chaos Engineering 将是第 4 或第 5 级学科，将包含在 DevOps 实践的保护范围内。正如将单独的测试/质量保证小组的传统角色纳入 DevOops 的学科一样，Chaos Engineering 也应如此。”\n更关注即时日志以快速验证成功或失败 —— ADESA 平台稳定性总监 “在后期部署中使用日志来验证发布是否成功，或存在严重错误。人们需要建立的最大联系是定义手动流程，然后实现自动化的巨大飞跃。一键部署，即时日志可快速验证成功或失败，然后触发回滚。随之而来的是复杂性以及跨服务依赖性，是否可以回滚某些内容，或者是否需要对其他服务进行进一步测试。想象一下 100 种微服务（又称管道，甚至还有 100 个容器）。\n作为一项，我总是庆祝成功的回滚，因为它不会对服务产生影响，而且是成功的。” -ADESA平台稳定性总监Craig Schultz\nDevSecOps 将成为 DevOps 的默认部分 —— JFrog 的 DevOps 架构师 DevSecOps 的 “Sec” 部分将越来越成为软件开发生命周期中不可或缺的一部分，真正的安全性 “向左移动” 方法将成为新的规范，CI/CD 管道中的专用安全性步骤将更少从开发人员的 IDE 到依赖关系和静态代码分析，安全和自动识别和采取措施将是所有流程步骤的一部分。在没有适当（自动？）解决这些问题的情况下，不会发布软件组件。真正的安全问题是免费软件。”\n希望你喜欢我们对 DevOps 趋势的专家综述，并在 2021 年关注。如果你认为这里缺少应考虑的内容，请在评论中分享你的观点。\n原文 15 DevOps Trends to Expect in 2021 的翻译。\n","date":"2021-01-21","externalUrl":null,"permalink":"/posts/devops-trends-2021/","section":"Posts","summary":"本文介绍了 2021 年 DevOps 领域的主要趋势，包括微服务架构、无服务器计算、Kubernetes 的普及以及 DevSecOps 的兴起。","title":"预测 2021 年的 DevOps 趋势","type":"posts"},{"content":" 这是一篇记录 result 与 currentResult 区别的笔记。\n1. Declarative Pipeline 示例 # 以下是来自 JENKINS-46325 的测试代码：\npipeline { agent any stages { stage (\u0026#39;Init\u0026#39;) { steps { echo \u0026#34;Init result: ${currentBuild.result}\u0026#34; echo \u0026#34;Init currentResult: ${currentBuild.currentResult}\u0026#34; } post { always { echo \u0026#34;Post-Init result: ${currentBuild.result}\u0026#34; echo \u0026#34;Post-Init currentResult: ${currentBuild.currentResult}\u0026#34; } } } stage (\u0026#39;Build\u0026#39;) { steps { echo \u0026#34;During Build result: ${currentBuild.result}\u0026#34; echo \u0026#34;During Build currentResult: ${currentBuild.currentResult}\u0026#34; sh \u0026#39;exit 1\u0026#39; } post { always { echo \u0026#34;Post-Build result: ${currentBuild.result}\u0026#34; echo \u0026#34;Post-Build currentResult: ${currentBuild.currentResult}\u0026#34; } } } } post { always { echo \u0026#34;Pipeline result: ${currentBuild.result}\u0026#34; echo \u0026#34;Pipeline currentResult: ${currentBuild.currentResult}\u0026#34; } } } 运行结果（部分）：\nInit result: null Init currentResult: SUCCESS Post-Init result: null Post-Init currentResult: SUCCESS During Build result: null During Build currentResult: SUCCESS + exit 1 Post-Build result: FAILURE Post-Build currentResult: FAILURE Pipeline result: FAILURE Pipeline currentResult: FAILURE Finished: FAILURE 结论：\ncurrentResult 在流水线运行中会反映当前阶段的执行结果（初始为 SUCCESS，阶段失败时更新为 FAILURE）。 result 在 Declarative Pipeline 中为 null，直到流水线执行结束或阶段显式设置时才有值。 2. Scripted Pipeline 示例 # 以下是来自 CloudBees 支持文档 的例子。\n失败情况 # node { try { sh \u0026#34;exit 1\u0026#34; currentBuild.result = \u0026#39;SUCCESS\u0026#39; } catch (Exception err) { currentBuild.result = \u0026#39;FAILURE\u0026#39; } echo \u0026#34;RESULT: ${currentBuild.result}\u0026#34; } 输出：\n+ exit 1 RESULT: FAILURE Finished: FAILURE 成功情况 # node { try { echo \u0026#34;I\u0026#39;m not going to fail\u0026#34; currentBuild.result = \u0026#39;SUCCESS\u0026#39; } catch (Exception err) { currentBuild.result = \u0026#39;FAILURE\u0026#39; } echo \u0026#34;RESULT: ${currentBuild.result}\u0026#34; } 输出：\nI\u0026#39;m not going to fail RESULT: SUCCESS Finished: SUCCESS 3. 总结 # 属性 类型 何时更新 典型用途 currentResult 只读（String） 始终反映当前构建或阶段的实时状态，默认 SUCCESS 判断当前流水线/阶段执行是否失败 result 可读写（String） 默认 null，可在脚本中显式设置，或构建结束后自动赋值 手动控制或覆盖最终构建结果 简单来说：\ncurrentResult = “当前状态” result = “最终结果（可手动覆盖）” ","date":"2021-01-14","externalUrl":null,"permalink":"/posts/jenkinsresult-vs-currentresult/","section":"Posts","summary":"本文解释了 Jenkins Pipeline 中 \u003ccode\u003eresult\u003c/code\u003e 与 \u003ccode\u003ecurrentResult\u003c/code\u003e 的区别，并通过 Declarative Pipeline 与 Scripted Pipeline 示例展示它们在不同阶段的表现。","title":"Jenkins 中 `result` 与 `currentResult` 的区别","type":"posts"},{"content":"","date":"2021-01-12","externalUrl":null,"permalink":"/tags/openssh/","section":"标签","summary":"","title":"OpenSSH","type":"tags"},{"content":"我管理团队的 Git 仓库已经超过两年，日常主要使用 Bitbucket，这里以 Bitbucket 为例，介绍一些推荐开启的设置。\n1. 禁止强制推送（Reject Force Push） # 这是我最推荐的设置。\n如果不禁止，当有人执行 git push -f 且本地代码比远程旧时，就可能导致提交丢失。\n一旦发生这种情况，只能手动恢复，非常麻烦。我身边听说过三次类似事故。\n建议尽快启用 “Reject Force Push” 钩子。\n2. 防止重要分支被删除（Branch Prevent Deletion） # 对于非常重要的分支，建议开启防止分支被删除功能，避免因误操作丢失关键分支。\n3. 为每次 Hotfix/GA 发布打标签 # 每次 Hotfix 或 GA 版本发布后，建议立即创建标签（tag），方便后续追溯版本。\n4. 合并检查（Merge Check） # 使用 Pull Request 是团队良好的协作流程之一。\n为了避免有人直接合并代码而未经过审核，我们启用了最低 1 次审批的限制。\n这样，任何想要合并到主分支的分支都必须添加其他人作为 Reviewer，并且 Reviewer 必须点击 “Approve”，否则 “Merge” 按钮不可用。\n5. 提交信息检查（Yet Another Commit Checker） # Yet Another Commit Checker (YACC) 是非常强大的插件，可以帮助规范提交信息和分支命名。\n我写过一篇中文文章介绍如何使用它来落地提交规范，有兴趣可以阅读：提交信息规范实践\n✅ 总结\n开启这些设置，可以有效防止误操作、提升代码质量、保障协作流程。 GitHub 上也有类似功能，例如 Branch Protection Rules、Required Reviews、Tagging 等，可根据实际情况启用。 ","date":"2021-01-12","externalUrl":null,"permalink":"/posts/git-repository-settings/","section":"Posts","summary":"列出 Bitbucket 和 GitHub 仓库推荐启用的设置，包括禁止强制推送、分支保护、标签管理、合并检查以及提交信息规范等。","title":"建议在 Bitbucket/GitHub 中启用的仓库设置","type":"posts"},{"content":"最近我们的 Bamboo 服务器在连接 Windows 构建机时出现错误：\nCan not connect to the host XXXX 22 port. 登录到构建机后，通过以下命令检查 22 端口：\nnetstat -aon | findstr \u0026#34;22\u0026#34; 结果并没有发现 22 端口在监听。\n1. 开放入站 22 端口 # 网上有很多关于在 Windows 上开放端口的文章，例如： How to open a port in Windows Firewall\n2. 仍然无法监听的原因 # 我在防火墙中开放了 22 端口，但执行上面的命令依然看不到 22 端口处于监听状态。\n原因是：系统中没有运行 SSH 服务。\n3. 解决方法：安装 Win32-OpenSSH # 我通过安装 Win32-OpenSSH 解决了问题。\n安装完成后会启动两个服务（SSH Server 和 SSH Agent），此时端口 22 就会处于监听状态。\n安装步骤可参考官方 Wiki： Install Win32-OpenSSH\n✅ 总结\n防火墙开放端口只是第一步，还需要确保有进程在监听该端口。 对于 SSH，必须运行 sshd 服务才能让 22 端口真正处于监听状态。 ","date":"2021-01-12","externalUrl":null,"permalink":"/posts/how-to-open-port-22/","section":"Posts","summary":"本文介绍如何在 Windows 上开启 22 端口并确保其处于监听状态，以便支持 SSH 连接。内容包括安装 OpenSSH 以及防火墙配置步骤。","title":"在 Windows 上开启 22 端口并让其处于监听状态","type":"posts"},{"content":"本文是我在自己环境中测试可行的笔记，并未在更多场景中验证。\n启用 sparse-checkout # 有时在 Windows 平台上克隆仓库会遇到某些文件夹问题，可以用 sparse-checkout 作为一种只检出部分目录的解决方案。\n情况 1：尚未克隆仓库时 # mkdir git-src cd git-src git init git config core.sparseCheckout true echo \u0026#34;/assets/\u0026#34; \u0026gt;\u0026gt; .git/info/sparse-checkout git remote add origin git@github.com:shenxianpeng/shenxianpeng.git git fetch git checkout master 情况 2：已克隆仓库时 # cd git-src git config core.sparseCheckout true echo \u0026#34;/assets/\u0026#34; \u0026gt;\u0026gt; .git/info/sparse-checkout rm -rf \u0026lt;不需要的文件或目录\u0026gt; git checkout 禁用 sparse-checkout # 如果需要恢复到完整检出状态，可以执行：\ngit config core.sparseCheckout false git read-tree --empty git reset --hard ✅ 提示\nsparse-checkout 适合只需要仓库部分内容的场景，例如减少下载量或规避平台限制。 Git 2.25+ 提供了 git sparse-checkout 子命令，可以更方便地管理此功能。 ","date":"2021-01-11","externalUrl":null,"permalink":"/posts/git-sparse-checkout/","section":"Posts","summary":"介绍如何启用与禁用 Git sparse-checkout，包括配置只检出指定目录的示例，以及如何恢复到完整检出状态。","title":"启用与禁用 Git sparse-checkout","type":"posts"},{"content":"","date":"2021-01-06","externalUrl":null,"permalink":"/tags/codesign/","section":"标签","summary":"","title":"CodeSign","type":"tags"},{"content":"","date":"2021-01-06","externalUrl":null,"permalink":"/tags/timestamp/","section":"标签","summary":"","title":"Timestamp","type":"tags"},{"content":"相信许多程序员在新年开始在做 code sign (数字签名)的时候可能遇到 Verisign Timestamp 服务器不好用了 http://timestamp.verisign.com/scripts/timstamp.dll 的情况。出现了如下错误：\n原因是 code sign 默认的时间戳服务器无法访问了。\n在 Stack overflow 这个 post 里上面有人给出了答案，是来自于 Verisign Support 的回复：\n他们的身份验证服务已出售给赛门铁克(Symante)，现在的服务商是 Digicert。该服务器已弃用了。\n他们建议联系 Digicert 或在网络上找免费的 timestamp servers\n以上是别人的回复，我在网络上没有找到一个官方回复，因此打算发邮件正式确认一下，发完不一会就得到了回复：\n和上面的回复类似：几年前，Verisign 的身份验证和证书业务被出售给赛门铁克(Symantec)，目前已过渡到Digicert。您将需要与当前供应商合作以获得支持或更新的时间戳URL。请访问 http://www.digicert.com 了解更多信息。\n好了，这下实锤了，放心大胆的开始动手修改到新的时间戳了。\n我找到了 Digicert 的时间戳服务器是 http://timestamp.digicert.com。更换到新的时间戳服务器后，数字签名恢复正常。\n除了上面 Digicert 那个网址，还有如下网址可以作为替换:\nhttp://timestamp.comodoca.com/authenticode http://timestamp.globalsign.com/scripts/timestamp.dll http://tsa.starfieldtech.com 但我都没有选用，我还是选择了官方的时间戳服务，留作备用吧。一旦又抽风 “官方” 哪天又被卖了呢？\n","date":"2021-01-06","externalUrl":null,"permalink":"/posts/verisign-server-not-working/","section":"Posts","summary":"本文介绍了如何解决 Verisign 时间戳服务器不可用的问题，提供了替代的时间戳服务器地址，帮助开发者顺利完成代码签名。","title":"解决 Code Sign 默认时间戳服务器 http://timestamp.verisign.com/scripts/timstamp.dll 不可用","type":"posts"},{"content":" 前言 # 自 2020 年因疫情开始，越来越多的 IT 公司都因不得不在家办公从而彻底转为 WFH（Work From Home） 公司，因此对于 IT 从业者来说，工作机会今后将会是全球性的。\n如果你有意想进入一个跨国公司工作，想与世界各地的人在一起工作，那么就不能仅仅的关注国内的这些大厂，要将眼光放眼到全世界，看看这些耳熟能详的公司对于工程师的职位要求有哪些。\n今天就先来看看 DevOps 岗位的需求是什么样的，了解这些，一来可以帮助我们在2021 年树立学习方向，而来如果你有意向去这些公司，了解并提早做准备才能有机会获取你想要的岗位。\n由于这些职位的介绍和要求会很长，因此我就先说结论。\n主要技能 # 国外很多公司他们使用的云服务商主要是 AWS，因此熟悉和使用 AWS 熟练使用 DevOps 工具，如 Jenkins, Ansible（Chef）, Git 等 Docker 和 Kubernetes 是每个想从事 DevOps 需要掌握的 熟悉操作系统，至少要 Linux 大多数都是要求 Python 很熟练，高级一些的岗位会要求熟悉 Go, Java 语言 最后，乐于学习，积极主动，具有创造性思维是每个 DevOps 最重要的特质，因此新的技术和工具层出不穷，我们需要保持和新进同行 具体的职位要求细节，请看后面的职位介绍吧 \u0026hellip;\nZOOM：DevOps Engineer # 工作地点：San Jose, CA\n岗位链接 https://www.linkedin.com/jobs/view/2337488435\nZOOM 不用过多介绍了，2020 年因为疫情，业务极具增长的一家视频会议的公司。也多次被推荐为最佳雇主，以及最佳的工作场所。\n岗位职责\n设计、部署、监控 ZOOM 平台服务 提升 ZOOM 平台服务从规划到上线的全生命周期策略 与跨职能干系人紧密合作，分析和解决复杂的生产问题 构建弹性和可扩展的服务基础设施，以适应基于区域的数据中心 优化当前CI/CD流程，简化服务器配置和部署的自动化工作 支持测试自动化和部署策略，以优化服务性能，确保产品质量 职位要求\n本科/硕士(CS或相关专业优先) 至少 5 年 DevOps 或 SRE 经验 对 AWS 基础架构(如DynamoDB, S3, Nginx, CloudWatch)，Linux 批处理命令，ELK 堆栈和容器编排(如 K8s, Docker)有深入的了解 熟练使用 Jenkins, Ansible 和 Git 仓库 能够监控，调试和自动化日常任务 熟悉云基础设施技术和基于云的测试自动化解决方案 乐于学习，积极主动，具有创造性思维 苹果：DevOps Engineer (CI/CD) # 地点：Cupertino, CA\n岗位链接：https://www.linkedin.com/jobs/view/2346233023/?eBP=JOB_SEARCH_ORGANIC\u0026amp;recommendedFlavor=COMPANY_RECRUIT\u0026amp;refId=RA%2BGKQ6QNrbwK1PHj3opBQ%3D%3D\u0026amp;trackingId=%2FCHeWxW%2FHm%2BULb2XD5Yyyg%3D%3D\u0026amp;trk=flagship3_search_srp_jobs\n我们的团队为苹果的应用程序运行 CI/C D管道，支持全球成千上万的开发者。我们对不断改进软件开发生命周期的方式充满热情，并为大规模工程问题重新发明前沿解决方案开辟边界。作为团队的一员，你将开发应用程序和微服务来构建和改进我们的下一代 CI/CD 管道。\n职位要求\n精通 Python 编程 有 Unix/Linux 平台工作经验 熟练使用 DevOps 工具，如 Chef, Docker, Kubernetes 具有软件开发过程的经验，如构建、单元测试、代码分析、发布过程和代码覆盖 有 CI/CD 流程和平台经验，如 Jenkins 较强的分析和解决问题的能力 优秀的书面和口头沟通能力，能够与大型开发团队合作 工作内容\n开发和维护应用开发团队的 CI/CD 流程 跨团队合作，改进产品的构建、集成和发布流程 开发和维护应用服务 CI/CD 管道的服务和集成 维护和管理包含 Linux/Unix/macOS 系统的动态构建场 能够参与下班后的轮班工作 教育和经验\n计算机科学或同等学历 其他要求\n有使用 Django/Flask 开发基于 Python 的微服务的经验 熟悉GitHub开发流程 有 Jenkins 管理和扩展的经验 具备扩展 CI/CD 系统和微服务的经验 有 Xcode 和开发 iOS, macOS 和其他苹果平台应用的经验 Oracle：Software Developer 4 for DevOps # 工作地点：Pleasanton, CA 职位链接：https://www.linkedin.com/jobs/view/2351849053/?eBP=JOB_SEARCH_ORGANIC\u0026amp;recommendedFlavor=COMPANY_RECRUIT\u0026amp;refId=RA%2BGKQ6QNrbwK1PHj3opBQ%3D%3D\u0026amp;trackingId=XQXcRFZiVeqSGJApqI4Grw%3D%3D\u0026amp;trk=flagship3_search_srp_jobs\n作为 Oracle Analytics Cloud 团队的开发人员，你将有机会在广泛分布的多租户云环境中构建和运行一套大规模集成云服务。你将需要在分布式系统方面有丰富的经验，熟练地解决大型数据系统中的实际挑战，对如何构建具有弹性，高可用性，可扩展性的解决方案有深刻的了解，并且需要具有可靠的设计，开发和交付回溯能力，终端系统。\n职位要求\n计算机科学，计算机工程学士学位或更高学位，或具有8年以上应用经验的同等学历 关于 Oracle Cloud Infrastructure 的动手经验–用户/策略管理，创建资源，配置和部署软件，自动化端到端到端供应 精通 Python 编程技巧 精通关系数据库，擅长 SQL 有使用容器技术的经验，尤其是 Docker 在 Linux 环境中的经验 有配置 Git 等源代码系统的经验 成为 CI/CD 和开发最佳实践的拥护者，尤其是在自动化和测试方面 具有构建高性能，弹性，可扩展性和精心设计的系统的经验 良好的沟通能力，能够清楚地阐明工程设计 敏捷软件开发经验 所需技能：\n熟悉 Kubernetes，Mesos 等（编排） 身份管理，安全性，网络等 NVIDIA DevOps Engineer # 工作地点：Santa Clara, CA 职位链接：https://www.linkedin.com/jobs/view/2249840303/?alternateChannel=search\u0026amp;refId=Y17t502dpy%2FAU43SIQPRIA%3D%3D\n工作内容\n为多个内部服务开发和支持暂存和生产环境 与全球各地的各个团队进行协调，以促进和改进 CI/CD 的实践 使用内部 Kubernetes 和商业云帮助构建服务 你将在团队中灵活地做出技术决策 岗位要求\n计算机工程，计算机科学或相关技术学科的理学学士学位或同等工作经验 6年以上经验 出色的脚本语言编程和调试技能 成熟的 Linux 系统管理经验（强烈建议使用 CentOS 和 Ubuntu） 对 CI/CD管道和工具有很好的了解（GitLab 或类似的工具） 数据库管理和性能调优经验 具有容器（Docker，Kubernetes）Web服务（SOAP/REST）和可扩展存储（HDFS/Ceph）的经验 从人群中脱颖而出的方法\n你具有 Python 编程方面的专业知识 熟悉 Windows 系统管理是一个巨大的优势 你曾经使用过云服务（AWS，Azure等） Cisco CX Cloud - Senior DevOps Release Engineer # 工作地点：San Jose, CA 职位链接：https://www.linkedin.com/jobs/view/2310027987/?alternateChannel=search\u0026amp;refId=COCpYBYNZm9w483i3SERXg%3D%3D\u0026amp;trackingId=FpZJedsrgIhNxlm961X75Q%3D%3D\n工作范围\n你可以跨 CX E\u0026amp;PI 和更广泛的客户体验（CX）组织进行协作，以启用我们的DevOps Release转换功能 你将以思想和实践的开发人员的身份开展工作，他不仅会认识并建立我们的组织实力，还将为团队带来新的观点和想法 与多元化的包容性软件工程师团队合作，在发布过程中实现端到端的DevOps，以确保加速的吞吐量和系统可靠性 与安全，应用程序和基础架构团队合作，从开发，测试，阶段，预生产和生产环境中检测简化的变更生命周期 运用你的经验将基础架构实现为代码，转换发布管道，并在高性能 DevOps 管道上以 NoOps 的心态部署到生产中 促进纪律严明的方法以确保部署的可预测性和质量 基准化和优化关键运营指标，确保我们符合运营SLA 测试并控制安全性，可靠性，可伸缩性和性能标准 积极寻求持续改进和学习的机会 岗位要求\n至少 10 年以上的设计，架构，启用和执行 DevOps 管道 以诚信，信任和透明的态度进行道德领导 具有在面向敏捷 DevOps 的团队和文化中担任高级主管的经验，并使用现代框架，技术，将基础架构用作代码工具的 DevOps 实践 将自动进行配置管理（使用Ansible等工具） 精通 DevOps，Blue/Green 和 Canary 部署以及云工程中的最佳实践 已经证明了微服务，作为代码的基础架构，监视和日志记录方面的专业知识 将编写代码，列出在 AWS 上运行的框架和架构 与跨职能团队合作时，你在复杂产品体系结构的持续集成和连续部署方面拥有深厚的专业知识 具有出色的能力，可以为支持全球客户群的云原生 SaaS 应用程序实现高可用性，灾难恢复，监视和警报，自动化以及持续的高性能 具有 Terraform 和 CloudFormation 模板的专业知识 在代码管道解决方案方面拥有深厚的专业知识 使用 Docker 和 Kubernetes 管理容器 使用 Jenkins 等平台管理构建管道 具有出色的组织和人际关系技巧，可以促进协作；在多功能矩阵管理环境中可以很好地工作 应该能够为 AWS 编写 Terraform 模块 已经展示了领域的专业知识，可以使用 SaaS 或消费者云软件公司的 DevOps 团队使用AWS之类的技术执行发布管道转换 在整合复杂的，跨公司的流程和信息策略方面拥有丰富的经验，包括技术规划和执行以及策略制定和维护 对临时性工作负载具有丰富的经验 体验 Kubernetes 有 Atlantis 知识者优先+ 在 DevOps 执行或工程职位上有10年以上的经验 具有丰富的经验，可在云体系结构上实现发布管道，并符合代码和临时工作负载 将基础架构作为代码和将配置作为代码技术的丰富经验 使用 Terraform，Kubernetes 和 Docker 等工具构建，扩展和保护应用程序云基础架构的丰富经验 在 GitHub 中管理多个代码库的丰富经验 在大型公共云中构建 Cloud Native 应用程序的丰富经验 具有实施可观察性，应用程序监视和日志聚合解决方案的经验 与跨职能团队合作并指导跨部门团队提供激情和经验，以提供受 DevOps 启发的解决方案 为大型数据管道开发高度可扩展的云原生架构 将异构构建，测试，部署和发布活动转换为在AWS上运行的同类企业级 DevOps 实施 交付和创作持续集成构建和部署自动化方面的丰富经验，例如 CI/CD 管道对 CloudFormation 模板，Ansible 和类似框架等解决方案的丰富经验 与应用程序和基础架构部署相关的 AWS 平台的丰富经验 能够在 Python，Go和 Java 中流畅开发 ","date":"2021-01-03","externalUrl":null,"permalink":"/posts/2021-devops-job-requirement/","section":"Posts","summary":"了解国外 IT 公司对 DevOps 工程师的技能要求，帮助你在 2021 年树立学习方向，获取理想岗位。","title":"2021 年国外 IT 公司对于 DevOps 工程师的要求有哪些？","type":"posts"},{"content":"时间飞快，马上就迎来了 2021 年，又到了总结自己这一年来的所有的经历和收获的时候了。\n2020 是非比寻常的一年，对于每个人不论是生活还是工作，都因为新冠病毒被影响着。\n但生活总归要继续。刷微博、抖音各种社交媒体是一天；工作学习也是一天。我是那种一段时间感觉没进步就特别恐慌的人，做完后者总会让我感觉更安心一些。\n回顾 2020 # 关于工作 # 对于 2020 年，我一直努力想要去做到最好，年尾回头看看，还算满意吧。\n在年底写下工作工作总结的时候，发现自己这一年确实做了很多工作。除了做好构建和发布的本职工作之外，做了很多改进流程和提高效率工作，捡一些重要的来说：\n通过 Jenkins Shared Libraries 和 Multi-branch Pipeline 完成了无人值守的自动化构建，为团队和我节省了大量的时间；同时也通过验证 PR 构建和测试，提高签入代码的稳定和质量。\n输出文章 每个 Jenkins 用户都应该知道这三个最佳实践\n推动团队迁移到企业级的 Artifactory 来下载和存储构建，从而改进 CI/CD 的效率和无人值守的能力。为更好的自动化下载、安装、测试等提供方便。\n输出文章 写给那些想使用 JFrog Artifactory 管理制品的人\n使用 Python 来做了很多关于 Jira, BitBucket 的集成，通过自动化提高效率。另外还发布了一个 Python 项目 UOPY。\n输出文章 在 GitHub 上发布一个 Python 项目需要注意哪些\n落地了 Git 提交信息和分支创建规范，统一了 Git 提交信息和分支创建规范。\n输出文章 程序员自我修养之Git提交信息和分支创建规范\n学习和使用了一些新工具，比如 Ansible playbook、ELK、JaCoCo 等并应用到项目中。\n输出文章 初识 Ansible，JaCoCo 实践\n作为 DevOps/软件工程师还需要有良好的表达能力，否则你做的东西再好，但无法很清晰的跟同事及领导分享出来也是事倍功半。尤其像我在外企，还需要用英文去做分享。\n另外，我想通过坚持阅读和实践，让自己在技术上有比较大的提高。有一段时间我坚持的比较好，完成了好几本技术书籍的阅读，但好的习惯就怕被打破，一旦被打破就很难坚持了。之前一般都是中午午休的时间会阅读半个小时以上的技术书，后来常常因为中午要去新房盯装修，加上每天要做的事情很多（工作、学英语、阅读、装修）的时候，很多时候往往是事情做了，但效果并没有达到预期。\n分享上 # 2020 年博客上一共更新了 41 篇文章，在公众号『DevOps攻城狮』上面一共更新了 26 篇文章。\n这个数量和质量跟一些其他的技术公众号真的没法比，但对于我个人：一个不以写作为生、只写原创文章的攻城狮，只要能不间断的输出，达到这个数量我已经比较满意了。\n而且在年初有一个清华大学的编辑老师找到我，想让我出一本书，这真是让我受宠若惊。我知道自己几斤几两，但能够受到邀请，并寄给我合同，就已经是对我写作的最大鼓励了。\n当时打算写一本《Jenkins 2实践指南》，如果签了合同，我就需要投入至少一年的业余时间死磕自己才有可能完成。\n最后考虑到自己当时的状况：优先做好工作；其次还有比Jenkins更重要的技术我需要去学习；另外周末要装修等一系列的事情\u0026hellip;我最终放弃了这个机会。\n我知道自己还处在输入比输出更加迫切的阶段，我想只要心中的小火苗还在，但行好事，莫问前程。\n工作 flag # 对于个人年终总结，不仅可以回顾自己过去的一年做了哪些，哪些没做好，有什么可以改进的，还能通过回顾和总结为新年立一个 flag。\n2021 我希望自己能更加专注，利用好时间，更加有效的工作和学习。\n除了做好工作，最重要的是提高英语口语和深入学习技术，其次才是输出。\n光有 flag 不行，还要落到具体的行动上。\n提高英语口语 # 🚩 2021 继续坚持至少每天半小时口语打卡练习；另外每周一次 50 分钟的与外教的口语交流。希望能够脱稿进行英文分享，同时能通过英语托业考试。\n深入学习技术 # 我现在学习技术的方式有这么几种：直接去官网阅读英文文档；如果读完还是没搞明白，就去 Linkedin Learning 或是 Udemy 上去搜相应的技术，不但学习了技术，同时也练习了英文听力；如果需要深入系统的去学习一门编程语言以及一些底层技术，会去读一些经典技术书籍。\n🚩 2021 年坚持每天也至少半个小时来阅读技术，一个月至少读完一本技术书籍并且有输出。\n生活 flag # 生活上希望家人和朋友都能身体健康，生活幸福。\n🚩 财务稳定，最好能增长；自己能够健康生活，减重 10 斤。\n最后，不时的来回顾自己 2021 年的 flag，时不时的看看自己是否偏离了最初的方向。2021 我来了！\n过去的年终总结 # 2019 年终总结 2018 从测试到开发的五个月\n","date":"2020-12-31","externalUrl":null,"permalink":"/misc/2020-summary/","section":"Miscs","summary":"\u003cp\u003e时间飞快，马上就迎来了 2021 年，又到了总结自己这一年来的所有的经历和收获的时候了。\u003c/p\u003e","title":"2020 年终总结","type":"misc"},{"content":"","date":"2020-11-24","externalUrl":null,"permalink":"/tags/backup/","section":"标签","summary":"","title":"Backup","type":"tags"},{"content":"大多数人可能已经在使用 Jenkins Configuration as Code 的理念，把构建/测试/发布流程写成代码。\n虽然这样很好，但并不是所有配置都在代码中，部分 Jenkins 系统配置是存储在 Jenkins 服务本地的，因此仍然需要定期备份，以防灾难发生。\n备份 Jenkins 有两种方式：\n使用插件 编写 Shell 脚本 方法一：使用插件备份 # 我使用的是 ThinBackup 插件，以下是我的配置示例：\n备份到 jenkins 用户有写权限的文件夹（非常重要）\n之前我将 Jenkins 备份到挂载目录，结果失败。后来切换到 jenkins 用户登录发现该目录无法访问，而我的个人用户可以，所以问题出在权限上。\n每天备份（周一到周六）\n最多保留 3 份备份（每份备份超过 400MB）\n其他勾选项：\n备份构建结果 备份 userContent 文件夹 备份下一个构建号文件 备份插件包 将旧备份压缩为 ZIP 方法二：使用 Shell 脚本备份 # 推荐参考以下资源：\nGitHub 仓库：sue445/jenkins-backup-script Gist 示例：abayer/jenkins-backup 脚本备份方式适合需要更多定制化的场景，比如结合 cron 定时任务或云存储同步。\n💡 建议\n无论使用哪种方法，都应定期验证备份可用性，确保在需要恢复时能快速上线。\n","date":"2020-11-24","externalUrl":null,"permalink":"/posts/jenkins-backup/","section":"Posts","summary":"介绍如何使用 ThinBackup 插件或 Shell 脚本备份 Jenkins，确保 Jenkins 配置和构建数据安全存储。","title":"备份 Jenkins 的方法","type":"posts"},{"content":"想要对 Java 项目进行代码覆盖率的测试，很容易就找到 JaCoCo 这个开源代码覆盖率分析工具是众多工具中最后欢迎的哪一个。\n本篇仅仅是在学习 JaCoCo 时对其实现设计文档 https://www.jacoco.org/trunk/doc/implementation.html 的粗略翻译。\n实现设计(Implementation Design) # 这是实现设计决策的一个无序列表，每个主题都试图遵循这样的结构:\n问题陈述 建议的解决方案 选择和讨论 覆盖率分析机制(Coverage Analysis Mechanism) # 覆盖率信息必须在运行时收集。为此，JaCoCo 创建原始类定义的插装版本，插装过程发生在加载类期间使用一个叫做 Java agents 动态地完成。\n有几种收集覆盖率信息的不同方法。每种方法都有不同的实现技术。下面的图表给出了 JaCoCo 使用的技术的概述:\n字节码插装非常快，可以用纯 Java 实现，并且可以与每个 Java VM 一起工作。可以将带有 Java 代理钩子的动态插装添加到 JVM 中，而无需对目标应用程序进行任何修改。\nJava 代理钩子至少需要 1.5 个 JVMs。用调试信息(行号)编译的类文件允许突出显示源代码。不幸的是，一些 Java 语言结构被编译成字节代码，从而产生意外的突出显示结果，特别是在使用隐式生成的代码时（如缺省构造函数或 finally 语句的控制结构）。\n覆盖 Agent 隔离(Coverage Agent Isolation) # Java 代理由应用程序类装入器装入，因此代理的类与应用程序类生活在相同的名称空间中，这可能会导致冲突，特别是与第三方库 ASM。因此，JoCoCo 构建将所有代理类移动到一个唯一的包中。\nJaCoCo 构建将包含在 jacocoagent.jar 中的所有类重命名为具有 org.jacoco.agent.rt_\u0026lt;randomid\u0026gt; 前缀，包括所需的 ASM 库类。标识符是从一个随机数创建的，由于代理不提供任何 API，因此没有人会受到此重命名的影响，这个技巧还允许使用 JaCoCo 验证 JaCoCo 测试。\n最低的Java版本(Minimal Java Version) # JaCoCo 需要 Java 1.5 及以上版本。\nJava 1.5 VMs 提供了用于动态插装的 Java 代理机制。使用 Java 1.5 语言级别进行编码和测试比使用旧版本更有效、更少出错——而且更有趣。JaCoCo 仍然允许运行针对这些编译的 Java 代码。\n字节码操纵(Byte Code Manipulation) # 插装需要修改和生成 Java 字节码的机制。JaCoCo 在内部使用 ASM 库来实现这个目的。\n实现 Java 字节码规范将是一项广泛且容易出错的任务。因此，应该使用现有的库。ASM库是轻量级的，易于使用，在内存和 CPU 使用方面非常高效，它被积极地维护并包含为一个巨大的回归测试套件，它的简化 BSD 许可证得到了 Eclipse 基金会的批准，可以与 EPL 产品一起使用。\nJava类的身份(Java Class Identity) # 在运行时加载的每个类都需要一个唯一的标识来关联覆盖率数据，JaCoCo 通过原始类定义的 CRC64 哈希代码创建这样的标识。\n在多类加载器环境中，类的纯名称不能明确地标识类。例如，OSGi 允许在相同的虚拟机中加载相同类的不同版本。在复杂的部署场景中，测试目标的实际版本可能与当前开发版本不同。代码覆盖率报告应该保证所呈现的数字是从有效的测试目标中提取出来的。类定义的散列代码允许区分类和类的版本。CRC64 哈希计算简单而快速，结果得到一个小的64位标识符。\n类加载器可能加载相同的类定义，这将导致 Java 运行时系统产生不同的类。对于覆盖率分析来说，这种区别应该是不相关的。类定义可能会被其他基于插装的技术(例如 AspectJ)改变。在这种情况下，哈希码将改变，标识将丢失。另一方面，基于被改变的类的代码覆盖率分析将会产生意想不到的结果。CRC64 代码可能会产生所谓的冲突，即为两个不同的类创建相同的哈希代码。尽管 CRC64 在密码学上并不强，而且很容易计算碰撞示例，但对于常规类文件，碰撞概率非常低。\n覆盖运行时依赖(Coverage Runtime Dependency) # 插装代码通常依赖于负责收集和存储执行数据的覆盖运行时。JaCoCo 只在生成的插装代码中使用 JRE 类型。\n在使用自己的类加载机制的框架中，使运行时库对所有插装类可用可能是一项痛苦或不可能完成的任务。自 Java 1.6 java.lang.instrument.Instrumentation。插装有一个扩展引导带加载器的API。因为我们的最低目标是 Java 1.5，所以 JaCoCo 只通过官方的 JRE API 类型来解耦插装类和覆盖运行时。插装的类通过 Object.equals(Object) 方法与运行时通信。插装类可以使用以下代码检索其探测数组实例。注意，只使用 JRE APIs:\nObject access = ... // Retrieve instance Object[] args = new Object[3]; args[0] = Long.valueOf(8060044182221863588); // class id args[1] = \u0026#34;com/example/MyClass\u0026#34;; // class name args[2] = Integer.valueOf(24); // probe count access.equals(args); boolean[] probes = (boolean[]) args[0]; 最棘手的部分发生在第 1 行，上面的代码片段中没有显示必须获得通过 equals() 方法提供对覆盖运行时访问的对象实例。到目前为止，已经实施和测试了不同的方法:\nSystemPropertiesRuntime: 这种方法将对象实例存储在系统属性下。这个解决方案打破了系统属性必须只包含 java.lang.String 的约定。字符串值，因此会在依赖于此定义的应用程序(如Ant)中造成麻烦。 LoggerRuntime: 这里我们使用共享的 java.util.logging.Logger。并通过日志参数数组而不是 equals() 方法进行通信。覆盖运行时注册一个自定义处理程序来接收参数数组。这种方法可能会破坏安装自己日志管理器的环境(例如Glassfish)。 ModifiedSystemClassRuntime: 这种方法通过插装将公共静态字段添加到现有的 JRE 类中。与上面的其他方法不同，此方法仅适用于活动 Java 代理的环境。 InjectedClassRuntime：这个方法使用 Java 9 中引入的 java.lang.invoke.MethodHandles.Lookup.defineClass 定义了一个新类。 从 0.8.3 版本开始，在 JRE 9 或更高版本上运行时，JaCoCo Java 代理实现使用 InjectedClassRuntime 在引导类装入器中定义新类，否则使用ModifiedSystemClassRuntime 向现有 JRE 类添加字段。从版本 0.8.0 开始，字段被添加到类 java.lang.UnknownError 中。version 0.5.0 - 0.7.9 向类 java.util.UUID 中添加了字段，与其他代理发生冲突的可能性较大。\n内存使用(Memory Usage) # 对于具有数千类或数十万行代码的大型项目，覆盖率分析应该是可能的。为了允许合理的内存使用，覆盖率分析是基于流模式和“深度优先”遍历的。\n一个庞大的覆盖率报告的完整数据树太大了，无法适合合理的堆内存配置。因此，覆盖率分析和报告生成被实现为“深度优先”遍历。也就是说，在任何时间点，工作记忆中只需要保存以下数据:\n当前正在处理的单个类。 这个类的所有父类(包、组)的汇总信息。 Java元素标识符(Java Element Identifiers) # Java 语言和 Java VM 对Java 元素使用不同的字符串表示格式。例如，Java 中的类型引用读起来像 java.lang.Object。对象，VM 引用的类型与 Ljava/lang/Object 相同。JaCoCo API 仅基于VM标识符。\n直接使用 VM 标识符不会在运行时造成任何转换开销。有几种基于 Java VM 的编程语言可能使用不同的符号。因此，特定的转换应该只在用户界面级别发生，例如在报表生成期间。\nJaCoCo实现的模块化(Modularization of the JaCoCo implementation) # JaCoCo 是在提供不同功能的几个模块中实现的。这些模块是作为带有适当清单文件的 OSGi 包提供的。但是它不依赖于 OSGi 本身。\n使用 OSGi bundle 允许在开发时和运行时在 OSGi 容器中定义良好的依赖关系。由于对 OSGi 没有依赖关系，捆绑包也可以像普通的 JAR 文件一样使用。\n","date":"2020-11-17","externalUrl":null,"permalink":"/posts/jacoco/","section":"Posts","summary":"介绍 JaCoCo 的实现设计，包括覆盖率分析机制、Java 版本要求、字节码操纵、内存使用等方面的内容。","title":"JaCoCo 实现原理 (JaCoCo Implementation Design)","type":"posts"},{"content":"最近在思考如何将团队里的所有的虚拟机都很好的管理并监控起来，但是由于我们的虚拟机的操作系统繁多，包括 Windows, Linux, AIX, HP-UX, Solaris SPARC 和 Solaris x86. 到底选择哪种方式来管理比较好呢？这需要结合具体场景来考虑。\nAnsible 和其他工具的对比 # 这里有一个关于 Chef，Puppet，Ansible 和 Saltstack 的对比文章\nhttps://www.edureka.co/blog/chef-vs-puppet-vs-ansible-vs-saltstack/\n选择合适的工具 # 仅管理 Windows 和 Linux # 如果你的虚拟机没有这么多平台，只是 Windows 和 Linux，假如你已经有了 VMware vSphere 来管理了，那么可以通过 VMware vSphere API 来查看这些机器的状态。\n这里是 VMware 官方的 API Library 供使用：\nVMware vSphere API Python Bindings Go library for the VMware vSphere API 管理多个操作系统 # 如果你和我的情况一下，想监控很多个操作操作系统，那么就只能通过 ssh 来登录到每一台机器上去查看，比如执行 uptime 等命令。可以写 shell 脚本来完成这些登录、检测等操作。\n另外就是使用 Ansible 的 Playbook。Playbook 里描述了你要做的操作，这是一个权衡，学习 Ansible 的 Playbook 需要花些时间的。\n如果想了解下 Ansible 那么可以试试 Ansible Playbook。以下是我使用 Ansible 做了一些练习。\nPlaybook结构 # +- vars | +- vars.yml | +- ... +- hosts # save all hosts you want to monitor +- run.yml # ansible executable file Playbook具体代码 # vars/vars.yml\n--- # system ip: \u0026#34;{{ ansible_default_ipv4[\u0026#39;address\u0026#39;] }}\u0026#34; host_name: \u0026#34;{{ ansible_hostname }}\u0026#34; os: \u0026#34;{{ ansible_distribution }}\u0026#34; version: \u0026#34;{{ ansible_distribution_version }}\u0026#34; total_mb: \u0026#34;{{ ansible_memtotal_mb }}\u0026#34; vcpus: \u0026#34;{{ ansible_processor_vcpus }}\u0026#34; hosts\n[unix-vm] aix ansible_host=walbld01.dev.company.com ansible_user=test ansible_become_pass=test hp-ux ansible_host=walbld04.dev.company.com ansible_user=test ansible_become_pass=test linux ansible_host=walbld05.dev.company.com ansible_user=test ansible_become_pass=test [win-vm] win-bld02 ansible_host=walbld02.dev.company.com ansible_user=Administrator ansible_password=admin ansible_port=5985 ansible_connection=winrm ansible_winrm_server_cert_validation=ignore [other-vm] solaris ansible_host=walbld07.dev.company.com ansible_user=test ansible_become_pass=test win-udb03 ansible_host=walbld03.dev.company.com ansible_user=administrator ansible_become_pass=admin run.yml\n--- # this playbook is simple test - name: \u0026#34;get unix build machine info\u0026#34; hosts: unix-vm gather_facts: True tasks: - name: get uname, hostname and uptime shell: \u0026#34;uname \u0026amp;\u0026amp; hostname \u0026amp;\u0026amp; uptime\u0026#34; register: output - debug: var=output[\u0026#39;stdout_lines\u0026#39;] - name: \u0026#34;get windows build machine os info\u0026#34; hosts: win-vm gather_facts: True tasks: - debug: var=hostvars[\u0026#39;win-bld02\u0026#39;].ansible_facts.hostname - debug: var=hostvars[\u0026#39;win-bld02\u0026#39;].ansible_distribution 如何执行 # 首先需要安装了 ansible，然后执行\n# run with playbook ansible-playbook -i hosts run.yml 注：上面的代码是脱敏过的，需要根据你的环境进行调整才能执行成功。\nAnsible TroubleShotting # \u0026quot;msg\u0026quot;: \u0026quot;winrm or requests is not installed: No module named winrm\u0026quot;\nNeed install pywinrm on your master server.\n\u0026ldquo;msg\u0026rdquo;: \u0026ldquo;plaintext: auth method plaintext requires a password\u0026rdquo;\nwhen I run ansible mywin -i hosts -m win_ping -vvv, I notice the output used Python2.7, so I install pywinrm with command sudo pip2 install pywinrm, then my problem was resolved.\nmywin | UNREACHABLE! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;plaintext: auth method plaintext requires a password\u0026#34;, \u0026#34;unreachable\u0026#34;: true } Result: You should be using ansible_password and not ansible_pass. link\n","date":"2020-10-28","externalUrl":null,"permalink":"/posts/ansible-practice/","section":"Posts","summary":"通过 Ansible 实践，探索如何高效管理和监控多种操作系统的虚拟机。","title":"Ansible 实践","type":"posts"},{"content":"我在使用 Artifactory 做持续集成已经有一段时间了，对企业级 Artifactory 也有了一些经验和总结，希望能通过本篇的分享帮助刚接触这个工具的人了解什么是Artifactory，它能做什么，为什么要选择它，以及在使用过程中应该注意什么。\n什么是Artifactory # 一句话概括：Artifactory 是一个存放制品（Artifacts）的工具。当前，Artifactory 是一个非常有影响力，功能非常强大的工具。\nArtifactory有哪些优势 # 可能你的团队已经有了自己的管理制品的方式，比如 FTP 等。Artifactory 能带来什么呢？让我先来看看它有哪些优势。\n注：以下优势都是针对 JFrog Aritifacvtory 企业版来介绍的。开源版，即 OSS 版本不具备以下丰富的功能。\n优势1：它是一个通用管理仓库 # JFrog Artifactory 企业版完全支持所有主要包格式的存储库管理器。它不但可以管理二进制文件，也可以对市面上几乎所有语言的包的依赖进行管理，如下图所示\n因此，使用 Artifactory 能够将所有的二进制文件和包存储在一个地方。\n优势2：跟 CI 工具更好的集成 # 它支持所有主流 CI 工具（如下图所示），并在部署期间能捕获详尽的构建环境信息，以实现可完全复制的构建\n另外通过提供的丰富的 REST API，因此 GUI 页面上的任何操作都可以通过代码以编程方式完成，方便实现 CI/CD。\n优势3：提供强大的搜索功能 # 如果你的构建是存储在 FTP 上，想从大量的制品中找到你要找的那一个，如果不知道它的名字，那么真的很难找到它。\nArtifactory 提供了强大的搜索功能，可以通过带有正则表达的名字进行搜索；还可以通过文件的 checksum；以及通过属性（Properties）等方式进行快速搜索，如下示例\n例1：通过名字搜索 # 你想找某一个提交点的构建制品，比如那个提交点的 commit hash 是 a422912，那么你就可以直接输入 *a422912* 回车，就能快速的从众多的制品中找到，例如 Demo_Linux_bin_a422912.zip\n例2：通过属性搜索 # 比如要找属性 release.status 为 released 的所有构建那么就可以这样搜索。\n例3：通过 checksum 搜索 # 如果只知道文件的 checksum，同样也可以进行搜索。例如通过 sha1sum 计算出文件的 checksum\n$ sha1sum test.zip ad62c72fb097fc4aa7723e1fc72b08a6ebcacfd1 *test.zip 优势4：管理制品的生命周期 # 通过定义不同成熟度存储库，然后使用 Artifactory Promote 功能可以将制品移动到不同的成熟度存储库，以及通过元数据属性，更好的管理和维护制品的生命周期。\n除了这些优势之外，Artifactory 还有更多的特点，我就不一一介绍了。\n更多功能可以浏览 JFrog Artifactory 的官方介绍 https://jfrog.com/artifactory/features/\n接下来通过一个 Demo 来介绍 Artifactory 应该怎么使用，以及其中有哪些最佳实践，避免走弯路。\nArtifactory首页介绍 # 页面顶部 # 你可以看到这个 Artifactory 已经服务了超过 5000 件的制品。还可以看到 Artifactory 的当前版本号，以及最新版本。\n页面中部，从左到右 # 最左边是搜索功能，通过丰富的搜索条件可以轻松找到制品。然后是一些用户手册、视频、REST API 文档等信息。\n中间是 Set Me Up，使用它可以选择和筛选你想要操作的存储库，点击特定的存储库可以弹出关于如何使用它的详细说明。\n最右边是显示的是最近部署的构建和最多下载量的制品（95代表的是下载次数）\n页面底部 # 在底部是一些与 Artifactory 集成的相关工具和技术用户文档，方便做集成时快速找到找到最权威的技术资料。\n实践和工作流 # 设置关注的仓库 # 在首页的 Set Me Up 里你也看到了我们有很多仓库（Repository），然而在众多仓库中，大多数成员只对其中一些仓库感兴趣，那么就可以只关注部分仓库。添加喜欢，然后点击喜欢按钮就可以只列出你关注的 Artifact Repository 了。\n仓库权限与保留策略 # 仓库(maturity) 保留策略(Retention) 个人账户(Personal Account) 服务账户(Service Account) 管理员(Admin) dev 通常不清理 read/write read/write all int 一周或是几天 read read/write all stage 永不清理 read read/write all release 永不清理 read read/write all 通过表格很容易了解这个权限的设置和保留策略，这适合大多数的情况，但不一定适合所有企业情况。\nArtifactory仓库命名方法 # 在这个列表仓库中，你可以从这些仓库的名称中看到遵循了某些命名约定，这里遵循了 JFrog Artifactory 推荐的官方命名 方法，强烈建议你也这么做。它是由四部分组成：\n\u0026lt;team\u0026gt;-\u0026lt;technology\u0026gt;-\u0026lt;maturity\u0026gt;-\u0026lt;locator\u0026gt;\n图上的 team 我做了脱敏，我们叫它叫 team1 吧。 然后是技术，这里有很多可选的，比如 generic, Docker, Maven, NPM 等等。我用的 generic，这是由于我们的产品是 C/C++ 编译出来的二进制文件，它属于 generic 类别。 接下来是成熟度（maturity），一个仓库通常由四个级别的成熟度组成，从低到高这里分别是 dev, int, stage 和 release。 最后是表示制品的所在位置。比如一个跨国公司，它可能在不同区域都有 Aritfacotory 实例来保证上传/下载速度等需求。图上的 den 就是当前 Artifactory 所在位置的缩写。 从构建生成到发布了解它的工作流 # dev 意味着 development（开发），该仓库对所有产品成员都具有读写权限，他们可以上传一些库或其他一些二进制文件。\nint 表示 integration（集成），比如从 Jenkins 里成功构建的制品将首先放在这个存储库下，如果构建失败，它将不会被上传到 Artifactory。\nstage 表示预发布仓库，通过 Unit Test/Smoke Test 的制品会被 Promote 这个仓库待进一步测试，比如手动测试。\nrelease 通过测试的制品会被 Promote 到这个仓库下。\n为了更好的管理Artifactory目录和制品的生命周期，我建议规范分支命名和对不同阶段的制品添加属性。\n1. 规范分支命名有利于Artifactory的目录清晰 # 例如，一个产品叫 ART，它的 Git 仓库也叫 ART，它下面有这样一个分支 feature/ART-1234。\nJenkins Pipeline 里的环境变量设置如下：\nenvironment { INT_REPO_PATH = \u0026#34;team1-generic-int-den/ART/${BRANCH_NAME}/${BUILD_NUMBER}/\u0026#34; } 我们来看看这个分支构建是如何流转的。\n这个分支通过 Jenkins 第1构建成功后，它首先会被 team1-generic-int-den 仓库下的 ART/feature/ART-1234/1/ 的目录下面，如果进行第2次构建，并成功，那么它的制品目录会是： team1-generic-int-den/ART/feature/ART-1234/2/ 以此类推。\n为了更好的管理仓库下面的目录，建议团队事先约定分支命名规范，这样同一种类型的分支的所有构建都会出现在一个目录下面。\n关于命名规范可参见这篇文章程序员自我修养之Git提交信息和分支创建规范\n对于 Pull Request Build 如果也想放到 Artifactory 上面，建议像下面这样设置：\nenvironment { PR_INT_REPO_PATH = \u0026#34;team1-generic-int-den/ART/PRs/${BRANCH_NAME}/${BUILD_NUMBER}/\u0026#34; } 这样所有的 Pull Request Build 构建成功后都会被放到 PRs 这个目录下，方便查找和管理。\n2. 不同阶段添加不同的属性 # 如果以上的构建通过了一些质量关卡，比如通过了单元测试、自动化测试以及 SonaQube 的扫描等等，建议添加不同的属性，例如：\nunit.test.status=passed automated.test.status=passed sonaqube.scan.status=passed\n然后根据上面的状态，将符合条件的制品从 int 仓库 Promote 到 stage 仓库，测试工程师进去 stage 仓库下去获取构建并进行测试。通过测试后，对制品添加相应的属性状态，比如在 Property 中添加 manual.test.status=passed。\n之后发布流水线中去到 stage 仓库里去找满足所有条件的构建进行发布。\nunit.test.status=passed automated.test.status=passed sonaqube.scan.status=passed manual.test.status=passed\n发布成功后，将构建从 stage 仓库 promote 到 release 仓库中，并添加属性 release.status=released，这样就完成了发布。\n最后 # 在软件交付中，质量可信、安全可信是评估版本可靠性的两个重要标准。在这个过程中，就像使用漏斗一样将构建通过层层筛选，从 int 仓库到 stage 仓库，最后到 release 仓库完成了制品的发布。通过 Artifactory 为制品管理的打造一个单一可信源，从而为软件的持续交付铺路。\n往期相关文章 # 初识 JFrog Artifactory Artifactory 与 Jenkins 集成 解决 Jenkins Artifactory Plugin 仅在 AIX 上传制品到 https 协议的 Artifactory 失败的问题 ","date":"2020-10-04","externalUrl":null,"permalink":"/posts/what-is-artifactory/","section":"Posts","summary":"本文介绍了 JFrog Artifactory 的概念、优势、工作原理以及最佳实践，帮助读者了解如何使用 Artifactory 管理软件制品。","title":"写给那些想使用 JFrog Artifactory 管理制品的人","type":"posts"},{"content":" 为什么要制定规范 # 古话说，没有规矩不成方圆。在团队协作开发时，每个人提交代码时都会写 commit message，但如果没有规范，每个人都会有自己的书写风格，因此在翻看 git log 时经常看到的是五花八门，十分不利于阅读和维护。\n通过下面两个例子来看看没规范和有规范的对比，以及有规范能带来哪些好处。\n提交信息 没规范 vs 有规范 # 从这个提交信息里你不知道他修改了什么，修改意图是什么。\n这是 Angular 的提交信息，它遵循了 Conventional Commits，直译过来为常规提交。\n这也是行业内使用最为广泛的 Git 提交信息规范，已经有不少的项目在使用，如果你的项目还没有制定 Git 提交信息规范，建议照搬或参考这个规范来制定。\n对于一个团队，当很多人在一起合作开发一个项目的时候，预先制定好提交信息规范，对于项目的长远发展以及后续人员加入和维护都非常有帮助。\n总结有以下几个好处：\n有助于他人更好的理解你的变更意图，更容易贡献/修改代码。 结构化的提交信息有助于自动化脚本的识别和 CI/CD。 提供自动化生成 CHANGELOGs 的能力。 最后，这也是体现了一个程序员的自我修养。 分支创建 没规范 vs 有规范 # 如果创建分支没有规范，不加以限制，很多分支会是这样的 ABC-1234-Test, ABC-2345-demo, Hotfix-ABC-3456, Release-1.0，甚至更糟。当分支很多的时候会显得混乱，并且不方便检索。\n如果制定分支创建规范，比如上面的分支在创建时，通过 Hook 强制限制分支开头必须以类型开始，那么新创建的分支将会是这样的：bugfix/ABC-1234, feature/ABC-2345, hotfix/ABC-3456, release/1.0 这不但有助于检索，还方便他人通过类型了解分支用途，以及方便后续的 CI/CD 流水线的开发。\n如何解决规范问题 # 应该从两方面着手：\n首先，为团队制定提交信息以及创建分支规范，让团队成员了解规范并遵守。 然后，在提交代码或创建分支时，通过设置的 Git Hook 将不规范的禁止提交到远程仓库。 制定Git提交信息规范 # 制定合理的规范，最有效的方法是参考软件行业里是否有通用的规范。目前行业最为广泛规范是 Conventional Commits 很多项目包括 Auglar 也在使用。\n可以根据以上规范制定适合自己团队的规范，例如：\nJIRA-1234 feat: support for async execution ^-------^ ^--^: ^-------------------------^ | | | | | +--\u0026gt; Summary in present tense. | | | +--\u0026gt; Type: feat, fix, docs, style, refactor, perf, test or chore. | +--\u0026gt; Jira ticket number Type Must be one of the following: feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing or correcting existing tests chore: Changes to the build process, .gitignore or auxiliary tools and libraries such as documentation generation, etc. 设置 Git Hooks # 这里以 Bitbuket 为例，其他 Git 工具比如 GitHub，Gitlab 都有类似的功能。\nBitbucket 使用的是 Yet Another Commit Checker 这个免费插件。\n首先，开启 Yet Another Commit Checker。\n然后逐一介绍 Yet Another Commit Checker 的一些常用的设置。\n1. 开启 Require Valid JIRA Issue(s) # 开启这个功能，在提交信息的时通过 Hook 自动验证是否有 Jira 单号，单号是否存在。如果是否定的，提交失败。这样就强制在提交代码时 commit message 与 Jira 单号进行关联。\n2. Commit Message Regex # 比如设置一个这样简单的正则表达式 [A-Z\\-0-9]+ .* ，这要求 Jira 单号必须以这种格式 ABCD-1234 开头，并且描述信息要与 Jira 单号之间留一个空格。\n通过以上设置，就将提交信息限定为如下格式：\nABCD-1234 Balabala...... 再比如这个更为复杂的正则表达式，如下\n^[A-Z-0-9]+ .*(?\u0026lt;type\u0026gt;chore|ci|docs|feat|fix|perf|refactor|revert|style|test|Bld|¯\\\\_\\(ツ\\)_\\/¯)(?\u0026lt;scope\u0026gt;\\(\\w+\\)?((?=:\\s)|(?=!:\\s)))?(?\u0026lt;breaking\u0026gt;!)?(?\u0026lt;subject\u0026gt;:\\s.*)?|^(?\u0026lt;merge\u0026gt;Merge.* \\w+)|^(?\u0026lt;revert\u0026gt;Revert.* \\w+) 这个正则表达式不但限制了开头必须以 JIRA 单号开始，中间有一个空格，还必须在描述信息里填写 type 类型，最后才是描述信息。另外还支持如果是如果是 Merge 或是 Revert 会产生其他的描述信息。\n通过下面的测试用例来具体理解上述的正则表达式会产生什么样的提交信息规范限制。\n# 测试通过的用例 NV-1234 chore: change build progress DT-123456 docs: update xdemo usage QA-123 ci: update jenkins automatic backup CC-1234 feat: new fucntional about sync Merge branch master into develop Reverted: Revert support feature \u0026amp; bugfix branches build Merge pull request from develop to master # 测试不通过的用例 NV-1234 build: update NV-1234 Chore: change progress DT-123456 Docs: update xdemo QA-123ci: update jenkins automatic backup CC-1234 Feat: new fucntional about sync DT-17734: 8.2.2 merge from CF1/2- Enhance PORT.STATUS DT-17636 fix AIX cord dump issue DT-18183 Fix the UDTHOME problem for secure telnet DT-18183 Add new condition to get UDTHOME DT-15567 code merge by Xianpeng Shen. 测试结果也可以在这里 https://regex101.com/r/5m0SIJ/10 找到​。​\n建议：如果你要在你的 Git 仓库里也要设置这样严格并且复杂的正则表达式，建议一定要经过充分的考虑和测试才把它正式放入你的 Git 仓库的 Hooks 设置中。\n3. Commit Regex Error # 这个设置是用来提示错误信息的。当团队成员在提交时，如果不符合规范提交失败了，会给出合理的提示信息，这有助于找到问题所在。比如提交失败了，会在命令行里看到如下信息：\nCommit Message Specifications: \u0026lt;Jira-ticket-number\u0026gt; \u0026lt;type\u0026gt;: \u0026lt;Description\u0026gt; Example: ABC-1234 feat: Support for async execution 1. Between Jira ticket number and type MUST has one space. 2. Between type and description MUST has a colon and a space. Type MUST be one of the following and lowercase feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing or correcting existing tests chore: Changes to the build process, .gitignore or auxiliary tools and libraries such as documentation generation, etc. 根据这个描述信息，提交者就能很容易知道正确的规范是什么样子的，然后通过 git commit --amend 命令对自己最近一次提交信息做出修改。\n4. Branch Name Regex # 这个是对创建分支时做出的规范限制。在设置了相应的正则表达后，开发在创建分支时，只有符合正则表达式的条件才可以推送到远程仓库中。\n例如这个创建分支的正则表达式 ^(bugfix|feature|release|hotfix).*|(master)|(.*-dev)\n这里限制了所有的分支必须以 bugfix, feature, release, hotfix 开头或是也可以这样的 v1.0-dev 这种类。\n你可以根据上面的正则表达式来设计属于自己项目的分支正则表达式。\n5. Branch Name Regex Error # 这个设置是提示推送不规范的分支时的错误信息。预先设置好相应的错误提示信息，有助于用户快速找到推送失败的原因。比如下面的错误信息：\nBranches must begin with these types: bugfix/ feature/ release/ hotfix/ 告诉用户，分支必须以 bugfix/ feature/ release/ hotfix/ 开头。\n6. 其他设置 # 另外还有一些其他设置，比如关联的 Jira 单子必须处于什么样的状态。这个可以防止已经是关闭状态 Jira 单子，开发还往上面偷偷的提交代码，这样可能导致未经测试的代码进入仓库。\n还有 Require Matching Committer Email 和 Require Matching Committer Name 来限定开发者必须配置好与登录用户名和邮箱相匹配的用户名和邮箱，来规范提交信息里显示的用户名和邮箱，也方便进行 Git 信息的统计等后续数据的收集。\n参考文档 # Conventional Commits https://www.conventionalcommits.org/en/v1.0.0/ Angular Commit Guidelines: https://github.com/angular/angular.js/blob/master/DEVELOPERS.md#commits Projects Using Conventional Commits: https://www.conventionalcommits.org/en/v1.0.0/#projects-using-conventional-commits Yet Another Commit Checker: https://mohamicorp.atlassian.net/wiki/spaces/DOC/pages/1442119700/Yet+Another+Commit+Checker+YACC+for+Bitbucket\n","date":"2020-09-24","externalUrl":null,"permalink":"/posts/commit-messages-specification/","section":"Posts","summary":"本文介绍了如何制定和实施 Git 提交信息和分支创建规范，以提高代码质量和团队协作效率。","title":"程序员自我修养之Git提交信息和分支创建规范","type":"posts"},{"content":"","date":"2020-09-13","externalUrl":null,"permalink":"/tags/pypi/","section":"标签","summary":"","title":"PyPI","type":"tags"},{"content":"","date":"2020-09-13","externalUrl":null,"permalink":"/tags/release/","section":"标签","summary":"","title":"Release","type":"tags"},{"content":"本篇介绍个人或企业在 GitHub 上发布一个 Python 项目需要了解和注意哪些内容\n如何配置setup.py 如何发布到PyPI 生成pydoc 版本号的选择 License的选择 配置setup.py # 打包和发布一项都是通过准备一个 setup.py 文件来完成的。假设你的项目目录结构如下：\ndemo ├── LICENSE ├── README.md ├── MANIFEST.in # 打包时，用来定制化生成 `dist/*.tar.gz` 里的内容 ├── demo │ └── __init__.py ├── setup.py ├── tests │ └── __init__.py │ └── __pycache__/ └── docs 在使用打包命令 python setup.py sdist bdist_wheel，将会生成在 dist 目录下生成两个文件 demo-1.0.0-py3-none-any.whl 和 demo-1.0.0.tar.gz\n.whl 文件是用于执行 pip install dist/demo-1.0.0-py3-none-any.whl 将其安装到 ...\\Python38\\Lib\\site-packages\\demo 目录时使用的文件。\n.tar.gz 是打包后的源代码的存档文件。而 MANIFEST.in 则是用来控制这个文件里到底要有哪些内容。\n下面例子是如何使用 MANIFEST.in 来定制化生成 dist/*.tar.gz 里的内容。MANIFEST.in 文件内容如下：\ninclude LICENSE include README.md include MANIFEST.in graft demo graft tests graft docs global-exclude __pycache__ global-exclude *.log global-exclude *.pyc 根据以上文件内容，在使用命令 python setup.py sdist bdist_wheel 生成 demo-1.0.0.tar.gz 文件时会包含 LICENSE, README.md, MANIFEST.in 这三个文件，并且还会包含 demo, tests, docs 三个目录下的所有文件，最后排除掉所有的 __pycache__, *.log, *.pyc 文件。\n更多关于 MANIFEST.in 文件的语法请参看 https://packaging.python.org/guides/using-manifest-in/\n官方有详细的示例和文档 https://packaging.python.org/tutorials/packaging-projects/\nPython sample 项目供你参考 https://github.com/pypa/sampleproject\n攒点耐心将上面的链接看完，就完全满足一般项目的发布要求了。\n发布到PyPI # 使用 Python 大家都知道可以通过以下命令来下载你要是使用的外部库，Python 有着大量的第三方库，将开源项目发布到 PyPI 上方便用户使用。\npip install xxxx 什么是 PyPI # PyPI 是 The Python Package Index 的缩写，意思是 Python 包索引仓库，用来查找、安装和发布 Python 包。\nPyPI 有两个环境\n测试环境 TestPyPI 正式环境 PyPI 准备 # 如果想熟悉 PyPI 发布工具和发布流程可以使用测试环境 TestPyPI 如果已经熟悉了 PyPI 的发布工具和流程可以直接使用正式环境 PyPI TestPyPI 和 PyPI 需要单独注册，即在正式环境注册了，如果去使用测试环境也同样需要注册。注意：同一个账号不能在 PyPI 和 TestPyPI 同时注册。 假设你的项目已经完成了，准备要发布到 PyPI 了，执行下面的命令，就可以将项目发布到 PyPI 上了。\nrm dist/* # 生成代码存档 .tar.gz 文件和构建文件 .whl 文件 python setup.py sdist bdist_wheel # 如果发布到TestPyPI使用以下命令 twine upload --repository testpypi dist/* # 如果发布到PyPI使用以下命令 twine upload dist/* 关于pydoc # Python 内置了 doc 的功能，叫 pydoc。执行 python -m pydoc 可以看到它有哪些选项和功能。\ncd docs python -m pydoc -w ..\\ # 生成全部文档 执行 python -m pydoc -b 可以在本地立即启动一个 web 页面来访问你 ...\\Python38\\Lib\\site-packages\\ 目录下所有 Libraries 文档。\n这些本地的 web 文档如何在外网进行访问？可以通过 GitHub 有内置的 GitHub Pages 功能，很容易提供一个在线网址。\n打开你的 GitHub python 项目设置选项 -\u0026gt; 找到 GitHub Pages -\u0026gt; Source 选择你的分支和路径，保存后就立刻拥有了一个网址。例如：\nhttps://xxxxx.github.io/demo/ 是你的项目主页，显示是 README.md 信息 https://xxxxx.github.io/demo/docs/demo.html 是你的项目的 pydoc 文档 关于版本号 # 另外如果是正式版本，在发布还需要注意版本号的选择。\n如果是功能简单，完成度也不高，建议从 0.0.1 版本开始。 如果是功能完善，且完成度很高，那么可以从 1.0.0 版本开始。 比如一个项目从准备发布到正式发布有四个阶段：Alpha, Beta, 候选发布以及正式发布。假如正式发布的版本号是 1.1.0 版本，根据以下的版本标识的规范：\nX.YaN # Alpha release X.YbN # Beta release X.YrcN # Release Candidate X.Y # Final release 得到 Alpha, Beta, 候选发布及正式发布版本分别如下：\nAlpha release 版本号是 1.1.0a1, 1.1.0a1, 1.1.0aN...\nBeta release 版本号是 1.1.0b1, 1.1.0b1, 1.1.0bN...\nRelease Candidate 版本号是 1.1.0rc1, 1.1.0rc2, 1.1.0rcN...\nFinal release 版本号 1.1.0, 1.1.1, 1.1.N...\nPython 官方的版本标识和依赖规范文档\n选择License # 企业级的项目 License 一般由公司的法律团队来提供，发布者只需拿到 License 文件做一些格式化工作（比如将 license.txt 文件格式化为每行 70~80 个字符）。\n如果是个人项目或是想了解开源许可相关的介绍，常见的软件开源许可证（以下许可证是按条件数量排序的）\nGNU AGPLv3 GNU GPLv3 GNU LGPLv3 Mozilla Public License 2.0 Apache License 2.0 MIT License Boost Software License 1.0 The Unlicense 这里有一篇关于《Github仓库如何选择开源许可证》文章供参考。\n如何选项 License https://choosealicense.com/licenses\n如何选项 License GitHub 仓库 https://github.com/github/choosealicense.com\n如何选项 License 附录 https://choosealicense.com/appendix \\\n","date":"2020-09-13","externalUrl":null,"permalink":"/posts/how-to-release-python-project/","section":"Posts","summary":"本文介绍个人或企业在 GitHub 上发布一个 Python 项目需要了解和注意的内容，包括项目结构、依赖管理、版本控制等方面的建议。","title":"在 GitHub 上发布一个 Python 项目需要注意哪些","type":"posts"},{"content":" 背景 # 如果你计划将 Python 项目发布到 PyPI，就需要了解 PyPI 与 pip 的版本安装规则。\n我做了一些关于 pip install 的测试，来观察不同版本号和 --upgrade 参数下的安装行为。\n假设我的项目叫 demo-pip：\nBeta 版本号示例：1.1.0.xxxx 正式版本号示例：1.1.0 目标是确认在不同情况下是否能成功升级。\n总结 # 安装 PyPI 上的指定版本（无论是否加 --upgrade）都会成功。 当 PyPI 上的版本高于本地版本时，只有加 --upgrade 才会成功升级；不加则失败。 当 PyPI 上的版本低于本地版本时，无论是否加 --upgrade，都会失败。 使用 1.1.0.xxxx 作为 beta 版本号在 pip 中是合法的，但如果 beta 版本号大于正式版本（例如 1.1.0.1000），在发布正式版本 1.1.0 后，--upgrade 将无法降级安装。 方案 A：正式版本从 1.1.0.1000 开始，beta 版本从 1.1.0.0001 递增。 方案 B（推荐）：遵循 PEP 440 官方版本规范，beta 版本命名为 1.1b1, 1.1b2 …（测试结果正常工作）。 按 PEP 440 命名的 beta 版本（如 1.0b1）被视为小于正式版本，因此升级逻辑正常。 测试案例 # No. 测试步骤 结果 结论 1 安装本地 1.0.5 → PyPI 最新是 1.0.4 → pip install --upgrade 已是最新，无法降级 低版本无法覆盖高版本 2 安装本地 1.0.3 → pip install --upgrade（PyPI: 1.0.4） 升级成功 高版本可覆盖低版本（需 --upgrade） 3 安装本地 beta 1.0.3.1000 → pip install / --upgrade 无 --upgrade 不升级，有则升级 大版本比较逻辑生效 4 安装本地 beta 1.0.4.1000 → pip install 1.0.4 降级失败（版本比较更大） beta 高于正式版时降级失败 5 安装本地 beta 1.0b1 → pip install --upgrade（PyPI: 1.0.4） 升级成功 PEP 440 命名可正常比较版本 建议 # 优先遵循 PEP 440：这样可以保证 pip 对 beta、rc、final 等版本的比较符合预期。 在 CI/CD 中明确使用 --upgrade 逻辑，避免不必要的安装失败。 对 beta 与正式版本的发布顺序做好规划，避免版本号反转导致的降级问题。 ","date":"2020-08-30","externalUrl":null,"permalink":"/posts/pip-install-and-versioning/","section":"Posts","summary":"解释 pip install 在不同版本号场景下的行为，包括如何处理 beta 版本，以及在指定版本号时使用 \u003ccode\u003e--upgrade\u003c/code\u003e 的影响。","title":"关于 Python pip install 与版本管理","type":"posts"},{"content":"","date":"2020-08-17","externalUrl":null,"permalink":"/tags/jira/","section":"标签","summary":"","title":"Jira","type":"tags"},{"content":" 背景 # 在 CI/CD 中使用 Jira Server 账号（例如 robot）时，如果希望该账号在 Jira 中的头像更专业，但该账号无法登录 Jira GUI，就需要通过 REST API 更新头像。\n以下提供 Python 与 Postman 两种示例。\nPython 示例 # import http.client conn = http.client.HTTPSConnection(\u0026#34;jira.your-company.com\u0026#34;) payload = \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;id\u0026#34;: \u0026#34;24880\u0026#34;, \u0026#34;isSelected\u0026#34;: false, \u0026#34;isSystemAvatar\u0026#34;: true, \u0026#34;urls\u0026#34;: { \u0026#34;16x16\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=xsmall\u0026amp;avatarId=24880\u0026#34;, \u0026#34;24x24\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=small\u0026amp;avatarId=24880\u0026#34;, \u0026#34;32x32\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=medium\u0026amp;avatarId=24880\u0026#34;, \u0026#34;48x48\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?avatarId=24880\u0026#34; } }\u0026#34;\u0026#34;\u0026#34; headers = { \u0026#34;content-type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;authorization\u0026#34;: \u0026#34;Basic Ymx3bXY6SzhNcnk5ZGI=\u0026#34;, # Base64(username:password) \u0026#34;cache-control\u0026#34;: \u0026#34;no-cache\u0026#34; } conn.request(\u0026#34;PUT\u0026#34;, \u0026#34;/rest/api/latest/user/avatar?username=robot\u0026#34;, payload, headers) res = conn.getresponse() print(res.read().decode(\u0026#34;utf-8\u0026#34;)) Postman 示例 # URL \u0026amp; Method\nPUT https://jira.your-company.com/rest/api/latest/user/avatar?username=robot Authorization\nType: Basic Auth Username: server-account-username Password: server-account-password Body\n{ \u0026#34;id\u0026#34;: \u0026#34;24880\u0026#34;, \u0026#34;isSelected\u0026#34;: false, \u0026#34;isSystemAvatar\u0026#34;: true, \u0026#34;urls\u0026#34;: { \u0026#34;16x16\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=xsmall\u0026amp;avatarId=24880\u0026#34;, \u0026#34;24x24\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=small\u0026amp;avatarId=24880\u0026#34;, \u0026#34;32x32\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?size=medium\u0026amp;avatarId=24880\u0026#34;, \u0026#34;48x48\u0026#34;: \u0026#34;https://jira.your-company.com/secure/useravatar?avatarId=24880\u0026#34; } } 如何获取 avatar ID # 进入 Jira 个人资料页面。 在头像选择界面右键查看图片地址。 URL 中的 avatarId 参数即为头像 ID。 示例： https://jira.your-company.com/secure/useravatar?avatarId=24880\n","date":"2020-08-17","externalUrl":null,"permalink":"/posts/jira-update-account-avatar-with-rest-api/","section":"Posts","summary":"介绍如何通过 Jira REST API 更新 Jira Server 账号的头像，并提供 Python 和 Postman 示例。","title":"使用 REST API 更新 Jira Server 账号头像","type":"posts"},{"content":" 问题描述 # 在 Windows Server 2012 R2 上，有时会遇到 RDP 无法连接的问题：\nThe remote session was disconnected because there are no Remote Desktop client access licenses available for this computer. Please contact the server administrator. 解决方法 # 如果你有 vSphere Web Client 或其他方式可以登录服务器控制台，可以尝试以下步骤：\n打开注册表编辑器 在命令行或运行窗口输入 regedit.exe，回车。\n删除以下键值\nLicensingGracePeriod LicensingGracePeriodExpirationWarningDays 处理删除失败的情况 如果出现 unable to delete all specified values 错误，需要先修改该键的权限。 可参考 YouTube 视频教程。\n重启系统 如果仍无法连接，重启后再尝试。\n注意事项 # 在我的环境中，大约每 90~120 天 就会再次遇到此问题，这并不是最终解决方案。 如果你有更好的永久性解决办法，欢迎分享。 特别感谢 Bill K. 提供上述解决方案。\n","date":"2020-08-10","externalUrl":null,"permalink":"/posts/rdp-problem/","section":"Posts","summary":"修复 Windows Server 2012 R2 上的 RDP 连接问题，错误提示为没有可用的远程桌面客户端访问许可证。","title":"解决 “Remote session was disconnected because there are no Remote Desktop client access licenses available”","type":"posts"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/tags/daily/","section":"标签","summary":"","title":"Daily","type":"tags"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/misc/daily/","section":"Miscs","summary":"This is only supporting English, not Chinese.","title":"Daily Notes","type":"misc"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/tags/notes/","section":"标签","summary":"","title":"Notes","type":"tags"},{"content":"","date":"2020-08-07","externalUrl":null,"permalink":"/tags/webhook/","section":"标签","summary":"","title":"Webhook","type":"tags"},{"content":" 背景 # 本篇讨论如何通过 Jenkins generic webhook trigger 插件来获取 Git 仓库事件（Events）。比如获取仓库的 Pull Request ID 等。\n使用过 Jenkins Multi-branch pipeline Job 的用户知道，这个 Job 类型的环境变量中可以得到 Pull Request 的相关信息如下\n为了获取这个变量需要创建这种类型的 Job，并且可能需要 clone 该仓库的代码，有点杀鸡宰牛的意思，看起来并不是一个特别恰当的办法。\n如何通过创建一个普通的 Jenkins Job 就能实时获取 Bitbucket 仓库以及 Pull Request 事件呢？通过以下功能和插件可以实现。\n配置 Bitucket 的 Webhook 通过 Jenkins generic-webhook-trigger 插件接收 Webhook 的 Event 事件 实现步骤 # 设置 Bitbucket Webhook # 在需要监听的 Bitbucket 仓库中创建一个 webhook，如下：\nName: test-demo URL: http://JENKINS_URL/generic-webhook-trigger/invoke?token=test-demo 备注：Bitbucket 中还有一个另外一个设置项，根据我的测试，该设置项 Post Webhooks 与上面的 Webhooks 都能实现本文要实现的功能。\n2. 配置 Jenkins Job # 想获取其他 Event 信息，比如 PR title, commit 等，请参考这个链接 bitbucket-server-pull-request.feature，按照上面的设置即可。\n这里的 token 值 test-demo 可以任意起名，但要与 Bitbucket event URL 中的 token 保持一致。\n测试 # 在 Jenkins Job pipeline 里添加了这个代码片段 echo pr_id is ${pr_id} 用来检查输出 Pull Request ID 是否如预期。\n然后在配置好的 Bitbucket 仓库下面创建一个 Pull Request\nJenkins Job 被 Pull Request Open 事件自动触发并执行了\n通过 Jenkins 的输出日志看到成功获取到了这个 Pull Request ID 值\n使用扩展 # 假如你有个程序，可以通过传入的 Pull Request ID 并借助 Bitbucket REST API 来获取并分析指定 Pull Request 的内容的。比如获取相关文件的历史记录，从而知道这些文件谁修改的最多以及这次修改涉及到了哪些 Jira 单号，从而做一些 Review 或是执行回归测试的推荐等等。\n有了这个 PR ID 就可以通过 Jenkins 来自动触发去执行你程序了。\n以上的这种方法适合不想或是不知道如何监听 Git 服务器（Bitbucket、GitHub 或是 GitLab 等）事件而需要要单独创建一个服务而准备的。如果你有什么很好的实践，期待你的留言分享。\n","date":"2020-08-07","externalUrl":null,"permalink":"/posts/bitbucket-pull-request-event/","section":"Posts","summary":"本文介绍如何使用 Jenkins 的 generic-webhook-trigger 插件来实时获取 Bitbucket 仓库的事件信息，如 Pull Request ID 等。","title":"通过 generic-webhook-trigger 插件实时获取 Bitbucket Repository Events","type":"posts"},{"content":" 《Jenkins Tips 3》—— 每期用简短的图文描述一个 Jenkins 小技巧。\n问题 # 在使用 Jenkins pipeline 时，如果 Shell 的返回值不为零（也就是 Shell 命令执行时有错误），Jenkins Job 默认会标记当前的 stage 为失败。因此整个 Job 也会失败。\n在有些时候我们希望 Shell 虽然执行失败返回的不为零，但希望Jenkins Job 在执行成功后，要显示成功状态。\n例如：通过 Shell 命令列出以 fail-list- 开头的文件，如果存在则通知用户，如果不存在则不通知用户。\nls -a fail-list-* 默认情况是执行如上命令导致了整个 Job 失败。\n解决 # 经过一番调查，使用以下代码片段解决了如上问题。\nstage(\u0026#34;Send notification\u0026#34;) { steps { script { def fileExist = sh script: \u0026#34;ls -a fail-list-* \u0026gt; /dev/null 2\u0026gt;\u0026amp;1\u0026#34;, returnStatus: true if ( fileExist == 0 ) { // send email to user }else { // if not found fail-list-* file, make build status success. currentBuild.result = \u0026#39;SUCCESS\u0026#39; } } } } 分析 # 在执行 Shell 时，添加了 returnStatus: true。这是将状态码返回并保存起来，然后与 0 进行比较。\n如果不等于 0，如果不添加 currentBuild.result = 'SUCCESS'，Jenkins 的整个 Job 还是会标记为失败状态。添加后，人为的忽略错误，将 Job 状态置为成功。\n","date":"2020-07-22","externalUrl":null,"permalink":"/posts/jenkins-tips-3/","section":"Posts","summary":"如何在 Jenkins Pipeline 中处理 Shell 返回值不为0的情况，以确保作业(Job)在执行成功后仍然显示为成功状态。","title":"Jenkins 执行 Shell 如果返回值不为0，作业(Job)停止并失败怎么办？","type":"posts"},{"content":"在将 Jenkins 从 2.176.3 升级到 2.235.1 后，我的 Windows Agent 无法与 Master 成功连接，并提示：\n.NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service 这意味着需要升级 Windows Agent 的 .NET Framework。以下是将其升级到 .NET Framework 3.5 的步骤。\n安装 .NET Framework 3.5 # 打开 Programs and Features（程序和功能）\n勾选 .NET Framework 3.5 Features（截图中已安装）\n安装完成后，尝试重新连接 Jenkins Agent，一般即可恢复正常。\n安装 Jenkins Agent 服务 # 如果系统中找不到 Jenkins agent 服务，可以按以下步骤安装：\n# 安装 Jenkins agent 服务 cd c:\\\\jenkins .\\jenkins-agent.exe install net start jenkinsslave-C__agent # 卸载 Jenkins agent 服务 sc delete jenkinsslave-C__agent 手动安装 .NET Framework 3.5 # 如果在 Windows 功能中安装失败，可以手动安装：\n下载 microsoft-windows-netfx3-ondemand-package.cab 在命令行中指定路径进行安装（路径为 .cab 文件所在目录） 在我的案例中，安装完成后 无需重启 Windows Agent。\n希望此方法对你有帮助，如有更好的解决方案，欢迎分享。\n","date":"2020-07-16","externalUrl":null,"permalink":"/posts/jenkins-windows-agent-connect-problem/","section":"Posts","summary":"当 Jenkins Windows Agent 因缺少 .NET Framework 无法连接时，通过安装 .NET Framework 3.5 并设置 Jenkins Agent 服务来解决问题。","title":"解决 “.NET Framework 2.0 or later is required on this computer to run a Jenkins agent as a Windows service” 问题","type":"posts"},{"content":"​在使用 Jenkins 实施了企业级的 CI/CD 工作，有如下三个最重要的实践和总结。\n第一，Configuration as Code（配置即代码）\n其次，Jenkins shared libraries（Jenkins 共享库）\n最后，Multi-Branch Pipeline（多分支流水线）\n配置即代码 # 配置即代码(Configuration as Code)是一种在代码仓库里管理配置的方法。\n它有什么好处 # 作业透明化 # 如果你使用过 Bamboo 或 Jenkins 1.x 版本，你知道想要从配置页面(GUI)来快速了解一个作业的逻辑是多么的困难，尤其是对于那些不太了解 CI 工具的人更是难上加难。\n因此如果你准备使用 Jenkins 来作为团队的 CI 工具，一定要使用配置即代码，因为代码对于工程师来说更易读和了解背后的逻辑。\n可追溯性 # 对于 GUI 页面配置带来的另外一个重要问题就是无法追溯修改历史，来看别人做了什么修改。能查看其他人的修改对于一些很重要的 Job 是非常重要的，比如像是 Build Jobs 等。把 Jenkins 的配置当作项目代码来管理，这样做的好处不仅在于可跟踪性，还在于在需要时可以回滚到指定版本。\n快速恢复 # 配置即代码的使用还有另一个好处：够在硬件或是系统出了问题后快速恢复 Jenkins。但是，如果 Jenkins 作业是通过 GUI 配置的，当托管 Jenkins 的服务器损坏时，你的业务可能面临丢失的风险。因此，从业务连续性角度来看，它也暗示我们要使用配置即代码。\nJenkins 共享库 # 就像编写任何应用程序代码一样，我们需要创建函数、子例程以实现重用和共享。同样的逻辑也适用于 Jenkins Pipeline 配置。比如发送电子邮件、打印日志、将 build 放到 FTP 或Artifactory 等功能都可以放到 Jenkins 共享库中。\n如你所见，以下这些 groovy 文件就是 Jenkins 共享库的一部分，它们用来完成发送电子邮件、git 等操作、更新开源、代码扫描（Polaris）及触发其他任务等工作。\nxshen@localhost MINGW64 /c/workspace/cicd/src/org/devops (develop) $ ls -l total 28 -rw-r--r-- 1 xshen 1049089 5658 Jun 18 09:23 email.groovy -rw-r--r-- 1 xshen 1049089 898 Jun 13 20:05 git.groovy -rw-r--r-- 1 xshen 1049089 1184 Jun 8 12:10 opensrc.groovy -rw-r--r-- 1 xshen 1049089 1430 Jul 3 10:33 polaris.groovy -rw-r--r-- 1 xshen 1049089 2936 Jul 3 10:32 trigger.groovy drwxr-xr-x 1 xshen 1049089 0 Jun 8 12:10 utils/ -rw-r--r-- 1 xshen 1049089 787 May 12 13:24 utils.groovy 好处 # 这就是为什么要使用共享库，它不但可以减少重复代码，也更容易维护。比如当你需要管理很多个仓库里的自动化流水线的时候，使用 Jenkins 共享库不需要更新每个代码仓库里的 Jenkinsfile，只更改共享库里的代码即可。\n这样的方式也鼓励重用和跨团队共享。例如，我创建的共享库也被公司其他团队在使用。\n分支流水线 # 在下面这张图中，开发的每个 Pull Request 通过 Webhook 触发自动构建和冒烟测试，只有通过构建测试和冒烟测试的修改才允许被合并到主干分支上。\n以上的工作流程是利用 Jenkins 多分支流水线来实现的。在进入细节之前，让我们先看看它是什么样子的。\n在这个页面看到的所有分支都是在代码仓库里创建后就自动生成的，这样开发者都通过这个 Jenkins Job 可以在自己的分支进行自动化构建和测试。\n注：如果分支已经从代码仓库删掉了，这些分支则也会相应从这个 Jenkins Job 里删除掉或是像上面那样显示划掉状态（这里取决你 Jenkins 的设置）。Pull Request 也是同理。\n这样，当开发人员完成他们的工作时，他们可以使用这些 Jenkins Job 来自己创建正式的 Build，而不再需要 Build 工程师的参与。这在引入多分支流水线之前所达不到的，以前开发总是需要 Build 工程师为他们创建 Build 用来测试，对于一个 20 多人的开发团队，可以想象满足这些需求所要花费多少努力。\n好处 # 以上介绍了这个多分支流水线的第一个好处：为团队创建了一个自助服务，节省了开发的时间，也节省了 Build 工程师的时间。\n另一个好处是：使主分支将更加稳定，再也不用花大量时间去查找是谁的提交破坏了主干分支的构建或是功能。因为只有通过构建、安装以及冒烟测试的代码才会被合并到主干分支上。\n价值 # 从人力成本：这样的自助服务，节省了至少 0.5 人以上的人力成本。让过去的重复劳动者，变成现在基础设施的维护者和开发者。\n从质量成本：以我的项目最近一个月大约 30 个 Pull Request 为例，发现其中 6 个在某些平台上存在 Build 问题。你知道如果能在开发阶段就发现问题，而不是被测试、Support 人员甚至是客户发现问题，那么这种发现缺陷的成本就会非常低。\n","date":"2020-07-06","externalUrl":null,"permalink":"/posts/jenkins-best-practice/","section":"Posts","summary":"本文介绍了 Jenkins 的三个最佳实践：配置即代码、Jenkins 共享库和多分支流水线，帮助用户提升 Jenkins 的使用效率和质量。","title":"每个 Jenkins 用户都应该知道这三个最佳实践","type":"posts"},{"content":" 《Jenkins Tips 2》 —— 每期用简短的图文描述一个 Jenkins 小技巧。\n问题 # 想要把 Linux 上不同的文本数据通过 Jenkins 发送邮件给不同的人。\n思路 # 想通过 Shell 先对数据进行处理，然后返回到 Jenkins pipeline 里，但只能得到 Shell 返回的字符串，因此需要在 Jenkinsfile 里把字符串处理成数组，然后通过一个 for 循环对数组中的值进行处理。\n以下是要处理的文本数据：\n# Example $ ls fail-list-user1.txt fail-list-user2.txt fail-list-user3.txt 要将以上文件通过 Jenkins 分别进行处理，得到用户 user1，user2，user3 然后发送邮件。\n解决 # 字符串截取 # 通过 Shell 表达式只过滤出 user1 user2 user3\n# list 所有以 fail-list 开头的文件，并赋给一个数组 l l=$(ls -a fail-list-*) for f in $l; do f=${f#fail-list-} # 使用#号截取左边字符 f=${f%.txt} # 使用%号截取右边字符 echo $f # 最终输出仅包含 user 的字符串 done 测试结果如下：\n$ ls fail-list-user1.txt fail-list-user2.txt fail-list-user3.txt $ l=$(ls -a fail-list-*) \u0026amp;\u0026amp; for f in $l; do f=${f#fail-list-}; f=${f%.txt}; echo $f ; done; user1 user2 user3 处理字符串为数组 # 以下在 Jenkinsfile 使用 groovy 将 Shell 返回的字符串处理成字符数组。\n// Jenkinsfile // 忽略 stage, steps 等其他无关步骤 ... scripts { // 将 Shell 返回字符串赋给 owners 这个变量。注意在 $ 前面需要加上 \\ 进行转义。 def owners = sh(script: \u0026#34;l=\\$(ls -a fail-list-*) \u0026amp;\u0026amp; for f in \\$l; do f=\\${f#fail-list-}; f=\\${f%.txt}; echo \\$f ; done;\u0026#34;, returnStdout:true).trim() // 查看 owners 数组是否为空，isEmpty() 是 groovy 内置方法。 if ( ! owners.isEmpty() ) { // 通过 .split() 对 owners string 进行分解，返回字符串数组。然后通过 .each() 对返回的字符串数组进行循环。 owners.split().each { owner -\u0026gt; // 打印最终的用户返回 println \u0026#34;owner is ${owner}\u0026#34; // 发送邮件，例子 email.SendEx([ \u0026#39;buildStatus\u0026#39; : currentBuild.currentResult, \u0026#39;buildExecutor\u0026#39;: \u0026#34;${owner}\u0026#34;, \u0026#39;attachment\u0026#39; : \u0026#34;fail-list-${owner}.txt\u0026#34; ]) } } } 最终完成了通过 Groovy 将 Shell 返回的字符串处理成字符数组，实现上述例子中对不同人进行邮件通知的需求。\n希望以上例子对你做其他类似需求的时候有所启示和帮助。\n","date":"2020-06-22","externalUrl":null,"permalink":"/posts/jenkins-tips-2/","section":"Posts","summary":"如何在 Jenkins Pipeline 中将 Shell 返回的字符串处理为字符数组，以便在后续步骤中进行处理和使用。","title":"将 Jenkins Shell 返回的字符串处理为字符数组","type":"posts"},{"content":" 《Jenkins Tips 1》 —— 每期用简短的图文描述一个 Jenkins 小技巧。\n问题 # 不希望 Shell 脚本因失败而中止 想一直运行 Shell 脚本并报告失败 解决 # 方法一 # 运行 Shell 时，你可以通过使用内置的 +e 选项来控制执行你的脚本错误。这可以禁用“非 0 退出”的默认行为。\n请参考如下四个示例中的测试 Shell 和测试结果 Console Output。\n示例一 # 执行的时候如果出现了返回值为非零（即命令执行失败）将会忽略错误，继续执行下面的脚本。\nset +e ls no-exit-file whoami 示例二 # 执行的时候如果出现了返回值为非零，整个脚本就会立即退出。\nset -e ls no-exit-file whoami 方法二 # 示例三 # 还有一种方式，如果不想停止失败的另一种方法是添加 || true 到你的命令结尾。\n# 做可能会失败，但并不关注失败的命令时 ls no-exit-file || true 示例四 # 如果要在失败时执行某些操作则添加 || \u0026lt;doSomethingOnFailure\u0026gt; 。\n# 做可能会失败的事情，并关注失败的命令 # 如果存在错误，则会创建变量 error 并将其设置为 true ls no-exit-file || error=true # 然后去判断 error 变量的值。如果为真，则退出 Shell if [ $error ] then exit -1 fi ","date":"2020-06-21","externalUrl":null,"permalink":"/posts/jenkins-tips-1/","section":"Posts","summary":"如何在 Jenkins 中使用 \u003ccode\u003eset +e\u003c/code\u003e 和 \u003ccode\u003eset -e\u003c/code\u003e 来控制 Shell 脚本的执行行为，以便在出现错误时不终止整个构建流程。","title":"忽略 Jenkins Shell 步骤中的故障","type":"posts"},{"content":" 背景 # 实现定期批量登录远程虚拟机然后进行一些指定的操作，还支持用户添加新的 hostname。\n需求分解 # 通过一个简单的 shell 脚本可实现定期进行 ssh 登录操作，但如何实现的更优雅一些就需要花点时间了，比如：\n定期自动执行 输出比较直观的登录测试结果 支持用户添加新的虚拟机 hostname 到检查列表中 执行完成后，通知用户等等 希望在不引入其他 Web 页面的情况下通过现有的工具 Jenkins 使用 Shell 脚本如何实现呢？\n写一个脚本去循环一个 list 里所有的 hostname，经考虑这个 list 最好是一个 file，这样方便后续处理。 这样当用户通过执行 Jenkins job 传入新的 hostname 时，使用新的 hostname 到 file 里进行 grep，查看是否已存在。 如果 grep 到，不添加；如果没有 grep 到，将这个 hostname 添加到 file 里。 将修改后的 file 添加到 git 仓库里，这样下次 Jenkins 的定时任务就会执行最近添加的 hostname 了。 实现重点 # 使用 expect。在使用 ssh 连接远程虚拟机的时候需要实现与远程连接时实现交互，例如：可以期待屏幕上的输出，然后进而进行相应的输入。在使用 expect 之前需要先安装，以 Redhat 的安装命令为例： sudo yum install expect 来进行安装。\n更多有关 expect 使用的可以参看这个连接：http://xstarcd.github.io/wiki/shell/expect.html\n使用了 Shell 数组。使用 Shell 读取文件数据，进行登录操作，将操作失败的记录到一个数组里，然后打印出来。\n在通过 Jenkins 提交新的 hostname 到 Git 仓库时，origin 的 URL 需要是 https://${USERNAME}:${PASSWORD}@git.company.com/scm/vmm.git 或 git@company.com:scm/vmm.git（需要提前在执行机器上生成了 id_rsa.pub）\n代码已经上传 GitHub 请参看 https://github.com/shenxianpeng/vmm.git\n最终效果 # 开始执行，提供输入新的 hostname # 执行完成，将执行结果归档以便查看 # 打开归档结果如下 # ##################################################### ######### VM login check via SSH results ############ ##################################################### # # # Compelted (success) 14/16 (total) login vm check. # # # # Below 2 host(s) login faied, need to check. # # # abc.company.com xyz.company.com # # ##################################################### 最后 # 现在技术的更新非常快，尤其作为 DevOps 工程师，各种工具层出不穷，想要每一样工具都掌握几乎是不可能的。\n只会工具不了解其背后的原理，等到新工具出现替换掉旧的工具，其实这些年是没有进步的。\n只有认真的把在工作中遇到的每个问题背后来龙去脉去搞懂，才能地基打的稳，这样不论工具怎么变，学习起来都会很快。\n掌握操作系统，Shell，以及一门擅长的编程语言之后再去学习那些工具，要不永远都是漂浮在空中。\n","date":"2020-06-13","externalUrl":null,"permalink":"/posts/vm-status-check-via-jenkins/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 实现定期批量登录远程虚拟机，并支持用户添加新的主机名称，提供了完整的实现代码和步骤。","title":"这也能用Jenkins？快速实现一个定期批量登录远程虚拟机并支持添加新的主机名称的Job","type":"posts"},{"content":" 本文对于同样在 AIX 遇到这个问题的人会非常有帮助。另外，不要被标题无聊到，解决问题的过程值得参考。\n分享一个花了两天时间才解决的一个问题：使用 Jenkins Artifactory 插件上传制品到 https 协议的企业级的 Artifactory 失败。该问题只在 AIX 平台上出现的，其他 Windows，Linux, Unix 均正常。\n前言 # 最近计划将之前使用的 Artifactory OSS（开源版）迁移到 Artifactory Enterprise（企业版）上。为什么要做迁移？这里有一个 Artifactory 对比的矩阵图 https://www.jfrog.com/confluence/display/JFROG/Artifactory+Comparison+Matrix\n简单来说，开源版缺少与 CI 工具集成时常用的 REST API 功能，比如以下常用功能\n设置保留策略(Retention)。设置上传的制品保留几天等，达到定期清理的目的。 提升(Promote)。通过自动化测试的制品会被提升到 stage（待测试）仓库，通过手工测试的提升到 release（发布）仓库。 设置属性(set properties)。对于通过不同阶段的制品通过 CI 集成进行属性的设置。 正好公司已经有企业版了，那就开始迁移吧。本以为会很顺利的完成，没想到唯独在 IBM 的 AIX 出现上传制品失败的问题。\n环境信息\nJenkins ver. 2.176.3 Artifactory Plugin 3.6.2 Enterprise Artifactory 6.9.060900900 AIX 7.1 \u0026amp;\u0026amp; java version 1.8.0 以下是去掉了无相关的信息的错误日志。\n[consumer_0] Deploying artifact: https://artifactory.company.com/artifactory/generic-int-den/database/develop/10/database2_cdrom_opt_AIX_24ec6f9.tar.Z Error occurred for request GET /artifactory/api/system/version HTTP/1.1: A system call received a parameter that is not valid. (Read failed). Error occurred for request PUT /artifactory/generic-int-den/database/develop/10/database2_cdrom_opt_AIX_24ec6f9.tar.Z;build.timestamp=1591170116591;build.name=develop;build.number=10 HTTP/1.1: A system call received a parameter that is not valid. (Read failed). Error occurred for request PUT /artifactory/generic-int-den/database/develop/10/database2_cdrom_opt_AIX_24ec6f9.tar.Z;build.timestamp=1591170116591;build.name=develop;build.number=10 HTTP/1.1: A system call received a parameter that is not valid. (Read failed). [consumer_0] An exception occurred during execution: java.lang.RuntimeException: java.net.SocketException: A system call received a parameter that is not valid. (Read failed) at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:44) at org.jfrog.build.extractor.producerConsumer.ConsumerRunnableBase.run(ConsumerRunnableBase.java:11) at java.lang.Thread.run(Thread.java:785) Caused by: java.net.SocketException: A system call received a parameter that is not valid. (Read failed) at java.net.SocketInputStream.socketRead(SocketInputStream.java:127) at java.net.SocketInputStream.read(SocketInputStream.java:182) at java.net.SocketInputStream.read(SocketInputStream.java:152) at com.ibm.jsse2.a.a(a.java:227) at com.ibm.jsse2.a.a(a.java:168) at com.ibm.jsse2.as.a(as.java:702) at com.ibm.jsse2.as.i(as.java:338) at com.ibm.jsse2.as.a(as.java:711) at com.ibm.jsse2.as.startHandshake(as.java:454) at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:436) at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384) at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:374) at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.ServiceUnavailableRetryExec.execute(ServiceUnavailableRetryExec.java:85) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) at org.jfrog.build.client.PreemptiveHttpClient.execute(PreemptiveHttpClient.java:89) at org.jfrog.build.client.ArtifactoryHttpClient.execute(ArtifactoryHttpClient.java:253) at org.jfrog.build.client.ArtifactoryHttpClient.upload(ArtifactoryHttpClient.java:249) at org.jfrog.build.extractor.clientConfiguration.client.ArtifactoryBuildInfoClient.uploadFile(ArtifactoryBuildInfoClient.java:692) at org.jfrog.build.extractor.clientConfiguration.client.ArtifactoryBuildInfoClient.doDeployArtifact(ArtifactoryBuildInfoClient.java:379) at org.jfrog.build.extractor.clientConfiguration.client.ArtifactoryBuildInfoClient.deployArtifact(ArtifactoryBuildInfoClient.java:367) at org.jfrog.build.extractor.clientConfiguration.util.spec.SpecDeploymentConsumer.consumerRun(SpecDeploymentConsumer.java:39) ... 2 more Failed uploading artifacts by spec 很奇怪会出现上述问题，从开源版的 Artifactory 迁移到企业版的 Artifactory，它们之间最直接的区别是使用了不同的传输协议，前者是 http 后者是 https。\nHTTPS 其实是有两部分组成：HTTP + SSL/TLS，也就是在 HTTP 上又加了一层处理加密信息的模块，因此更安全。\n本以为 Google 一下就能找到此类问题的解决办法，可惜这个问题在其他平台都没有，只有 AIX 上才有，肯定这个 AIX 有什么“过人之处”和其他 Linux/Unix 不一样。\n使用 curl 来替代 # 由于上述问题重现在需要重新构建，比较花时间，就先试试直接用 curl 命令来调用 Artifactory REST API 看看结果。\n做了以下测试，查看 Artifactory 的版本\ncurl https://artifactory.company.com/artifactory/api/system/version curl: (35) Unknown SSL protocol error in connection to artifactory.company.com:443 # 打开 -v 模式，输出更多信息 bash-4.3$ curl -v https://artifactory.company.com/artifactory/api/system/version * Trying 10.18.12.95... * Connected to artifactory.company.com (10.18.12.95) port 443 (#0) * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH * TLSv1.2 (OUT), TLS handshake, Client hello (1): * TLSv1.2 (IN), TLS handshake, Server hello (2): * NPN, negotiated HTTP1.1 * TLSv1.2 (IN), TLS handshake, Certificate (11): * TLSv1.2 (OUT), TLS alert, Server hello (2): * Unknown SSL protocol error in connection to artifactory.company.com:443 * Closing connection 0 curl: (35) Unknown SSL protocol error in connection to artifactory.company.com:443 果然也出错了，curl 也不行，可能就是执行 curl 命令的时候没有找到指定证书，查了 curl 的 help，有 --cacert 参数可以指定 cacert.pem 文件。\nbash-4.3$ curl --cacert /var/ssl/cacert.pem https://artifactory.company.com/artifactory/api/system/version { \u0026#34;version\u0026#34; : \u0026#34;6.9.0\u0026#34;, \u0026#34;revision\u0026#34; : \u0026#34;60900900\u0026#34;, \u0026#34;addons\u0026#34; : [ \u0026#34;build\u0026#34;, \u0026#34;docker\u0026#34;, \u0026#34;vagrant\u0026#34;, \u0026#34;replication\u0026#34;, \u0026#34;filestore\u0026#34;, \u0026#34;plugins\u0026#34;, \u0026#34;gems\u0026#34;, \u0026#34;composer\u0026#34;, \u0026#34;npm\u0026#34;, \u0026#34;bower\u0026#34;, \u0026#34;git-lfs\u0026#34;, \u0026#34;nuget\u0026#34;, \u0026#34;debian\u0026#34;, \u0026#34;opkg\u0026#34;, \u0026#34;rpm\u0026#34;, \u0026#34;cocoapods\u0026#34;, \u0026#34;conan\u0026#34;, \u0026#34;vcs\u0026#34;, \u0026#34;pypi\u0026#34;, \u0026#34;release-bundle\u0026#34;, \u0026#34;replicator\u0026#34;, \u0026#34;keys\u0026#34;, \u0026#34;chef\u0026#34;, \u0026#34;cran\u0026#34;, \u0026#34;go\u0026#34;, \u0026#34;helm\u0026#34;, \u0026#34;rest\u0026#34;, \u0026#34;conda\u0026#34;, \u0026#34;license\u0026#34;, \u0026#34;puppet\u0026#34;, \u0026#34;ldap\u0026#34;, \u0026#34;sso\u0026#34;, \u0026#34;layouts\u0026#34;, \u0026#34;properties\u0026#34;, \u0026#34;search\u0026#34;, \u0026#34;filtered-resources\u0026#34;, \u0026#34;p2\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;webstart\u0026#34;, \u0026#34;support\u0026#34;, \u0026#34;xray\u0026#34; ], \u0026#34;license\u0026#34; : \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; } 试了下成功了。\n到这里问题已经解决了，只要使用 curl 调用 Artifactory REST API 就能完成上传操作了。但我用的 Jenkins Artifactory Plugin，如果使用 curl 我需要把之前的代码重新再实现一遍，然后再测试，就为了 AIX 一个平台的问题，实在是“懒”的重新开始。本着这样懒惰的性格，还得继续解决 Jenkins 调用 agent 去执行上传失败的问题。\n最终解决办法 # 尝试设置 SSL_CERT_FILE 环境变量 # 想试试用上述的办法来解决 Jenkins 的问题。如果能有一个环境变量能设置指定 cacert.pem 文件的路径，那样在 Jenkins 调用 agent 执行上传时候就能找到证书，可能就能解决这个问题了。果然是有这样的环境变量的 SSL_CERT_FILE，设置如下\nset SSL_CERT_FILE=/var/ssl/cacert.pem 设置好环境变量之后，通过 curl 调用，再不需要使用 --cacert 参数了。这下看起来有戏了，带着喜悦的心情把这个环境变量加到 agent 机器上，设置如下：\n或者可以修改 agent 机器上的 /etc/environment 文件。\n结果经测试错误信息依旧，看来 Jenkins 执行的 remote.jar 进行上传时跟本地配置环境没有关联，看来需要从执行 remote.jar 着手，把相应的设置或是环境变量在启动 remote.jar 时传进去。\nJenkins 管理 agent 的原理是通过在 agent 上启动一个 remote.jar 实现的\n在启动 remote.jar 时设置环境变量 # java 的 -D 参数可以完成这一点。\n进行了大量的搜索和尝试，最终在 IBM 的官方找到了这篇文档 https://www.ibm.com/support/knowledgecenter/SSYKE2_8.0.0/com.ibm.java.security.component.80.doc/security-component/jsse2Docs/matchsslcontext_tls.html\n文档大意是，IBM SDK 系统属性 com.ibm.jsse2.overrideDefaultTLS=[true|false] 有 true 和 false 两个值，如果想要与 Oracle SSLContext.getInstance(\u0026quot;TLS\u0026quot;) 的行为相匹配，请将此属性设置为 true，默认值为 false。\n下表显示了系统属性对 SSLContext.getInstance(\u0026ldquo;TLS\u0026rdquo;) 的影响\nProperty value setting Protocol enabled false TLS V1.0 true TLS V1.0, V1.1, and V1.2 绝大多数的 Java 应用都是使用 Oracle 的 JDK 来开发的，这里要与 Oracle 的行为保持一致；另外 IBM 的 SDK 默认协议只有 TLS V1.0，而上面的 log 可以看到使用的 TLSv1.2 协议，因此需要将属性设置为 true。\n最终在 Jenkins 的 agent 配置里将 JVM Options 区域加上这句 -Dcom.ibm.jsse2.overrideDefaultTLS=true，断开连接，重新启动 agent，再次执行 Pipeline，成功的把 AIX 上的制品上传到 Artifactory 上了，问题解决了。\n总结 # 遇到问题并解决问题是一件非常爽的事，从中也学到了很多之前不曾了解过的知识，解决问题的过程比 Google 随便查查更让人印象深刻，再遇到此类问题可能就会举一反三了。\n另外，凡事如果觉得自己在短时间内没有头绪、自己搞不定的时候尽快寻求有经验的同事的帮助。感谢帮助我的同事们，没有他们的帮助和指导就不能这么快的解决问题。\n","date":"2020-06-03","externalUrl":null,"permalink":"/posts/java-net-socketexception-on-aix/","section":"Posts","summary":"本文介绍了在 AIX 上使用 Jenkins Artifactory 插件上传制品到 https 协议的 Artifactory 失败的问题及其解决方法，包括设置环境变量和调整 Java 系统属性。","title":"解决 Jenkins Artifactory Plugin 仅在 AIX 上传制品到 https 协议的 Artifactory 失败的问题","type":"posts"},{"content":"","date":"2020-05-30","externalUrl":null,"permalink":"/tags/others/","section":"标签","summary":"","title":"Others","type":"tags"},{"content":"在程序员圈子里比较流行这样一句话“会写程序的干不过会写 PPT 的”，还记得 2019 年新东方年会的一首《放飞自我》里有这样一句歌词戳中了绝大大多数程序员的内心\n“干的累死累活，有成果那又如何，到头来干不过写PPT的”。\n一时间大家好像都认同了这个说法，表达着自己的不满和无奈。\n（一） # 随着自己从业超过 10 年有余，不管是从网络上还是工作中见识了各种各样的能人之后，对于真正的“能力”也有了新的认识。\n在一开始我会觉得那些沉默寡言并且技术还好的才是真正的大神，那些会点技术就开始给人讲的是在班门弄斧。然而，当给团队分享一些技术分享的时候，发现想把一件事给绝大多数不了解的人说明白其实也同样是一种能力。\n首先，在给人讲之前首先得自己完全弄明白 其次，还要想别人可能会问的问题自己否能答出来 最后，故事线是什么，怎样逻辑清晰的讲出来 （二） # 最近吴军老师在得到上开设了一门《阅读和写作》里他举了一个例子\n一个项目组中有三个人，第一个人擅长于做专业工作，组里最重要的工作都是他做的，其他人也经常要靠他来指导；第二个人擅长把大家组织到一起，当大家遇到困难时，他能够鼓舞大家的士气，带领大家克服困难取得胜利；第三个人能把他们组的工作讲清楚。\n如果老板要从这几个人中提拔一个人，谁的机会最大呢？我相信很多人会觉得是那个专业能力特别强的。但是，在现实生活中往往是第三个人最大。为什么呢？\n我们假想一下三个人去做汇报的场景\n第一个人去做汇报，他讲了一堆专业细节，领导听得无趣，最后他既没有让全组的工作被上级认可，也没有争取到什么资源。也许第二次大家就不推举他做代表了，或者这个项目受不到重视解散了。\n如果是那个善于组织的人去作报告呢？他给人的印象就是一个行政管理人员，在具体的项目上一直在外围转悠，指手画脚，细节根本讲不清楚，当然汇报的效果也好不了。\n如果是大家找那个会讲的人去汇报，那么全组的工作最有可能得到认可，而且还争取来很多资源。于是大家都有好处，接下来还会让他代表大家去汇报。久而久之，在外人的印象里，他的功劳第一。\n这不是编的故事，大家在单位里看看周围，就会发现情况和这里说的大同小异，这是在一个单位里自然选择的结果。通常由表达能力好的人去汇报工作和作报告，这样的人也因此更容易被看见，被提拔。\n（三） # 很多在外企工作的小伙伴会注意到，很多公司的高管都是印度人，我自己也听过印度人在跟领导汇报工作，发现他们讲的确实比我讲的好太多，抛开英语层面不说，在内容和准备方面，确实有这么几点需要像他们学习的。\nPPT 做的很好，用结构化的图表达 有核心，有重点 能串起一个故事来，把事情说清楚 最后你发现印度人做的你都有，比他做的还好，他没有的你也有。但是，他讲的比你好，另在场的所有人都印象深刻，都觉得很牛逼，只有你呵呵一笑。\n作为程序员，吃技术这碗饭，提高技术固然不可缺少，但同时锻炼自己的写作和表达的能力才会更有机会打造自己的品牌，让自己走的更高更远。\n最后 # 最近自己要准备一个英文的 PPT 给国外的老大做分享，我的领导的审核给了我极大的帮助，并深刻的发现，同样想表达一个意思，自己的表达和领导的表达存在相当大的差异。\n举一个例子，为什么用 Jenkins 共享库？来对比修改前后的差别\n修改前：In the early stage of doing this work one year ago, I wrote many duplicate code such as sending emails, printing logs. Shared Libraries could help to solve this problem \u0026hellip;\n修改后：Just like writing any application code, that we need to create functions, subroutines for reuse and sharing purpose. The same logic applies to Jenkins configuration code \u0026hellip;\n其实没有必要说自己一开始走的一些弯路和错误，虽然很诚实，但这种表达对自己并没有什么好处。另外，从开场白，每页 PPT 之间的衔接，结束语都给出了修改意见。\n到这里，我彻底抛开了对写 PPT 曾经有过的偏见，并认为写好 PPT 是一种能力。\n不说了，继续改我的 PPT、背稿去了 \u0026hellip;\n","date":"2020-05-30","externalUrl":null,"permalink":"/posts/programmers-read-and-write/","section":"Posts","summary":"本文探讨了程序员写作的重要性，强调了写作和表达能力在职业发展中的作用，并分享了个人在写作方面的经验和体会。","title":"从 “会写程序的干不过会写PPT的” —— 聊程序员写作","type":"posts"},{"content":"我在做 Jenkins 声明式流水线开发时常会遇到的问题是：修改后的 Pipeline 看起来没有问题，当提交到代码仓库后进行 Jenkins 构建时发现原来有语法错误，然后再去修改、提交、构建，结果可能还有有其他没有注意到的语法问题。\n为了减少这种因为语法错误而需要频繁像代码库去提交的情况，如果能在提交之前进行基本的语法校验，来检查当前的 Pipeline 是否存在语法错误就好了。\n经过调查发现 Jenkins 本身提供了这样的语法检查 REST API，可以直接使用这个 API 来对 Pipeline 声明式进行语法校验，这个方式需要执行一长串的 curl 命令，看起来似乎很麻烦，如果能在 IDE 里直接运行就好了。\nVS Code 作为当前当前最流行 IDE 工具，果然找到了相关的插件。\n以下就介绍两种方法：针对 Jenkins 声明式流水线中的 Jenkinsfile 文件进行语法错误检查，这两种方式的原理都是通过调用 Jenkins REST API 来实现的。\n注意：\n当前只有声明式流水线支持语法校验，脚本式流水线不支持。\n如果使用 Jenkins 回放功能或是使用 Jenkins Web 页面开发 Pipeline 不存在上述问题。\nREST API # 如果你的项目使用了 Jenkins Shared Libraries，为了方便使用 REST API，那么不妨在该仓库下面创建一个 linter.sh 文件，并将这个文件加到你的 .gitignore 里，这样你可以在这个文件里配置你的用户名和密码就不会意外的被提交到 Git 仓库中。\n以下是 linter.sh 脚本内容如下，供参考。\n# 如何使用 # sh linter.sh your-jenkinsfile-path # 替换为你的 Jenkins 用户名 username=admin # 替换为你的 Jenkins 密码 password=admin # 替换为你的 Jenkins URL JENKINS_URL=http://localhost:8080/ PWD=`pwd` JENKINS_FILE=$1 curl --user $username:$password -X POST -F \u0026#34;jenkinsfile=\u0026lt;$PWD/$JENKINS_FILE\u0026#34; $JENKINS_URL/pipeline-model-converter/validate 让我们来测试一下效果 sh linter.sh your-jenkinsfile-path\n实例 1\n$ sh linter.sh Jenkinsfile Errors encountered validating Jenkinsfile: WorkflowScript: 161: Expected a stage @ line 161, column 9. stages { ^ 实例 2\nsh linter.sh Jenkinsfile Errors encountered validating Jenkinsfile: WorkflowScript: 60: Invalid condition \u0026#34;failed\u0026#34; - valid conditions are [always, changed, fixed, regression, aborted, success, unsuccessful, unstable, failure, notBuilt, cleanup] @ line 60, column 9. failed{ ^ # 将 failed 改为 failure，再次执行，成功。 sh linter.sh Jenkinsfile Jenkinsfile successfully validated. 当 Pipeline 写的很长的时候，总是很难发现有什么地方没有匹配或是缺了括号什么的。有了这个脚本就可以在提交之前检查是否有问题。\nJenkinsfile successfully validated. Jenkins Pipeline Linter Connector 插件 # 第二种方式就是通用了，只要是声明式流水线，就可以使用这个插件去验证是否存在语法错误问题。\n安装插件 # 在 VSCode 插件里搜索 Jenkins Pipeline Linter Connector\n配置插件 # 打开 File -\u0026gt; Preferences -\u0026gt; Settings -\u0026gt; Extensions， 找到 Jenkins Pipeline Linter Connector，参考如下配置。\n运行插件 # 右键 -\u0026gt; Command Palette -\u0026gt; Validate Jenkinsfile\n或\n执行快捷键 Shift + Alt + V\n执行效果 # 总结 # 如果使用 VSCode 作为开发工具，推荐使用 Jenkins Pipeline Linter Connector 插件。\n如果是 Jenkins Shared Libraries 仓库不妨可以创建一个 shell 脚本，通过执行脚本来进行校验。\n当然，如果只是简单的使用 Jenkinfile 也可以在 Jenkins Web Pipeline 页面里编写，那里自带语法检查。\n如果您还有别的方式，欢迎留言告诉我。\n","date":"2020-05-23","externalUrl":null,"permalink":"/posts/jenkins-pipeline-linter-connector/","section":"Posts","summary":"本文介绍了两种方法来确保在提交 Jenkins Pipeline 前没有语法错误：使用 REST API 进行语法校验和使用 VSCode 插件进行语法检查。","title":"如何确保在提交 Jenkins Pipeline 前没有语法错误","type":"posts"},{"content":"","date":"2020-05-17","externalUrl":null,"permalink":"/tags/nightwatch/","section":"标签","summary":"","title":"Nightwatch","type":"tags"},{"content":"Nightwatch js 是我之前写自动化测试用例使用了很长一段时间的测试框架，我当时的使用 v0.9 版本并且对使用和 API 进行了翻译。作为一名前测试工程师，对于自动化的知识不能不更新下自己的知识库，一转眼 Nightwatch 1.3 版本已经发布了，可以看到它在 GitHub 上的使用和关注度还是很高的。\nNightwarch.js 是一个端到端的基于 Node.js 使用 W3C Webdriver （以前是 Selenium ）的自动化测试框架。它是一个完整的集成解决方案，用于 web 应用程序和网站的端到端测试，以及 Node.js 单元测试和集成测试。\n查看了一下 Nightwatch 的发布历史（https://github.com/nightwatchjs/nightwatch/releases），可以看到这期间修复了不少 Bug，而且在 v1.3 ​版本还新增一些新的功能。\n新增 BDD describe Interface - 可以同时运行以 BDD 描述和导出接口编写的测试，无需其他配置。\n新增 assert.not 断言\n// 原来这么写 browser.assert.elementNotPresent() // 现在可以这么写 browser.assert.not.elementPresent(\u0026#39;.not_present\u0026#39;) 新增一些 APIs - 比如 getElementProperty, domPropertyContains, domPropertyEquals, .property\n新增了 CLI 选项\n--headless - 以无头模式启动浏览器（Chrome或Firefox） --timeout - 设置断言失败之前重试断言的全局超时 如果要从 v1.0 之前的版本升级，参阅如下升级指南。\n升级和启动 # 从 NPM 安装 Nightwatch\nnpm install nightwatch --save-dev 安装浏览器驱动程序 # Geckodriver（Firefox） # Geckodriver 是用于驱动 Mozilla Firefox 浏览器的 WebDriver 服务。\nnpm install geckodriver --save-dev Chromedriver # Chromedriver 是用于驱动 Google Chrome 浏览器的 WebDriver 服务。\nnpm install chromedriver --save-dev 或用一行安装所有内容：\nnpm i nightwatch geckodriver chromedriver --save-dev 运行演示测试 # Nightwatch 带有一个 example 文件夹，其中包含一些示例测试。\n下面将运行一个基本测试，该测试打开搜索引擎 Ecosia.org，搜索 “nightwatch” 一词，并验证术语 “第一个结果” 是否是 Nightwatch.js 网站。\n./node_modules/.bin/nightwatch node_modules/nightwatch/examples/tests/ecosia.js Windows 用户可能需要运行节点 node node_modules/.bin/nightwatch\n手动下载浏览器驱动程序 # Nightwatch 使用兼容 WebDriver 的服务器来控制浏览器。 WebDriver 是 W3C 规范和行业标准，提供了与浏览器进行交互的平台和 HTTP 协议。\nNightwatch 包括对自动管理以下服务的支持：\nChromeDriver # 针对 Chrome 浏览器运行测试 下载网址 https://sites.google.com/a/chromium.org/chromedriver/downloads。 从版本 75 开始，Chromedriver 默认启用 W3C Webdriver 协议。如果你现在想坚持使用 JSONWire，请调整 chromeOptions：\ndesiredCapabilities : { browserName : \u0026#39;chrome\u0026#39;, chromeOptions: { w3c: false } } GeckoDriver # 针对 Mozilla Firefox 浏览器运行测试。下载网址：https://github.com/mozilla/geckodriver/releases.\nSelenium Standalone Server # 在一个地方管理多个浏览器配置，还可以利用 Selenium Grid 服务 可以从 Selenium 发布页面下载 selenium 服务器 jar 文件 selenium-server-standalone-3.x.x.jar：https://selenium-release.storage.googleapis.com/index.html 重要的是要注意，尽管较早的 Nightwatch 版本（v0.9 及更低版本）需要 Selenium Server，但从 1.0 版本开始不再需要 Selenium。\n特定的 WebDriver 设置指南可在 Docs 网站上找到。旧版 Selenium 驱动程序安装指南以及调试说明可以在 Wiki 上找到。\n例子 # 示例文件夹中包含示例测试，这些示例演示了多个 Nightwatch 功能的用法。\n你还可以查看 nightwatch-website-tests (https://github.com/nightwatchjs/nightwatch-website-tests) 存储库，例如针对 nightwatchjs.org (https://nightwatchjs.org/) 网站的测试。\nNightwatch 单元测试 # Nightwatch 的测试是使用 Mocha 编写的。\n1.克隆项目\ngit clone https://github.com/nightwatchjs/nightwatch.git cd nightwatch npm install 2.运行测试\n要运行完整的测试套件：\nnpm test 要检查测试范围，请运行以下命令：\nnpm run mocha-coverage 然后在浏览器中打开生成的 coverage/index.html 文件。\nNightwatch 使用示例 # 以下是我写的一个使用 NightwatchJS 对 Nightwatch 官网 https://nightwatchjs.org 进行测试的一个测试示例（已经升级到 v1.3 版本）以展示在实际项目中使用 Nightwatchjs 如何组织目录结构，区别配置全局和本地环境。\n示例仓库的 GitHub 地址是 https://github.com/nightwatchjs-cn/nightwatch-e2e，欢迎 Star 和 Fork。\n","date":"2020-05-17","externalUrl":null,"permalink":"/posts/nightwatch-v1-3/","section":"Posts","summary":"本文介绍了 Nightwatch.js v1.3 版本的新特性和改进，包括 BDD 接口、断言方法和浏览器驱动程序的更新。","title":"Nightwatch v1.3 介绍","type":"posts"},{"content":"","date":"2020-05-09","externalUrl":null,"permalink":"/tags/jmeter/","section":"标签","summary":"","title":"JMeter","type":"tags"},{"content":" 录制 JMeter 脚本 # 使用 JMeter 的 HTTP(S) Test Script Recorder，参考官方文档：\nhttps://jmeter.apache.org/usermanual/jmeter_proxy_step_by_step.html\n运行 JMeter 脚本 # GUI 模式调试\n在 GUI 模式下调试录制的脚本，直到无报错为止。\n非 GUI 模式运行（推荐）\njmeter -n -t ..\\extras\\Test.jmx -l Test.jtl 在 Jenkins 中运行 JMeter 脚本 # 需要的工具 # JMeter - Web Request Load Testing\nJMeter Plugins\nServerAgent-2.2.1（PerfMon Agent，用于性能监控） 测试环境 # 两台虚拟机：\n被测系统（System under test） JMeter 执行机（同时作为 Jenkins 服务器） 实现步骤 # 1. 开发测试脚本 # 录制脚本可参考官方文档： https://jmeter.apache.org/usermanual/jmeter_proxy_step_by_step.html\n2. 在 Jenkins 中创建任务运行 JMeter 脚本 # 新建 Freestyle Project\n添加构建步骤 → Execute Windows batch command 清理上次测试结果：\ncd C:\\Users\\peter\\.jenkins\\jobs\\TEST-122 Upload large data\\workspace del /Q \u0026#34;jtl\u0026#34;\\* del /Q \u0026#34;PerfMon Metrics Collector\u0026#34;\\* 添加构建步骤 → Execute Windows batch command 执行 JMeter 脚本：\njmeter -n -t script/UploadLargeData-1.jmx -l jtl/UploadLargeData-1.jtl 配置 Jenkins 邮件通知（Extended E-mail Notification）：\nSMTP Server: smtp.gmail.com Recipient List: xianpeng.shen@gmail.com Content Type: HTML 触发器：Always 生成测试报告 # 在 JMeter 中添加监听器： jp@gc - PerfMon Metrics Collector → 选择 .jtl 文件 → 右键导出为 CSV。\n分析测试结果 # 测试场景 # 使用 1、5、10、20、30（50） 个并发用户进行压测，记录各组结果。\n常用指标解释：\nSample (label)：虚拟用户请求数 Average：平均响应时间 Median：中位数 % Line：90%、95%、99% 分位数 Min/Max：最短/最长响应时间 Error %：失败率 Throughput：每秒请求数（越大越好） KB/Sec：吞吐量（KB/s） 示例结果：\nUser # Samples Average Median 90% Line 95% Line Min Max Error % Throughput Received Send KB/sec 1 31 348 345 452 517 5 773 0.00% 2.85 2.50 0 5 155 1166 1164 1414 1602 9 1821 0.00% 4.26 3.73 0 10 310 2275 2299 2687 2954 20 4104 0.00% 4.38 3.84 0 20 620 4479 4620 5113 6152 39 6571 0.00% 4.42 3.88 0 30 930 6652 6899 7488 9552 4 10060 0.00% 4.47 3.91 0 可视化分析 # ","date":"2020-05-09","externalUrl":null,"permalink":"/posts/jmeter-performance-testing/","section":"Posts","summary":"介绍如何使用 JMeter 进行性能测试，包括录制脚本、在 GUI 和非 GUI 模式下运行测试，以及在 Jenkins 中实现自动化测试。","title":"使用 JMeter 进行性能测试","type":"posts"},{"content":" 背景 # 最近我们团队需要将一些示例和例子从内部的 Bitbucket 同步到 GitHub。我了解 GitHub 可以创建公共的或是私人的仓库，但我们需要保持以下两点\n只分享我们想给客户分享的内容 不改变当前的工作流程，即继续使用 Bitbucket 因此我们需要在 GitHub 上创建相应的仓库，然后将内部 Bitbucket 仓库中对应的 master 分支定期的通过 CI job 同步到 BitHub 上去。\n分支策略 # 首先，需要对 Bitbucket 进行分支权限设置\nmaster 分支只允许通过 Pull Request 来进行修改 Pull Request 默认的 reviewer 至少需要一人，并且只有同意状态才允许合并 其次，为了方便产品、售后等人员使用，简化分支策略如下\n从 master 分支上创建 feature 或是 bugfix 分支（取决于你的修改目的） 然后将你的更改提交到自己的 feature 或 bugfix 分支 在你自己的分支通过测试后，提交 Pull Request 到 master 分支 当 reviewer 同意状态，才能进行合并进入到 master 分支 Jenkins Pipeline # 基于这样的工作不是特别的频繁，也为了方便维护 Jenkins Pipeline 的简单和易于维护，我没有在需要同步的每个仓库里添加 Jenkinsfile 或在 Bitbucket 里添加 webhooks。有以下几点好处：\n只创建一个 Jenkins Job，用一个 Jenkinsfile 满足所有仓库的同步 减少了冗余的 Jenkinsfile 的代码，修改时只需更维护一个文件 不需要在每个仓库里添加一个 Jenkinsfile，更纯粹的展示示例，避免给非 IT 人员造成困扰 不足之处，不能通过 SCM 来触发构建，如果想通过 webhooks 来触发，有的公司需要申请权限来添加 webhooks 比较麻烦；另外可能无法区分从哪个仓库发来的请求，实现指定仓库的同步。\n因此如果不是特别频繁的需要同步，提供手动或是定时同步即可。\n// 这个 Jenkinsfile 是用来将 Bitbucket 仓库的 master 分支同步到 GitHub 仓库的 master 分支 // This Jenkinsfile is used to synchronize Bitbucket repositories master branches to GitHub repositories master branches. @Library(\u0026#39;jenkins-shared-library@develop\u0026#39;) _ def email = new org.cicd.email() pipeline { agent { label \u0026#34;main-slave\u0026#34; } parameters { booleanParam(defaultValue: false, name: \u0026#39;git-repo-win\u0026#39;, summary: \u0026#39;Sync internal git-repo-win master branch with external git-repo-win on GitHub\u0026#39;) booleanParam(defaultValue: true, name: \u0026#39;git-repo-lin\u0026#39;, summary: \u0026#39;Sync internal git-repo-lin master branch with external git-repo-lin on GitHub\u0026#39;) booleanParam(defaultValue: false, name: \u0026#39;git-repo-aix\u0026#39;, summary: \u0026#39;Sync internal git-repo-aix master branch with external git-repo-aix on GitHub\u0026#39;) booleanParam(defaultValue: false, name: \u0026#39;git-repo-sol\u0026#39;, summary: \u0026#39;Sync internal git-repo-sol master branch with external git-repo-sol on GitHub\u0026#39;) } options { timestamps() buildDiscarder(logRotator(numToKeepStr:\u0026#39;50\u0026#39;)) } stages { stage(\u0026#34;Synchronous master branch\u0026#34;){ steps{ script { try { params.each { key, value -\u0026gt; def repoName = \u0026#34;$key\u0026#34; if ( value == true) { echo \u0026#34;Start synchronizing $key Bitbucket repository.\u0026#34; sh \u0026#34;\u0026#34;\u0026#34; rm -rf ${repoName} return_status=0 git clone -b master ssh://git@git.your-company.com:7999/~xshen/${repoName}.git cd ${repoName} git config user.name \u0026#34;Sync Bot\u0026#34; git config user.email \u0026#34;bot@your-company.com\u0026#34; git remote add github git@github.com:shenxianpeng/${repoName}.git git push -u github master return_status=\u0026#34;\\$?\u0026#34; if [ \\$return_status -eq 0 ] ; then echo \u0026#34;Synchronize ${repoName} from Bitbucket to GitHub success.\u0026#34; cd .. rm -rf ${repoName} exit 0 else echo \u0026#34;Synchronize ${repoName} from Bitbucket to GitHub failed.\u0026#34; exit 1 fi\u0026#34;\u0026#34;\u0026#34; } else { echo \u0026#34;${repoName} parameter value is $value, skip it.\u0026#34; } } cleanWs() } catch (error) { echo \u0026#34;Some error occurs during synchronizing $key process.\u0026#34; } finally { email.Send(currentBuild.currentResult, env.CHANGE_AUTHOR_EMAIL) } } } } } } 以上的 Jenkinsfile 的主要关键点是这句 params.each { key, value -\u0026gt; }，可以通过对构建时选择参数的进行判断，如果构建时参数已勾选，则会执行同步脚本；否则跳过同步脚本，循环到下一个参数进行判断，这样就实现了可以对指定仓库进行同步。\nBackground # Recently our team need to share code from internal Bitbucket to external GitHub. I know GitHub can create private and public repository, but we have these points want to keep.\nonly share the code what we want to share not change current work process, continue use Bitbucket. So we have created corresponding repositories in the internal Bitbucket, and the master branches of these repositories will periodically synchronize with the master branches of corresponding repositories on GitHub via Jenkins job.\nBranch Strategy # Then the work process will be like\nCreate a feature or bugfix branch (it depends on the purpose of your modification).\nCommit changes to your feature/bugfix branch.\nPlease pass your feature/bugfix branch test first then create a Pull Request from your branch to master branch, at least one reviewer is required by default.\nAfter the reviewer approved, you or reviewer could merge the Pull Request, then the changes will be added to the master branch.\nTiming trigger CI job will sync code from internal repositories master branch to GitHub master branch by default. also support manual trigger.\nJenkins Job # Base on this work is not very frequency, so I want make the Jenkins job simple and easy to maintain, so I don\u0026rsquo;t create every Jenkinsfile for every Bitbucket repositories.\nPros\nOnly one Jenkinsfile for all Bitbucket repositories. Less duplicate code, less need to change when maintenance. Don\u0026rsquo;t need to add Jenkinsfile into very Bitbucket repositories. Cons\nCan not support SCM trigger, in my view this need add Jenkinsfile into repository. The main part for this Jenkinsfile is below, use this function params.each { key, value -\u0026gt; } can by passing in parameters when start Jenkins build.\n","date":"2020-05-05","externalUrl":null,"permalink":"/posts/sync-from-bitbucket-to-github/","section":"Posts","summary":"介绍如何通过 Jenkins 将 Bitbucket 仓库的 master 分支同步到 GitHub。","title":"如何将 Bitbucket 仓库同步到 GitHub","type":"posts"},{"content":" 问题 # Jenkins 的 multi-branch pipeline 想必很多人已经在用了，使用这种类型的 Jenkins Job 最显著的作用就是可以对 Git 仓库里的任何分支和任何 Pull Request（以下简写为 PR）进行构建。\n在做 Jenkins 与 Bitbucket 的集成时，需要安装插件：Bitbucket Branch Source，可以通过该插件在 Jenkins 里进行 webhook 的配置。这种方式对于没有 Bitbucket 仓库的管理权限，CI/CD 暂且处于变更比较频繁的阶段，不想麻烦的去申请添加 webhook 的同学来说是非常友好的，就是可以不用通过管理员在 Bitbucket 设置里添加 webhook 也可以实现创建 PR 后触发 Jenkins 构建。\n但我最近遭遇了两次：在创建 PR 后没有触发 Jenkins 自动构建，查了 Jenkins 和 Bitbucket Branch Source 插件的配置，并没有任何改动，也各种 Google 之后也没有找到相应的解决办法（如果有遇到此情况的小伙伴欢迎一起交流）。\n那既然这条路不稳定，不好走，那就走一条可以走通的路、直接的硬路，即在 Bitbucket 对应的仓库中添加 webhooks。\n原理 # 通过设置 Webhook 事件，可以监听 git push，创建 Pull Request 等事件，在这些事件发生时自动触发 Jenkins 扫描，从而 Jenkins 可以获取到最新的创建（或删除）的分支（或Pull Request），从而自动构建Jenkins Job。\n配置 # 在申请添加 webhooks 之前，我先在个人的私人仓库下，创建了测试仓库对 webhook 进行了测试，在经过反复的测试，觉得没有问题后，将相应的配置通过管理员添加到对应的 Repository 中。如下示例：\nWebhook name: test-multibranch Webhook URL: http://localhost:8080/multibranch-webhook-trigger/invoke?token=test-multibranch Test connection: 返回 200, 连接测试通过。 Events: Repository: Push Pull Request: Opened, Merged, Declined, Deleted. Active: enable 放两张截图方便参考\nBitbucket webhooks 设置\nJenkins multi-branch pipeline 设置\n效果 # 通过以上的设置，开发人员在每次创建 PR 都会立即触发 Jenkins 构建，显著的变化有两个：\n比以前依赖插件响应速度要快很多，之前的响应速度一般在 1~2 分支才能触发构建 稳定程度大大提高，目前为止没有再出现创建 PR 之后没有触发 Jenkins 构建的情况 最终的与 Jenkins 的集成效果是这样的：\n当创建 (Opened) 一个 Pull Request 时，会自动在 Jenkins 上创建相应的 Pull Request 任务（比如 PR-123）并开始构建。 当合并 (Merged) 这个 Pull Request 时，会自动删除 Jenkins 中的 PR-123 任务。 当拒绝 (Declined) 这个 Pull Request 时，会自动删除 Jenkins 中的 PR-123 任务。 当删除 (Deleted) 这个 Pull Request 时，会自动删除 Jenkins 中的 PR-123 任务。 已经 Merge 的分支，会显示已经划掉了，灰色的，这种分支不可以再进行执行构建；而 develop 和 master 分支则可以继续手动或自动构建。\n已经 Merge 了的 Pull Request 同样显示为已经划掉了，灰色的。PR-12, PR-13, PR-14 可以继续手动或自动构建。\n补充 # 这里说一下我为什么没有添加 Modified 事件。此前我是添加了 Modified 事件，我发现一些处于待合并的分支不知不觉被 webhook 触发了很多次，由于我们的全平台构建、扫描、以及测试需要至少 2~3 小时的时间，当处于待合并的 Pull Request 过多时，对构建资源的占用可能会是全天的。\n我想应该是其中的哪个事件的特性所导致引起的，果不其然，这时候才注意到 Modified 事件的这段解释：A pull request\u0026rsquo;s description, title, or target branch is changed. 从解释里可以看到 Modified 事件包含修改 PR 描述、标题、还包括了目标分支的变更都会触发构建。\n其实这个 Modified 事件的这个特性本身是特别好的，可以不断的将已经合并到目标分支的代码拉取到源分支进行构建，保证源分支的代码一直是与最新的代码进行集成、构建和测试，这样集成的结果才是最准确可靠的。但只是不适合目前的我们，因此暂且没有开启 Modified 事件。\n这里没有添加其他 webhook 事件，比如对于主分支的触发事件，这个可以根据具体需要进行添加。如果不是那么频繁，每日构建满足需求，那么在 Pipeline 里添加一个 trigger 就可以了。\n","date":"2020-04-28","externalUrl":null,"permalink":"/posts/bitbucket-webhooks/","section":"Posts","summary":"本文介绍如何在 Jenkins 中配置 Bitbucket Webhooks，以便在创建 Pull Request 时自动触发 Jenkins 构建。","title":"Jenkins 与 Bitbucket Webhooks 的配置和使用","type":"posts"},{"content":"","date":"2020-04-20","externalUrl":null,"permalink":"/tags/pipeline/","section":"标签","summary":"","title":"Pipeline","type":"tags"},{"content":"这是我第二次在使用 Jenkins 声明式流水线的时候遇到了这个问题，第一次遇到这个问题的时候是在一个 Pipeline 里大概写到 600 多行时候遇到如下错误\norg.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed: General error during class generation: Method code too large! java.lang.RuntimeException: Method code too large! at groovyjarjarasm.asm.MethodWriter.a(Unknown Source) [...] 当时我也使用了 Jenkins Shared Libraries，但那时候的代码组织的并不是很好，有不少步骤还没来得及单独抽离出来作为单独的方法。为了解决这个问题，经过一番重构，我将原来的 600 多行的 Pipeline 变成了现在的 300 多行，很不巧，随着继续添加功能，最近又遇到了这个问题。\n出现这个问题的原因是 Jenkins 将整个声明性管道放入单个方法中，并且在一定大小下，JVM 因 java.lang .RuntimeException 失败：方法代码太大！看来我还是有什么方法超过了 64k。\nJenkins JIRA 上已经有了该问题的单子，但目前为止还是尚未解决。针对这个问题目前有三种方案，但他们都有各自的利弊。\n方法1：将步骤放到管道外的方法中 # 自2017年中以来，你可以在管道的末尾声明一个方法，然后在声明性管道中调用它即可。 这样，我们可以达到与共享库相同的效果，但是避免了维护开销。\npipeline { agent any stages { stage(\u0026#39;Test\u0026#39;) { steps { whateverFunction() } } } } void whateverFunction() { sh \u0026#39;ls /\u0026#39; } 优点 缺点 没有额外的维护费用 这个解决方案不知道会不会一直有效 所有的功能都反映在Jenkinsfile中 有的方法在多个Jenkinsfile里用到时，这种方法还是会写很多重复的代码 方法2：从声明式迁移到脚本式管道 # 最后，我们可以迁移到脚本化的管道。有了它，我们就有了所有的自由。但是也就会失去我们最初决定使用声明式管道的原因。有了专用的DSL，就很容易理解管道是如何工作的\n优点 缺点 完全没有限制 需要比较大的重构 更容易出错 可能需要更多的代码来实现相同的功能 方法3：使用 Shared Libraries # 我当前使用的就是 Jenkins Shared Libraries，有一个共享库来执行一些复杂的步骤。共享库目前看来使用的非常广泛，尤其是在维护一些比较大型的、复杂的项目里用的很多。\n最终我的解决办法是进一步缩减 Pipeline 里的代码，这里我也用到 方法1 的解决方案，将一些步骤提到 Pipeline {} 括号的外面，尤其是那些重复调用的​步骤。​\n优点 缺点 减少了大量重复的代码 任何一个修改都会影响到所有的引用，要测试好了再将变更放到引用分支里 可以分块使用 不熟悉的话很难理解一个步骤到底是做什么的 生成的Jenkinsfile将易于阅读 结论 # 方法1：对于单一的 Repository 的集成，可以快速实现，大多数人上手会很快。 方法2：脚本化提供了很少的限制，适合熟悉 Java，Groovy 的高级用户和有更复杂需求的人使用。 方法3：对于企业级项目，拥有很多 Repositories，需要进行大量集成，并且想了解共享库，推荐使用此方法。\n","date":"2020-04-20","externalUrl":null,"permalink":"/posts/jenkins-troubleshooting/","section":"Posts","summary":"本文介绍了三种方法来解决 Jenkins 声明式流水线中出现的 \u0026ldquo;Method code too large\u0026rdquo; 异常，包括将步骤放到管道外的方法、从声明式迁移到脚本式管道以及使用 Shared Libraries。","title":"三种方法解决 Jenkins 声明式流水线 Exception Method code too large !","type":"posts"},{"content":"\u0026ldquo;Quality at Speed\u0026rdquo; 是软件开发中的新规范。\n企业正在朝着 DevOps 方法论和敏捷文化迈进，以加快交付速度并确保产品质量。在 DevOps 中，连续和自动化的交付周期使快速可靠的交付成为可能的基础。\n这导致我们需要适当的持续集成和持续交付（CI/CD）工具。 一个好的 CI/CD 工具可以利用团队当前的工作流程，以最佳利用自动化功能并创建可靠的 CI/CD 管道为团队发展提供所需的动力。\n随着市场上大量 CI/CD 工具的出现，团队可能难以做出艰难的决定来挑选合适的工具。该列表包含市场上最好的 14 种 CI/CD 工具及其主要特性，使你和团队在选择过程中更加轻松。\n以下罗列出了目前市场上最流行的 14 种最佳 CI/CD 工具，希望该列表为你在选择 CI/CD 前提供了足够的信息，更多详细信息你也可以查看官网做更深入的了解。最终结合你的需求以及现有基础架构以及未来潜力和改进的空间是将影响你最终选择的因素，帮助你选择到最适合你的规格的 CI/CD 软件。\nJenkins # Jenkins 是一个开源自动化服务器，在其中进行集中构建和持续集成。它是一个独立的基于 Java 的程序，带有 Windows，macOS，Unix 的操作系统的软件包。Jenkins 支持软件开发项目的构建，部署和自动化，以及成百上千的插件来满足你的需求。它是市场上最具影响力的 CI/CD 工具之一。\nJenkins 主要特性：\n易于在各种操作系统上安装和升级 简单易用的界面 可通过社区提供的巨大插件资源进行扩展 在用户界面中轻松配置环境 支持主从架构的分布式构建 根据表达式构建时间表 在预构建步骤中支持 Shell 和 Windows 命令执行 支持有关构建状态的通知 许可：免费。Jenkins 是一个拥有活跃社区的开源工具。\n主页：https://jenkins.io/\nCircleCI # CircleCI 是一种 CI/CD 工具，支持快速的软件开发和发布。CircleCI 允许从代码构建，测试到部署的整个用户管道自动化。\n你可以将 CircleCI 与 GitHub，GitHub Enterprise 和 Bitbucket 集成，以在提交新代码行时创建内部版本。CircleCI 还可以通过云托管选项托管持续集成，或在私有基础架构的防火墙后面运行。\nCircleCI 主要特性:\n与 Bitbucket，GitHub 和 GitHub Enterprise 集成 使用容器或虚拟机运行构建 简易调试 自动并行化 快速测试 个性化的电子邮件和IM通知 连续和特定于分支机构的部署 高度可定制 自动合并和自定义命令以上传软件包 快速设置和无限构建 许可：Linux 计划从选择不运行任何并行操作开始。开源项目获得了另外三个免费容器。在注册期间，将看到价格以决定所需的计划。\n主页： https://circleci.com/\nTeamCity # TeamCity 是 JetBrains 的构建管理和持续集成服务器。\nTeamCity 是一个持续集成工具，可帮助构建和部署不同类型的项目。 TeamCity 在 Java 环境中运行，并与 Visual Studio 和 IDE 集成。该工具可以安装在 Windows 和 Linux 服务器上，支持 .NET 和开放堆栈项目。\nTeamCity 2019.1 提供了新的UI和本机 GitLab 集成。它还支持 GitLab 和 Bitbucket 服务器拉取请求。该版本包括基于令牌的身份验证，检测，Go测试报告以及 AWS Spot Fleet 请求。\nTeamCity主要特性:\n提供多种方式将父项目的设置和配置重用到子项目 在不同环境下同时运行并行构建 启用运行历史记录构建，查看测试历史记录报告，固定，标记以及将构建添加到收藏夹 易于定制，交互和扩展服务器 保持CI服务器正常运行 灵活的用户管理，用户角色分配，将用户分组，不同的用户身份验证方式以及带有所有用户操作的日志，以透明化服务器上所有活动 许可：TeamCity 是具有免费和专有许可证的商业工具。\n主页： https://www.jetbrains.com/teamcity/\nBamboo # Bamboo 是一个持续集成服务器，可自动执行软件应用程序版本的管理，从而创建了持续交付管道。Bamboo 涵盖了构建和功能测试，分配版本，标记发行版，在生产中部署和激活新版本。\nBamboo主要特性:\n支持多达 100 个远程构建代理 并行运行批次测试并快速获得反馈 创建图像并推送到注册表 每个环境的权限，使开发人员和测试人员可以在生产保持锁定状态的情况下按需部署到他们的环境中 在 Git，Mercurial，SVN Repos 中检测新分支，并将主线的CI方案自动应用于它们 触发器基于在存储库中检测到的更改构建。 推送来自 Bitbucket 的通知，已设置的时间表，另一个构建的完成或其任何组合。 许可：Bamboo 定价层基于代理（Slave）而不是用户，代理越多，花费越多。\n主页：https://www.atlassian.com/software/bamboo\nGitLab # GitLab 是一套用于管理软件开发生命周期各个方面的工具。 核心产品是基于 Web 的 Git 存储库管理器，具有问题跟踪，分析和 Wiki 等功能。\nGitLab 允许你在每次提交或推送时触发构建，运行测试和部署代码。你可以在虚拟机，Docker 容器或另一台服务器上构建作业。\nGitLab主要特性:\n通过分支工具查看，创建和管理代码以及项目数据 通过单个分布式版本控制系统设计，开发和管理代码和项目数据，从而实现业务价值的快速迭代和交付 提供真实性和可伸缩性的单一来源，以便在项目和代码上进行协作 通过自动化源代码的构建，集成和验证，帮助交付团队完全接受CI。 提供容器扫描，静态应用程序安全测试（SAST），动态应用程序安全测试（DAST）和依赖项扫描，以提供安全的应用程序以及许可证合规性 帮助自动化并缩短发布和交付应用程序的时间 许可：GitLab 是一个商业工具和免费软件包。它提供了在 GitLab 或你的本地实例和/或公共云上托管 SaaS 的功能。\n主页：https://about.gitlab.com/\nBuddy # Buddy 是一个 CI/CD 软件，它使用 GitHub，Bitbucket 和 GitLab 的代码构建，测试，部署网站和应用程序。它使用具有预安装语言和框架的 Docker 容器以及 DevOps 来监视和通知操作，并以此为基础进行构建。\nBuddy主要特性:\n易于将基于 Docker 的映像自定义为测试环境 智能变更检测，最新的缓存，并行性和全面的优化 创建，定制和重用构建和测试环境 普通和加密，固定和可设置范围：工作空间，项目，管道，操作 与 Elastic，MariaDB，Memcached，Mongo，PostgreSQL，RabbitMQ，Redis，Selenium Chrome 和 Firefox 关联的服务 实时监控进度和日志，无限历史记录 使用模板进行工作流管理，以克隆，导出和导入管道 一流的Git支持和集成 许可：Buddy 是免费的商业工具。\n主页：https://buddy.works/\nTravis CI # Travis CI 是用于构建和测试项目的CI服务。Travis CI 自动检测新提交并推送到 GitHub 存储库的提交。每次提交新代码后，Travis CI 都会构建项目并相应地运行测试。\n该工具支持许多构建配置和语言，例如 Node，PHP，Python，Java，Perl 等。\nTravis 主要特性:\n快速设置 GitHub项目监控的实时构建视图 拉取请求支持 部署到多个云服务 预装的数据库服务 通过构建时自动部署 为每个版本清理虚拟机 支持 macOS，Linux 和 iOS 支持多种语言，例如 Android，C，C＃，C ++，Java，JavaScript（带有Node.js），Perl，PHP，Python，R，Ruby 等。 许可：Travis CI 是一项托管的 CI/CD 服务。私人项目可以在 travis-ci.com 上进行收费测试。可以在 travis-ci.org 上免费应用开源项目。\n主页：https://travis-ci.com\nCodeship # Codeship 是一个托管平台，可多次支持早期和自动发布软件。通过优化测试和发布流程，它可以帮助软件公司更快地开发更好的产品。\nCodeship 主要特性:\n与所选的任何工具，服务和云环境集成 易于使用。提供快速而全面的开发人员支持。 借助CodeShip的交钥匙环境和简单的UI，使构建和部署工作更快 选择AWS实例大小，CPU和内存的选项 通过通知中心为组织和团队成员设置团队和权限 无缝的第三方集成，智能通知管理和项目仪表板，可提供有关项目及其运行状况的高级概述。 许可：每月最多免费使用100个版本，无限版本从$49/月开始。你可以为更大的实例大小购买更多的并发构建或更多的并行管道。\n主页： https://codeship.com/\nGoCD # GoCD 来自 ThoughtWorks，是一个开放源代码工具，用于构建和发布支持 CI/CD 上的现代基础结构的软件。\n轻松配置相关性以实现快速反馈和按需部署 促进可信构件：每个管道实例都锚定到特定的变更集 提供对端到端工作流程的控制，一目了然地跟踪从提交到部署的更改 容易看到上游和下游 随时部署任何版本 允许将任何已知的良好版本的应用程序部署到你喜欢的任何位置 通过比较内部版本功能获得用于任何部署的简单物料清单 通过 GoCD 模板系统重用管道配置，使配置保持整洁 已经有许多插件 许可：免费和开源\n主页：https://www.gocd.org/\nWercker # 对于正在使用或正在考虑基于 Docker 启动新项目的开发人员，Wercker 可能是一个合适的选择。Wercker 支持组织及其开发团队使用 CI/CD，微服务和 Docker。\n2017 年 4 月 17 日，甲骨文宣布已签署最终协议收购 Wercker。\nWercker 主要特性:\nGit 集成，包括 GitHub，Bitbucket，GitLab 和版本控制 使用 Wercker CLI 在本地复制 SaaS 环境，这有助于在部署之前调试和测试管道 支持 Wercker 的 Docker 集成以构建最少的容器并使尺寸可管理 Walterbot – Wercker 中的聊天机器人 – 允许你与通知交互以更新构建状态 环境变量有助于使敏感信息远离存储库 Wercker 利用关键安全功能（包括源代码保护）来关闭测试日志，受保护的环境变量以及用户和项目的可自定义权限 许可：甲骨文在收购后未提供 Wercker 的价格信息。\n主页：https://app.wercker.com\nSemaphore # Semaphore 是一项托管的 CI/CD 服务，用于测试和部署软件项目。 Semaphore 通过基于拉取请求的开发过程来建立 CI/CD 标准。\nSemaphore 主要特性:\n与 GitHub 集成 自动执行任何连续交付流程 在最快的 CI/CD 平台上运行 自动缩放你的项目，以便你仅需支付使用费用 本机 Docker 支持。测试和部署基于 Docker 的应用程序 提供 Booster –一种功能，用于通过自动并行化Ruby项目的构建来减少测试套件的运行时间 许可：灵活。使用传统的CI服务，你会受到计划容量的限制。同时，Semaphore 2.0 将根据你团队的实际需求进行扩展，因此你无需使用该工具就不必付费\n主页：https://semaphoreci.com/\nNevercode # Nevercode 支持移动应用程序的 CI/CD。它有助于更​​快地构建，测试和发布本机和跨平台应用程序。\nNevercode 主要特性:\n自动配置和设置 测试自动化：单元和UI测试，代码分析，真实设备测试，测试并行化 自动发布：iTunes Connect，Google Play，Crashlytics，T​​estFairy，HockeyApp 你的构建和测试状态的详细概述 许可：灵活。针对不同需求进行持续集成的不同计划。你可以从标准计划中选择，也可以请求根据自己的需求量身定制的计划。\n主页：https://nevercode.io/\nSpinnaker # Spinnaker 是一个多云连续交付平台，支持在不同的云提供商之间发布和部署软件更改，包括 AWS EC2，Kubernetes，Google Compute Engine，Google Kubernetes Engine，Google App Engine 等。\nSpinnaker主要特性:\n创建部署管道，以运行集成和系统测试，旋转服务器组和降低服务器组以及监视部署。通过 Git 事件，Jenkins，Travis CI，Docker，cron 或其他 Spinnaker 管道触发管道 创建和部署不可变映像，以实现更快的部署，更轻松的回滚以及消除难以调试的配置漂移问题 使用它们的指标进行金丝雀分析，将你的发行版与诸如 Datadog，Prometheus，Stackdriver 或 SignalFx 的监视服务相关联 使用Halyard – Spinnaker的CLI管理工具安装，配置和更新你的 Spinnaker 实例 设置电子邮件，Slack，HipChat 或 SMS 的事件通知（通过 Twilio） 许可：开源\n主页：https://www.spinnaker.io/\nBuildbot # Buildbot 是一个基于 Python 的 CI 框架，可自动执行编译和测试周期以验证代码更改，然后在每次更改后自动重建并测试树。因此，可以快速查明构建问题。\nBuildbot 主要特性:\n自动化构建系统，应用程序部署以及复杂软件发布过程的管理 支持跨多个平台的分布式并行执行，与版本控制系统的灵活集成，广泛的状态报告 在各种从属平台上运行构建 任意构建过程并使用 C 和 Python 处理项目 最低主机要求：Python 和 Twisted 注意：Buildbot 将停止支持 Python 2.7，并需要迁移到 Python 3。 许可：开源\n主页：https://buildbot.net/\n英文原文\n","date":"2020-03-29","externalUrl":null,"permalink":"/posts/ci-cd-tools/","section":"Posts","summary":"本文列出了市场上最流行的 14 种 CI/CD 工具，包括 Jenkins、CircleCI、TeamCity 等，并介绍了它们的主要特性和使用场景。","title":"2021 年务必知道的最好用的 14 款 CI/CD 工具","type":"posts"},{"content":"","date":"2020-03-29","externalUrl":null,"permalink":"/tags/cicd/","section":"标签","summary":"","title":"CICD","type":"tags"},{"content":" DevOps术语和定义 # 什么是DevOps\n用最简单的术语来说，DevOps是产品开发过程中开发（Dev）和运营（Ops）团队之间的灰色区域。 DevOps是一种在产品开发周期中强调沟通，集成和协作的文化。因此，它消除了软件开发团队和运营团队之间的孤岛，使他们能够快速，连续地集成和部署产品。\n什么是持续集成\n持续集成（Continuous integration，缩写为 CI）是一种软件开发实践，团队开发成员经常集成他们的工作。利用自动测试来验证并断言其代码不会与现有代码库产生冲突。理想情况下，代码更改应该每天在CI工具的帮助下，在每次提交时进行自动化构建（包括编译，发布，自动化测试），从而尽早地发现集成错误，以确保合并的代码没有破坏主分支。\n什么是持续交付\n持续交付（Continuous delivery，缩写为 CD）以及持续集成为交付代码包提供了完整的流程。在此阶段，将使用自动构建工具来编译工件，并使其准备好交付给最终用户。它的目标在于让软件的构建、测试与发布变得更快以及更频繁。这种方式可以减少软件开发的成本与时间，减少风险。\n什么是持续部署\n持续部署（Continuous deployment）通过集成新的代码更改并将其自动交付到发布分支，从而将持续交付提升到一个新的水平。 更具体地说，一旦更新通过了生产流程的所有阶段，便将它们直接部署到最终用户，而无需人工干预。因此，要成功利用连续部署，软件工件必须先经过严格建立的自动化测试和工具，然后才能部署到生产环境中。\n什么是持续测试及其好处\n连续测试是一种在软件交付管道中尽早、逐步和适当地应用自动化测试的实践。在典型的CI/CD工作流程中，将小批量发布构建。因此，为每个交付手动执行测试用例是不切实际的。自动化的连续测试消除了手动步骤，并将其转变为自动化例程，从而减少了人工。因此，对于DevOps文化而言，自动连续测试至关重要。\n持续测试的好处\n确保构建的质量和速度。 支持更快的软件交付和持续的反馈机制。 一旦系统中出现错误，请立即检测。 降低业务风险。 在潜在问题变成实际问题之前进行评估。 什么是版本控制及其用途？\n版本控制（或源代码控制）是一个存储库，源代码中的所有更改都始终存储在这个代码仓库中。版本控件提供了代码开发的操作历史记录，追踪文件的变更内容、时间、人等信息忠实地了记录下来。版本控制是持续集成和持续构建的源头。\n什么是Git？\nGit是一个分布式版本控制系统，可跟踪代码存储库中的更改。利用GitHub流，Git围绕着一个基于分支的工作流，该工作流随着团队项目的不断发展而简化了团队协作。\n实施DevOps的原因 # DevOps为什么重要？DevOps如何使团队在软件交付方面受益？\n在当今的数字化世界中，组织必须重塑其产品部署系统，使其更强大，更灵活，以跟上竞争的步伐。\n这就是DevOps概念出现的地方。DevOps在为整个软件开发管道（从构思到部署，再到最终用户）产生移动性和敏捷性方面发挥着至关重要的作用。DevOps是将不断更新和改进产品的更简化，更高效的流程整合在一起的解决方案。\n解释DevOps对开发人员有何帮助\n在没有DevOps的世界中，开发人员的工作流程将首先建立新代码，交付并集成它们，然后，操作团队有责任打包和部署代码。之后，他们将不得不等待反馈。而且如果出现问题，由于错误，他们将不得不重新执行一次。沿线是项目中涉及的不同团队之间的无数手动沟通。\n由于CI/CD实践已经合并并自动化了其余任务，因此应用DevOps可以将开发人员的任务简化为仅构建代码。随着流程变得更加透明并且所有团队成员都可以访问，将工程团队和运营团队相结合有助于建立更好的沟通和协作。\n为什么DevOps最近在软件交付方面变得越来越流行？\nDevOps在过去几年中受到关注，主要是因为它能够简化组织运营的开发，测试和部署流程，并将其转化为业务价值。\n技术发展迅速。因此，组织必须采用一种新的工作流程-DevOps和Agile方法-来简化和刺激其运营，而不能落后于其他公司。DevOps的功能通过Facebook和Netflix的持续部署方法所取得的成功得到了清晰体现，该方法成功地促进了其增长，而没有中断正在进行的运营。\nCI/CD有什么好处？\nCI和CD的结合将所有代码更改统一到一个单一的存储库中，并通过自动化测试运行它们，从而在所有阶段全面开发产品，并随时准备部署。\nCI/CD使组织能够按照客户期望的那样快速，高效和自动地推出产品更新。\n简而言之，精心规划和执行良好的CI/CD管道可加快发布速度和可靠性，同时减轻产品的代码更改和缺陷。这最终将导致更高的客户满意度。\n持续交付有什么好处？\n通过手动发布代码更改，团队可以完全控制产品。 在某些情况下，该产品的新版本将更有希望：具有明确业务目的的促销策略。\n通过自动执行重复性和平凡的任务，IT专业人员可以拥有更多的思考能力来专注于改进产品，而不必担心集成进度。\n持续部署有哪些好处？\n通过持续部署，开发人员可以完全专注于产品，因为他们在管道中的最后任务是审查拉取请求并将其合并到分支。通过在自动测试后立即发布新功能和修复，此方法可实现快速部署并缩短部署持续时间。\n客户将是评估每个版本质量的人。新版本的错误修复更易于处理，因为现在每个版本都以小批量交付。\n如何有效实施DevOps # 定义典型的DevOps工作流程\n典型的DevOps工作流程可以简化为4个阶段：\n版本控制：这是存储和管理源代码的阶段。 版本控件包含代码的不同版本。 持续集成：在这一步中，开发人员开始构建组件，并对其进行编译，验证，然后通过代码审查，单元测试和集成测试进行测试。 持续交付：这是持续集成的下一个层次，其中发布和测试过程是完全自动化的。 CD确保将新版本快速，可持续地交付给最终用户。 持续部署：应用程序成功通过所有测试要求后，将自动部署到生产服务器上以进行发布，而无需任何人工干预。 DevOps的核心操作是什么？\nDevOps在开发和基础架构方面的核心运营是：\nSoftware development:\nCode building Code coverage Unit testing Packaging Deployment Infrastructure:\nProvisioning Configuration Orchestration Deployment 在实施DevOps之前，团队需要考虑哪些预防措施？\n当组织尝试应用这种新方法时，对DevOps做法存在一些误解，有可能导致悲惨的失败：\nDevOps不仅仅是简单地应用新工具和/或组建新的“部门”并期望它能正常工作。实际上，DevOps被认为是一种文化，开发团队和运营团队遵循共同的框架。 企业没有为其DevOps实践定义清晰的愿景。对开发团队和运营团队而言，应用DevOps计划是一项显着的变化。因此，拥有明确的路线图，将DevOps集成到您的组织中的目标和期望将消除任何混乱，并从早期就提供清晰的指导方针。 在整个组织中应用DevOps做法之后，管理团队需要建立持续的学习和改进文化。系统中的故障和问题应被视为团队从错误中学习并防止这些错误再次发生的宝贵媒介。 SCM团队在DevOps中扮演什么角色？\n软件配置管理（SCM）是跟踪和保留开发环境记录的实践，包括在操作系统中进行的所有更改和调整。\n在DevOps中，将SCM作为代码构建在基础架构即代码实践的保护下。\nSCM为开发人员简化了任务，因为他们不再需要手动管理配置过程。 现在，此过程以机器可读的形式构建，并且会自动复制和标准化。\n质量保证（QA）团队在DevOps中扮演什么角色？\n随着DevOps实践在创新组织中变得越来越受欢迎，QA团队的职责和相关性在当今的自动化世界中已显示出下降的迹象。\n但是，这可以被认为是神话。 DevOps的增加并不等于QA角色的结束。 这仅意味着他们的工作环境和所需的专业知识正在发生变化。 因此，他们的主要重点是专业发展以跟上这种不断变化的趋势。\n在DevOps中，质量保证团队在确保连续交付实践的稳定性以及执行自动重复性测试无法完成的探索性测试任务方面发挥战略作用。 他们在评估测试和检测最有价值的测试方面的见识仍然在缓解发布的最后步骤中的错误方面起着至关重要的作用。\nDevOps使用哪些工具？ 描述您使用任何这些工具的经验\n在典型的DevOps生命周期中，有不同的工具来支持产品开发的不同阶段。 因此，用于DevOps的最常用工具可以分为6个关键阶段：\n持续开发：Git, SVN, Mercurial, CVS, Jira 持续整合：Jenkins, Bamboo, CircleCI 持续交付：Nexus, Archiva, Tomcat 持续部署：Puppet, Chef, Docker 持续监控：Splunk, ELK Stack, Nagios 连续测试：Selenium，Katalon Studio\n如何在DevOps实践中进行变更管理\n典型的变更管理方法需要与DevOps的现代实践适当集成。 第一步是将变更集中到一个平台中，以简化变更，问题和事件管理流程。\n接下来，企业应建立高透明度标准，以确保每个人都在同一页面上，并确保内部信息和沟通的准确性。\n对即将到来的变更进行分层并建立可靠的策略，将有助于最大程度地降低风险并缩短变更周期。 最后，组织应将自动化应用到其流程中，并与DevOps软件集成。\n如何有效实施CI/CD # CI/CD的一些核心组件是什么？\n稳定的CI/CD管道需要用作版本控制系统的存储库管理工具。 这样开发人员就可以跟踪软件版本中的更改。\n在版本控制系统中，开发人员还可以在项目上进行协作，在版本之间进行比较并消除他们犯的任何错误，从而减轻对所有团队成员的干扰。\n连续测试和自动化测试是成功建立无缝CI / CD管道的两个最关键的关键。 自动化测试必须集成到所有产品开发阶段（包括单元测试，集成测试和系统测试），以涵盖所有功能，例如性能，可用性，性能，负载，压力和安全性。\nCI/CD的一些常见做法是什么？\n以下是建立有效的CI / CD管道的一些最佳实践：\n发展DevOps文化 实施和利用持续集成 以相同的方式部署到每个环境 失败并重新启动管道 应用版本控制 将数据库包含在管道中 监控您的持续交付流程 使您的CD流水线流畅 什么时候是实施CI/CD的最佳时间？\n向DevOps的过渡需要彻底重塑其软件开发文化，包括工作流，组织结构和基础架构。 因此，组织必须为实施DevOps的重大变化做好准备。\n有哪些常见的CI/CD服务器\nVisual Studio Visual Studio支持具有敏捷计划，源代码控制，包管理，测试和发布自动化以及持续监视的完整开发的DevOps系统。\nTeamCity TeamCity是一款智能CI服务器，可提供框架支持和代码覆盖，而无需安装任何额外的插件，也无需模块来构建脚本。\nJenkins 它是一个独立的CI服务器，通过共享管道和错误跟踪功能支持开发和运营团队之间的协作。 它也可以与数百个仪表板插件结合使用。\nGitLab GitLab的用户可以自定义平台，以进行有效的持续集成和部署。 GitLab帮助CI / CD团队加快代码交付，错误识别和恢复程序的速度。\nBamboo Bamboo是用于产品发布管理自动化的连续集成服务器。 Bamboo跟踪所有工具上的所有部署，并实时传达错误。\n描述持续集成的有效工作流程\n实施持续集成的成功工作流程包括以下实践：\n实施和维护项目源代码的存储库 自动化构建和集成 使构建自检 每天将更改提交到基准 构建所有添加到基准的提交 保持快速构建 在生产环境的克隆中运行测试 轻松获取最新交付物 使构建结果易于所有人监视 自动化部署 每种术语之间的差异 # 敏捷和DevOps之间有哪些主要区别？\n基本上，DevOps和敏捷是相互补充的。敏捷更加关注开发新软件和以更有效的方式管理复杂过程的价值和原则。同时，DevOps旨在增强由开发人员和运营团队组成的不同团队之间的沟通，集成和协作。\n它需要采用敏捷方法和DevOps方法来形成无缝工作的产品开发生命周期：敏捷原理有助于塑造和引导正确的开发方向，而DevOps利用这些工具来确保将产品完全交付给客户。\n持续集成，持续交付和持续部署之间有什么区别？\n持续集成（CI）是一种将代码版本连续集成到共享存储库中的实践。这种做法可确保自动测试新代码，并能快速检测和修复错误。\n持续交付使CI进一步迈出了一步，确保集成后，随时可以在一个按钮内就可以释放代码库。因此，CI可以视为持续交付的先决条件，这是CI / CD管道的另一个重要组成部分。\n对于连续部署，不需要任何手动步骤。这些代码通过测试后，便会自动推送到生产环境。\n所有这三个组件：持续集成，持续交付和持续部署是实施DevOps的重要阶段。\n一方面，连续交付更适合于活跃用户已经存在的应用程序，这样事情就可以变慢一些并进行更好的调整。另一方面，如果您打算发布一个全新的软件并且将整个过程指定为完全自动化的，则连续部署是您产品的更合适选择。\n连续交付和连续部署之间有哪些根本区别？\n在连续交付的情况下，主分支中的代码始终可以手动部署。 通过这种做法，开发团队可以决定何时发布新的更改或功能，以最大程度地使组织受益。\n同时，连续部署将在测试阶段之后立即将代码中的所有更新和修补程序自动部署到生产环境中，而无需任何人工干预。\n持续集成和持续交付之间的区别是什么？\n持续集成有助于确保软件组件紧密协作。 整合应该经常进行； 最好每小时或每天一次。 持续集成有助于提高代码提交的频率，并降低连接多个开发人员的代码的复杂性。 最终，此过程减少了不兼容代码和冗余工作的机会。\n持续交付是CI / CD流程中的下一步。 由于代码不断集成到共享存储库中，因此可以持续测试该代码。 在等待代码完成之前，没有间隙可以进行测试。 这样可确保找到尽可能多的错误，然后将其连续交付给生产。\nDevOps和持续交付之间有什么区别？\nDevOps更像是一种组织和文化方法，可促进工程团队和运营团队之间的协作和沟通。\n同时，持续交付是成功将DevOps实施到产品开发工作流程中的重要因素。 持续交付实践有助于使新发行的版本更加乏味和可靠，并建立更加无缝和短的流程。\nDevOps的主要目的是有效地结合Dev和Ops角色，消除所有孤岛，并实现独立于持续交付实践的业务目标。\n另一方面，如果已经有DevOps流程，则连续交付效果最佳。 因此，它扩大了协作并简化了组织的统一产品开发周期。\n敏捷，精益IT和DevOps之间有什么区别？\n敏捷是仅专注于软件开发的方法。 敏捷旨在迭代开发，建立持续交付，缩短反馈循环以及在整个软件开发生命周期（SDLC）中改善团队协作。\n精益IT是一种旨在简化产品开发周期价值流的方法。 精益专注于消除不必要的过程，这些过程不会增加价值，并创建流程来优化价值流。\nDevOps专注于开发和部署-产品开发过程的Dev和Ops。 其目标是有效整合自动化工具和IT专业人员之间的角色，以实现更简化和自动化的流程。\n准备好在下一次DevOps面试中取得成功吗？ # 目前有无数的DevOps面试问题，我们目前还不能完全解决。但是，我们希望这些问题和建议的答案能使您掌握DevOps和CI/CD的大量知识，并成功地帮助您完成面试。\n将来，我们将在此列表中添加更多内容。 因此，如果您对此主题有任何建议，请随时与我们联系。最后，我们祝您在测试事业中一切顺利！\n","date":"2020-03-29","externalUrl":null,"permalink":"/posts/top-30-devops-interview-questions/","section":"Posts","summary":"本文列出了 DevOps 领域的 30 多个常见面试问题，涵盖 DevOps 基础知识、CI/CD、DevOps 工具和实践等方面，帮助求职者准备 DevOps 面试。","title":"DevOps Top 30+ 面试问题","type":"posts"},{"content":"","date":"2020-03-29","externalUrl":null,"permalink":"/tags/interview/","section":"标签","summary":"","title":"Interview","type":"tags"},{"content":"对 Git 仓库的维护通常是为了减少仓库的大小。如果你从另外一个版本控制系统导入了一个仓库，你可能需要在导入后清除掉不必要的文件。本文主要讨论如何从 Git 仓库中删除不需要的文件。\n请格外小心\u0026hellip;..\n本文中的步骤和工具使用的高级技术涉及破坏性操作。确保您在开始之前仔细读过并备份了你的仓库，创建一个备份最容易的方式是使用 \u0026ndash;mirror 标志对你的仓库克隆，然后对整个克隆的文件进行打包压缩。有了这个备份，如果在维护期间意外损坏了你的仓库的关键元素，那么你可以通过备份的仓库来恢复。\n请记住，仓库维护对仓库的用户可能会是毁灭性的。与你的团队或者仓库的关注者进行沟通会是很有必要的。确保每个人都已经检查了他们的代码，并且同意在仓库维护期间停止开发。\n理解从 Git 的历史记录中删除文件 # 回想一下，克隆仓库会克隆整个历史记录——包括每个源代码文件的所有版本。如果一个用户提交了一个较大的文件，比如一个 JAR，则随后的每次克隆都会包含这个文件。即使用户最终在后面的某次提交中删除了这个文件，但是这个文件仍然存在于这个仓库的历史记录中。要想完全的从你的仓库中删除这个文件，你必须：\n从你的项目的当前的文件树中删除该文件; 从仓库的历史记录中删除文件——重写 Git 历史记录，从包含该文件的所有的提交中删除这个文件; 删除指向旧的提交历史记录的所有 reflog 历史记录; 重新整理仓库，使用 git gc 对现在没有使用的数据进行垃圾回收。 Git 的 gc（垃圾回收）将通过你的任何一个分支或者标签来删除仓库中所有的实际没用的或者以某种方式引用的数据。为了使其发挥作用，我们需要重写包含不需要的文件的所有 Git 仓库历史记录，仓库将不再引用它 git gc 将会丢弃所有没用的数据。\n重写存储库历史是一个棘手的事情，因为每个提交都依赖它的父提交，所以任何一个很小的改变都会改变它的每一个随后的提交的提交。有两个自动化的工具可以帮助你做这件事：\nBFG Repo Cleaner —— 快速、简单且易于使用，需要 Java 6 或者更高版本的运行环境。 git filter-branch —— 功能强大、配置麻烦，用于大于仓库时速度较慢，是核心 Git 套件的一部分。 切记，当你重写历史记录后，无论你是使用 BFG 还是使用 filter-branch，你都需要删除指向旧的历史记录的 reflog 条目，最后运行垃圾回收器来删除旧的数据。\n使用 BFG 重写历史记录 # BFG 是为将像大文件或者密码这些不想要的数据从 Git 仓库中删除而专门设计的，所以它有一一个简单的标志用来删除那些大的历史文件（不在当前的提交里面）：--strip-blobs-bigger-than\nBFG 下载地址\njava -jar bfg.jar --strip-blobs-than 100M 大小超过 100MB 的任何文件（不包含在你最近的提交中的文件——因为 BFG 默认会保护你的最新提交的内容）将会从你的 Git 仓库的历史记录中删除。如果你想用名字来指明具体的文件，你也可以这样做：\njava -jar bfg.jar --delete-files *.mp4 BFG 的速度要比 git filter-branch 快 10-1000 倍，而且通常更容易使用——查看完整的使用说明和示例获取更多细节。\n或者，使用 git filter-branch 来重写历史记录 # filter-branch 命令可以对 Git 仓库的历史记录重写，就像 BFG 一样，但是过程更慢和更手动化。如果你不知道这些大文件在哪里，那么你第一步就需要找到它们：\n手动查看你 Git 仓库中的大文件 # Antony Stubbs 写了一个可以很好地完成这个功能的 BASH 脚本。该脚本可以检查你的包文件的内容并列出大文件。在你开始删除文件之前，请执行以下操作获取并安装此脚本：\n下载脚本到你的本地的系统\n将它放在一个可以访问你的 Git 仓库的易于找到的位置\n让脚本成为可执行文件\nchmod 777 git_find_big.sh 克隆仓库到你本地系统\n改变当前目录到你的仓库根目录\n手动运行 Git 垃圾回收器\ngit gc --auto 找出 .git 文件夹的大小\n# 注意文件大小，以便随后参考 du -hs .git/objects 45M .git/objects 运行 git_find_big.sh 脚本来列出你的仓库中的大文件\ngit_find_big.sh All sizes are in kB\u0026#39;s. The pack column is the size of the object, compressed, inside the pack file. size pack SHA location 592 580 e3117f48bc305dd1f5ae0df3419a0ce2d9617336 media/img/emojis.jar 550 169 b594a7f59ba7ba9daebb20447a87ea4357874f43 media/js/aui/aui-dependencies.jar 518 514 22f7f9a84905aaec019dae9ea1279a9450277130 media/images/screenshots/issue-tracker-wiki.jar 337 92 1fd8ac97c9fecf74ba6246eacef8288e89b4bff5 media/js/lib/bundle.js 240 239 e0c26d9959bd583e5ef32b6206fc8abe5fea8624 media/img/featuretour/heroshot.png 大文件都是 JAR 文件，包的大小列是最相关的。aui-dependencies.jar 被压缩到 169kb，但是 emojis.jar 只压缩到 500kb。emojis.jar 就是一个待删除的对象。\n运行 filter-branch # 你可以给这个命令传递一个用于重写 Git 索引的过滤器。例如，一个过滤器可以可以将每个检索的提交删除。这个用法如下：\ngit filter-branch --index-filter \u0026#39;git rm --cached --ignore-unmatch\u0026amp;nbsp; _pathname_ \u0026#39; commitHASH --index-filter 选项可以修改仓库的索引，--cached 选项从索引中而不是磁盘来删除文件。这样会更快，因为你不需要在运行这个过滤器前检查每个修订版本。 git rm 中的 ignore-unmatch 选项可以防止在尝试移走不存在的文件 pathname 的时候命令失败。通过指定一个提交 HASH 值，你可以从每个以这个 HASH 值开始的提交中删除 pathname。要从开始处删除，你可以省略这个参数或者指定为 HEAD。\n如果你的大文件在不同的分支，你将需要通过名字来删除每个文件。如果大文件都在一个单独的分支，你可以直接删除这个分支本身。\n选项 1：通过文件名删除文件 # 使用下面的步骤来删除大文件：\n使用下面的命令来删除你找到的第一个大文件\ngit filter-branch --index-filter \u0026#39;git rm --cached --ignore-unmatch filename\u0026#39; HEAD 重复步骤 1 找到剩下的每个大文件\n在你的仓库里更新引用。 filter-branch 会为你原先的引用创建一个 refs/original/ 下的备份。一旦你确信已经删除了正确的文件，你可以运行下面的命令来删除备份文件，同时可以让垃圾回收器回收大的对象\ngit for-each-ref --format=\u0026#34;%(refname)\u0026#34; refs/original/ | xargs -n 1 git update-ref -d 选项 2：直接删除分支 # 如果你所有的大文件都在一个单独的分支上，你可以直接删除这个分支。删除这个分支会自动删除所有的引用。\n删除分支\ngit branch -D PROJ567bugfix 从后面的分支中删除所有的 reflog 引用\ngit reflog expire --expire=now PROJ567bugfix 对不用的数据垃圾回收 # 删除从现在到后面的所有 reflog 引用（除非你明确地只在一个分支上操作）\ngit reflog expire --expire=now --all 通过运行垃圾回收器和删除旧的对象重新打包仓库。\ngit gc --prune=now 把你所有的修改推送回仓库\ngit push --all --force 确保你所有的标签也是当前最新的\ngit push --tags --force 英文原文地址\n","date":"2020-03-21","externalUrl":null,"permalink":"/posts/maintaining-git-repository/","section":"Posts","summary":"如何从 Git 仓库中删除不需要的文件和历史记录，以减少仓库的大小，并提供了两种方法：使用 BFG Repo Cleaner 或 git filter-branch。","title":"如何给你的 Git 仓库“瘦身”","type":"posts"},{"content":"","date":"2020-02-16","externalUrl":null,"permalink":"/tags/cppcheck/","section":"标签","summary":"","title":"Cppcheck","type":"tags"},{"content":"由于历史遗留原因，我们当前产品的代码仓库里遗留很多 Warning，这些 Warning 不是一时半会可以解决掉的。只有通过不断的丰富自动化测试用例，来保障最后的质量关卡，才敢有条不紊的进行 Warining 的修复，在次之前，如何有效杜绝继续引入更多的 Warining 是当下应该做的。\n因此我想在 Pull Request 阶段加入 C/C++ 的静态代码扫描的集成，但是很多工具只要涉及的是 C/C++ 经常都是收费的，比如这里首选的 SonarQube，Community 版本不支持 C/C++ 代码扫描，只有 Developer 以及 Enterprise 等付费版本才支持，在静态代码扫描还没有带来收益之前，盲目的付费只会给产品带来更多的成本，因此决定先寻找其他开源工具来替代。\n最终我选择了 CPPCheck，主要有以下几个原因：\n这是为数不多的 C/C++ 开源静态代码扫描工具 可以与 Jenkins 集成，可以在 Jenkins 里查看结果报告 支持 Jenkins Pipeline 本文记录我调查和使用的经验，如果您也相关的需求，提供一点参考。\n安装 Cppcheck # 安装到 Linux\nsudo yum install cppcheck.x86_64 其他平台安装请参考 cppcheck 官网\n如果你在 Linux 无法通过命令一键安装，也可通过下载源代码构建 cppcheck。以下是从代码手动构建一个 cppcheck 可执行文件的步骤\ncd opt \u0026amp;\u0026amp; mkdir cppcheck \u0026amp;\u0026amp; cd cppcheck # 下载代码 wget https://github.com/danmar/archive/1.90.tar.gz # 解压 tar -xvf 1.90.tar.gz # make build cd cppcheck-1.90 mkdir build cd build cmake .. cmake --build . # link sudo ln -s /opt/cppcheck-1.90/cppcheck /usr/bin/cppcheck # 检查是否安装成功 which cppcheck /usr/bin/cppcheck cppcheck --version Cppcheck 1.90 使用 cppcheck 静态代码扫描 # 在与 Jenkins 集成之前，先看看这个工具怎么用。通过查阅Cppcheck 官方文档，一般的使用如下：\n# 例如扫描 src 下 public 和 themes 两个目录下的代码将结果输出到 cppcheck.xml cppcheck src/public src/themes --xml 2\u0026gt; cppcheck.xml Cppcheck 与 Jenkins 集成 # 首先，下载 Cppcheck Jenkins 插件，通过 Pipeline Syntax 生成了此代码 publishCppcheck pattern: 'cppcheck.xml'\n但是在读取 xml 文件进行报告展示时，我遇到了两个问题：\n问题1：分析 cppcheck.xml 我在有的 Linux 机器上成功，有的机器上会失败，我怀疑是我的 JDK 版本不同所致。Jenkins JIRA 我也找到了次问题 JENKINS-60077 但目前还没有人来解决。\n我之所以没有继续尝试去解决问题1，最主要的原因是它有一个对我来说是更致命的缺陷，那就是下面说的问题。\n问题2： 无法通过 Cppcheck Results 报告直接查看代码，这样就算扫描出来了问题还需要去 git 或是本地的 IDE 上去查看具体的问题，大大降低效率。\n# 查看代码文件时会出错 Can\u0026#39;t read file: Can\u0026#39;t access the file: file:/disk1/agent/workspace/cppcheck-ud113/src/public/dummy/err_printf.c 并且官方也相应的 Ticket 记录了该问题 JENKINS-42613 和 JENKINS-54209，JENKINS-42613 一直在等待 merge，截止发文，都还是暂时没有解决。\n最后我发现 Warnings Next Generation 这个插件将取代整个 Jenkins 静态分析套件，其中包含了这些插件 Android Lint, CheckStyle, Dry, FindBugs, PMD, Warnings, Static Analysis Utilities, Static Analysis Collector，最后通过 Warnings Next Generation 插件解决了报告展示的问题。\n这里可以通过 Pipeline Syntax 生成读取报告代码 recordIssues(tools: [codeAnalysis(pattern: 'cppcheck.xml')])\n更多有关 Warnings Next Generation 插件的使用，请参看文档\n最终 Pipeline 示例如下 # pipeline{ agent { node { label \u0026#39;cppcheck\u0026#39; customWorkspace \u0026#34;/agent/workspace/cppcheck\u0026#34; } } parameters { string(name: \u0026#39;Branch\u0026#39;, defaultValue: \u0026#39;develop\u0026#39;, summary: \u0026#39;Which branch do you want to do cppcheck?\u0026#39;) } options { timestamps () buildDiscarder(logRotator(numToKeepStr:\u0026#39;50\u0026#39;)) } stage(\u0026#34;Checkout\u0026#34;){ steps{ checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/${Branch}\u0026#39;]], browser: [$class: \u0026#39;BitbucketWeb\u0026#39;, repoUrl: \u0026#39;https://git.yourcompany.com/projects/repos/cppcheck-example/browse\u0026#39;], doGenerateSubmoduleConfigurations: false, extensions: [ [$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#39;**\u0026#39;], [$class: \u0026#39;CheckoutOption\u0026#39;, timeout: 30], [$class: \u0026#39;CloneOption\u0026#39;, depth: 1, noTags: false, reference: \u0026#39;\u0026#39;, shallow: true, timeout: 30]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;d1cbab74-823d-41aa-abb7\u0026#39;, url: \u0026#39;https://git.yourcompany.com/scm/cppcheck-example.git\u0026#39;]]]) } } stage(\u0026#34;Cppcheck\u0026#34;){ steps{ script { sh \u0026#39;cppcheck src/public src/themes --xml 2\u0026gt; cppcheck.xml\u0026#39; } } } stage(\u0026#39;Publish results\u0026#39;){ steps { recordIssues(tools: [cppCheck(pattern: \u0026#39;cppcheck.xml\u0026#39;)]) } } } 报告展示 # 我将 CPPCheck 应用到每个 Pull Request 里，当开发提交新的代码时，CPPCheck 就会去扫描代码，然后跟之前的历史记录做对比。CPPCheck 执行成功并生成报告，这里会出现一个按钮，点击进入。\n打开之后就会当前分支代码的扫结果。\nCPPCheck 有三个维度来来展示静态代码扫描结果：\n严重程度分布（Severities Distribution）：这里分为 High，Normal，Low 三种级别\n参考比较（Reference Comparison）：这里会参考之前的数据进行比较，如果有新增就会显示 New，如果是现存的就显示为 Outstanding，如果减少了就会显示 Fixed\n历史（History）：随着代码的增加和修改，这里会显示一个历史记录的趋势\n注意：cppcheck 相关的 xml 是存储在 Jenkins master 上，只有当前的 Jenkins Job 被人为删掉，那么 cppcheck xml 才会被删掉。\n-sh-4.2$ ls -l cppcheck* -rw-r--r-- 1 jenkins jenkins 418591 Feb 27 05:54 cppcheck-blames.xml -rw-r--r-- 1 jenkins jenkins 219 Feb 27 05:54 cppcheck-fixed-issues.xml -rw-r--r-- 1 jenkins jenkins 142298 Feb 27 05:54 cppcheck-forensics.xml -rw-r--r-- 1 jenkins jenkins 219 Feb 27 05:54 cppcheck-new-issues.xml -rw-r--r-- 1 jenkins jenkins 488636 Feb 27 05:54 cppcheck-outstanding-issues.xml 点击相应的连接就可以直接跳转到具体的代码警告位置了。\n它是不是还挺香的？\n","date":"2020-02-16","externalUrl":null,"permalink":"/posts/cppcheck/","section":"Posts","summary":"本文介绍了 Cppcheck 的安装、使用以及与 Jenkins 的集成方法，旨在提升 C/C++ 代码质量和静态分析能力。","title":"一款免费的 C/C++ 静态代码分析工具 Cppcheck 与 Jenkins 集成","type":"posts"},{"content":"如果你想使用 Linux 但又不想租用云厂商的虚拟机，那么 VirtualBox 是一个比较好的选择。我们可以在 VirtualBox 安装你需要的绝大数的操作系统。\n为了保证使用的流畅，这里最好使用内存建议 8G 及以上，硬盘 256 GB 的 Windows 电脑。\n下载 # 下载和安装 VirtualBox https://www.virtualbox.org/wiki/Downloads CentOS 镜像下载地址 https://mirror.umd.edu/centos/7/isos/x86_64/ Ubuntu 镜像下载地址 https://ubuntu.com/#download 安装和配置\n本文没有详细介绍每个安装步骤，只列出一些关键步骤。\n","date":"2020-02-08","externalUrl":null,"permalink":"/posts/setup-linux-in-virtualbox/","section":"Posts","summary":"本文介绍了如何在 VirtualBox 中配置一台 Linux 虚拟机，包括下载、安装和配置步骤，帮助用户快速搭建 Linux 环境。","title":"用 VirtualBox 配置一台 Linux 虚拟机","type":"posts"},{"content":"","date":"2020-02-05","externalUrl":null,"permalink":"/tags/hp-ux/","section":"标签","summary":"","title":"HP-UX","type":"tags"},{"content":" 安装 Java8 # 安装包下载链接是 https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=JDKJRE8018\n需要先注册，然后登陆后才能下载，我下载的是 Itanium_JDK_8.0.18_June_2019_Z7550-96733_java8_18018_ia.depot\n在线安装文档 https://support.hpe.com/hpesc/public/docDisplay?docId=emr_na-c04481894\nswinstall -s /tmp/Itanium_JDK_8.0.18_June_2019_Z7550-96733_java8_18018_ia.depot # if swinstall not found /usr/sbin/swinstall -s /tmp/Itanium_JDK_8.0.18_June_2019_Z7550-96733_java8_18018_ia.depot 安装成功后，会在根目录 opt 下多了一个 java8 目录，检查下 java 版本：\nbash-5.0$ pwd /opt/java8 bash-5.0$ cd bin bash-5.0$ java -version java version \u0026#34;1.8.0.18-hp-ux\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0.18-hp-ux-b1) Java HotSpot(TM) Server VM (build 25.18-b1, mixed mode) 创建软连接\nsudo ln -s /opt/java8/bin/java /bin/java 安装 gzip 和 gunzip # # 安装 gzip /usr/bin/sudo /usr/local/bin/depothelper gzip 如果你机器上已经有 zip 和 gunzip 了，只需要软连接一下即可，防止出现命令找不到的问题\n/usr/bin/sudo ln -s /usr/contrib/bin/gzip /usr/bin/gzip /usr/bin/sudo ln -s /usr/contrib/bin/gunzip /usr/bin/gunzip Can not use bash in HP-UX # For example, when you run bash command, you have the following error:\n$ bash /usr/lib/hpux64/dld.so: Unable to find library \u0026#39;libtermcap.so\u0026#39;. Here is the solution：https://community.hpe.com/t5/HP-UX-General/Unable-to-use-bash-for-ia-machine-11-23/m-p/3980789#M128592\nIt bcasue the LIBTERMCAP is not installed, you can go here to see bash\u0026rsquo;s dependencies include gettext libiconv termcap, etc.\nHere are two very useful commands of install and uninstall.\nDownload bash to command depothelper\ndepothelper bash If you wang to remove the package on your HP-UX system, you can run the command\nsudo /usr/sbin/swremove [package-name]\n","date":"2020-02-05","externalUrl":null,"permalink":"/posts/hpxu-tips/","section":"Posts","summary":"本文介绍了在 HP-UX 系统上安装 Java 8、gzip 和 gunzip 的方法，以及如何解决 HP-UX 上使用 bash 时遇到的库依赖问题。","title":"HP-UX 安装工具以及一些使用总结","type":"posts"},{"content":" ls 命令 # 列出当前目录的文件和文件夹。参数:\n-l 列出时显示详细信息\n-a 显示所有文件，包括隐藏的和不隐藏的\n可以组合使用，像这样\nls -la cp 命令 # 将源文件复制到目标。参数：\n-i 交互模式意味着等待确认，如果目标上有文件将被覆盖。\n-r 递归复制，意味着包含子目录（如果有的话）。\ncp –ir source_dir target_dir /tmp 空间不够怎么办 # 在 /etc/fstab 文件里增加一行\nsudo vi /etc/fstab # 添加如下一行 tmpfs /tmp tmpfs defaults,size=4G 0 0 重启之后，df -h 查看，/tmp 目录已经就变成 4G 了。\nMore, Refer to these links\nhttps://likegeeks.com/main-linux-commands-easy-guide/ https://dzone.com/articles/most-useful-linux-command-line-tricks ","date":"2020-02-05","externalUrl":null,"permalink":"/posts/linux-tips/","section":"Posts","summary":"本文介绍了一些最有用的 Linux 命令行技巧，以提高开发和运维的效率。","title":"最有用的 Linux 命令行技巧","type":"posts"},{"content":"","date":"2020-01-21","externalUrl":null,"permalink":"/tags/stats/","section":"标签","summary":"","title":"Stats","type":"tags"},{"content":"上一篇（GitStats - Git 历史统计信息工具），我已经给老板提供了他想看的所有仓库的 Git 提交历史分析报告了，并且把报告都部署到了一台虚拟机的 tomcat 上了，老板可以通过网址访问直接查看每个仓库的分析报告了，看看谁的贡献大，谁活跃，谁偷懒了，谁周末写代码了（这里不鼓励 996）。\n最近老板提需求了。\n老板：你弄个这个网址的数据咋不更新呢？报告上咋没见你们提交代码呢？ 小开：老板儿，您看到这些一个个仓库的数据都是小开我人肉手动生成的，要不您给我点时间，我来做个自动化任务吧。\n我这么积极主动，不是我奉承老板，我心里也知道老板如果觉得 Git Stats 这个工具好用，肯定希望看到的分析报告是最新的。既然老板先提了，那我就别磨蹭了，赶紧干吧。\n不过用啥实现呢？肯定是 Jenkins 了。一来我已经在 Jenkins 上做了很多的自动化任务了，轻车熟路；二来使用同一套系统不但可以减少繁多的系统入口，降低学习成本，也提高 Jenkins 服务器的利用率。\n设身处地的考虑了下老板的使用需求，他肯定不希望自己去 Jenkins 服务器上去运行 Job 来生成这个Git 仓库的多维度代码分析报告，那么，如果我是老板，我希望：\n这个 Jenkins 任务要定期执行，不需要太频繁，一周更新一次就行； 另外还要支持对单独仓库的单独执行，一旦老板要立即马上查看某个仓库的的分析报告呢。 最后实现的效果如下：\n手动执行 # 老板可以勾选他最关心的代码仓库进行更新\n每周末定时执行 # 老板在周一上班就能看到最新的分析数据了，可以看到这个任务 Started by timer\n最终的 Jenkinsfile 是这样的 # pipeline{ agent{ node { label \u0026#39;main-slave\u0026#39; customWorkspace \u0026#34;/workspace/gitstats\u0026#34; } } environment { USER_CRE = credentials(\u0026#34;d1cbab74-823d-41aa-abb7\u0026#34;) webapproot = \u0026#34;/workspace/apache-tomcat-7.0.99/webapps/gitstats\u0026#34; } parameters { booleanParam(defaultValue: true, name: \u0026#39;repo1\u0026#39;, summary: \u0026#39;uncheck to disable [repo1]\u0026#39;) booleanParam(defaultValue: true, name: \u0026#39;repo2\u0026#39;, summary: \u0026#39;uncheck to disable [repo2]\u0026#39;) } triggers { cron \u0026#39;0 3 * * 7\u0026#39; # 每周日早上进行定时运行，因此此时机器是空闲的。 } options { buildDiscarder(logRotator(numToKeepStr:\u0026#39;10\u0026#39;)) } stages{ stage(\u0026#34;Checkout gitstats\u0026#34;){ steps{ # 准备存放 html 报告目录 sh \u0026#34;mkdir -p html\u0026#34; # 下载 gitstats 代码 sh \u0026#34;rm -rf gitstats \u0026amp;\u0026amp; git clone https://github.com/hoxu/gitstats.git\u0026#34; } } stage(\u0026#34;Under statistics\u0026#34;) { parallel { stage(\u0026#34;reop1\u0026#34;) { when { expression { return params.repo1 } # 判断是否勾选了 } steps { # 下载要进行分析的仓库 repo1 sh \u0026#39;git clone -b master https://$USER_CRE_USR:\u0026#34;$USER_CRE_PSW\u0026#34;@git.software.com/scm/repo1.git\u0026#39; # 进行仓库 repo1 的历史记录分析 sh \u0026#34;cd gitstats \u0026amp;\u0026amp; ./gitstats ../repo1 ../html/repo1\u0026#34; } post { success { # 如果分析成功，则将分析结果放到 apache-tomcat-7.0.99/webapps/gitstats 目录下 sh \u0026#39;rm -rf ${webapproot}/repo1 \u0026amp;\u0026amp; mv html/repo1 ${webapproot}\u0026#39; # 然后删掉 repo1 的代码和 html 报告，以免不及时清理造成磁盘空间的过度占用 sh \u0026#34;rm -rf repo1\u0026#34; sh \u0026#34;rm -rf html/repo1\u0026#34; } } } } stage(\u0026#34;repo2\u0026#34;) { when { expression { return params.repo2 } } steps { sh \u0026#39;git clone -b master https://$USER_CRE_USR:\u0026#34;$USER_CRE_PSW\u0026#34;@git.software.com/scm/repo2.git\u0026#39; sh \u0026#34;cd gitstats \u0026amp;\u0026amp; ./gitstats ../repo2 ../html/repo2\u0026#34; } post { success { sh \u0026#39;rm -rf ${webapproot}/repo2 \u0026amp;\u0026amp; mv html/repo2 ${webapproot}\u0026#39; sh \u0026#34;rm -rf repo2\u0026#34; sh \u0026#34;rm -rf html/repo2\u0026#34; } } } } } } post{ always{ # 总是给执行者分送邮件通知，不论是否成功都会对工作空间进行清理 script { def email = load \u0026#34;vars/email.groovy\u0026#34; email.build(currentBuild.result, \u0026#39;\u0026#39;) } cleanWs() } } } 最后 # 如果你是测试、DevOps或是从事研发效能方面的工作，那么利用好开源工具，比如 Jenkins 和 Git Stats 就可以快速帮助老板或是你自己提供一个 Git 仓库的多维度代码分析报告，有助于更加了解产品的代码情况。\n","date":"2020-01-21","externalUrl":null,"permalink":"/posts/git-stats-jenkins/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 定期自动生成 Git 仓库的多维度代码分析报告，包括手动执行和定时执行的实现方式，帮助开发者和团队更好地了解代码质量和贡献情况。","title":"通过 Jenkins 定期自动给老板提供 Git 仓库的多维度代码分析报告","type":"posts"},{"content":"记录在使用 AIX 时所遇到的问题和解决办法，以备以后遇到同样问题不要再因为这些再浪费时间，希望也能帮助到你。\n在 AIX 上无法解压超一个大约 600 MB 文件 # bash-4.3$ ls data_cdrom_debug_AIX_05949fb.tar.Z bash-4.3$ gzip -d data_cdrom_debug_AIX_05949fb.tar.Z # 错误信息 gzip: data_cdrom_debug_AIX_05949fb.tar: File too large # 解决办法 bash-4.3$ sudo vi /etc/security/limits default: fsize = -1 # 修改为 -1 core = 2097151 cpu = -1 data = -1 rss = 65536 stack = 65536 nofiles = 2000 # 需要重启 bash-4.3$ sudo reboot Rebooting . . . 修改之后，重启，再解压就没有问题了。\n安装 Java Standard Edition on AIX # 下载地址 https://developer.ibm.com/javasdk/support/aix-download-service/\ndownload Java8_64.sdk.8.0.0.600.tar.gz Java8_64.jre.8.0.0.600.tar.gz gzip -d Java8_64.sdk.8.0.0.600.tar.gz and Java8_64.jre.8.0.0.600.tar.gz tar -xvf Java8_64.sdk.8.0.0.600.tar and Java8_64.jre.8.0.0.600.tar installp -agXYd . Java8_64.jre Java8_64.sdk 2\u0026gt;\u0026amp;1 | tee installp.log # install output Installation Summary -------------------- Name Level Part Event Result ------------------------------------------------------------------------------- Java8_64.sdk 8.0.0.600 USR APPLY SUCCESS Java8_64.jre 8.0.0.600 USR APPLY SUCCESS Java8_64.jre 8.0.0.600 ROOT APPLY SUCCESS smitty install_all Input: Type \u0026ldquo;./\u0026rdquo; in the field Acceptance Install the Agreement, then start install. Troubleshooting\nbash-4.4# ./java -version Error: Port Library failed to initialize: -70 Error: Could not create the Java Virtual Machine. Error: A fatal exception has occurred. Program will exit. ","date":"2020-01-09","externalUrl":null,"permalink":"/posts/aix-tips/","section":"Posts","summary":"记录在使用 AIX 时所遇到的问题和解决办法，以备以后遇到同样问题不要再因为这些再浪费时间，希望也能帮助到你。","title":"AIX 上安装工具以及一些使用总结","type":"posts"},{"content":"","date":"2020-01-07","externalUrl":null,"permalink":"/tags/solaris/","section":"标签","summary":"","title":"Solaris","type":"tags"},{"content":"记录在使用 Solaris 时所遇到的问题和解决办法，以备以后遇到同样问题不要再因为这些再浪费时间，希望也能帮助到你。\ninstall packages on solaris # https://www.opencsw.org/get-it/packages/\nInstall Git # https://www.opencsw.org/packages/git/\n","date":"2020-01-07","externalUrl":null,"permalink":"/posts/solaris-tips/","section":"Posts","summary":"本文记录了在使用 Solaris 时遇到的问题和解决办法，包括安装工具、配置网络、安装软件包等，帮助用户更高效地使用 Solaris 系统。","title":"Solaris 安装工具以及一些使用总结","type":"posts"},{"content":"时间过得飞快，转眼已经是 2020 年的第三天了，回顾 2019 年，我给自己的年终关键词是：尽力。\n这是我作为开发工程师的第二年，虽然 Title 是 SE (Software Engineer)，但主要的工作内容是产品的构建和发布以及 CI/CD/DevOps 的落地（自称打杂）。流水的记录一下 2019 年发生在工作上的“成绩”。\n2019 年在工作中除了完成日常产品构建、发布、Git 管理、VM 管理等，尝试在构建和发布自动化上做大的调整，将手工构建和部分自动构建从 Bamboo 迁移到 Jenkins，通过 Jenkins 的 multi-branch pipeline、Shared Libraries 与 Artifactory 做持续集成。\n2019 年在公司内部提交了一个创新项目，有幸拿到了第一名和首席产品奖。\n因此有机会再去美国，参加了公司的开发者大会。很开心这个项目最终进入到了产品的 Roadmap 里。\n2019 年底 12 月去北京参加了一次两天的《JFrog Jenkins，Artifactory \u0026amp; Kubernetes》训练营，跟 DevOps 行业工具里最有影响力（之一）的公司的工程师学习最佳实践。\n这一年关于持续集成和持续交付收获不少新的知识，但只更新了 11 篇公众号相关原创文章，33 篇 Blog。这差别是因为 Blog 更像笔记，随时记录修改不怕错；公众号更像报纸，发出去的内容无法修改和补充，每次更新都需要一字一句反复修改和阅览确认，最终输出一篇完整的原创内容需要花费比写 Blog 多几倍的时间。\n希望 2020 年能完成更多有价值的内容输出，有机会的话用 8 到 10 个月的业余时间完成一件以前一直不敢想的事（如果完成就写下来，没成就烂在肚子里）。\n分享在 2019 年最后一天收到的一封邮件：\n前段时间大家都做了年末总结。试问，有多少人对自己一年的表现满意？你是否在工作中体现出你的价值？你提交了多少次代码？提交的代码质量怎么样？你解决了多少个客户问题？你发现了多少 bug？ 你是否在技术和业务知识有所提高？和你的同事比较，你的进步速度你自己是否满意？又有多少人把时间都花在了微信和聊天上？有没有人是待着混日子，或者应付公事的思路在这里工作。 我们年龄都差不多，多半是三四十岁。这个年龄被认为是最好的工作年龄。希望你不要在这里浪费你最好的时光。 有人可能说，我已经实现或者接近时间财务自由了，我对工作没有大的要求。首先我恭喜你实现财务自由，但同时我想说的是工作绝不只是一份收入来源这么简单。你是否能在工作中得到同事和领导的认可，是否能体现自己的价值。 我喜欢和有求知欲，有责任感，有上进心的人一起工作。我同时不喜欢工作时间，整天拿着手机不离手的， 工作上没有上进心的人。如果确实有事情，可以离开作为去打个电话，迅速解决一下。没有事情的闲看，是不赞成的。 还有人，到点就准时下班，很像多干几分钟自己就吃亏了一样，这种心态千万不可取。试问，如果你的能力水平不及你的同事，你每天还不比别人多努力，你怎么才能接近或者超过别人？难道永远都想落后？每天多走一里路，每天多做一些吧。\n我读了好几遍，感谢上面的话，从个人角度非常认同以上观点，尤其是说工作绝不是一份收入来源这么简单，它是一个人的价值体现。如果喜欢这份工作，尽力去做到最好吧；如果不喜欢，还是趁早找到自己的乐趣所在。\n","date":"2019-12-28","externalUrl":null,"permalink":"/misc/2019-summary/","section":"Miscs","summary":"\u003cp\u003e时间过得飞快，转眼已经是 2020 年的第三天了，回顾 2019 年，我给自己的年终关键词是：尽力。\u003c/p\u003e","title":"2019 年终总结","type":"misc"},{"content":" Jenkins Warnings Next Generation 插件 # Jenkins Warnings Next Generation 插件可收集编译器警告或静态分析工具报告的问题并可视化结果，它内置了对众多静态分析工具（包括多个编译器）的支持，更多支持的报告格式。\n支持的项目类型 # Warnings Next Generation 插件支持以下 Jenkins 项目类型：\n自由式项目 Maven 项目 矩阵项目 脚本化管道（顺序和并行步骤） 声明式管道（顺序步骤和并行步骤） 多分支管道 功能概述 # 当作为后续构建任务操作（或步骤）添加时，Warnings Next Generation 插件提供以下功能：\n该插件会扫描 Jenkins 版本的控制台日志或你工作区中的文件中是否存在任何问题。支持一百多种报告格式，它可以检测到的问题包括： 来自编译器的错误（C，C＃，Java等） 来自静态分析工具（CheckStyle，StyleCop，SpotBugs 等）的警告 来自复制粘贴检测器（CPD, Simian 等）的重复 漏洞 在源文件的注释中打开任务 该插件会发布有关在构建中发现的问题的报告，因此可以从以下位置导航到摘要报告，主构建页面。你还可以从那里深入了解细节： 发行新的，固定的和未解决的问题 按严重性，类别，类型，模块或程序包分发问题 所有问题的列表，包括来自报告工具的有用评论 受影响文件的带注释的源代码 问题趋势图 该插件不会运行静态分析，它只是可视化此类工具报告的结果。你仍然需要在构建文件或 Jenkinsfile 中启用和配置静态分析工具。\n配置 # 你可以在 Jenkins 作业配置用户界面中配置插件的每个选项（在自由式，maven 或矩阵作业中）。在这里你需要在工作中添加并启用生成后操作“记录编译器警告和静态分析结果”。\n在管道中，将通过添加 recordIssues 激活插件。也可以使用相同的用户界面来配置此步骤（通过使用 Snippet 编辑器）。请注意，对于脚本化管道，一些其他功能可用于汇总和分组问题，有关详细信息，请参阅“高级管道配置”部分。\n在以下各节中，将同时显示图形配置和管道配置。\n工具选择 # 下图显示了插件的基本配置：\n首先，你需要指定用于创建问题的工具，根据所选工具，你可能还会配置一些其他参数。\n对于所有读取报告文件的解析器，你需要指定应分析和扫描问题的报告文件的模式。如果未指定模式，则将扫描构建的控制台日志。对于几种流行的工具，提供了默认模式，在这种情况下，如果模式为空，则将使用默认模式。\n为了让扫描程序正确解析你的报告，需要设置文件的编码，否则将使用平台编码，这可能不正确。\n每个工具都由一个 ID 标识，该 ID 用作分析结果的 URL。对于每个工具，都提供了一个默认 URL（和名称），可以根据需要进行更改。例如，如果你打算多次使用解析器，则需要为每个调用指定不同的 ID。\n你可以指定将用于同一配置的多个工具（和模式），由于 Jenkins 的技术（或市场）限制，无法通过使用多个后期构建操作来选择不同的配置。\n通过使用“汇总结果”复选框，可以使用一项新功能。如果选中此选项，则将创建一个结果，其中包含所选工具的所有问题的汇总。这是之前静态分析收集器插件提供的。激活此选项后，你将获得所有问题的唯一入口点。以下屏幕截图显示了此新行为：\n如果未启用此选项，则将为每个工具创建单独的结果。此结果具有唯一的 URL 和图标，因此你可以快速查看创建的报告之间的区别：\n在基本配置部分中，你还可以选择是否针对失败的构建也运行该步骤。默认情况下禁用此选项，因为如果构建失败，分析结果可能会不准确。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues( enabledForFailure: true, aggregatingResults: true, tools: [java(), checkStyle(pattern: \u0026#39;checkstyle-result.xml\u0026#39;, reportEncoding: \u0026#39;UTF-8\u0026#39;)] ) 如果使用单个工具，则可以使用属性工具代替工具：\nrecordIssues enabledForFailure: true, aggregatingResults: true, tool: checkStyle(pattern: \u0026#39;checkstyle-result.xml\u0026#39;) 创建对自定义工具的支持 # 如果你的项目中没有内置工具，则可以通过多种方式添加其他工具。\n将问题导出为受支持的格式 # 将工具的分析结果获取到 Warnings 插件中的一种简单方法是将信息导出为一种已经支持的格式。例如，几种工具将其问题导出为 CheckStyle 或 PMD 格式。如果要使用警告插件的所有功能，则最好将信息导出为本机 XML 或 JSON 格式（此解析器使用 ID 问题）。 这些格式已经在用户界面中注册，你可以直接使用它们。你甚至可以在包含单行 JSON 问题的简单日志文件中提供问题，请参见示例。\n这是一个示例步骤，可用于解析本机 JSON（或 XML）格式：\nrecordIssues(tool: issues()) 使用自定义插件部署新工具 # 最灵活的方法是通过编写将在你自己的小型 Jenkins 插件中部署的 Java 类来定义新工具，有关详细信息，请参见文档“为自定义静态分析工具提供支持”。\n使用Groovy解析器创建新工具 # 如果日志消息的格式非常简单，则可以通过在 Jenkins 的用户界面中创建简单的工具配置来定义对工具的支持。 出于安全原因（Groovy 脚本可能会危害你的主服务器），此配置仅在系统配置中可用。 新解析器的配置采用正则表达式，该正则表达式将用于匹配报告格式。 如果表达式匹配，则将调用 Groovy 脚本，该脚本将匹配的文本转换为问题实例。 这是基于 Groovy 的解析器的示例：\n以编程方式创建 Groovy 解析器 # 还可以使用 Groovy 脚本从管道，Jenkins 启动脚本或脚本控制台中创建基于 Groovy 的解析器，请参见以下示例：\ndef config = io.jenkins.plugins.analysis.warnings.groovy.ParserConfiguration.getInstance() if(!config.contains(\u0026#39;pep8-groovy\u0026#39;)){ def newParser = new io.jenkins.plugins.analysis.warnings.groovy.GroovyParser( \u0026#39;pep8-groovy\u0026#39;, \u0026#39;Pep8 Groovy Parser\u0026#39;, \u0026#39;(.*):(\\\\d+):(\\\\d+): (\\\\D\\\\d*) (.*)\u0026#39;, \u0026#39;return builder.setFileName(matcher.group(1)).setCategory(matcher.group(4)).setMessage(matcher.group(5)).buildOptional()\u0026#39;, \u0026#34;optparse.py:69:11: E401 multiple imports on one line\u0026#34; ) config.setParsers(config.getParsers().plus(newParser)) } 使用配置作为代码导入解析器（JCasC） # 还可以使用 JCasC yaml 文件中的部分来指定基于 Groovy 的解析器。这是一个小示例，展示了如何添加这样的解析器：\nunclassified: warningsParsers: parsers: - name: \u0026#34;Example parser\u0026#34; id: example-id regexp: \u0026#34;^\\\\s*(.*):(\\\\d+):(.*):\\\\s*(.*)$\u0026#34; script: | import edu.hm.hafner.analysis.Severity builder.setFileName(matcher.group(1)) .setLineStart(Integer.parseInt(matcher.group(2))) .setSeverity(Severity.WARNING_NORMAL) .setCategory(matcher.group(3)) .setMessage(matcher.group(4)) return builder.buildOptional(); example: \u0026#34;somefile.txt:2:SeriousWarnings:SomethingWentWrong\u0026#34; 使用定义的工具 # 一旦注册了 Groovy 解析器，就可以在作业的工具配置部分中使用它：\n首先，你需要选择工具 “Groovy Parser” 以获取 Groovy 解析器的配置屏幕。 然后，你可以从可用解析器列表中选择解析器。 该列表是根据 Jenkins 的“系统配置”部分中定义的解析器动态创建的。可以使用与其他工具相同的方式来设置自定义 ID 和名称属性。\n为了在管道中使用 Groovy 解析器，你需要使用以下形式的脚本语句：\nrecordIssues sourceCodeEncoding: \u0026#39;UTF-8\u0026#39;, tool: groovyScript(parserId: \u0026#39;groovy-id-in-system-config\u0026#39;, pattern:\u0026#39;**/*report.log\u0026#39;, reportEncoding:\u0026#39;UTF-8\u0026#39;) 处理受影响的源代码文件的属性 # 为了让插件解析并显示你的源代码文件，需要为这些文件设置正确的编码。 此外，如果你的源代码不在工作区中（例如，它已签出到共享代理文件夹中），则该插件将不会自动找到你的源文件。 为了让插件显示这些文件，你可以添加一个附加的源目录：\n以下代码段显示了带有这些选项的示例管道，请注意，如果需要，可以不同地设置报告文件的编码：\nrecordIssues sourceCodeEncoding: \u0026#39;ISO-8859-1\u0026#39;, sourceDirectory: \u0026#39;/path/to/sources\u0026#39;, tool: java(reportEncoding: \u0026#39;UTF-8\u0026#39;) 请注意，工作区外部的文件内容可能很敏感。 为了防止意外显示此类文件，你需要在 Jenkins 系统配置屏幕中提供允许的源代码目录的白名单：\n另外，此配置设置可以由 JCasC yaml 文件中的以下子节提供\nunclassified: warningsPlugin: sourceDirectories: - path: \u0026#34;C:\\\\Temp\u0026#34; - path: \u0026#34;/mnt/sources\u0026#34; 控制参考构建的选择（基准） # 警告下一代插件的一项重要功能是将问题分类为新问题，未解决问题和已解决问题：\n新增：所有问题，属于当前报告的一部分，但未在参考报告中显示 已修复：所有问题，属于参考报告的一部分，但不再存在于当前报告中 未解决：所有问题，是当前报告和参考报告的一部分 为了计算此分类，插件需要参考构建（基准）。 然后，通过比较当前版本和基准中的问题来计算新的，已修复的和未解决的问题。 有三个选项可控制参考构建的选择。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(), ignoreQualityGate: false, ignoreFailedBuilds: true, referenceJobName: \u0026#39;my-project/master\u0026#39; 筛选问题 # 创建的问题报告可以随后进行过滤。 你可以指定任意数量的包含或排除过滤器。 当前，支持按模块名称，程序包或名称空间名称，文件名，类别或类型过滤问题。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(pattern: \u0026#39;*.log\u0026#39;), filters: [includeFile(\u0026#39;MyFile.*.java\u0026#39;), excludeCategory(\u0026#39;WHITESPACE\u0026#39;)] Quality gate 配置 # 你可以定义几个 Quality gate (质量门)，在报告问题后将对其进行检查。这些质量门使你可以修改詹金斯的生产状态，以便立即查看是否满足所需的产品质量。对于这些质量门中的每一个，都可以将构建设置为不稳定或失败。所有质量门都使用一个简单的度量标准：给定质量门将失败的问题数量。\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(pattern: \u0026#39;*.log\u0026#39;), qualityGates: [[threshold: 1, type: \u0026#39;TOTAL\u0026#39;, unstable: true]] 类型确定将用来评估质量门的属性。请参阅枚举 QualityGateType 以查看支持哪些不同类型。\n健康报告配置 # 该插件可以参与你项目的运行状况报告。你可以更改将运行状况更改为 0％ 和 100％ 的问题数。此外，可以选择在创建运行状况报告时应考虑的严重性。\n健康报告配置!\n以下代码段显示了具有这些选项的示例管道：\nrecordIssues tool: java(pattern: \u0026#39;*.log\u0026#39;), healthy: 10, unhealthy: 100, minimumSeverity: \u0026#39;HIGH\u0026#39; 该作业根据严重性为 HIGH 和错误的所有警告来调整构建运行状况。如果内部版本包含 10 条或更少的警告，则运行状况为 100％。如果内部版本有 100 个以上的警告，则运行状况为 0％。\n管道配置 # 在 Jenkins Pipeline 中使用 Warnings 插件的要求可能很复杂，有时会引起争议。为了尽可能灵活，我决定将主要步骤分为两个独立的部分，然后可以彼此独立使用。\n简单的管道配置 # 步骤 recordIssues 提供了简单的管道配置，它提供了与构建后操作相同的属性（请参见上文）。此步骤扫描给定文件集（或控制台日志）中的问题，并在构建中报告这些问题。你可以使用代码片段生成器来创建一个有效的代码片段，以调用此步骤。以下示例显示了此步骤的典型示例：\nrecordIssues( enabledForFailure: true, tool: java(pattern: \u0026#39;*.log\u0026#39;), filters: [includeFile(\u0026#39;MyFile.*.java\u0026#39;), excludeCategory(\u0026#39;WHITESPACE\u0026#39;)] ) 在此示例中，将扫描文件 * .log 中的 Java 问题。仅包括文件名与模式 MyFile.*.java 匹配的问题。类别 WHITESPACE 的问题将被排除，即使构建失败，也会执行该步骤。\n为了查看所有配置选项，你可以研究步骤实现。\n声明式管道配置 # 声明性管道作业中的插件配置与脚本管道中的配置相同，请参见以下示例，该示例在 Jenkins 上构建分析模型库：\npipeline { agent \u0026#39;any\u0026#39; tools { maven \u0026#39;mvn-default\u0026#39; jdk \u0026#39;jdk-default\u0026#39; } stages { stage (\u0026#39;Build\u0026#39;) { steps { sh \u0026#39;${M2_HOME}/bin/mvn --batch-mode -V -U -e clean verify -Dsurefire.useFile=false -Dmaven.test.failure.ignore\u0026#39; } } stage (\u0026#39;Analysis\u0026#39;) { steps { sh \u0026#39;${M2_HOME}/bin/mvn --batch-mode -V -U -e checkstyle:checkstyle pmd:pmd pmd:cpd findbugs:findbugs spotbugs:spotbugs\u0026#39; } } } post { always { junit testResults: \u0026#39;**/target/surefire-reports/TEST-*.xml\u0026#39; recordIssues enabledForFailure: true, tools: [mavenConsole(), java(), javaDoc()] recordIssues enabledForFailure: true, tool: checkStyle() recordIssues enabledForFailure: true, tool: spotBugs() recordIssues enabledForFailure: true, tool: cpd(pattern: \u0026#39;**/target/cpd.xml\u0026#39;) recordIssues enabledForFailure: true, tool: pmdParser(pattern: \u0026#39;**/target/pmd.xml\u0026#39;) } } } 高级管道配置 # 有时仅使用一个步骤发布和报告问题是不够的。例如，如果你使用多个并行步骤来构建产品，并且想要将所有这些步骤中的问题合并为一个结果。然后，你需要拆分扫描和聚合。该插件提供以下两个步骤：\nscanForIssues 此步骤使用特定的解析器扫描报告文件或控制台日志，并创建一个包含报告的中 间 AnnotatedReport 对象。有关详细信息，请参见步骤实现。 publishIssues：此步骤在你的构建中发布一个新报告，其中包含几个 scanForIssues 步骤的汇总结果。有关详细信息，请参见步骤实现。 node { stage (\u0026#39;Checkout\u0026#39;) { git branch:\u0026#39;5.0\u0026#39;, url: \u0026#39;git@github.com:jenkinsci/warnings-plugin.git\u0026#39; } stage (\u0026#39;Build\u0026#39;) { def mvnHome = tool \u0026#39;mvn-default\u0026#39; sh \u0026#34;${mvnHome}/bin/mvn --batch-mode -V -U -e clean verify -Dsurefire.useFile=false\u0026#34; junit testResults: \u0026#39;**/target/*-reports/TEST-*.xml\u0026#39; def java = scanForIssues tool: java() def javadoc = scanForIssues tool: javaDoc() publishIssues issues: [java, javadoc], filters: [includePackage(\u0026#39;io.jenkins.plugins.analysis.*\u0026#39;)] } stage (\u0026#39;Analysis\u0026#39;) { def mvnHome = tool \u0026#39;mvn-default\u0026#39; sh \u0026#34;${mvnHome}/bin/mvn --batch-mode -V -U -e checkstyle:checkstyle pmd:pmd pmd:cpd findbugs:findbugs\u0026#34; def checkstyle = scanForIssues tool: checkStyle(pattern: \u0026#39;**/target/checkstyle-result.xml\u0026#39;) publishIssues issues: [checkstyle] def pmd = scanForIssues tool: pmdParser(pattern: \u0026#39;**/target/pmd.xml\u0026#39;) publishIssues issues: [pmd] def cpd = scanForIssues tool: cpd(pattern: \u0026#39;**/target/cpd.xml\u0026#39;) publishIssues issues: [cpd] def spotbugs = scanForIssues tool: spotBugs(pattern: \u0026#39;**/target/findbugsXml.xml\u0026#39;) publishIssues issues: [spotbugs] def maven = scanForIssues tool: mavenConsole() publishIssues issues: [maven] publishIssues id: \u0026#39;analysis\u0026#39;, name: \u0026#39;All Issues\u0026#39;, issues: [checkstyle, pmd, spotbugs], filters: [includePackage(\u0026#39;io.jenkins.plugins.analysis.*\u0026#39;)] } } 新功能 # 以下各节介绍了最重要的新功能。\n发行记录：New, Fixed, Outstanding 问题 # 该插件的一大亮点是能够将后续版本的问题分类为 New, Fixed, Outstanding。\n使用此功能可以更轻松地控制项目的质量：你只能专注于最近引入的警告。\n注意：新警告的检测基于复杂的算法，该算法试图在源代码的两个不同版本中跟踪同一警告。根据源代码的修改程度，它可能会产生一些误报，即，即使应该没有警告也可能会收到一些新的固定警告。该算法的准确性仍在研究中，并将在接下来的几个月中进行完善。\nSeverities 严重程度 # 该插件在图表中显示问题严重性的分布，它定义了以下默认严重级别，但是扩展警告插件的插件可能会添加其他默认级别。\nError：表示通常会导致构建失败的错误 Warning (High, Normal, Low)：指示给定优先级的警告。映射到优先级取决于各个解析器。 请注意，并非每个解析器都能产生不同严重性的警告。某些解析器仅对所有问题使用相同的严重性。\nBuild trend 构建趋势 # 为了查看分析结果的趋势，几个图表显示了每个构建的问题数量。这些图表用于详细信息页面和作业概述中。当前提供以下不同的趋势图类型：\n问题的严重程度分布 # 默认趋势图显示问题总数，按严重性堆叠。使用此图表，你可以查看哪种严重程度对问题总数贡献最大。\n每种静态分析类型的问题 # 如果你要汇总几个静态分析结果，则类型图将使用单独的一行显示每个工具的问题数量。你可以通过单击相应的图例符号暂时隐藏工具。\n新问题与已修复问题 # 如果你对积压的问题感兴趣，可以打开新的与固定的图表。它映射了引入的问题与通过一系列构建解决的问题。这可以帮助你了解整个待办事项列表是在增加还是在减少。\n项目健康 # 仅当启用了运行状况报告后，运行状况图表才可用。在这种情况下，趋势图将显示健康和不健康区域中的警告数量。你的项目目标应该是使警告数量不逃避图表的绿色部分。\n缩放 # 细节视图中的所有趋势图都支持使用图表底部的范围滑块缩放构建轴。\n构建与日期轴 # 详细信息视图中的所有趋势图都可以显示每个构建或每天的警告数量。你可以通过选择右上角的相应图标在X轴变体之间切换，每天显示平均警告数。\n问题概述 # 你可以在几个聚合视图中快速，高效地查看报告的问题集。根据问题的数量或类型，你将看到问题的分布\nStatic Analysis Tool（静态分析工具） Module（模组） Package or Namespace（包或命名空间） Severity（严重程度） Category（类别） Type（类型） 这些详细信息视图中的每一个都是交互式的，即，你可以导航到已分类问题的子集。\n问题详情 # 一组已报告的问题显示在一个现代化的响应表中。该表使用 Ajax 调用按需加载，它提供以下功能：\nPagination（分页）：问题的数量分为几个页面，可以使用提供的页面链接进行选择。请注意，目前分页是在客户端进行的，即从服务器获取整个问题表可能要花费一些时间。 Sorting（排序）：可以通过单击表列中的仅一个来对表内容进行排序。 Filtering, Searching（过滤，搜索）：你可以通过在搜索框中输入一些文本来过滤显示的问题。 Content Aware（内容感知）：仅当有必要显示的内容时才显示列。也就是说，如果工具未报告问题类别，则该类别将被自动隐藏。 Responsive（响应式）：布局应适应实际的屏幕阈值。 Details（详细信息）：问题的详细信息消息（如果由相应的静态分析工具提供）在表中显示为子行。 源代码 Blame（归咎于） # 这个功能需要安装其他插件：Git Forensics 插件\n如果未在作业配置中禁用，则插件将执行 git blame 以确定谁是问题的负责 author。在相应的 SCM Blames 视图中，所有问题将与 auther name, email, 和 commit ID 一起列出。\n为了禁用 git blame 功能，请将属性 blameDisabled 设置为 true，请参见以下示例：\nrecordIssues blameDisabled: true, tool: java(pattern: \u0026#39;*.log\u0026#39;) Git 仓库取证 # 此功能需要安装其他插件：Git Forensics 插件\n如果未在作业配置中禁用，则该插件将以“犯罪现场代码”的样式（Adam Tornhill，2013年11月）挖掘源代码存储库，以确定受影响文件的统计信息。在相应的 “SCM 取证” 视图中，将列出所有问题以及受影响文件的以下属性：\n提交总数 不同作者总数 创作时间 最后修改时间 源代码控制概述 为了禁用 Git 取证功能，请将属性 forensicsDisabled设置为 true，请参见以下示例：\nrecordIssues forensicsDisabled: true, tool: java(pattern: \u0026#39;*.log\u0026#39;) 源代码视图 # 现在，源代码视图使用 JS 库 Prism 在受影响的文件中显示警告。该库为最流行的语言提供语法高亮显示，并在客户端呈现所有内容。\n发行总数栏 # 你可以在 Jenkins 作业表的单独列中显示作业的总数。 默认情况下，Jenkins 主列表视图将显示一个新列，该列计算所有工具的发行总数。 你可以添加可以配置的其他列\n列名 应考虑的实际工具 要显示的总计类型（总体警告，新警告，特定严重性等），请参阅 “token宏支持” 部分。 仪表板视图支持 # 还提供对 Jenkins 仪表板视图的支持。当前，以下 portlet 可用：\n每个工具和作业表的问题 # 问题表显示了作业的问题总数（由每个工具分开）。\n问题趋势 # 可以将趋势图添加为 portlet，该趋势图显示所有作业的发行总数。\n远程API # 该插件提供以下 REST API 端点。\n所有分析结果的汇总摘要 # 可以使用 URL [build-url]/warnings-ng/api/json（或 [build-url]/warnings-ng/api/xml）查询构建中已配置的所有静态分析工具。此汇总显示每个工具的 ID，名称，URL 和问题总数。\n{ \u0026#34;_class\u0026#34;: \u0026#34;io.jenkins.plugins.analysis.core.restapi.AggregationApi\u0026#34;, \u0026#34;tools\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;maven\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/maven\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Maven Warnings\u0026#34;, \u0026#34;size\u0026#34;: 9 }, { \u0026#34;id\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/java\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Java Warnings\u0026#34;, \u0026#34;size\u0026#34;: 1 }, { \u0026#34;id\u0026#34;: \u0026#34;javadoc\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/javadoc\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;JavaDoc Warnings\u0026#34;, \u0026#34;size\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;checkstyle\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/checkstyle\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;CheckStyle Warnings\u0026#34;, \u0026#34;size\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;pmd\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/pmd\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;PMD Warnings\u0026#34;, \u0026#34;size\u0026#34;: 671 }, { \u0026#34;id\u0026#34;: \u0026#34;spotbugs\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/spotbugs\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;SpotBugs Warnings\u0026#34;, \u0026#34;size\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: \u0026#34;cpd\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/cpd\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;CPD Warnings\u0026#34;, \u0026#34;size\u0026#34;: 123 }, { \u0026#34;id\u0026#34;: \u0026#34;open-tasks\u0026#34;, \u0026#34;latestUrl\u0026#34;: \u0026#34;http://localhost:8080/view/White%20Mountains/job/New%20-%20Pipeline%20-%20Simple%20Model/26/open-tasks\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Open Tasks Scanner Warnings\u0026#34;, \u0026#34;size\u0026#34;: 11 } ] } 分析结果汇总 # 你可以使用 URL [build-url]/[tool-id]/api/xml（或 [build-url]/[tool-id]/api/json）获得特定分析报告的摘要。摘要包含问题数量，质量门状态以及所有信息和错误消息。\n这是一个示例 XML 报告：\n\u0026lt;analysisResultApi _class=\u0026#39;io.jenkins.plugins.analysis.core.restapi.AnalysisResultApi\u0026#39;\u0026gt; \u0026lt;totalSize\u0026gt;3\u0026lt;/totalSize\u0026gt; \u0026lt;fixedSize\u0026gt;0\u0026lt;/fixedSize\u0026gt; \u0026lt;newSize\u0026gt;0\u0026lt;/newSize\u0026gt; \u0026lt;noIssuesSinceBuild\u0026gt;-1\u0026lt;/noIssuesSinceBuild\u0026gt; \u0026lt;successfulSinceBuild\u0026gt;-1\u0026lt;/successfulSinceBuild\u0026gt; \u0026lt;qualityGateStatus\u0026gt;WARNING\u0026lt;/qualityGateStatus\u0026gt; \u0026lt;owner _class=\u0026#39;org.jenkinsci.plugins.workflow.job.WorkflowRun\u0026#39;\u0026gt; \u0026lt;number\u0026gt;46\u0026lt;/number\u0026gt; \u0026lt;url\u0026gt;http://localhost:8080/view/White%20Mountains/job/Full%20Analysis%20-%20Model/46/\u0026lt;/url\u0026gt; \u0026lt;/owner\u0026gt; \u0026lt;infoMessage\u0026gt;Searching for all files in \u0026#39;/tmp/node1/workspace/Full Analysis - Model\u0026#39; that match the pattern \u0026#39;**/target/spotbugsXml.xml\u0026#39; \u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; found 1 file\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Successfully parsed file /tmp/node1/workspace/Full Analysis - Model/target/spotbugsXml.xml\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; found 3 issues (skipped 0 duplicates)\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Post processing issues on \u0026#39;node1\u0026#39; with encoding \u0026#39;UTF-8\u0026#39;\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Resolving absolute file names for all issues\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; affected files for all issues already have absolute paths\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Copying affected files to Jenkins\u0026#39; build folder /Users/hafner/Development/jenkins/jobs/Full Analysis - Model/builds/46 \u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; 2 copied, 0 not in workspace, 0 not-found, 0 with I/O error\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Resolving module names from module definitions (build.xml, pom.xml, or Manifest.mf files)\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; all issues already have a valid module name\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Resolving package names (or namespaces) by parsing the affected files\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; all affected files already have a valid package name\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Creating fingerprints for all affected code blocks to track issues over different builds\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;No filter has been set, publishing all 3 issues\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;No valid reference build found - all reported issues will be considered outstanding\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Evaluating quality qualityGates\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; WARNING - Total number of issues: 3 - Quality Gate: 1\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;-\u0026gt; Some quality qualityGates have been missed: overall result is WARNING\u0026lt;/infoMessage\u0026gt; \u0026lt;infoMessage\u0026gt;Health report is disabled - skipping\u0026lt;/infoMessage\u0026gt; \u0026lt;/analysisResultApi\u0026gt; Token 宏支持 # Warnings 插件提供了 token ANALYSIS_ISSUES_COUNT，可用于其他后期构建处理步骤，例如在邮件中。为了使用此 token，你需要安装 token 宏插件。token 具有以下可选参数：\ntool：选择特定的分析结果，如果未定义，则将所有结果相加 type：选择要使用的计数器的类型，请选择以下之一 Total（任何严重性） Total（仅错误） Total（仅严重度高） Total（仅严重级别正常） Total（仅限严重性低） New （任何严重程度） New （仅限错误） New （仅限严重性高） New （仅严重性为正常） New （仅限严重性低） Delta（任何严重程度） Delta（仅错误） Delta（仅严重度高） Delta（仅严重等级正常） Delta（仅严重度低） Fixed（任何严重性） 例子：\n${ANALYSIS_ISSUES_COUNT}：扩展到所有分析工具的合计数量\n${ANALYSIS_ISSUES_COUNT, tool=\u0026quot;checkstyle\u0026quot;}：扩展到CheckStyle问题的总数\n${ANALYSIS_ISSUES_COUNT, tool=\u0026quot;checkstyle\u0026quot;, type: \u0026quot;NEW\u0026quot;}：扩展到新的 CheckStyle 问题数\n从静态分析套件过渡 # 以前，静态分析套件的插件提供了相同的功能集（CheckStyle，PMD，FindBugs，静态分析实用工具，Analysis Collector，任务扫描器，Warnings 等）。为了简化用户体验和开发过程，这些插件和核心功能已合并到Warnings Next Generation 插件中。这些旧的静态分析插件不再需要，现在已经停产。如果当前使用这些旧插件之一，则应尽快迁移到新的记录器和步骤。我仍然会保留旧代码一段时间，但是主要的开发工作将花在新的代码库中。\n迁移 Pipelines 调用旧的静态分析步骤（例如，findbug，checkstyle 等）的管道需要立即调用新的 recordIssues 步骤。所有静态分析工具都使用相同的步骤，使用 step 属性工具选择实际的解析器。有关可用参数集的更多详细信息，请参见“配置”部分。\n迁移其他所有工作 使用旧版 API 的 Freestyle，Matrix 或 Maven Jobs 使用了由每个插件提供的所谓的 Post Build Action。例如，FindBugs 插件确实提供了构建后操作“发布 FindBugs 分析结果”。这些旧的插件特定操作不再受支持，它们现在在用户界面中标记为 [Deprecated]。现在，你需要添加一个新的后期构建步骤-对于所有静态分析工具，此步骤现在称为“记录编译器警告和静态分析结果”。工具的选择是此后期构建步骤配置的一部分。注意：新的后期制作操作无法读取使用旧 API 的后期制作步骤所产生的警告。也就是说，你看不到新旧结果的合并历史记录-你仅看到两个不相关的结果。也不会自动转换以旧格式存储的结果。\n插件的迁移取决于分析核心 以下插件已集成到此警告插件的新版本中：\nAndroid-Lint 插件 Analysis Collector 插件 CheckStyle 插件 CCM 插件 Dry 插件 PMD 插件 FindBugs 插件 Tasks Scanner 插件 Warnings 插件 所有其他插件仍需要集成或需要重构以使用新的 API\n","date":"2019-12-28","externalUrl":null,"permalink":"/posts/jenkins-warnings-next-generation-plugin/","section":"Posts","summary":"本文介绍了 Jenkins Warnings Next Generation 插件的功能和配置方法，包括如何收集编译器警告和静态分析工具报告的问题，并可视化结果。","title":"Jenkins Warnings Next Generation 插件","type":"posts"},{"content":"如果你是研发效能组的一员或者在从事 CI/CD 或 DevOps，除了提供基础设施，指标和数据是也是一个很重要的一环，比如需要分析下某个 Git 仓库代码提交情况：\n这个仓库的代码谁提交的代码最多 这个仓库的活跃度是什么样子的 各个时段的提交分析数据 每个版本的贡献排名 每周/每月/每年的贡献排名等等 几天前发现一个 Git 历史统计信息生成工具叫 GitStats (https://github.com/shenxianpeng/gitstats)\n这是一个用 python 写的，代码量很少，功能却非常强大的分析工具，也是我目前发现为数不多的可以生成漂亮的报告并且使用很方便的开源项目。\ngitstats 的报告也很强大 (https://shenxianpeng.github.io/gitstats/previews/main/index.html) 感兴趣的可以试试\n如何使用 # 需要的依赖：Git，Python3，Gnuplot。\n如果有 Linux，推荐在 Linux 上下载和安装。我曾尝试用 Windows 来准备环境，要配置 Cygwin，还要手动配置 Gnuplot （Gnuplot 是一个可移植的命令行驱动的图形工具），挺麻烦的。以下是我在 Linux 上来的安装和使用步骤。\n# 安装 Gnuplot sudo yum -y install gnuplot # 安装 gitstats pip install gitstats # 下载你要分析的代码仓库 git clone https://github.com/alibaba/fastjson.git # 执行命令，生产报告 gitstats ../fastjson ../html/fastjson # 经过 15 秒钟的执行，生成报告 Generating report... [0.00393] \u0026gt;\u0026gt; git --git-dir=/workspace/gitstats/.git --work-tree=/workspace/gitstats rev-parse --short HEAD [0.00236] \u0026gt;\u0026gt; git --version [0.00716] \u0026gt;\u0026gt; gnuplot --version Generating graphs... [0.01676] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/day_of_week.plot\u0026#34; [0.01571] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/files_by_date.plot\u0026#34; [0.01281] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/month_of_year.plot\u0026#34; [0.09293] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/lines_of_code_by_author.plot\u0026#34; [0.01340] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/commits_by_year.plot\u0026#34; [0.01799] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/hour_of_day.plot\u0026#34; [0.01627] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/domains.plot\u0026#34; [0.01268] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/commits_by_year_month.plot\u0026#34; [0.09435] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/commits_by_author.plot\u0026#34; [0.01522] \u0026gt;\u0026gt; gnuplot \u0026#34;/workspace/html/fastjson/lines_of_code.plot\u0026#34; Execution time 15.16208 secs, 4.58247 secs (30.22 %) in external commands) You may now run: sensible-browser \u0026#39;/workspace/html/fastjson/index.html\u0026#39; 报告分析 # 常规统计 # 可以看到这个项目叫 fastjson，从 2011 年 7 月 31 日开始提交第一次代码，距离当前已经 3060 天了，893 个活跃天。总共文件是 2960 个，一共的代码行数是 25 万行。\n活跃度 # 每天中的每小时的、每周中的每天、每周中的每小时，每年中的每月的、每年的提交数量。\n在过去的 32 周里，其中前 12 周到 17 周这段时间很活跃，提交很多。另外可以看到在白天 12 点到晚上 20 点这段时间提交很少，大部分程序员在这段时间都在应对公司里的工作，无法贡献开源项目。\n晚上 20 点之后提交开始增多，看来是一个非常努力的开发者。另外可以看到周一到周日都有提交，周六相对最少，应该是周六休息和放松一下。周日提交明显比周六多，看来还是花费了不少的个人宝贵周末时间。\n还能看到该项目主要完成时间在 2016 年到 2017 年，完成的时区是在东八区，这个维度可对不同区域的开发者贡献数量进行分析。\n贡献者 # 列举了所有的作者，提交数，第一次提交及最近一次提交。\n这张图也是可以看到谁是该项目的创建者，以及这几年以来每年的谁的贡献最大。还有可以看到提交者所用的邮箱账户。\n文件及行数 # 文件总数是 2960 个，代码行数是 25 万行。\n另外还可以看到每年的文件增加趋势图以及这些文件类型数量的排名，可以看到 java 文件占了 96.08%，其次是 json。\nTags # Tags 对于团队是一个很重要分析指标（前提是所要分析的仓库在版本发布后创建了 Tag）可以看到每个版本的贡献度排名。\n最后 # 如果感兴趣可以分析一下自己的项目或是去 GitHub 上找一个有意思、有影响力的项目拿来分析一下，比如 996.ICU 以及 vue 等，玩的开心。\n","date":"2019-12-17","externalUrl":null,"permalink":"/posts/git-stats/","section":"Posts","summary":"GitStats，一个用 Python 编写的 Git 历史统计信息生成工具，能够生成详细的代码提交统计报告，帮助开发者分析项目活跃度和贡献者情况。","title":"Git 历史统计信息生成器","type":"posts"},{"content":"最近做了 Black Duck 与 Jenkins 的集成，目标是给测试和开发提供定制化、定时的对各个开发代码仓库的进行源码扫描。\n为什么要做源码扫描 # 在产品开发中经常需要引入一些开源组件，但这些开源的代码会给产品风险。因此我们在发布自己产品的时候需要对这些开源组件的漏洞和许可信息进行评估。 Black Duck（黑鸭）是一款对源代码进行扫描、审计和代码管理的软件工具（同类型的工具还有 JFrog Xray）。能够搜索安全的开源代码，检测产品的开源代码使用情况，以检查外来代码的开源代码使用情况和风险情况。\n如果不能及时的进行代码扫描，在产品发布快要发布才进行扫描，如果发现问题这时候再去解决就会变得非常被动，因此团队需要尽早发现并解决问题，将 CI 工具进行集成，进行每日、每周、每月扫描就变得十分重要。\nBlack Duck 手动执行一般步骤 # 手动下载指定 Git 仓库及分支代码 去掉不相关的代码（也可以通过 Black Duck 参数去指定要扫描的特定文件或文件夹） 手动执行 Black Duck 扫描命令​ 扫描成功后，结果传到内部 Black Duck 网站供相关人员进行审查 Black Duck 与 Jenkins 的集成目标 # 一个流水线支持定制化仓库的代码下载 给开发和测试提供简单的、可随时可以执行源码扫描的界面 支持定期自动扫描，以及与其他 Jenkins 任务联动执行​ Black Duck 参数介绍 # --blackduck.url # 你的 Black Duck 网址 --blackduck.username # 你的登录用户 --blackduck.api.token # 你的登录用户 Token --detect.project.name # Black Duck 下面的项目 --detect.project.version.name # 项目版本号 --detect.source.path # 要扫描的代码目录 --logging.level.com.synopsys.integration # 扫描日志级别 --blackduck.trust.cert=TRUE # 是否信任 socket (SSL) --detect.blackduck.signature.scanner.snippet.matching # 扫描片段模式 更多其他参数可以参照官方的 CI 集成文档 Synopsys Detect for Jenkins\nBlack Duck 配置 # 首先，安装 Black Duck 插件 Synopsys Detect 到 Jenkins\n然后，配置 Synopsys Detect 插件\nJenkins -\u0026gt; Confiruration（系统配置） Black Duck URL： 公司内部的 Black Duck 网址，例如 https://yourcompany.blackducksoftware.com Black Duck credentials： 注意要选择 credentials 类型为 Secret text, Secret 填写你用户的 Token 配置完成后点击 Test connections to Black Duck，显示 Connection successful 表示配置成功。 Black Duck 流水线任务效果 # Black Duck 流水线代码 # pipeline{ agent { node { label \u0026#39;black-duck\u0026#39; customWorkspace \u0026#34;/agent/workspace/blackduck\u0026#34; } } parameters { choice( name: \u0026#39;VERSION\u0026#39;, choices: [\u0026#39;MVSURE_v1.1\u0026#39;, \u0026#39;MVSURE_v1.2\u0026#39;, \u0026#39;MVSURE_v2.2\u0026#39;], summary: \u0026#39;Which version do you want scan on black duck? MVSURE_v1.1, MVSURE_v1.2 or others?\u0026#39;) choice( name: \u0026#39;REPO\u0026#39;, choices: [\u0026#39;blog-server\u0026#39;, \u0026#39;blog-client\u0026#39;, \u0026#39;blog-docker\u0026#39;], summary: \u0026#39;Which repository code does above VERSION belong to?\u0026#39;) string( name: \u0026#39;BRANCH\u0026#39;, defaultValue: \u0026#39;develop\u0026#39;, summary: \u0026#39;Which branch does above VERSION belong to?\u0026#39;) choice( name: \u0026#39;SNIPPET-MODES\u0026#39;, choices: [\u0026#39;SNIPPET_MATCHING\u0026#39;, \u0026#39;SNIPPET_MATCHING_ONLY\u0026#39;, \u0026#39;FULL_SNIPPET_MATCHING\u0026#39;, \u0026#39;FULL_SNIPPET_MATCHING_ONLY\u0026#39;, \u0026#39;NONE\u0026#39;], summary: \u0026#39;What snippet scan mode do you want to choose?\u0026#39;) } environment { ROBOT = credentials(\u0026#34;d1cbab74-823d-41aa-abb7-858485121212\u0026#34;) hub_detect = \u0026#39;https://blackducksoftware.github.io/hub-detect/hub-detect.sh\u0026#39; blackduck_url = \u0026#39;https://yourcompany.blackducksoftware.com\u0026#39; blackduck_user = \u0026#39;robot@yourcompany.com\u0026#39; detect_project = \u0026#39;GITHUB\u0026#39; detect_project_version = \u0026#39;${VERSION}\u0026#39; detect_source_path = \u0026#39;${WORKSPACE}/${REPO}/src\u0026#39; } # 只保留最近十次 Jenkins 执行结果 options {buildDiscarder(logRotator(numToKeepStr:\u0026#39;10\u0026#39;))} # 定时触发可以在这里添加 stages { stage(\u0026#34;git clone\u0026#34;){ # 参数化 git clone 代码过程 steps{ sh \u0026#39;\u0026#39;\u0026#39; if [ -d ${REPO} ]; then rm -rf ${REPO} fi git clone -b ${BRANCH} --depth 1 https://$ROBOT_USR:\u0026#34;$ROBOT_PSW\u0026#34;@git.yourcompany.com/scm/github/${REPO}.git \u0026#39;\u0026#39;\u0026#39; } } stage(\u0026#34;black duck scan\u0026#34;){ # 参数化 Black Duck 所用到的参数值 steps { withCredentials([string(credentialsId: \u0026#39;robot-black-duck-scan\u0026#39;, variable: \u0026#39;TOKEN\u0026#39;)]) { # 用 withCredentials 来获得 Token synopsys_detect \u0026#39;bash \u0026lt;(curl -s ${hub_detect}) --blackduck.url=${blackduck_url} --blackduck.username=${blackduck_user} --blackduck.api.token=${TOKEN} --detect.project.name=${detect_project} --detect.project.version.name=${detect_project_version} --detect.source.path=${detect_source_path} --logging.level.com.synopsys.integration=debug --blackduck.trust.cert=TRUE --detect.blackduck.signature.scanner.snippet.matching=${SNIPPET-MODES}\u0026#39; } } } } post { # 不论结果任何都给执行者发送邮件通知 always { script { def email = load \u0026#34;vars/email.groovy\u0026#34; wrap([$class: \u0026#39;BuildUser\u0026#39;]) { def user = env.BUILD_USER_ID email.build(currentBuild.result, \u0026#34;${user}\u0026#34;) } } } success { echo \u0026#34;success, cleanup blackduck workspace\u0026#34; cleanWs() } } } ","date":"2019-12-08","externalUrl":null,"permalink":"/posts/blackduck-interate-with-jenkins/","section":"Posts","summary":"本文介绍如何将 Black Duck 与 Jenkins 集成，实现对代码仓库的自动化安全扫描和漏洞检测。","title":"Black Duck 与 Jenkins 集成","type":"posts"},{"content":" Docker 常用命令小纸条\nDocker start|stop|restart # # 查看 Docker 版本 docker -v # or docker --version # 重启 docker sudo systemctl restart docker.service # 停止 docker sudo systemctl stop docker.service # 启动 docker sudo systemctl start docker.service Docker run # 我们通过 docker 的两个参数 -i -t，让 docker 运行的容器实现\u0026quot;对话\u0026quot;的能力：\ndocker run -i -t ubuntu:15.10 /bin/bash Login Artifactory # 注意：Open Source 版本 Artifactory 不支持 Docker，需要下载 JFrog Container Registry 或是 Artifactory 企业版。\ndocker login -u \u0026lt;USER_NAME\u0026gt; -p \u0026lt;USER_PASSWORD\u0026gt; devasvm.dev.org.com:\u0026lt;REPOSITORY_PORT\u0026gt; -sh-4.2$ sudo docker login devasvm.dev.org.com:8040 Username: admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 把 Docker image 推送到远程仓库\n// docker tag SOURCE_IMAGE[:TAG] devasvm.dev.org.com:8040/docker-local/IMAGE[:TAG] -sh-4.2$ sudo docker tag ubuntu:15.10 devasvm.dev.org.com:8040/docker-local/ubuntu:15.10 // docker push devasvm.dev.org.com:8040/docker-local/IMAGE[:TAG] -sh-4.2$ sudo docker push devasvm.dev.org.com:8040/docker-local/ubuntu:15.10 The push refers to repository [devasvm.dev.org.com:8040/docker-local/ubuntu] 98d59071f692: Pushed af288f00b8a7: Pushed 4b955941a4d0: Pushed f121afdbbd5d: Pushed 15.10: digest: sha256:a3f5e428c0cfbfd55cffb32d30b1d78fedb8a9faaf08efdd9c5208c94dc66614 size: 1150 TODO # 更多 Docker 常用命令记录到这里。\n","date":"2019-12-02","externalUrl":null,"permalink":"/posts/docker-commands/","section":"Posts","summary":"一个 Docker 常用命令小纸条，记录一些常用的 Docker 命令和操作，方便日常使用和参考。","title":"Docker 常用命令","type":"posts"},{"content":" Docker 可分为三个版本 # Docker Engine - Community\nDocker Engine - Enterprise\nDocker Enterprise\nDocker Engine - Community 是希望开始使用 Docker 并尝试基于容器的应用程序的个人开发人员和小型团队的理想选择。\nDocker Engine - Enterprise 专为企业开发容器运行时而设计，同时考虑了安全性和企业级SLA。\nDocker Enterprise 专为企业开发和IT团队而设计，他们可以大规模构建，交付和运行关键业务应用程序。\n能力 Docker Engine - Community Docker Engine - Enterprise Docker Enterprise 容器引擎和内建的编配，网络，安全 √ √ √ 认证的基础设施，插件和ISV容器 √ √ 镜像管理 √ 容器应用程序管理 √ 镜像安全扫描 √ 安装 Docker 社区版本 # 以 CentOS 安装为例： https://docs.docker.com/install/linux/docker-ce/centos/ 其他 Docker 版本安装 # 参考 Docker 官网：https://docs.docker.com/install/overview/ ","date":"2019-12-01","externalUrl":null,"permalink":"/posts/overview-of-docker-editions/","section":"Posts","summary":"概述 Docker 的不同版本，包括社区版、企业版和企业级解决方案，适用于不同规模和需求的用户。","title":"Docker 版本概述","type":"posts"},{"content":"本周二下班我没有像往常一样加会班（我一般都会赶在晚6点后下班来躲过晚高峰期），而是直接挤地铁奔向机场，准备坐八点半去往北京的一班飞机，因为第二天要参加 JFrog 中国在北京望京举办的 Jenkins, Artifactory \u0026amp; Kubernetes 实战训练营。\n一是由于公司每人每年都有两天带薪培训假期，如果有特别适合我的，我会自费前往。二是培训内容本身也十分贴近我目前的工作内容，想了解下行业最佳实践与相关同行交流。\n在出行前，跟领导请了假说明意向，顺便问了下去参加类似培训是否会有预算，领导让我报一下，我大概算了车票和酒店的钱，领导就跟公司申请，并且还附带帮我申请了饭钱。公司同意，前提只有一个就是回来要做好分享。就这样我就带着任务去参加这次 JFrog DevOps 训练营了，这里我特别感谢我的领导以及公司。\n再次见北京 # 想想离开北京已经5年多了，这是唯一一次在北京停留时间最长的 48 小时，之前有过出差路过北京。以前虽然在北京工作，但那时候我还没有太多关注过生活，这次有意的注意了下，脑子里一直思考一个问题，如果再让我回到北京我还会像当初的执念仅仅想着北京工作几年而已吗？\n下了飞机已经十点半了，从机场到望京这段的机场高速车流量还是很大的，北京的夜生活跟以前的感觉一样，比起二三线城市足足晚了两个多小时。五年前在北我京通常都是九点后从公司走，赶上上线的时候后半夜才到家，第二天中午到公司，每天的时间比二三线城市现在我足足延后了五六个小时。\n我现在的生活是早九晚六，不推崇加班，上班期间也是很忙，但绝大多数人也不会加班，加不加班是自愿。期间去过美国出差，美国的同事很多是早上七八点上班，他们一般中午吃的很简单也很快，吃完饭如果工作忙，他们会立马投入工作，下午三四点钟就离开办公室了，待到五六点钟的极少数，但工作效率其实感觉不差。这让我想起了今年轰轰烈烈的讨论过的996，可能归根到底是我们的社会发展阶段所造成的，年轻人压力很大需要努力赚钱买房，有娃的人有自己的父母来帮忙照顾孩子，年老的人也不考虑自己的退休生活，更多的是希望能帮忙自己的孩子解决一些后顾之忧，就这样年轻人就可以安心的的996了。但是美国人他们不行，很现实的他们如果不三四点钟下班，他们的孩子就没人接，他们需要自己的家庭生活。因此，我们本应该由企业甚至是社会来承担事情，被全社会的来承担了，996就自然而然不可避免的发生了。\n早高峰时段，望京有的路口有年长的大爷大妈在指挥交通，每当绿灯时，大量的电动车和自行车与行人一同穿行，我总担心会被刮到。晚上下班吃完饭回去的时候，路口没有指挥了，行人、自行车以及电动车在红灯时过马路的情况还是挺普遍的，造成了绿灯时机动车通行效率很低。另外，就是走在路上，后面不时的有自行车、电动车骑过，我总是挺担心被撞到，这种走路时候担心的感觉其实是不太舒服的。\n中午跟朋友约了吃饭，听他聊起过去五年多的工作情况，期间他换了好几个公司，有勾心斗角的，有 P2P 黄的（工资还欠着的，还在仲裁中），谈起最近一年北京大厂裁员以及网易最近的裁员风波，都让我感受在哪混着都挺不容易的。随着他要当爸爸了，从没有考虑过要离开北京的他也有了离开的念头，如果不能在将来在北京购买个小房子，他可能就回到家乡，住他自己已经购买房子里生活。\n随着企业成本控制，不少企业已经去二三线城市发展，那里的租金甚至比北京便宜四五倍，如果能招到人的话，他们就可以落地二三线了。当初我在北京的时候，我的室友就在中国移动研究院，后来中国移动研究院搬到了苏州，他和几个小伙伴也都去了苏州，现在已经早早在那里买房了，那边环境很好，有自己的房子，工资也不低，其实生活挺舒服的。尤其是软件行业，有的公司允许远程办公，那只要能满足岗位要求，其他也挺好的。身边就有朋友他们的公司在北京撤除了办公室，他们现在就在家办公，隔一段时间可以去出差去二三线城市的办公室与同事工作交流。\n此时，脑海中的问题一直缠绕，再给你一次机会你还想在留在北京生活吗？我的决定和当初一样回到现在的城市。\n我喜欢这里的工作生活的平衡，加班也都是主动加班学习，没有被迫的加班给领导看；我喜欢这里的海，中午吃完饭就可以跟同事一起散步走到海边看海；我喜欢这里离父母很近，开车半个小时就能到，可以经常与他们相聚照顾他们；我喜欢住在自己的房子，不用担心搬家了，可以不断的改善生活环境；我喜欢这里人不太多、该有的专卖店和商城都有、有地铁，去哪里都不算太远。\n周四下午 5 点培训完，吃了个饭就直奔机场连夜回到家，第二天回来继续上班了。这短短的两天，往返两座城市之间工作和培训，飞机上只需要40分钟，就像没有离开过一样，让人感叹交通的便利。\n祝愿每个人都收获自己享受的生活。\n2019 年 12 月 1 日 23 : 55 : 00\n","date":"2019-12-01","externalUrl":null,"permalink":"/posts/48h-in-beijing/","section":"Posts","summary":"记录我在北京参加 JFrog DevOps 训练营的经历，分享培训内容和个人感受，以及对未来工作的思考。","title":"北京48小时：记一次参加 DevOps 训练营","type":"posts"},{"content":"对于如何备份 Jenkins 除了用 Jenkins 插件来定期备份之外，如果把 Jenkins 安装到 Docker 里，定期备份一个 Docker Image 最后传到 Artifactory 中，也是一个不错的方案。\n安装 Docker 版 Jenkins # 在 CentOS 上安装 Docker 版 Jenkins，这里推荐用 Long-term Support (LTS) 版本，可以从 Jenkins 官网下载。\n# 下载指定 lts 版本 2.130 sudo docker pull jenkins/jenkins:2.130 # 运行指定 docker Jenkins sudo docker run -p 8080:8080 -p 50000:50000 jenkins/jenkins:2.130 # 如果想下载最新的 lts 版 sudo docker pull jenkins/jenkins:lts # 运行最新的 lts 版 docker Jenkins sudo docker run -p 8080:8080 -p 50000:50000 jenkins/jenkins:lts 启动成功后即可打开 http://hostname:8080/ 网址\n修改登录密码 # 显示所有的 image 以及正在运行的 container\n# 列出来所有 image sudo docker image list # 列出当前运行的 container sudo docker ps # 进入容器，使用 -it 参数 sudo docker exec -it 39bc7a8307d9 /bin/bash # 查看默认 admin 密码 jenkins@a6195912b579:/$ cat /var/jenkins_home/secrets/initialAdminPassword 5193d06c813d46d3b18babeda836363a 建议登录之后，修改 admin 密码，方便下次登录\nsudo docker commit 39bc7a8307d9 myjenkins:v0.1 将宿主机目录映射到 Jenkins Docker 中 # 如果想让 Docker 里的 Jenkins 可以访问宿主机的目录，在运行 docker 时使用 -v 参数进行 mount volume\nsudo docker run -p 8080:8080 -p 50000:50000 --name mydata -v /data/backup:/home/backup jenkins/jenkins:2.130 # 映射成功，可以看到宿主机上的备份文件了 jenkins@c85db3f88115:/home/backup$ ls FULL-2019-09-14_02-00 FULL-2019-09-28_02-00 FULL-2019-10-19_02-00 FULL-2019-11-02_02-00 FULL-2019-11-23_02-00 FULL-2019-09-21_02-00 FULL-2019-10-05_02-00 FULL-2019-10-26_02-00 FULL-2019-11-09_02-00 FULL-2019-11-30_02-00 将 Jenkins Docker Image 保存在 Artifactory # 下载并安装 Artifactory 企业版或是 JFrog Container Registry，注意 Artifactory Open Source 版本不支持 Docker Registry。\n例如我的 JFrog Container Registry 是：dln.dev.mycompany.com:8040，并创建了一个 docker repository 叫 docker-local。\n上传 Docker Image 一共分为三步：\ndocker login\n# 在登录前需要添加如下配置到 /etc/docker/daemon.json { \u0026#34;insecure-registries\u0026#34; : [\u0026#34;dln.dev.mycompany.com:8040\u0026#34;] } # docker login \u0026lt;DOCKER_SERVER\u0026gt;, example below: sudo docker login dln.dev.mycompany.com:8040 docker tag\n# docker tag \u0026lt;IMAGE_ID\u0026gt; artprod.mycompany/\u0026lt;DOCKER_REPOSITORY\u0026gt;:\u0026lt;DOCKER_TAG\u0026gt;, example below: sudo docker tag myjenkins:v0.1 dln.dev.mycompany.com:8040/docker-local/myjenkins:v0.1 docker push\n# docker push artprod.mycompany/\u0026lt;DOCKER_REPOSITORY\u0026gt;:\u0026lt;DOCKER_TAG\u0026gt;, example below: $ sudo docker push dln.dev.mycompany.com:8040/docker-local/myjenkins::v0.1 The push refers to repository [dln.dev.mycompany.com:8040/docker-local/myjenkins] 98d59071f692: Pushed af288f00b8a7: Pushed 4b955941a4d0: Pushed f121afdbbd5d: Pushed 15.10: digest: sha256:a3f5e428c0cfbfd55cffb32d30b1d78fedb8a9faaf08efdd9c5208c94dc66614 size: 1150 登录 JFrog Container Registry 刷新就可以到已经上次的 Image 了。说明：截图是我上传的另外一个镜像 ubuntu:15.10\n","date":"2019-12-01","externalUrl":null,"permalink":"/posts/install-docker-jenkins/","section":"Posts","summary":"如何定制一个 Docker 版 Jenkins 镜像，并将其备份到 Artifactory，便于在需要时快速恢复 Jenkins 环境。","title":"定制一个 Docker 版 Jenkins 镜像","type":"posts"},{"content":"上一篇 初识 JFrog Artifactory，介绍了什么是 Artifactory，以及如何安装、启动和升级。\n本篇介绍 Artifactory 与 Jenkins 的集成，因为没有与 CI 工具集成的 Artifactory 是没有灵魂的。\n通过集成，可以让 Jenkins 在完成构建之后，可以直接将制品（比如 build）推送到 Artifactory，供测试下载、部署或是后续的 Jenkins 任务去继续进行持续集成。\nJenkins 里配置 Artifactory # 打开 Manage Jenkins-\u0026gt;Configure System，找到 Artifactory，点击 Add Artifactory Server， 输入 Server ID 和 URL\nServer ID 是给你的 Artifactory 起个别名，这样使用 Jenkins pipeline 的时候会用到 URL 是你的 Artifactory 服务器的地址，例如 http://art.company.com:8040/artifactory 配置完成后，点击Test Connection，返回 Found Artifactory 6.14.0 表示配置成功。 如图所示: 使用 Pipeline 调用 Artifactory # 这里演示了两种方式，我在项目中用的是 Jenkins Shared Library；当然你也可以仅仅使用 Jenkinsfile，把如下两个 groovy 文件组合成一个 Jenkinsfile。\n方式1：Jenkins Shared Library # build.groovy\ndef call() { pipeline { # 省略其他代码 post { # 这里只有在 Jenkins Job 成功的时候才将 build post 到 artifactory success { script { if (env.BRANCH_NAME == \u0026#39;develop\u0026#39;) { # 如果当前是 develop 分支，则将 release 和 debug build 都 post 到 artifactory artifactory(\u0026#34;${PATTERN_RELEASE_PATH}\u0026#34;, \u0026#34;${TARGET_PATH}\u0026#34;, \u0026#34;${BUILD_NAME}\u0026#34;, \u0026#34;${BUILD_NUMBER}\u0026#34;) artifactory(\u0026#34;${PATTERN_DEBUG_PATH}\u0026#34;, \u0026#34;${TARGET_PATH}\u0026#34;, \u0026#34;${BUILD_NAME}\u0026#34;, \u0026#34;${BUILD_NUMBER}\u0026#34;) } else if (env.BRANCH_NAME.startsWith(\u0026#39;PR\u0026#39;)) { # 如果当前是 pull request 分支，则只将 release build 都 post 到 artifactory artifactory(\u0026#34;${PATTERN_RELEASE_PATH}\u0026#34;, \u0026#34;${TARGET_PATH}\u0026#34;, \u0026#34;${BUILD_NAME}\u0026#34;, \u0026#34;${BUILD_NUMBER}\u0026#34;) } } } } } } artifactory.groovy\nimport groovy.transform.Field @Field artifactoryServerId = \u0026#34;art-1\u0026#34; @Field artifactoryURL = \u0026#34;http://art.company.com:8040/artifactory\u0026#34; @Field artifactoryCredential = \u0026#34;d1cbab74-823d-41aa-abb7\u0026#34; def call(String patternPath, String targetPath, String buildName, String buildNumber) { rtServer ( id: \u0026#34;${artifactoryServerId}\u0026#34;, url: \u0026#34;${artifactoryURL}\u0026#34;, credentialsId: \u0026#34;${artifactoryCredential}\u0026#34; ) rtPublishBuildInfo ( serverId: \u0026#34;${artifactoryServerId}\u0026#34; ) rtUpload ( serverId: \u0026#34;${artifactoryServerId}\u0026#34;, spec: \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;files\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;${patternPath}\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;${targetPath}\u0026#34; } ] }\u0026#34;\u0026#34;\u0026#34;, buildNumber: \u0026#34;${buildNumber}\u0026#34;, buildName: \u0026#34;${buildName}\u0026#34;, ) } 方式2：Jenkinsfile # pipeline { # 省略其他代码 stage(\u0026#39;config art\u0026#39;){ rtServer ( id: \u0026#34;art-1\u0026#34;, url: \u0026#34;http://art.company.com:8040/artifactory\u0026#34;, credentialsId: \u0026#34;d1cbab74-823d-41aa-abb7\u0026#34;) } post { # 这里只有在 Jenkins Job 成功的时候才将 build post 到 artifactory success { script { if (env.BRANCH_NAME == \u0026#39;develop\u0026#39;) { rtUpload ( serverId: \u0026#34;art-1\u0026#34;, spec: \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;files\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;/release/build/*.zip\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;demo/develop/\u0026#34; } ] }\u0026#34;\u0026#34;\u0026#34;, buildNumber: \u0026#34;${buildNumber}\u0026#34;, buildName: \u0026#34;${buildName}\u0026#34;, ) } else if (env.BRANCH_NAME.startsWith(\u0026#39;PR\u0026#39;)) { rtUpload ( serverId: \u0026#34;art-1\u0026#34;, spec: \u0026#34;\u0026#34;\u0026#34;{ \u0026#34;files\u0026#34;: [ { \u0026#34;pattern\u0026#34;: \u0026#34;/release/build/*.zip\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;demo/pull-request/\u0026#34; } ] }\u0026#34;\u0026#34;\u0026#34;, buildNumber: \u0026#34;${buildNumber}\u0026#34;, buildName: \u0026#34;${buildName}\u0026#34;, ) } } } } } Jenkins 与 Artifactory 集成成功 # 蓝色表示构建成功，绿色圆圈表示 Build 已经 Post 到 Artifactory 上了。\n点击绿色圆圈可以跳转到 Artifactory 看到制品。\nJenkins 与 Artifactory 打通了。完！\n","date":"2019-11-17","externalUrl":null,"permalink":"/posts/artifactory-integrate-with-jenkins/","section":"Posts","summary":"本文介绍如何将 JFrog Artifactory 与 Jenkins 集成，实现持续集成和制品管理。","title":"Artifactory 与 Jenkins 集成","type":"posts"},{"content":" 什么是 Artifactory # Artifactory 是 JFrog 的一个产品，用作二进制存储库管理器。二进制存储库可以将所有这些二进制统一托管，从而使团队的管理更加高效和简单。\n就跟你用 Git 一样，Git 是用来管理代码的，Artifactory 是用来管理二进制文件的，通常是指 jar, war, pypi, DLL, EXE 等 build 文件。\n我觉得使用 Artifactory 的最大优势是创造了更好的持续集成环境，有助于其他持续集成任务去 Artifactory 里调用，再部署到不同的测试或开发环境，这对于实施 DevOps 至关重要。\n如果想了解更多有关 Artifactory，请参看 中文官网 以及 English Website。\n安装 Artifactory # 从官网下载 Open Source Artifactory，这里演示的是安装到 Linux，所以点击 Download RPM 下载 将下载好的 jfrog-artifactory-oss-6.14.0.rpm 上传到 Linux 上 # 在根目录创建一个文件，你也可以在任何目录创建文件夹 sudo mkdir /artifactory cd /artifactory # 将下载好的 jfrog-artifactory-oss-6.15.0.rpm 上传到你的 Linux 上 $ ls jfrog-artifactory-oss-6.14.0.rpm # 安装 artifactory sudo rpm -ivh jfrog-artifactory-oss-6.14.0.rpm Artifactory 服务启动和关闭 # # 启动服务 sudo systemctl start artifactory.service # 在使用上面的命令启动服务的时候遇到如下错误： # Job for artifactory.service failed because a configured resource limit was exceeded. See \u0026#34;systemctl status artifactory.service\u0026#34; and \u0026#34;journalctl -xe\u0026#34; for details. # 详情：https://www.jfrog.com/jira/browse/RTFACT-19988 # 可尝试如下命令启动 cd /opt/jfrog/artifactory/app/bin \u0026amp;\u0026amp; ./artifactory.sh start \u0026amp; # 停止服务 sudo systemctl stop artifactory.service # 查看服务状态 sudo systemctl status artifactory.service 访问 Artifactory # Artifactory 默认端口是8040，安装成功后访问：http://hostname:8040 即可登录（默认用户名 admin 密码 password） Artifactory 升级 # 从官网下载最新的 Artifactory\n将下载好的 jfrog-artifactory-oss-6.15.0.rpm（目前最新）上传到你的 Linux 上\ncd /artifactory ls jfrog-artifactory-oss-6.14.0.rpm jfrog-artifactory-oss-6.15.0.rpm # 停止服务 sudo systemctl stop artifactory.service # 进行升级 sudo rpm -U jfrog-artifactory-oss-6.15.0.rpm # 输出日志，显示升级成功 warning: jfrog-artifactory-oss-6.15.0.rpm: Header V4 DSA/SHA1 Signature, key ID d7639232: NOKEY Checking if ARTIFACTORY_HOME exists Removing tomcat work directory Removing Artifactory\u0026#39;s exploded WAR directory Initializing artifactory service with systemctl... ************ SUCCESS **************** The upgrade of Artifactory has completed successfully. Start Artifactory with: \u0026gt; systemctl start artifactory.service Check Artifactory status with: \u0026gt; systemctl status artifactory.service NOTE: Updating the ownership of files and directories. This may take several minutes. Do not stop the installation/upgrade process. Artifactory 卸载 # 停止 Artifactory 服务 systemctl stop artifactory.service 使用 root 用户执行 RPM uninstall 命令 # remove OSS version yum erase jfrog-artifactory-oss # remove PRO version, etc. yum erase jfrog-artifactory-pro 更多关于 JFrog 产品的卸载，请看：https://www.jfrog.com/confluence/display/JFROG/Uninstalling+JFrog+Products\n安装 JFrog CLI # # ON MAC brew install jfrog-cli-go # WITH CURL curl -fL https://getcli.jfrog.io | sh # WITH NPM npm install -g jfrog-cli-go # WITH DOCKER docker run docker.bintray.io/jfrog/jfrog-cli-go:latest jfrog -v CLI for JFrog Artifactory\n如何在 Artifactory 上使用 JFrog CLI\n","date":"2019-11-10","externalUrl":null,"permalink":"/posts/artifactory-install-and-upgrade/","section":"Posts","summary":"JFrog Artifactory 是一个强大的二进制存储库管理器，本文介绍其安装、升级和使用方法。","title":"初识 JFrog Artifactory","type":"posts"},{"content":"如果想让 Jenkins Console Output 出来一些重要日志醒目的显示，可以让一些日志显示颜色方便查看\n首先需要安装插件： https://wiki.jenkins.io/display/JENKINS/AnsiColor+Plugin\n安装成功后进入系统设置\n","date":"2019-09-24","externalUrl":null,"permalink":"/posts/jenkins-output-display-color/","section":"Posts","summary":"本文介绍了如何在 Jenkins 的 Console Output 中使用颜色来突出显示重要日志信息，提升可读性和易用性。","title":"Jenkins Console Output 显示彩色","type":"posts"},{"content":"如何针对 Jenkins 里的不同 Job 进行不同的策略管理。比如某个 Job 所有人都可以查看，但仅限于某些人可以执行，这时候就需要对 Job 行程权限设置。\n这里用的插件是 Role-based Authorization Strategy。安装成功后，打开要设置的 Job, 设置如下：\n","date":"2019-09-24","externalUrl":null,"permalink":"/posts/jenkins-privilege-management/","section":"Posts","summary":"本文介绍了如何在 Jenkins 中进行权限管理，包括如何设置 Job 的访问权限和执行权限，以确保安全和高效的 CI/CD 流程。","title":"Jenkins privilege management","type":"posts"},{"content":"","date":"2019-09-10","externalUrl":null,"permalink":"/tags/nfs/","section":"标签","summary":"","title":"NFS","type":"tags"},{"content":"例如我有一个共享仓库的代码所在用的空间非常大（超过 20 G），在每个产品构建时候都需要用到这个仓库的代码（从里面 copy 第三方库），如果每个人都要 git clone 这个第三方仓库，一是网络开销非常大，二是 git clone 时间长，而且占用大量的物理空间。\n这可以通过 NFS 共享来解决。\n另外希望这个代码仓库能自动更新，这里引入了 Jenkins。用它来检查如果这个容量巨大的仓库有代码提交就自动执行 git pull 操作，更新最新的代码到共享服务器上。\n什么是 NFS？NFS（Network File System）即网络文件系统，是 FreeBSD 支持的文件系统中的一种，它允许网络中的计算机之间共享资源。在 NFS 的应用中，本地 NFS 的客户端应用可以透明地读写位于远端 NFS 服务器上的文件，就像访问本地文件一样，Windows 上俗称共享。\n设置 NFS # # 例如在 Linux 上, 共享服务器的 ip 是 192.168.1.1 sudo vi /etc/exports # 以下是我的 exports 文件的配置 # 假设内网 ip 是这样的区间 192.168.1.1 ~ 192.168.1.250 # ro 表示只读 # all_squash 表示不管使用 NFS 的用户是谁，他的身份都会被限定成为一个指定的普通用户身份(nfsnobody) /agent/workspace/opensrc 192.168.1.*(ro,all_squash) /agent/workspace/opensrc dev-team-a*.com(ro,all_squash) /agent/workspace/opensrc dev-team-b*.com(ro,all_squash) /agent/workspace/opensrc dev-ci*(ro,all_squash) NFS 操作 # 启动 NFS 服务 # 启动 NFS 服务，需要启动 portmap 和 nfs 两个服务，并且 portmap 一定要先于 nfs 启动\nservice portmap start service nfs start # 查看 portmap 状态 service portmap status 查看服务状态 # service nfs status 停止服务 # service nfs stop 导出配置 # 当改变/etc/exports配置文件后，不用重启 NFS 服务直接用这个 exportfs 即可\nsudo exportfs -rv 挂载到不同平台 # Windows # # Install the NFS Client(Services for NFS) # Step 1: Open Programs and Features # Step 2: Click Turn Windows features on or off # Step 3: Find and check option Services for NFS # Step 4: Once installed, click Close and exit back to the desktop. refer to https://graspingtech.com/mount-nfs-share-windows-10/ $ mount -o anon 192.168.1.1:/agent/workspace/opensrc Z: Linux/Unix # # Linux sudo mount -t nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc # AIX sudo nfso -o nfs_use_reserved_ports=1 # should only first time mount need to run this command sudo mount -F nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc # HP-UX sudo mount -F nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc # Solaris-SPARC # 如果你不能直接在命令行执行 mount sudo /usr/sbin/mount -F nfs 192.168.1.1:/agent/workspace/opensrc /agent/workspace/opensrc ","date":"2019-09-10","externalUrl":null,"permalink":"/posts/nfs/","section":"Posts","summary":"本文介绍了如何设置 NFS 共享以及在不同平台（Windows/Linux/Unix）上进行挂载的步骤和命令。","title":"如何设置 NFS 共享以及在不同的平台 Windows/Linux/Unix 进行挂载","type":"posts"},{"content":"最近我在运行 Jenkins Job 时候突然发现 git clone 代码的时候突然报了这个错误：\n$ git clone ssh://git@git.companyname.com:7999/repo/opensrc.git Cloning into \u0026#39;opensrc\u0026#39;... fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 这个错误只在我刚开始使用 git 的时候遇到过，那时候我还不知道如何使用 ssh 的方式来 clone 代码。怎么会出现这个错误呢？我也没改过什么，非常不理解。\n常见解决方案 # Google 了没有找到我遇到的这个问题，绝大多数都是应为没有生成 ssh-key，生成后将 pub key 添加到 Github 或是其他 Web git 管理平台，基本就解决了。以 GitHub 为例\n首先，生成 SSH key\n# 记得替换成你自己的邮箱账号 ssh-keygen -t rsa -C xianpeng.shen@gmail.com 其次，拷贝 SSH pub key 到你使用的 git web 平台，比如 Github 等等。\ncd %userprofile%/.ssh # 打开 id_rsa.pub 并拷贝内容 notepad id_rsa.pub 最后，打开 https://github.com/settings/ssh/new 把你复制的内容贴进去保存即可。\n对于我遇到的问题，这种解决方式是无效的，因为同样的账号在别的虚拟机上并不存在这个问题，因为同样是 HP-UX 虚拟机，我用另外一个账号生成 ssh-key, git clone 代码是没有问题的，那我猜测是这两个账号的之间存在差异。\n通过 SSH 连接测试排查 # 首先，我查看了这两个账号的 .gitconfig 文件，确实有差异。当我将好用的账号的 .gitconfig 内容复制到不好用的账号的 .gitconfig 文件时，并不好用。\n其次，我发现执行 git clone 的时候在当前目录下生成了一个 core 文件，说明已经 coredump 了，但是这个 core 直接打开大部分都是乱码，错误信息很难准确定位。\n最后，我通过命令来测试 SSH 连接的。对于 Github 是这个命令\nssh -T git@github.com 我当前使用和出问题的是 Bitbucket，它的 SSH 连接测试命令是：\nssh -vvv git\\@bitbucket.org 我先用好 git clone 好用的账号，测试结果如下，这里我省略一些其他返回信息。\n$ ssh -vvv git\\@bitbucket.org OpenSSH_6.2p1+sftpfilecontrol-v1.3-hpn13v12, OpenSSL 0.9.8y 5 Feb 2013 # OpenSSH 版本不同 HP-UX Secure Shell-A.06.20.006, HP-UX Secure Shell version # 原来是调用路径不同 debug1: Reading configuration data /opt/ssh/etc/ssh_config debug3: RNG is ready, skipping seeding debug2: ssh_connect: needpriv 0 debug1: Connecting to bitbucket.org [18.205.93.1] port 22. debug1: Connection established. ... ... debug2: we did not send a packet, disable method debug1: No more authentication methods to try. Permission denied (publickey). 我再用 git clone 不好用的账号进行测试，结果返回如下：\n$ ssh -vvv git\\@bitbucket.org OpenSSH_8.0p1, OpenSSL 1.0.2s 28 May 2019 # OpenSSH 版本不同 debug1: Reading configuration data /usr/local/etc/ssh_config # 原来是调用路径不同 debug2: resolving \u0026#34;bitbucket.org\u0026#34; port 22 debug2: ssh_connect_direct debug1: Connecting to bitbucket.org [180.205.93.10] port 22. debug1: Connection established. Memory fault(coredump) $ 明显看到他们使用了不同版本的 OpenSSH，说明他们的环境变量有所不同。我之前查看过环境变量，但由于变量很多，不能一下判断那个变量可能会有影响。\n最终解决方案 # 回到 git clone 失败的那个账号下面的 .profile 文件查看，这里确实添加了一个 /usr/bin 的环境变量，导致这个账号在执行 git clone 时候用了另外版本的 OpenSSH，我用的是 HP-UX，它对于包之前的依赖以及版本要求都非常高，把这个环境变量去掉之后，保存，重新登录到虚拟机，执行 git clone 恢复正常。\n","date":"2019-09-01","externalUrl":null,"permalink":"/posts/could-not-read-from-remote-repository/","section":"Posts","summary":"解决在使用 Git 克隆代码时遇到的 \u0026ldquo;Could not read from remote repository\u0026rdquo; 错误，分析原因并提供解决方案。","title":"解决 Could not read from remote repository 问题","type":"posts"},{"content":" 本地提交尚未推送到远程 # 如果要将本地的多个提交合并为一个，可以按以下流程操作。\n这里有一个 3 分钟短视频 讲解了 git rebase -i 的用法。\n查看本地仓库的日志： git log --oneline ``\n假设要合并最近的 3 个提交（add6152、3650100、396a652）：\ngit rebase -i HEAD~3 将需要合并到前一个提交的记录改为 s 或 squash： 保存退出（ESC → :wq!）。\n在提交信息编辑界面中，注释掉不需要保留的提交信息： 查看日志，确认已合并为一个提交： 提交已推送到远程 # 如果提交已推送到远程，建议新建分支进行 squash 以避免影响已有分支历史。\n查看日志： 新建分支：\ngit checkout -b bugfix/UNV-1234-for-squash 合并最近 2 个提交：\ngit rebase -i HEAD~2 修改提交信息，例如：\nUNV-1234 combine all commit to one commit 推送新分支到远程：\ngit push -u origin bugfix/UNV-1234-for-squash ","date":"2019-08-20","externalUrl":null,"permalink":"/posts/git-commit-squash/","section":"Posts","summary":"介绍如何将多个 Git 提交合并为一个提交，包括本地和已推送到远程的情况，分别使用交互式 rebase 和在 Bitbucket 中的合并策略。","title":"Git 提交合并（Squash）","type":"posts"},{"content":"","date":"2019-08-20","externalUrl":null,"permalink":"/tags/squash/","section":"标签","summary":"","title":"Squash","type":"tags"},{"content":" 业务场景 # 日常工作中需要切换到不同平台（包括 Linux, AIX, Windows, Solris, HP-UX）不同的版本进行开发和验证问题，但是由于虚拟机有限，并不能保证每个开发和测试都有所以平台的虚拟机并且安装了不同的版本，因此准备各种各样的开发和测试环境会花费很长时间。\n需求分析 # 对于这样的需求，一般都会首先想到 Docker；其次是从 Artifactory 取 Build 然后通过 CI 工具进行安装；最后从 Source Code 进行构建然后安装。\n先说 Docker，由于我们所支持的平台繁多，包括 Linux, AIX, Windows, Solris, HP-UX, Docker 只适用于 Linux 和 Windows，因此不能满足这样的需求。\n由于其他原因我们的 Artifactory 暂时还不能使用，最后只能选择用 Source Code 进行构建然后进行安装。这两种方式都需要解决锁定资源以及释放资源的问题。如果当前环境有人正在使用，那么这台虚拟机的资源应该被锁住，不允许 Jenkins 再去调用这台正在使用的 node，以保证环境在使用过程中不被破坏。\n本文主要介绍如何通过 Jenkins Lockable Resources Plugin 来实现资源的上锁和解锁。\n演示 Demo # 设置 Lockable Resources\nJenkins -\u0026gt; configuration -\u0026gt; Lockable Resources Manager -\u0026gt; Add Lockable Resource 这里的 Labels 是你的 node 的 Label，在 Jenkins -\u0026gt; Nodes 设置 查看 Lockable Resources 资源池\n测试锁资源\n这里我配置的是参数化类型的 Job，可以选择不同平台，不同仓库进行构建 build-with-parameters 运行第一个 Job 查看当前可用资源数量 Free resources = 1，看到已经被 #47 这个 Job 所使用 继续运行第二个 Job 查看当前可用资源数量 Free resources = 0，看到已经被 #48 这个 Job 所使用 最关键是这一步，如果继续运行第三个 Job，是否能够被继续行呢 可以看到这个任务没有开始执行，看下 log 是否真的没有被执行。通过日志发现，当前正在等待可用的资源 测试释放锁\n现在释放一个资源，看下第三个 Job 是否能拿到资源，并且执行 从下图可以看到 第三个 Job 已经运行成功了 Jenkins pipeline 代码 # 整个 pipeline 最关键的部分就是如何上锁和释放，这里是通过 lock 和 input message 来实现。\n当前 Job 只要用户不点击 Yes，就会一直处于没有完成的状态，那么的它的锁会一直生效中。直到点击 Yes， Job 结束，锁也就释放了。\n具体可以参考下面的 Jenkinsfile。\npipeline { agent { node { label \u0026#39;PreDevENV\u0026#39; } } options { lock(label: \u0026#39;PreDevENV\u0026#39;, quantity: 1) } parameters { choice( name: \u0026#39;platform\u0026#39;, choices: [\u0026#39;Linux\u0026#39;, \u0026#39;AIX\u0026#39;, \u0026#39;Windows\u0026#39;, \u0026#39;Solris\u0026#39;, \u0026#39;HP-UX\u0026#39;], summary: \u0026#39;Required: which platform do you want to build\u0026#39;) choice( name: \u0026#39;repository\u0026#39;, choices: [\u0026#39;repo-0.1\u0026#39;, \u0026#39;repo-1.1\u0026#39;, \u0026#39;repo-2.1\u0026#39;, \u0026#39;repo-3.1\u0026#39;, \u0026#39;repo-4.1\u0026#39;], summary: \u0026#39;Required: which repository do you want to build\u0026#39;) string( name: \u0026#39;branch\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Required: which branch do you want to build\u0026#39;) } stages { stage(\u0026#39;git clone\u0026#39;){ steps { echo \u0026#34;git clone source\u0026#34; } } stage(\u0026#39;start build\u0026#39;){ steps { echo \u0026#34;start build\u0026#34; } } stage(\u0026#39;install build\u0026#39;){ steps{ echo \u0026#34;installing\u0026#34; } } stage(\u0026#39;unlock your resource\u0026#39;){ steps { input message: \u0026#34;do you have finished?\u0026#34; echo \u0026#34;Yes, I have finished\u0026#34; } } } } ","date":"2019-08-10","externalUrl":null,"permalink":"/posts/jenkins-lock-resource/","section":"Posts","summary":"本文介绍了如何使用 Jenkins 的 Lockable Resources 插件来管理和锁定资源，确保在多任务环境中资源的独占性和安全性。","title":"如何通过 Jenkins 进行资源的锁定和释放","type":"posts"},{"content":"Jenkins 是 DevOps 领域里非常好的 CI/CD 工具，它凭借其独特的功能，几乎可以满足你一切的的业务要求。其中一个独特的功能是多分支流水线(Multi-branch 流水线)，可以动态配置流水线。但是，随着公司的发展，单独的多分支流水线并不能完全满足你的所有需求，特别是在涉及大型企业时，你需要考虑流水线的集中管理，治理，稳定性，限制和安全性等其他事项。因此对于具有 Jenkins 流水线的大规模 CI/CD 环境，你需要添加之前没有想到的更多功能。\n动态配置流水线 # 当一个开发人员创建一个新分支并将其推送到远程代码仓库时，Jenkins 会为这个新分支动态创建流水线。根据代码仓库，甚至也可以作为动态创建 Pull Request 流水线。这个动态功能在使用 Feature 分支或其他类似功能的团队中非常有用，由于本文的主题不是多分支流水线，你可以在端到端多分支流水线项目创建中找到详细信息和一些示例。\n流水线即代码 # 在多分支流水线中，脚本存储在项目代码仓库中，这就是“流水线即代码”的概念。此外，当你拥有小型开发人员团队或项目没有大量分支时，它非常有用。这样，开发人员可以根据需要更改流水线，将更改推送到分支，并立即看到更改生效，但对于拥有数百或数千名拥有大量项目的开发人员的大型企业而言，这种方案就完成不可行了。\n集中式库 # 当你的团队或项目增加时，是时候考虑一种方法，比如通过共享的的方式应该在所有项目中。从长远来看，这种“集中式库”变得非常关键，因为随着规模的扩大，流水线中出现了新的要求或变化，在这种情况下，手动更改每个流水线或脚本对管理员来说将是一场噩梦。因此，如果你在一个地方进行更改并且每个流水线都得到更新，那么拥有该集中式库将更加实用。这是 Jenkins 共享库概念的用武之地。有关详细信息，你可以访问该站点。\n即使你只有一个流水线，仍然可以使用集中式库。\n治理与稳定 # 如果你的团队有对 CI/CD 一定了解的开发人员，并且你确信他们不会做出重大更改或编写脚本错误导致影响环境的稳定性，那么将流水线脚本放在代码中是很好的。但是，你真的确定吗？\n有人很可能会意外删除流水线文件或者可能出现小错误，这些小错误都会影响 CI/CD 的稳定性。如果你在早期发现这些错误时很容易解决这些错误，如果没有，这些微小的变化或错误将可能比你想象的更严重的影响 CI/CD，它将被传播到不同项目中的所有分支或 tag，这会变得很难解决。\n你需要将正确的流水线脚本推送到所有分支和/或代码仓库，或是要求每个开发人员提取最新的脚本，这种类型的问题集中式库这种更高级的方式来解决，除此之外，你的环境会因为有人可能会删除 Jenkins 文件或输入一些拼写错误带来风险。\n远程文件插件 # 为了消除不必要的更改的风险并降低使用的库的复杂性，我们需要以某种方式将流水线脚本与项目/代码代码仓库分开，同时仍继续使用多分支流水线功能。为此，我们有远程文件插件。\n这个插件使多分支流水线能够从其他代码仓库运行/加载流水线脚本，而不是将它们放在项目/代码代码仓库中，通过这个功能，你可以拥有一个单独的代码仓库，你可以在其中放置所有流水线脚本，并且只能为你自己提供访问权限。这样，你将拥有与集中式库相同的集中式流水线脚本代码仓库。此外，你可以将流水线脚本存储在集中式库本身中。\n这个功能的好处是除了有访问权限的人之外，没有人能够在流水线脚本中进行更改。你在集中流水线脚本中所做的任何更改都将影响使用该脚本文件的所有多分支流水线。这样，你无需等待所有开发人员获取更新版本或将脚本推送到所有代码仓库上的所有分支。\n另一个好处是，如果你将集中式流水线脚本放入 BitBucket 或 GitHub 等代码仓库中，你还将拥有代码审查功能。这样，你可以与其他人共享代码仓库，同时仍可限制或查看其他人所做的更改。\n结论 # 在大型企业中创建 CI/CD 流水线并不容易，你需要考虑治理，限制，稳定性和安全性等概念。在此上下文中，借助 Jenkins 的其他功能，Remote File Plugin 提供了一个独特的功能，用于集中，维护和共享流水线脚本。\n有关插件的详细信息，你可以访问插件的 Wiki 页面。\n","date":"2019-08-06","externalUrl":null,"permalink":"/posts/jenkins-multi-branch-pipeline/","section":"Posts","summary":"如果没有适当的解决方案，在大型企业可能难以创建和维护多分支流水线。","title":"在大型企业里维护多分支流水线","type":"posts"},{"content":"随着近些年 Git 的快速普及，想必无论开发还是测试在日常工作中都要用到 Git。\n对于刚刚接触的 Git 的人来说，打开一个 Git 仓库，面对十几个甚至几十个分支时，有的人不理解，有的人云里雾里，为什么会创建这么多分支？\n对于开发需要知道如何通过 Git 分支来管理产品的开发和发布，尤其是对于大型的项目的开发，只有 master 和 develop 分支是无法满足产品管理和发布要求的，我们还需要其他分支以便更好的管理产品代码。\n对于测试更多的了解开发过程及分支管理有助于测试及开展自动化测试用例，可以针对不同的分支进行测试用例的编写，在以后回归测试里可以通过分支或是 tag 找到对应的测试用例。\nGit 分支策略 # 这是一个大型的项目的 Git 分支管理策略，了解这张图可以涵盖 99% 的产品需求。\n上面这张图大体上分为 master, hotfix, release, develop 分支：\nmaster - 只用于存放稳定版本的提交，且只限于 merge 操作。每次发布成功后，要将 release 分支的代码 merge 到 master 和 develop 分支，并且在 master 上打上相应的 tag，如图里的 v1.1, v1.1.01 等。\ndevelop - 开发分支是所有开发者最常用的分支，当前的 Bug 和 Features 都需要修复到这个分支上面去。需要每次创建 bugfix 或 feature 类型的分支，创建 Pull Request 进行代码 review，然后才能 merge 分支到 develop 分支上。\nrelease - 发布分支是在产品 code freeze 后创建的， 这时候测试要开始大规模的测试了，新创建的 release 分支是不允许开发再往里面添加有关 feature 的代码，只有测试发现 bug 并被开发修复的代码才允许通过 Pull Request 的方式 merge 到 release 分支里。如果开发要提交 feature 的代码只能提交到 develop 分支里。等到产品成功发布后会将 release 分支 merge 到 master 分支并打上相应的 tag （版本号），还要将 release 分支 merge 到 develop 分支。\ndevelop - 开发分支是所有开发者最常用的分支，当前的 Bug 和 Features 都需要修复到这个分支上面去。\n这个图有几个关键点：\nhotfix 分支是从最新的 hotfix 分支上创建的 hotfix 分支发布后将会合并到 develop 分支 release 分支是从 develop 分支上创建的 release 分支发布后将会合并到 develop 和 master 分支 release 分支上发现的缺陷将会修复到 release 分支 如果你是那 1% 不能满足的产品需求，欢迎留言。\n","date":"2019-07-28","externalUrl":null,"permalink":"/posts/git-branching-strategy/","section":"Posts","summary":"介绍大型项目的 Git 分支策略，包括 master、develop、release 和 hotfix 分支的作用和使用方法，帮助团队更好地管理代码和版本发布。","title":"Git 分支策略","type":"posts"},{"content":"在持续集成中，你可能需要通过 Jenkins 来修改代码，并且将修改后的代码提交到Git仓库里。怎么做呢？最方便的做法还是 Jenkins 提供对应的插件，但是很遗憾我没找到合适的。另外我也觉得通过脚本的方式来实现会更加稳定，不用担心 Jenkins 以及插件升级带来潜在不好用的可能。\n以下 pipeline 片段供参考使用：\n// This pipeline is used for bumping build number pipeline { environment { MYGIT = credentials(\u0026#34;d1cbab74-823d-41aa-abb7\u0026#34;) } stages { stage(\u0026#39;Git clone repo\u0026#39;) { steps { sh \u0026#39;git clone -b develop --depth 1 https://$MYGIT_USR:\u0026#34;$MYGIT_PSW\u0026#34;@github.com/shenxianpeng/blog.git\u0026#39; } } stage(\u0026#39;Change code stage\u0026#39;){ steps { sh \u0026#39;\u0026#39; } } stage(\u0026#39;Git push to remote repo\u0026#39;) { steps { sh label: \u0026#39;\u0026#39;, script: \u0026#39;\u0026#39;\u0026#39; cd blog git add . git commit -m \u0026#34;Bld # 1001\u0026#34; git push https://$MYGIT_USR:\u0026#34;$MYGIT_PSW\u0026#34;@github.com/shenxianpeng/blog.git --all\u0026#39;\u0026#39;\u0026#39; } } } } 这里面我所遇到最大的坑，我之前脚本是这样写的：\nstage(\u0026#39;Git push to remote\u0026#39;) { // not works script steps { sh \u0026#39;cd blog\u0026#39; sh \u0026#39;git add .\u0026#39; sh \u0026#39;git commit -m \u0026#34;${JIRA_NO} Bld # ${BUILD_NO}\u0026#34;\u0026#39; sh \u0026#39;git push https://$MYGIT_USR:\u0026#34;$MYGIT_PSW\u0026#34;@github.com/shenxianpeng/blog.git --all\u0026#39; } } 在最后一个阶段提交代码时，shell 脚本不能使用单引号 \u0026lsquo;\u0026rsquo;，要使用三引号才行\u0026rsquo;\u0026rsquo;\u0026rsquo; \u0026lsquo;\u0026rsquo;\u0026rsquo;。我在这里花了很多时间，一直找不到问题所在，因为我在上面的shell脚本使用的时候用单引号 \u0026rsquo;\u0026rsquo; 可以正常 git clone 代码，但在提交代码时不行，最后我 Jenkins 的 Pipeline Syntax 生成的脚本，提交代码成功。\n","date":"2019-07-22","externalUrl":null,"permalink":"/posts/git-push-by-jenkins/","section":"Posts","summary":"如何通过 Jenkins Pipeline 脚本来提交修改的代码到 Git 仓库，包括克隆仓库、修改代码和推送更改等步骤。","title":"通过 Jenkins 来提交修改的代码 git push by Jenkins","type":"posts"},{"content":"在使用 Jenkins pipeline 的时候，在 Linux 需要用 root 来执行，我想通过 Jenkins pipeline 的语法来解决，但是只找到这种方式：SSH Pipeline Steps\ndef remote = [:] remote.name = \u0026#39;test\u0026#39; remote.host = \u0026#39;test.domain.com\u0026#39; remote.user = \u0026#39;root\u0026#39; remote.password = \u0026#39;password\u0026#39; remote.allowAnyHosts = true stage(\u0026#39;Remote SSH\u0026#39;) { sshCommand remote: remote, command: \u0026#34;ls -lrt\u0026#34; sshCommand remote: remote, command: \u0026#34;for i in {1..5}; do echo -n \\\u0026#34;Loop \\$i \\\u0026#34;; date ; sleep 1; done\u0026#34; } * command * Type: String * dryRun (optional) * Type: boolean * failOnError (optional) * Type: boolean * remote (optional) * Nested Choice of Objects * sudo (optional) * Type: boolean 从 example 来看需要提供的参数比较多，很多参数我已经在 Pipeline 的 environment 已经设置过了，这里再设置就显得不够优美，且限于没有足够的 example，你知道的 Jenkinsfile 调试非常痛苦和麻烦，我就没通过这种方式来尝试解决。\n通过 Linux 设置来解决\n// open a shell console and type $ sudo visudo // type your user name jenkins ALL=(ALL) NOPASSWD: ALL 但即使这样设置，通过 Jenkins 执行 shell 脚本的时候还是出现如下问题\nsudo: no tty present and no askpass program specified 最后通过如下脚本解决了我的问题\n// Jenkinsfile environment { JENKINS = credentials(\u0026#34;d1cbab74-823d-41aa-abb7-85848595\u0026#34;) } sh \u0026#39;sudo -S \u0026lt;\u0026lt;\u0026lt; \u0026#34;$JENKINS_PSW\u0026#34; sh test.sh\u0026#39; 如果你有更好的方式，欢迎留言评论，谢谢。\n","date":"2019-07-16","externalUrl":null,"permalink":"/posts/execute-sudo-without-password/","section":"Posts","summary":"本文介绍了如何在 Jenkins Pipeline 中执行 sudo 命令而无需输入密码，提供了具体的实现方法和示例代码。","title":"Execute sudo without password","type":"posts"},{"content":"","date":"2019-07-16","externalUrl":null,"permalink":"/tags/os/","section":"标签","summary":"","title":"OS","type":"tags"},{"content":"","date":"2019-07-07","externalUrl":null,"permalink":"/tags/disqus/","section":"标签","summary":"","title":"Disqus","type":"tags"},{"content":" 查找是否有遗漏提交 # 从一个分支找到所有的 commit 和 ticket 号，然后去另外一个分支去查找这些提交是否也在这个分支里。\n找一个分支的所有 commit 和 ticket 号\n# 从 develop 分支上获取所有的 commit 和 ticket 号，然后根据 ticket 号进行排序 git log origin/develop --pretty=oneline --abbrev-commit | cut -d\u0026#39; \u0026#39; -f2,1 | sort -t \u0026#39; \u0026#39; -k 2 \u0026gt;\u0026gt; develop_involve_tickets.txt --pretty=oneline # 显示为一行 --abbrev-commit # 显示短的提交号 cut --help # 切出来所需要的字段 -d # 字段分隔符, \u0026#39; \u0026#39;分隔空格 -f # 只选择某些字段 sort --help # 利用 sort 将剪出来的字段进行排序 -t # 字段分隔， \u0026#39; \u0026#39;分隔空格 -k # 通过键进行键定义排序;KEYDEF 给出位置和类型 然后去另外一个分支去找是否有次提交\n由于在 SVN 时代时，每次修改都会在描述里添加 ticket 号，所以切换到 master 分支后，直接搜索所有 ticket 号是否存在就好了.\n#!/bin/bash filename=\u0026#39;C:\\develop_involve_tickets.txt\u0026#39; while read line do echo $line var=`grep -ir $line src` if [[ -z $var ]];then echo \u0026#34;not found\u0026#34; echo $line \u0026gt;\u0026gt; ../not_found_in_master.txt else echo \u0026#34;found\u0026#34; echo $line \u0026gt;\u0026gt; ../found_in_master.txt fi done \u0026lt; \u0026#34;$filename\u0026#34; ","date":"2019-07-07","externalUrl":null,"permalink":"/posts/git-management/","section":"Posts","summary":"本文介绍了 Git 的常见管理操作，包括分支管理、提交规范、代码审查等，帮助开发者更好地使用 Git 进行版本控制。","title":"Git 管理","type":"posts"},{"content":"","date":"2019-07-07","externalUrl":null,"permalink":"/tags/hexo/","section":"标签","summary":"","title":"Hexo","type":"tags"},{"content":" 在你的 Hexo 网站添加 Disqus # 去 Disqus 创建一个账号，在这个过程中有需要选择一个 shortname，完成后，你可以在设置页码找到你的 shortname\nhttps://YOURSHORTNAMEHERE.disqus.com/admin/settings/general 在你 Hexo 博客里打开 _config.yml, 然后输入 disqus_shortnameand: YOURSHORTNAMEHERE，像这样：\ndisqus_shortname: myshortnamegoeshere comments: true 也需要更改 _config.yml 文件如下，例如我的：\n# 修改默认 url: http://yoursite.com 为： url: https://shenxianpeng.github.io 复制这段代码到 blog\\themes\\landscape\\layout\\_partial\\footer.ejs\n\u0026lt;% if (config.disqus_shortname){ %\u0026gt; \u0026lt;script\u0026gt; var disqus_shortname = \u0026#39;\u0026lt;%= config.disqus_shortname %\u0026gt;\u0026#39;; \u0026lt;% if (page.permalink){ %\u0026gt; var disqus_url = \u0026#39;\u0026lt;%= page.permalink %\u0026gt;\u0026#39;; \u0026lt;% } %\u0026gt; (function(){ var dsq = document.createElement(\u0026#39;script\u0026#39;); dsq.type = \u0026#39;text/javascript\u0026#39;; dsq.async = true; dsq.src = \u0026#39;//go.disqus.com/\u0026lt;% if (page.comments){ %\u0026gt;embed.js\u0026lt;% } else { %\u0026gt;count.js\u0026lt;% } %\u0026gt;\u0026#39;; (document.getElementsByTagName(\u0026#39;head\u0026#39;)[0] || document.getElementsByTagName(\u0026#39;body\u0026#39;)[0]).appendChild(dsq); })(); \u0026lt;/script\u0026gt; \u0026lt;% } %\u0026gt; 也需要复制这些文件到 footer.ejs 到最底部：\n\u0026lt;div id=\u0026#34;disqus_thread\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 最后 footer.ejs 文件是这样的：\n\u0026lt;% if (theme.sidebar === \u0026#39;bottom\u0026#39;){ %\u0026gt; \u0026lt;%- partial(\u0026#39;_partial/sidebar\u0026#39;) %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;% if (config.disqus_shortname){ %\u0026gt; \u0026lt;script\u0026gt; var disqus_shortname = \u0026#39;\u0026lt;%= config.disqus_shortname %\u0026gt;\u0026#39;; \u0026lt;% if (page.permalink){ %\u0026gt; var disqus_url = \u0026#39;\u0026lt;%= page.permalink %\u0026gt;\u0026#39;; \u0026lt;% } %\u0026gt; (function(){ var dsq = document.createElement(\u0026#39;script\u0026#39;); dsq.type = \u0026#39;text/javascript\u0026#39;; dsq.async = true; dsq.src = \u0026#39;//go.disqus.com/\u0026lt;% if (page.comments){ %\u0026gt;embed.js\u0026lt;% } else { %\u0026gt;count.js\u0026lt;% } %\u0026gt;\u0026#39;; (document.getElementsByTagName(\u0026#39;head\u0026#39;)[0] || document.getElementsByTagName(\u0026#39;body\u0026#39;)[0]).appendChild(dsq); })(); \u0026lt;/script\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;footer id=\u0026#34;footer\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;outer\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;footer-info\u0026#34; class=\u0026#34;inner\u0026#34;\u0026gt; \u0026amp;copy; \u0026lt;%= date(new Date(), \u0026#39;YYYY\u0026#39;) %\u0026gt; \u0026lt;%= config.author || config.title %\u0026gt;\u0026lt;br\u0026gt; \u0026lt;%= __(\u0026#39;powered_by\u0026#39;) %\u0026gt; \u0026lt;a href=\u0026#34;http://hexo.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Hexo\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;div id=\u0026#34;disqus_thread\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 最后清理和构建\nhexo clean hexo generate \u0026amp;\u0026amp; hexo server 现在你可以看到我的博客已经可以添加评论了 : )\n","date":"2019-07-07","externalUrl":null,"permalink":"/posts/add-disqus-to-hexo/","section":"Posts","summary":"在 Hexo 博客中集成 Disqus 评论系统，允许读者留言和互动。","title":"Hexo 添加 Disqus 留言功能","type":"posts"},{"content":"这个pipeline里包含了如下几个技术：\n如何使用其他机器，agent 如何使用环境变量，environment 如何在build前通过参数化输入，parameters 如何使用交互，input 如何同时clone多个repos 如何进行条件判断，anyOf pipeline { agent { node { label \u0026#39;windows-agent\u0026#39; } } environment { MY_CRE = credentials(\u0026#34;2aee7e0c-a728-4d9c-b25b-ad5451a12d\u0026#34;) } parameters { // Jenkins parameter choice( name: \u0026#39;REPO\u0026#39;, choices: [\u0026#39;repo1\u0026#39;, \u0026#39;repo2\u0026#39;, \u0026#39;repo3\u0026#39;, \u0026#39;repo4\u0026#39;], summary: \u0026#39;Required: pick a repo you want to build\u0026#39;) string( name: \u0026#39;BRANCH\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Required: chose a branch you want to checkout\u0026#39;) string( name: \u0026#39;BUILD_NO\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Required: input build number\u0026#39;) string( name: \u0026#39;JIRA_NO\u0026#39;, defaultValue: \u0026#39;\u0026#39;, summary: \u0026#39;Optional: input jira ticket number for commit message\u0026#39;) } stages { stage(\u0026#34;Are you sure?\u0026#34;){ steps{ // make sure you want to start this build input message: \u0026#34;${REPO}/${BRANCH}:${BUILD_NO}, are you sure?\u0026#34; echo \u0026#34;I\u0026#39;m sure!\u0026#34; } } stage(\u0026#39;Git clone repos\u0026#39;) { steps { // git clone one repo source code checkout([ $class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;refs/heads/${BRANCH}\u0026#39;]], browser: [$class: \u0026#39;GitHub\u0026#39;, repoUrl: \u0026#39;https://github.com/${REPO}\u0026#39;], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;CleanBeforeCheckout\u0026#39;], [$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#39;**\u0026#39;], [$class: \u0026#39;RelativeTargetDirectory\u0026#39;, relativeTargetDir: \u0026#39;../${REPO}\u0026#39;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;2aee7e0c-a728-4d9c-b25b\u0026#39;, url: \u0026#39;https://github.com/${REPO}.git\u0026#39;]]]) // git clone another repo source code checkout([ $class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;refs/heads/${BRANCH}\u0026#39;]], browser: [$class: \u0026#39;GitHub\u0026#39;, repoUrl: \u0026#39;https://github.com/${REPO}\u0026#39;], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;CleanBeforeCheckout\u0026#39;], [$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#39;**\u0026#39;], [$class: \u0026#39;RelativeTargetDirectory\u0026#39;, relativeTargetDir: \u0026#39;../${REPO}\u0026#39;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;2aee7e0c-a728-4d9c-b25b\u0026#39;, url: \u0026#39;https://github.com/${REPO}.git\u0026#39;]]]) } } stage(\u0026#39;Build repo1 and repo2\u0026#39;) { when { // if REPO=repo1 or REPO=repo2, execute build_repo12.sh anyOf { environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo1\u0026#39; environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo2\u0026#39; } } steps { sh label: \u0026#39;\u0026#39;, script: \u0026#39;${REPO}/build_repo12.sh ${REPO} ${BUILD_NO} ${JIRA_NO}\u0026#39; } } stage(\u0026#39;Build repo3 and repo4\u0026#39;) { when { // if REPO=repo3 or REPO=repo4, execute build_repo34.sh anyOf { environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo3\u0026#39; environment name: \u0026#39;REPO\u0026#39;, value: \u0026#39;repo4\u0026#39; } } steps { sh label: \u0026#39;\u0026#39;, script: \u0026#39;${REPO}/build_repo34.sh ${REPO} ${BUILD_NO} ${JIRA_NO}\u0026#39; } } stage(\u0026#39;Git push to remote repo\u0026#39;) { steps { // commit code to remote repo sshagent([\u0026#39;2aee7e0c-a728-4d9c-b25b\u0026#39;]) { sh \u0026#34;git push https://%MY_CRE_USR%:%MY_CRE_PSW%@github.com/${REPO}.git\u0026#34; } } } } } ","date":"2019-07-07","externalUrl":null,"permalink":"/posts/jenkinsfile-example/","section":"Posts","summary":"这个 Jenkinsfile 示例展示了如何在 Jenkins Pipeline 中实现交互式输入、克隆多个 Git 仓库，并在构建完成后将代码推送到远程仓库。","title":"Jenkinsfile example - 实现交互、clone 多个仓库以及 git push","type":"posts"},{"content":"","date":"2019-05-21","externalUrl":null,"permalink":"/tags/c/","section":"标签","summary":"","title":"C","type":"tags"},{"content":" 什么是代码覆盖率 # 代码覆盖率（Code Coverage）是衡量代码在自动化测试中被执行比例的指标，通常以百分比表示。覆盖率越接近 100%，意味着未被测试覆盖、潜在存在缺陷的代码越少。\n它能帮助团队了解测试的全面性，也能作为向管理层展示测试成果的重要数据。\n常见 C/C++ 代码覆盖率工具 # 工具 支持语言 价格 合作伙伴 Squish Coco C, C++, C#, SystemC, Tcl, QML 未公开 客户列表 BullseyeCoverage C, C++ $800 / 年起 - Testwell C, C++, C#, Java 未公开 - Parasoft C/C++test C, C++ 未公开 合作伙伴 VECTOR Code Coverage C, C++ 未公开（有试用） 合作伙伴 JaCoCo Java 开源 Java 领域最知名的覆盖率工具 ","date":"2019-05-21","externalUrl":null,"permalink":"/posts/code-coverage-tools/","section":"Posts","summary":"代码覆盖率衡量自动化测试覆盖的代码行、语句或代码块的比例，是评估 QA 质量的重要指标。本文列出了常见的 C/C++ 代码覆盖率工具及其特性。","title":"C/C++ 代码覆盖率工具","type":"posts"},{"content":"","date":"2019-05-21","externalUrl":null,"permalink":"/tags/squishcoco/","section":"标签","summary":"","title":"SquishCoco","type":"tags"},{"content":" 准备工作 # 申请 免费试用许可 并安装 会收到包含用户名/密码的邮件，用于登录下载。 本文测试环境为 Windows，因此下载并安装 Windows 版本的 Squish Coco 及其 VS 插件。 安装 Visual Studio 2010 及以上版本（本文使用 VS2017 Professional）。 安装 VS 插件 # 进入 ..squishcoco\\Setup 目录，双击 SquishCocoVSIX2017.vsix，重新打开 VS2017 后即可看到插件入口。\n创建示例项目 # 在 Visual Studio 中选择 File → New → Project\u0026hellip; 选择 Visual C++ → Win32 Console Application 模板。 项目命名为 squishcoco_sample，点击 OK → Finish。 此时程序尚未插桩，需要为构建添加新的配置。\n打开 Build → Configuration Manager\u0026hellip; 在 Configuration 列点击 New\u0026hellip; 在 New Project Configuration 窗口中： Name 填写 Code Coverage Copy settings from 选择 Release 或 Debug 点击 OK 添加测试代码 # squishcoco_sample.cpp\n#include \u0026#34;stdafx.h\u0026#34; extern int myprint(); int _tmain(int argc, _TCHAR* argv[]) { int age; printf(\u0026#34;Enter your age: \u0026#34;); scanf(\u0026#34;%d\u0026#34;, \u0026amp;age); if (age \u0026gt; 0 \u0026amp;\u0026amp; age \u0026lt;= 40) printf(\u0026#34;You\u0026#39;re young guys\\n\u0026#34;); else if (age \u0026gt; 40 \u0026amp;\u0026amp; age \u0026lt;= 70) printf(\u0026#34;You\u0026#39;re midle guys\\n\u0026#34;); else if (age \u0026gt; 70 \u0026amp;\u0026amp; age \u0026lt;= 100) printf(\u0026#34;You\u0026#39;re old guys\\n\u0026#34;); else printf(\u0026#34;You\u0026#39;re awesome\\n\u0026#34;); myprint(); return 0; } myprint.cpp\n#include \u0026#34;stdafx.h\u0026#34; int myprint() { printf(\u0026#34;you have call printf function\\n\u0026#34;); return 0; } 启用代码覆盖率插桩 # 使用 VS 插件：\n点击 Tools → Code Coverage Build Mode\u0026hellip; Project 选择 squishcoco_sample Configuration 选择 Code Coverage 底部选择 Modify → 点击 Enable code coverage for C++ projects 执行后，SquishCoco 会在输出窗口显示对编译器、链接器等附加的参数配置。\n构建项目 # 构建后会生成：\n可执行文件 squishcoco_sample.exe 插桩信息文件 squishcoco_sample.exe.csmes 双击 .csmes 文件会在 CoverageBrowser 中打开，此时因未运行程序，所有插桩行会显示为灰色。\n收集和查看覆盖率结果 # 双击运行 squishcoco_sample.exe，会生成 squishcoco_sample.exe.csexe 覆盖率快照文件。\n在 CoverageBrowser 中：\n点击 File → Load Execution Report\u0026hellip; 选择快照文件并 Import 代码窗口中已执行的行会以绿色高亮显示。\n最终结果示例 # ","date":"2019-05-21","externalUrl":null,"permalink":"/posts/squishcoco/","section":"Posts","summary":"介绍代码覆盖率工具 Squish Coco，并展示在 Visual Studio C++ 项目中如何安装、配置、执行和查看覆盖率结果。","title":"代码覆盖率工具 Squish Coco 使用示例","type":"posts"},{"content":"最近遇到一个 regression bug，是产品完成构建之后，build commit number 不对，显示的 HEAD 而不是常见的 97b34931ac HASH number,这是什么原因呢？ 我检查了 build 脚本没有发现问题，branch 的输出是正确的，那我怀疑是引入 Jenkins 的原因，果然登录到远程的 agent 上去查看分支名称如下：\nC:\\workspace\\blog\u0026gt;git branch * (HEAD detached at 97b3493) 果然问题出在了 Jenkins 上。这个问题有简单办法解决，就是直接使用git命令来clone代码，而不使用Git插件\ngit clone --depth 1 -b u2opensrc https://username:\u0026#34;passwowrd\u0026#34;@git.github.com/scm/blog.git blog 这种方式固然简单，不会出错，但它是明码显示，我岂能容忍这种不堪的处理方式吗？肯定还是要在 Git 插件上找到解决办法的。 随后google一下，果然有遇到和我一样问题的人，问题链接 这里。\n他说他做了很多调查，还跟专业的 Jenkins 人士联系，试了很多次，最后找到这个办法\ncheckout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/feature/*\u0026#39;]], doGenerateSubmoduleConfigurations: false, extensions: [[$class: \u0026#39;LocalBranch\u0026#39;, localBranch: \u0026#34;**\u0026#34;]], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;99f978af-XXXX-XXXX-8147-2cf8f69ef864\u0026#39;, url: \u0026#39;http://TFS_SERVER:8080/tfs/DefaultCollection/Product/_git/Project\u0026#39;]]]) 主要是在 extensions:[] 中加入这句 [$class: \u0026lsquo;LocalBranch\u0026rsquo;, localBranch: \u0026ldquo;**\u0026rdquo;]\n这是 Jenkins 的 Bug 吗？带着这个疑问随后通过 Pipeline Syntax，找到 checkout: Check out from version control，在 Additional Behaviours 里有 Check out to specific local branch 这个配置项\nIf given, checkout the revision to build as HEAD on this branch. If selected, and its value is an empty string or \u0026ldquo;**\u0026rdquo;, then the branch name is computed from the remote branch without the origin. In that case, a remote branch origin/master will be checked out to a local branch named master, and a remote branch origin/develop/new-feature will be checked out to a local branch named develop/newfeature.\n看介绍原来 Jenkins 自带这个设置，只是它不是默认选项，所以才遇到刚才那个问题。随后选择这个设置，然后填入\u0026quot;**\u0026quot;，然后生成 Pipeline 脚本，就跟上面的脚本一样了。\n","date":"2019-05-14","externalUrl":null,"permalink":"/posts/gitscm-clone-code-don-t-display-branch/","section":"Posts","summary":"如何在 Jenkins 中使用 GitSCM插件克隆代码时，确保正确显示分支信息，避免出现 HEAD detached 状态的问题。","title":"GitSCM clone code don't display branch","type":"posts"},{"content":"","date":"2019-05-13","externalUrl":null,"permalink":"/tags/automation/","section":"标签","summary":"","title":"Automation","type":"tags"},{"content":"","date":"2019-05-13","externalUrl":null,"permalink":"/tags/ftp/","section":"标签","summary":"","title":"FTP","type":"tags"},{"content":"实现 CI/CD 过程中，常常需要将构建好的 build 上传到一个公共的服务器，供测试、开发来获取最新的 build。如何上传 build 成果物到 FTP server，又不想把 FTP server登录的用户名和密码存在脚本里，想做这样的参数化如何实现呢？\nupload_to_ftp.bat [hostname] [username] [password] [local_path] [remote_pat] windows batch 由于它的局限性，在实现上是比较麻烦的，但还是有办法。如何用 windows batch 来实现呢？借助一个临时文件，把需要的参数写入到临时文件里，然后通过 ftp -s 参数读取文件，最后把临时文件删除的方式来实现。\n@echo off set ftp_hostname=%1 set ftp_username=%2 set ftp_password=%3 set local_path=%4 set remote_path=%5 if %ftp_hostname%! == ! ( echo \u0026#34;ftp_hostname not set correctly\u0026#34; \u0026amp; goto USAGE ) if %ftp_username%! == ! ( echo \u0026#34;ftp_username not set correctly\u0026#34; \u0026amp; goto USAGE ) if %ftp_password%! == ! ( echo \u0026#34;ftp_password not set correctly\u0026#34; \u0026amp; goto USAGE ) if %local_path%! == ! ( echo \u0026#34;local_path not set correctly\u0026#34; \u0026amp; goto USAGE ) if %remote_path%! == ! ( echo \u0026#34;remote_path not set correctly\u0026#34; \u0026amp; goto USAGE ) echo open %ftp_hostname% \u0026gt; ftp.txt echo user %ftp_username% %ftp_password% \u0026gt;\u0026gt; ftp.txt echo cd %remote_path% \u0026gt;\u0026gt; ftp.txt echo lcd %local_path% \u0026gt;\u0026gt;ftp.txt echo prompt off \u0026gt;\u0026gt;ftp.txt echo bin \u0026gt;\u0026gt; ftp.txt echo mput * \u0026gt;\u0026gt; ftp.txt echo bye \u0026gt;\u0026gt; ftp.txt ftp -n -s:ftp.txt del ftp.txt goto END :USAGE echo. echo. - ------------------------------------------------------------------------------- echo. - upload_to_ftp.bat [hostname] [username] [password] [local_path] [remote_pat] - echo. - Example: - echo. - upload_to_ftp.bat 192.168.1.1 guest guest D:\\Media\\* C:\\Builds\\ - echo. - ------------------------------------------------------------------------------- echo. :END ","date":"2019-05-13","externalUrl":null,"permalink":"/posts/upload-to-ftp-parameterization-by-bat/","section":"Posts","summary":"本文介绍了如何使用 Windows Batch 脚本通过参数化的方式上传文件到 FTP 服务器，避免在脚本中硬编码 FTP 凭据。","title":"通过参数化上传文件到 FTP 服务器","type":"posts"},{"content":" 准备 Java 运行时 # 检查是否已安装 Java # $ java -version openjdk version \u0026#34;1.8.0_65\u0026#34; OpenJDK Runtime Environment (build 1.8.0_65-b17) OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode) 如果未安装，可参考 这篇文章 进行安装 # 创建节点 # 1. 在 Jenkins 首页进入 # Manage Node → New Node，例如创建 window-build-machine\n2. Linux Agent 设置示例 # 项目 配置 Name Linux-build-machine Description 用于 Linux 构建 # of executors 1 Remote root directory /home/agent Labels Linux, build Usage 尽可能多地使用此节点 Launch method 通过 SSH 启动 Agent Host 192.168.1.112 Credentials username/password Host Key Verification Strategy Manually trusted key Verification Strategy Availability 尽可能保持此 Agent 在线 3. 凭据配置 # 凭据项 配置 Domain Global credentials (unrestricted) Kind Username with password Scope Global (Jenkins、nodes、items 及其子项) Username root Password mypassword Description Linux agent 用户名和密码 4. 保存并连接 # 示例日志：\nRemoting version: 3.29 This is a Unix agent Evacuated stdout Agent successfully connected and online SSHLauncher{host=\u0026#39;192.168.1.112\u0026#39;, port=22, credentialsId=\u0026#39;d1cbab74-823d-41aa-abb7-8584859503d0\u0026#39;, jvmOptions=\u0026#39;\u0026#39;, javaPath=\u0026#39;/usr/bin/java\u0026#39;, prefixStartSlaveCmd=\u0026#39;\u0026#39;, suffixStartSlaveCmd=\u0026#39;\u0026#39;, launchTimeoutSeconds=210, maxNumRetries=10, retryWaitTime=15, sshHostKeyVerificationStrategy=hudson.plugins.sshslaves.verifiers.ManuallyTrustedKeyVerificationStrategy, tcpNoDelay=true, trackCredentials=true} [05/11/19 01:33:37] [SSH] Opening SSH connection to 192.168.1.112:22. [05/11/19 01:33:37] [SSH] SSH host key matches key seen previously for this host. Connection will be allowed. [05/11/19 01:33:37] [SSH] Authentication successful. [05/11/19 01:33:37] [SSH] The remote user\u0026#39;s environment is: 常见问题排查 # 问题 解决方法 [SSH] WARNING: No entry currently exists in the Known Hosts file for this host... 执行 ssh-keyscan HOSTNAME \u0026gt;\u0026gt; known_hosts /var/lib/jenkins/.ssh/known_hosts [SSH] No Known Hosts file was found... 在 Launch method 中将 Host key verification strategy 从 \u0026ldquo;Known Hosts file verification strategy\u0026rdquo; 改为 \u0026ldquo;Manually trusted key verification strategy\u0026rdquo; ","date":"2019-05-12","externalUrl":null,"permalink":"/posts/jenkins-linux-agent/","section":"Posts","summary":"本文提供了 Jenkins Linux Agent 的逐步配置指南，包括 Java 运行时的准备、节点创建以及常见问题的排查方法。","title":"Jenkins Linux Agent 配置","type":"posts"},{"content":" 准备 Java 运行时 # 1. 下载 Java # 2. 配置 Windows 系统环境变量 # JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_201 CLASSPATH=.;%JAVA_HOME%\\lib;%JAVA_HOME%\\jre\\lib 创建节点 # 1. 在 Jenkins 首页进入 # Manage Node → New Node，例如创建 window-build-machine\n2. Windows Agent 设置示例 # 项目 配置 Name window-build-machine Description 用于 Windows 构建 # of executors 1 Remote root directory C:\\agent Labels windows, build Usage 尽可能多地使用此节点 Launch method 让 Jenkins 以 Windows 服务的方式控制此 Agent Administrator user name .\\Administrator Password mypassword Host 192.168.1.111 Run service as 使用上述 Administrator 账户 Availability 尽可能保持此 Agent 在线 3. 保存并连接 # [windows-slaves] Connecting to 192.168.1.111 Checking if Java exists java -version returned 1.8.0. [windows-slaves] Copying jenkins-slave.xml [windows-slaves] Copying slave.jar [windows-slaves] Starting the service [windows-slaves] Waiting for the service to become ready \u0026lt;===[JENKINS REMOTING CAPACITY]===\u0026gt;Remoting version: 3.29 This is a Windows agent Agent successfully connected and online 常见问题排查 # 1. ERROR: Message not found for errorCode: 0xC00000AC # 需要安装 JDK，并配置 JAVA 环境变量。\n2. 添加 Windows 节点作为服务时报错 # 参考 JENKINS-16418。\n3. org.jinterop.dcom.common.JIException: Message not found for errorCode: 0x00000005 # 修复以下注册表项权限：\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Classes\\Wow6432Node\\CLSID{72C24DD5-D70A-438B-8A42-98424B88AFB8} HKEY_CLASSES_ROOT\\CLSID{76A64158-CB41-11D1-8B02-00600806D9B6} 步骤：\n打开 regedit（以管理员身份），找到对应注册表项 修改所有者为 Administrators 组，并赋予 完全控制 权限 将所有者改回 NT Service\\TrustedInstaller 重启 Remote Registry Service 4. ERROR: Unexpected error in launching an agent # 登录远程机器，在服务中找到 jenkinsslave-C__agent 启动类型设为 Automatic Log On 选择 This account 并输入正确账号密码 启动服务 5. errorCode: 0x800703FA # Agent 使用域账号运行时，在组策略中：\n打开 gpedit.msc 计算机配置 → 管理模板 → 系统 → 用户配置文件 启用 Do not forcefully unload the user registry at user logoff 6. errorCode: 0xC0000001 ... Failed to connect # 需要启用 SMB1：\n打开 启用或关闭 Windows 功能 勾选 SMB 1.0/CIFS File Sharing Support 确认并重启 7. .NET Framework 2.0 or later is required # 升级 .NET Framework，参考 这篇文章。\n8. 更多 Windows Agent 连接问题 # 参考：https://github.com/jenkinsci/windows-slaves-plugin/blob/master/docs/troubleshooting.adoc\n","date":"2019-05-12","externalUrl":null,"permalink":"/posts/jenkins-windows-agent/","section":"Posts","summary":"本文提供 Jenkins Windows Agent 的详细配置步骤，包括 Java 运行时准备、节点创建以及常见问题的排查方法。","title":"Jenkins Windows Agent 配置","type":"posts"},{"content":"每当工作闲暇，我都会时常想起好久没有更新微信公众号的文章了，总想等工作不忙的时候赶紧跟大家分享我从测试转开发这段时间的经历和感受，但工作总是有忙不完的忙，一刻都停不下来。\n终于等到这一周有两天工作不是那么忙碌了，才决定将前几天写到一半的文章更新完。这是我这几个月下来感受最轻松的两天，暂时没有bug需要去调查和测试，不用去看十几年的C代码，终于有大块时间去写我负责的Python Client端代码了。这种写着代码，听着歌曲去重构，Debug，修改Unit Test Suite感觉真是幸福。\n幸福的时光总是短暂的，今天就又来了两个Bug需要去调查 ε=(´ο｀*)))唉\u0026hellip;\n又把我打回原形，调查大半天之后发现原来是QA测的不对，可以松口气晚上可以不用工作更新下微信公众号了。\n这五个月来，几乎每天都是白天八小时，晚上继续背着电脑回家准备继续工作，周日偶尔去公司，经常在家学习。因为角色的转变，新的项目，需要学习的地方很多。从业务到技术，再加上产品发布在即，作为一名开发新人也肩负起Bug Fix的任务，十年前的代码，全英文的文档，复杂的系统，如果不全力一搏，真担心自己转型失败，那就太打脸了。\n一天的工作忙碌和压力，使得我晚上总是吃的停不下来，吃饭是我一天当中最轻松的时刻。去年我跟别人打赌减肥赢奖金，我毫无怨念的拿到了第一的奖金，可是今年再和别人打赌减肥，至今我都还没开始，马上年底了，输掉奖金是毫无悬念的。总结下来，大概是因为今年工作太忙，工作压力大的缘故，使得我无法在八小时之余安心去继续练习吉他，做keep，年假还没来得及休，真是计划不如变化快。\n虽然我还是个小开发，当角色变了，角度也会有变化。\n自动化测试是本分，DevOps是阶梯 # 这几年下来相信你也会真切感受到，如果一名测试人员不懂自动化测试，不会写自动化测试脚本，不但难有升职或是跳槽的机会，很有可能会被企业所淘汰。\n个人觉得DevOps是未来一段时间很多企业要走的路，一般的二线城市能把DevOps讲明白并且实施的人太少了，所以尽早掌握实施DevOps的人，就有机会成为DevOps教练或是测试架构师这样的角色。\n没有做好抗压的准备，不要去做开发 # 这几个月来遇到压力非常多，从刚开始的学习C语言，到C语言考核；从学习全英文的业务文档，到业务文档的分享（也是一种考核）；从调研C代码的代码覆盖率、Git分享，到调查并解决Bug；从每天的站立会汇报到每周与国外同事的例会。终于等到九月份，Title从Quality Assurance Engineer变成了Software Engineer，这其中的压力、痛苦和短暂的喜悦只有走过的人才知道。\n与年龄想匹配的能力 # 这点非常重要，如果现在问你，你与刚毕业两三年的同行年轻人有哪些优势？如果你不能肯定和清楚的说出自己优势的话，那就要好好反思一下了。\n如果从开发角度来说，我现在就是与年龄不相匹配的能力，因此测试相关的技能以及DevOps相关知识依旧是我要好好掌握的功课。\n学好英语 # 对于国内公司来说，工作上不会用到英语，但我想说如果想在测试和开发领域有更长远发展，英文非常重要。一般最流行开源的自动化测试框架、技术、DevOps相关的工具以及搜索最有效的解决问题的方案一般都是英文。如果你的英语不好，坚持一年半载去硬啃一手英文资料，形成习惯，受益终生。\n","date":"2018-12-26","externalUrl":null,"permalink":"/misc/from-qa-to-dev/","section":"Miscs","summary":"从测试转开发的五个月，工作、学习、生活的感悟和总结。","title":"从测试转开发","type":"misc"},{"content":"随着技术的进步和自动化技术的出现，市面上出现了一些自动化测试框架。只需要进行一些适用性和效率参数的调整，这些自动化测试框架就能够开箱即用，大大节省了开发时间。\n本文整理了当前最受欢迎的 Python 自动化测试框架。\nRobot Framework # 这是最流行的开源 Python 自动化测试框架，表格式的测试数据语法和关键词驱动测试使得它在全球的测试人员中非常流行。它还拥有众多可用的工具和库，并且留有 API 扩展空间，使得这个框架非常先进和健壮。\nRobot Framework 完全用 Python 开发，对于验收测试非常有用。该框架可以运行在 Java 和.NET 环境，同时支持跨平台，如 Windows、MacOS 和 Linux。它无疑是最易用的自动化测试框架，能允许开发者进行并行测试。\nRedwoodHQ # 这是一个流行的自动化测试工具，它之所以流行是因为它支持大部分流行的编程语言，如 Java、Python、C# 等。它还支持多个测试人员在一个平台上协作并运行测试用例。\nRedwoodHQ 有一个内置的 IDE（集成开发环境），可以在那里创建、修改以及运行测试用例。RedwoodHQ 是对用户最友好或对测试人员最友好的平台之一，它关注一个重大项目的全部测试过程。\nJasmine # 这是一套 Javascript 行为驱动开发测试框架（BDD），不依赖于其他任何框架和 DOM，适用于任何使用 JavaScript 的地方。除了 JavaScript 之外，Jasmine 还被用于 Python 和 Ruby 自动化测试。\n因此，使用 Jasmine 可以并行运行客户端测试用例和服务端测试用例。它是一个将客户端和服务端单元测试结合起来的完美的测试框架，而且被认为是测试领域的未来。\nPytest # 如果项目比较小、复杂度比较低，Pytest 是最适合的自动化测试平台。大部分 Python 开发者用它来进行单元测试。它也具有 Robot Framework 所闻名的验收测试能力。\nPytest 最好的特性之一是，它提供了测试用例的详细失败信息，使开发者可以快速准确地改正问题。它还有各种可用插件来给现有测试技术和测试用例增加更多功能和多样性。\n","date":"2018-09-26","externalUrl":null,"permalink":"/posts/most-popular-python-automated-testing-framework/","section":"Posts","summary":"本文整理了当前最受欢迎的 Python 自动化测试框架，包括 Robot Framework、RedwoodHQ、Jasmine 和 Pytest 的介绍和比较。","title":"最受欢迎的 Python 自动化测试框架推荐","type":"posts"},{"content":"","date":"2018-08-07","externalUrl":null,"permalink":"/tags/functiontest/","section":"标签","summary":"","title":"FunctionTest","type":"tags"},{"content":"当你第一次开始接触测试这个行业的时候，首先听说的应该都是功能测试。\n功能测试是通过一些测试手段来验证开发做出的代码是否符合产品需求。这些年功能测试好像不太受欢迎了，不少同学开始尝试自动化测试，测试开发等等，结果是功能测试、自动化测试、测试开发一样都没做好。\n我们通常认为的功能测试是根据需求，采取以下测试流程：需求分析，用例编写，用例评审，提测验证，Bug回归验证，上线与线上回归等测试。如此日复一日，年复一年，可是等准备换工作的时候却得不到认可，你也遇到这种情况吗？\n那么如何做好功能测试？功能测试用到哪些知识？有哪些相关的建议呢？\n需求分析 # 业务方在提出需求的时候，产品是要分析这个需求的价值，影响范围和实现代价的。在需求评审的时候，作为一个测试人员必须了解这次需求的内容，影响到哪些现有的功能，涉及到的操作系统或是类别等，然后准确的评估出工作量，防止因评估不足造成后期测试不充分。\n再者，关注开发和产品的讨论，关注需求最后如何实现？其中做出的变动和难点就是测试的时候必须重点关注的部分，不能因为这些暂时和你没有关系就不去关注，防止欠债越来越多，不能做好充分的的测试。\n第三，需求评审结束后，要求产品更新此次评审过程中的所有改动部分，同时确保之后的任何需求变化都及时更新。\n第四，根据产品需求，同时与在会人员进行探讨，设计测试方案及时间安排，此时可以粗粒度考虑，时间上要合理。\n用例设计与评审 # 测试用例是每个测试人员工作过程中必须要完成的工作，它对测试工作起到指导作用，也是相关业务的一个文档沉淀。在以往面试的经验中，有许多人的测试用例写的没有章法，他们是凭着感觉去写测试用例，也没有从用户的角度来思考如何编写测试用例，对于测试用例设计较为常见的方法论也不清楚。\n假如面试的时候给你一个场景：一个全新的App要发布，如果让你来测试，你能想到哪些测试方案？如果你只能想到如何去测试app的功能的话，作为功能测试人员就考虑不够全面。此时除了App的功能以外，还应关注App的兼容性，易用性，接口的功能测试和性能测试，数据的存储以及容灾情况等等都应考虑在内。\n测试用例可设计为两类： 一类是开发自测和验收提测试标准的冒烟测试用例；一类是针对需求的全面测试用例。\n编写完测试用例后主动联系相关人员进行用例评审，在评审过程中及时修改不合适的用例。\n测试流程，注重项目控制 # 项目的流程控制在需求开始的时候就应该重视起来，只是很多时候我们没有意识到这是测试的工作，有的是产品来控制，有的是专门的项目经理来控制。\n测试人员需要有关注整体项目的意识。如果你不关注项目进度，什么时候提测什么时候开始测试，那么在测试过程中会遇到测试的内容和最初的需求不一致时候就会额外需要时间来解决，导致项目延期。另外主动关注项目，长此以往，你的这份主动性也会是你有效的竞争力。\n需求一旦明确了由你来负责的时候，就要时刻来关注项目的情况。中间变更需求的时候，要评估是否影响项目进度，如果影响了重新进行排期。如果开发提测试晚了，是否影响上线时间，如果影响需要及时跟相关的人员沟通，发风险邮件，通知大家详细的情况。\n同时在测试过程中，发现了bug需要详细描述问题，以方便开发去进行重现和修改。同时给bug准确分级，实时跟踪进度，保证项目高质量的按期完成。\n上线回归与项目总结 # 一个需求上线完成后，要及时进行线上回归，同时必须回归我们在需求评审的时候考虑到的可能影响到的原有的功能，以确保新功能完全上线成功。\n在一个项目完成后，最好有一份个人总结报告，总结整个项目过程中遇到的问题及最后的解决办法，有哪些需要注意的问题？有什么可以借鉴的方案或是改进策略？项目中有没有通用性的问题等等。\n能力的总结和沉淀 # 在找工作的时候，很多做功能测试多年的同学都遭遇过面试失败，究其原因，我觉得最核心的原因是：不具备相应工作年限应该具备的能力。\n我们应该时常问自己一句话：离开现有的平台，我还有什么？如果仅仅是对现在公司业务和工具的熟悉，那是没有任何优势可言的。\n对同类业务流程的掌握，项目的整体把控，快速了解业务并能根据需求选择测试方案，引入提高测试效率测试方案和工具，测试过程中遇到问题的预判和解决办法等才是功能测试人员必须具备的能力。\n这些方面你做到了吗？不要抱怨功能测试如何如何，认清行业现状和自己的优缺点，做好自己的职业规划。\n如果你不善于编码，那么做务专家也是功能测试人员一个很好的选择。\n","date":"2018-08-07","externalUrl":null,"permalink":"/posts/how-to-do-functional-testing/","section":"Posts","summary":"介绍功能测试的基本流程、用例设计、项目控制、上线回归等方面的建议，帮助测试人员提升功能测试的质量和效率。","title":"如何做好功能测试","type":"posts"},{"content":"最近几个月以来一直没有更新公众号文章，是因为五月开始，因为项目原因我有机会转为开发，我非常珍惜这一机会，所以一直在努力学习开发相关的技能。\n做了9年测试，我为何转开发？\n从三年前我在心里就种下了做开发的种子，因为这些年做自动化测试的原因，在写了很多自动化测试用例代码之后，觉得自己还是喜欢写代码，我想在技术上有更深入的学习，无疑作为开发是最直接的办法，所以一直在努力多看、多写代码，一直准备着等待能成为开发测试工程师，或是开发工程师的那一天。\n最近茹炳晟的一篇文章我看了也很受启发《我为何从开发转测试，并坚持了16年，我们正好是相反的职业历程，虽然如此，但是都是想往更好的职业发展方向上去努力，他在视频里提到了很多未来测试可以做的工作，不了解的可以去看看，可以开阔大家的思路，个人觉得良好的代码能力是做好工程师相关的工作基础。测试不是只要认真仔细的点点点就可以了，不是测试这工作更适合女生，不是做测试比做开发轻松，不是可以不思进取还能高枕无忧，做好测试同样需要比别人更多的努力才能看起来轻而易举。\n因为我学的是毕业之后再没接触的C语言，在这几个月的学习过程中，深刻体会了做开发我还有太多未知的领域知识需要去学习，作为开发语言需要深入的学习，这跟学自动化测试不一样，初学自动化测试脚本语言可以边用边查边学，但C语言不一样，它需要很系统的去学，从数组，指针，结构体，链表，二叉树，数据机构都要一个个突破，了解算法、操作系统、编译原理等等。\n虽然是转开发，但是作为测试出身，我会一如既往的关注测试。\n希望通过角色的转变能让我有更全方位的角度来看待产品质量，测试相关的思考和技术，分享更多有价值的内容。\n","date":"2018-07-21","externalUrl":null,"permalink":"/posts/why-i-move-to-development/","section":"Posts","summary":"本文记录了我从测试转为开发的经历和感悟，分享了在陪产假期间的学习和工作安排，包括阅读书籍、参与开源项目、体育锻炼等，强调了如何在照顾家庭的同时保持学习和成长。","title":"做了9年测试，我为何转开发？","type":"posts"},{"content":"如何打印下面的字符？\n$ ## $$$ ### $$$ ## $ 示例 1：\nint main() { char array[] = {\u0026#39;#\u0026#39;, \u0026#39;$\u0026#39;}; for (int row = 1; row \u0026lt;= 7; row++) { for (int hashNum = 1; hashNum \u0026lt;= 4 - abs(4 - row); hashNum++) { printf(\u0026#34;%c\u0026#34;, array[row % 2]); } printf(\u0026#34;\\n\u0026#34;); } } ","date":"2018-07-08","externalUrl":null,"permalink":"/posts/c-print/","section":"Posts","summary":"一个 C 语言打印字符的示例，展示如何使用循环和条件语句打印特定模式的字符。","title":"C-print","type":"posts"},{"content":"","date":"2018-07-08","externalUrl":null,"permalink":"/tags/language/","section":"标签","summary":"","title":"Language","type":"tags"},{"content":"#include \u0026#34;stdafx.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; /* 计算图形的面积： 1. 圆的面积 = π * radius * radius 2. 矩形面积 = weight * height 3. 三角形面积 = 1/2 * weight * height @author Xianpeng Shen */ double calcCircle(double); double calcSquare(double, double); double calcTriangle(double, double); int validate(double); int main() { int choice; // 用户选择 double area; // 图形面积 double radius; // 圆半径 double weight, height; // 图形的宽和高 printf(\u0026#34;1. 圆\\n2. 矩形\\n3. 三角形\\n\u0026#34;); printf(\u0026#34;本系统支持三种图形面积计算，请选择：\u0026#34;); scanf_s(\u0026#34;%d\u0026#34;, \u0026amp;choice); while (choice \u0026gt; 3 || choice \u0026lt; 1) { printf(\u0026#34;只能输入1~3整数，请重新输入：\u0026#34;); scanf_s(\u0026#34;%d\u0026#34;, \u0026amp;choice); } switch (choice) { case 1: printf(\u0026#34;请输入圆的半径：\u0026#34;); do { scanf_s(\u0026#34;%lf\u0026#34;, \u0026amp;radius); if (!(validate(radius))) { printf(\u0026#34;不能为负数，请重新输入一个整数：\u0026#34;); } } while (!validate(radius)); area = calcCircle(radius); break; case 2: printf(\u0026#34;请输入矩形的长和宽：\u0026#34;); do { scanf_s(\u0026#34;%lf%lf\u0026#34;, \u0026amp;weight, \u0026amp;height); if (!validate(weight) || !validate(height)) { printf(\u0026#34;不能为负数，请重新输入两个正数：\u0026#34;); } } while (!validate(weight) || !validate(height)); area = calcSquare(weight, height); break; case 3: printf(\u0026#34;请输入三角形的底和高：\u0026#34;); do { scanf_s(\u0026#34;%lf%lf\u0026#34;, \u0026amp;weight, \u0026amp;height); if (!validate(weight) || !validate(height)) { printf(\u0026#34;不能为负数，请重新输入两个正数：\u0026#34;); } } while (!validate(weight) || !validate(height)); area = calcTriangle(weight, height); break; default: printf(\u0026#34;只能输入1~3整数，请重新输入：\u0026#34;); break; } printf(\u0026#34;图形面积为：%.2lf\\n\u0026#34;, area); } double calcCircle(double radius) { return 3.14 * radius * radius; } double calcSquare(double weight, double height) { return weight * height; } double calcTriangle(double weight, double height) { return weight * height / 2; } int validate(double num) { return num \u0026gt; 0; // 如果 num\u0026gt;0, 返回一个非零值，表示真。 } ","date":"2018-05-16","externalUrl":null,"permalink":"/posts/calculate-graph-area/","section":"Posts","summary":"一个 C 语言程序，用于计算圆、矩形和三角形的面积，支持用户输入和验证。","title":"C-Language 计算图形的面积","type":"posts"},{"content":"求次幂函数power\n#include \u0026lt;stdio.h\u0026gt; double power(double, int); // 形式参数 int main() { printf(\u0026#34;%.2lf的%d次幂等于:%.2lf\\n\u0026#34;, 5.2, 2, power(5.2, 2)); //实际参数 return 0; } double power(double num1, int num2) // 形式参数 { double result = 1; int i; for (i = 0; i \u0026lt; num2; i++) { result *= num1; // 累乘 } return result; } ","date":"2018-05-15","externalUrl":null,"permalink":"/posts/c-self-defining-function/","section":"Posts","summary":"介绍 C 语言中自定义函数的基本概念和使用方法，包括函数的声明、定义和调用。","title":"C-Language 自定义函数","type":"posts"},{"content":"如果你想在一台电脑上配置 github 和 bitbucket，如何配置多个 SSH git key？ 输入以下命令生成 SSH Key，注意在生成过程中最好输入新的名字，比如 id_rsa_github 和 id_rsa_bitbucket\nssh-keygen -t rsa -C \u0026#34;your_email@youremail.com\u0026#34; 然后将生成的 SSH key 文件内容复制到对应网址的个人用户设置中即可。但是明明按照官方教程做的但是在 git clone 的时候还是遇到以下问题： Error: Permission denied (publickey) 困恼了几天的错误终于解决了。\n参看这个文档\n由于我用的是macOS Sierra 10.13.3，文档这里写着如果是macOS Sierra 10.12.2 及以后的版本需要在 ~/.ssh 目录下创建一个 config 文件 congfig 文件的具体配置如下：\nHost * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa_github Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa_bitbucket 配置了这个文件之后，再次尝试\ngit clone git@github.com:shenxianpeng/blog.git 可以 download 代码了，两个 SSH git 都好用了 : )\n","date":"2018-05-06","externalUrl":null,"permalink":"/posts/permission-denied-publickey/","section":"Posts","summary":"本文介绍了如何在配置多个 SSH Git Key 时解决 \u0026ldquo;Permission denied (publickey)\u0026rdquo; 错误，确保 GitHub 和 Bitbucket 的 SSH 连接正常工作。","title":"Error: Permission denied (publickey)","type":"posts"},{"content":"满足什么样的技术和经验才算高级工程师呢？说说我心中的高级工程师。\n具有丰富的行业测试经验 # 最好有传统和互联网大公司工作经验，没有的话至少与这些公司的高级测试工程师有交流，了解他们是如何开展测试的，有助提高自己的眼界。\n有良好的测试基础 # 掌握必要的测试理论，熟悉测试流程，需求分析，测试用例设计方法，根据项目实际需要制定测试方案。\n有丰富的业务能力 # 做好功能测试的前提是熟悉业务，能更好的站在产品的角度去设计测试用例，才能发现基本功能以外的问题，能给产品提出建设性的需求和意见。\n熟悉相关的测试工具 # 软件测试用到的相关工具非常多，了解和使用过这些工具，能更好的结合公司的要求及项目的需求来权衡引入哪些工具，提高工作效率。\n管理工具：比如JIRA，Testlink，Wiki，Confluence 持续集成：Jenkins，Bamboo，Travis CI等，了解他们之间的区别以及如何实施。 自动化测试：web和mobile平台之间是如何做自动化才测试的，用到哪些工具。了解Selenium，WebDriver，Appium，Robotium测试框架，以及用哪些语言去开发自动化测试用例，Python？Java？JavaScript？知道如何选择如何实施。 性能测试：了解Jmeter，LoadRunner这两个主要的性能测试工具，如何开展性能测试。 有良好的代码能力 # 良好的代码能力可以快速掌握自动化测试，甚至可以开发测试平台。另外，当你跳槽到任何一家公司可以让你快速熟悉Java、Python、Javascript等任何语言编写的自动化测试用例。\n语言能力 # 包括沟通能力和外语能力。沟通是一个测试人员在工作中必不可少的一项基本技能，良好的沟通会让开发人员了解问题所在，接受你的意见，从产品人员那里更好的了解需求。虽然只有在外企的时候才会用到英语，但是随着测试人员也需要学习很多的技术，开源社区的发展，很多第一手资料都是用英文写的，所以学好英文对于扩展和学习新知识有很大帮助。\n所以说成为一名优秀的高级测试工程师所要求的能力还是很多的，一起努力吧！💪\n","date":"2018-05-06","externalUrl":null,"permalink":"/posts/senior-test-engineer/","section":"Posts","summary":"本文介绍了高级测试工程师所需的技能和经验，包括测试理论、业务能力、工具使用、代码能力等方面，帮助读者了解如何成为一名优秀的高级测试工程师。","title":"我眼中的高级测试工程师","type":"posts"},{"content":"我想大多数的团队都面临这样的问题：\n发布周期长\n开发和测试时间短\n开发和测试是两个独立的团队\n不稳定的交付质量\n低收益难维护的UI自动化测试脚本\n不合理的测试权重分配\n解决方法：\n引入 DevOps 和分层自动化\n组件化产品 产品开发引入模块化，数据驱动会使得产品更加容易实施 Unit，Server，UI 自动化测试 优化工程师 开发和测试在未来将没有界限，他们都是开发者，都会产品的质量和客户负责 分层自动化 更合理的测试权重分配，更底层的测试收益越高 引入工具 实施DevOps引入必要的工具，Bitbucket, Jenkins, Sonar, Pipelines, Docker, test framework … ","date":"2018-04-14","externalUrl":null,"permalink":"/posts/devops-practice/","section":"Posts","summary":"本文介绍了 DevOps 实践的核心概念、目标和实施方法，强调了持续集成、持续交付和自动化的重要性。","title":"DevOps 实践","type":"posts"},{"content":"最近在做有关 DevOps Build 的时候，学习了 Jenkins 的 Pipeline 的功能，不得不提到的就是 Jenkinsfile 这个文件。\n以下面是我配置的 Jenkinsfile 文件及简单说明，更多有关 Pipeline 请看官方文档。\npipeline { agent any stages { // Build 阶段 stage(\u0026#39;Build\u0026#39;) { steps { echo \u0026#39;Building...\u0026#39; bat \u0026#39;npm run build webcomponent-sample\u0026#39; } } // 单元测试阶段 stage(\u0026#39;Unit Test\u0026#39;) { steps { echo \u0026#39;Unit Testing...\u0026#39; bat \u0026#39;npm test webcomponent-sample\u0026#39; } post { success { // 执行成功后生产报告 publishHTML target: [ allowMissing: false, alwaysLinkToLastBuild: false, keepAll: true, reportDir: \u0026#39;components/webcomponent-sample/coverage/chrome\u0026#39;, reportFiles: \u0026#39;index.html\u0026#39;, reportName: \u0026#39;RCov Report\u0026#39; ] } } } // E2E 测试阶段 stage(\u0026#39;E2E Test\u0026#39;) { steps { bat \u0026#39;node nightwatch e2e/demo/test.js\u0026#39; } } stage(\u0026#39;Release\u0026#39;) { steps { echo \u0026#39;Release...\u0026#39; } } } post { // 执行成功是触发 success { mail bcc: \u0026#39;email@qq.com\u0026#39;, body: \u0026#34;\u0026lt;b\u0026gt;Project: ${env.JOB_NAME} \u0026lt;br\u0026gt;Build Number: ${env.BUILD_NUMBER} \u0026lt;br\u0026gt;Build URL: ${env.BUILD_URL} \u0026#34;, cc: \u0026#39;\u0026#39;, charset: \u0026#39;UTF-8\u0026#39;, from: \u0026#39;jenkins@qq.com\u0026#39;, mimeType: \u0026#39;text/html\u0026#39;, replyTo: \u0026#39;\u0026#39;, subject: \u0026#34;SUCCESS: Project Name -\u0026gt; ${env.JOB_NAME}\u0026#34;, to: \u0026#34;\u0026#34;; } // 执行失败时触发 failure { mail bcc: \u0026#39;email@qq.com\u0026#39;, body: \u0026#34;\u0026lt;b\u0026gt;Project: ${env.JOB_NAME} \u0026lt;br\u0026gt;Build Number: ${env.BUILD_NUMBER} \u0026lt;br\u0026gt;Build URL: ${env.BUILD_URL} \u0026#34;, cc: \u0026#39;\u0026#39;, charset: \u0026#39;UTF-8\u0026#39;, from: \u0026#39;jenkins@qq.com\u0026#39;, mimeType: \u0026#39;text/html\u0026#39;, replyTo: \u0026#39;\u0026#39;, subject: \u0026#34;FAILURE: Project Name -\u0026gt; ${env.JOB_NAME}\u0026#34;, to: \u0026#34;\u0026#34;; } } } ","date":"2018-04-14","externalUrl":null,"permalink":"/posts/jenkinsfile-configure/","section":"Posts","summary":"本文介绍了如何使用 Jenkinsfile 配置 Jenkins Pipeline，包括构建、测试和发布阶段的示例，以及如何处理邮件通知。","title":"Jenkinsfile 配置","type":"posts"},{"content":"有些git命令总是记不住，在我这台 Ubuntu 使用 web 版 OneNote 不方便，那就把他们记到 Blog 里吧，需要的时候翻看一下。\ngit remote\ngit remote -v # 查看当前位置的远程代码库 git remote remove origin # 取消远程仓库 git remote add orgin git@github.com:shenxianpeng/nightwatch.git # 关联新的仓库 git log\n# 得到某一时段提交日志 git log --after=\u0026#39;2017-12-04\u0026#39; --before=\u0026#39;2017-12-08\u0026#39; --author=xshen --pretty=oneline --abbrev-commit git tag\ngit tag -a v1.6.700 -m \u0026#39;Release v1.6.700\u0026#39; # 给前面的提交补上 tag git log --pretty=oneline git tag -a v1.6.700 -m \u0026#39;Release v1.6.700\u0026#39; e454ad98862 git push tag git push origin --tag 设置 npm install 代理\nnpm config set proxy=http://10.17.201.60:8080 # 设置代理 npm config set proxy null # 取消代理 设置 cnpm\nnpm install -g cnpm --registry=https://registry.npm.taobao.org cnpm install [name] cnpm sync connect cnpm info connect ","date":"2018-02-26","externalUrl":null,"permalink":"/posts/git-command-cheat/","section":"Posts","summary":"本文总结了 Git 的常用命令和技巧，帮助开发者快速查找和使用 Git 命令，提高工作效率。","title":"Git 命令备忘","type":"posts"},{"content":"如果是通过 https 方式来 pull 和 push 代码，每次都要输入烦人的账号和密码 可以通过切成成 ssh 方式：\n# 取消远程仓库 git remote remove origin # 关联远程仓库 git remote add origin git@github.com:shenxianpeng/blog.git ","date":"2018-02-06","externalUrl":null,"permalink":"/posts/remove-and-add-remote-repository/","section":"Posts","summary":"本文介绍了如何在 Git 中移除和添加远程仓库，帮助开发者管理代码仓库的远程连接。","title":"Git remove and add remote repository","type":"posts"},{"content":"","date":"2018-02-01","externalUrl":null,"permalink":"/tags/javascript/","section":"标签","summary":"","title":"Javascript","type":"tags"},{"content":"除了通过增加\nconsole.log(\u0026#39;===========\u0026#39;) 来调试 Nightwatch 代码，如何通过配置 VS code 来 Debug Nightwatch 代码？\nCtrl+Shift+D 打开 Debug 界面，配置如下：\n{ // Use IntelliSense to learn about possible Node.js debug attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;npm test\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceRoot}/node_modules/nightwatch/bin/runner.js\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;tests/DQA/DQA-221/login.js\u0026#34; ] } ] } ","date":"2018-02-01","externalUrl":null,"permalink":"/posts/debug-nightwatch-in-vs/","section":"Posts","summary":"本文介绍如何配置 VS Code 来调试 Nightwatch 测试代码，提供了详细的配置示例和步骤。","title":"Nightwatch 使用 VS code 进行调试","type":"posts"},{"content":" 在持续集成执行自动化测试用例时候会遇到那些问题呢 # 运行时间过长 因为某些错误程序卡住 异常处理 针对以上三种情况，通过下面的三种方式进行解决 # 运行时间过长, E2E 测试脚本中难免需要时间等待，例如 # this.pause(1000); // 尽可能将说有的 pause 换成 wait，例如： this.element(\u0026#39;@columns\u0026#39;).to.be.visible.before(2000); // 或 this.waitForElementVisible(\u0026#39;@columns\u0026#39;, 5000); 因为某些错误程序卡住, 在 TestCase 中进行验证时，例如 # this.assert.equal(result.value.length, 1); // 如果只想标注失败，继续执行后面的代码，则需将 assert 换成 verify this.veriry.equal(result.value.length, 1); // 在 waitForElementVisible 中加 abortOnFailure 参数，当设置为 false，在 wait 超时时，就会标志为 false 继续继续执行 this.waitForElementVisible(\u0026#39;@columns\u0026#39;, 5000, false); //还可以通过在 nightwatch.conf.js 设置全局变量 abortOnAssertionFailure: false 异常处理 # 当程序执行运行一次时，程序运行正常，一旦遇到异常时，下次执行就回出错。 例如：比如邀请账号登录系统的操作。管理员添加一个新用户，然后用这个新用户登录，之后管理员删除这个账户。但如果删除这个账号失败时，下次执行这个程序再邀请这个账号时就会提示这个账号存在的，可能这个时候这个程序就执行不下去了。这个时候就需要考虑这些异常情况处理，保证程序能够良好的执行下去。\n","date":"2018-01-15","externalUrl":null,"permalink":"/posts/nightwatch-ci-problem/","section":"Posts","summary":"介绍如何在 Nightwatch 持续集成中处理自动化测试用例的常见问题，包括运行时间过长、程序卡住和异常处理。","title":"Nightwatch 持续集成问题","type":"posts"},{"content":"如果想打开两个窗口并控制那个窗口怎么办？\nvar url = process.env.BASE_URL, newWindow; client.execute(function (url, newWindow) { window.open(url, newWindow, \u0026#39;height=768,width=1024\u0026#39;); }, [url, newWindow]); client.window_handles(function(result) { this.verify.equal(result.value.length, 2, \u0026#39;There should be 2 windows open\u0026#39;); newWindow = result.value[1]; this.switchWindow(newWindow); }) ","date":"2018-01-02","externalUrl":null,"permalink":"/posts/open-multiple-windows/","section":"Posts","summary":"如何在 Nightwatch 中打开多个浏览器窗口并切换控制。","title":"Nightwatch 打开多个窗口","type":"posts"},{"content":"","date":"2017-12-26","externalUrl":null,"permalink":"/tags/ubuntu/","section":"标签","summary":"","title":"Ubuntu","type":"tags"},{"content":"如何在 Ubuntu 上连接 Cisco AnyConnect VPN\n打开Terminal，执行：\nsudo /sbin/modprobe tun 安装OpenConnect，执行:\nsudo apt-get install openconnect 连接VPN，执行：\nsudo openconnect yourvpn.example.com 将提示你输入用户名和密码，输入争取后，VPN连接成功。\n原文 请点击 。\n","date":"2017-12-26","externalUrl":null,"permalink":"/posts/use-vpn-on-ubuntu/","section":"Posts","summary":"本文介绍了如何在 Ubuntu 上使用 VPN，包括安装和配置步骤，帮助用户安全地访问网络资源。","title":"Ubuntu 上使用 VPN","type":"posts"},{"content":"","date":"2017-12-26","externalUrl":null,"permalink":"/tags/vpn/","section":"标签","summary":"","title":"VPN","type":"tags"},{"content":"在 Ubuntu 下面安装 Visual Studio Code\nsudo add-apt-repository ppa:ubuntu-desktop/ubuntu-make sudo apt-get update sudo apt-get install ubuntu-make umake web visual-studio-code 亲测，好用。\n","date":"2017-12-25","externalUrl":null,"permalink":"/posts/install-vscode-on-ubuntu/","section":"Posts","summary":"在 Ubuntu 上安装 Visual Studio Code 的步骤和方法，帮助你快速开始使用这款强大的代码编辑器。","title":"Ubuntu 上安装 VS Code","type":"posts"},{"content":"在使用 Nightwatch 做自动化测试的时候，会遇到这样一种情况： 创建一个 query, 等待这个query的状态从 Wait 变成 Running 最后到 Available 时再执行操作。 Nightwatch 并没有提供这样的方法，可以通过下面的方式解决。\n\u0026#39;Wait for text\u0026#39;: function waitForText(client) { const query = client.page.query(); query.navigate(); for (let i = 0; i \u0026lt;= 10; i++) { client.getText(\u0026#39;status\u0026#39;, function (result) { if (result.value.indexOf(\u0026#39;Available\u0026#39;) == 0) { this.break; } else { client.pause(1000); i++; } }); } // TODO something } ","date":"2017-12-19","externalUrl":null,"permalink":"/posts/nightwatch-wait-for-text/","section":"Posts","summary":"本文介绍了如何在 Nightwatch.js 中等待特定文本出现的示例代码。","title":"Nightwatch wait For Text","type":"posts"},{"content":" 测试用例 # 验证登录 cookies 和清除 access_token。测试用例设计如下\n测试用例设计 # 登录系统时，不选择记住我按钮，验证 cookies\nclient.getCookies(function cb(result) { this.assert.equal(result.value.length, 3); this.assert.equal(result.value[0].name, \u0026#39;domain\u0026#39;); this.assert.equal(result.value[1].name, \u0026#39;user_id\u0026#39;); this.assert.equal(result.value[2].name, \u0026#39;access_token\u0026#39;); }); 登录系统时，选择记住我按钮，验证 cookies\nclient.getCookies(function cb(result) { this.assert.equal(result.value.length, 5); this.assert.equal(result.value[0].name, \u0026#39;domain\u0026#39;); this.assert.equal(result.value[1].name, \u0026#39;user_id\u0026#39;); this.assert.equal(result.value[2].name, \u0026#39;identifier\u0026#39;); this.assert.equal(result.value[3].name, \u0026#39;access_token\u0026#39;); this.assert.equal(result.value[4].name, \u0026#39;persistent_token\u0026#39;); }); 登录系统时，不选择记住我按钮，删除 cookies\nlet accesstoken; client.getCookies(function cb(result) { accesstoken = result.value[2].name; this.deleteCookie(accesstoken, function () { // refresh current page, logout this.refresh().waitForElementVisible(\u0026#39;div.login-form\u0026#39;, 5000); }); }); 登录系统时，选择记住我按钮，删除 cookies\nlet accesstoken; client.getCookies(function cb(result) { accesstoken = result.value[3].name; this.deleteCookie(accesstoken, function() { // refresh current page, still login this.refresh().waitForElementVisible(\u0026#39;.andes-header\u0026#39;, 5000); }); }); 如何知道登录都有哪些参数 # 事先在手动测试的时候打开 chrome 浏览器，然后按 F12，登录时查看 Network。\n以成功百度登录时为例，可以看到 Headers 里的参数，我们可以通过验证这些参数来确定登录成功了。\n这样我们就可以这些参数来实现对 cookie，token 等等参数进行自动化测试的验证。\n","date":"2017-12-14","externalUrl":null,"permalink":"/posts/nightwatch-handle-cookies/","section":"Posts","summary":"本文介绍了如何在 Nightwatch.js 中处理 cookies，包括登录时验证 cookies 的存在性和清除 access_token 的示例代码。","title":"Nightwatch 得到和验证 cookies","type":"posts"},{"content":" Nightwatch 元素常用验证方法 # 验证元素的值信息\nandesFormSection .assert.containsText(\u0026#39;@errorMessage\u0026#39;, \u0026#39;The email address is invalid.\u0026#39;) 验证元素是否可用\nandesFormSection .assert.attributeEquals(\u0026#39;@continueBtn\u0026#39;, \u0026#39;disabled\u0026#39;, \u0026#39;true\u0026#39;); 等待元素可用\nandesFormSection .expect.element(\u0026#39;@signInBtn\u0026#39;).to.be.visible.before(5000); 或者 andesFormSection waitForElementVisible(\u0026#39;signInBtn\u0026#39;, 5000); 等待元素呈现\nandesFormSection .expect.element(\u0026#39;@signInBtn\u0026#39;).to.be.present.before(5000); 或者 andesFormSection waitForElementPresent(\u0026#39;signInBtn\u0026#39;, 5000); ","date":"2017-12-14","externalUrl":null,"permalink":"/posts/nightwatch-element-check/","section":"Posts","summary":"本文介绍了如何在 Nightwatch.js 中验证元素的存在性和状态，包括常用的验证方法和示例代码。","title":"Nightwatch 元素判断","type":"posts"},{"content":" 对于一个不善于言表的我工作中遇到过 # 过度理解在与同事之间的Email和Chat中的意思； 同事之间的沟通中出现的分歧事后还会继续琢磨； 十分关注自己的工作失与得在上级领导中的看法。 以下方式对我来说还比较有效的 # 让自己的精力更多的聚焦在工作上； 工作中对事不对人，做对的事情； 眼光放长远，不忘初心，专注应该做的事情上； 领导说的一句“向前看”我印象深刻，过去的就过去了，别再纠结，向前看。 ","date":"2017-11-23","externalUrl":null,"permalink":"/misc/weather-setbacks-at-work/","section":"Miscs","summary":"工作中遇到挫折时，如何调整心态，专注于工作本身，而不是过度纠结于人际关系。","title":"度过工作中挫折心结","type":"misc"},{"content":"Hexo 默认主题代码高亮是黑色的，如果想换个风格？具体操作如下：\n# 修改 highlight.styl 文件，路径 themes/landscape/source/css/_partial/highlight.styl 修改默认代码主题 Tomorrow Night Eighties\nhighlight-background = #2d2d2d highlight-current-line = #393939 highlight-selection = #515151 highlight-foreground = #cccccc highlight-comment = #999999 highlight-red = #f2777a highlight-orange = #f99157 highlight-yellow = #ffcc66 highlight-green = #99cc99 highlight-aqua = #66cccc highlight-blue = #6699cc highlight-purple = #cc99cc 为主题 Tomorrow\nhighlight-background = #ffffff highlight-current-line = #efefef highlight-selection = #d6d6d6 highlight-foreground = #4d4d4c highlight-comment = #8e908c highlight-red = #c82829 highlight-orange = #f5871f highlight-yellow = #eab700 highlight-green = #718c00 highlight-aqua = #3e999f highlight-blue = #4271ae highlight-purple = #8959a8 更多详情请参考 tomorrow-theme 修改。\n","date":"2017-11-20","externalUrl":null,"permalink":"/posts/change-hexo-code-highlight/","section":"Posts","summary":"Hexo 默认主题代码高亮是黑色的，如果想换个风格？本文介绍如何修改 Hexo 主题代码高亮样式。","title":"Change Hexo code highlight","type":"posts"},{"content":"自动化测试中，有一个验证点，当测试通过时，后面的测试脚本继续执行； 当出现异常时，你希望标记出来这个错误，但不影响后面的测试脚本执行，在 Nightwatch 中如何做？\n下面的一段代码验证 home 页面的 body 是否显示。这里如果显示则将验证点置为 false，如下：\nhome.waitForElementVisible(\u0026#39;@body\u0026#39;, 3000, true, function(result) { if (result.value) { // 测试报告中会显示失败，但是会继续执行后面的测试脚本 client.verify.equal(result.value, false); } else { // 验证点通过 console.log(\u0026#39;Pass\u0026#39;); } }); 注意：这里如果用 assert，程序就会中断执行。\n// 中断执行 client.assert.equal(result.value, false); ","date":"2017-10-27","externalUrl":null,"permalink":"/posts/test-case-fails-to-continue-execution/","section":"Posts","summary":"本文介绍了如何在 Nightwatch 自动化测试中处理测试用例失败的情况，使得后续测试脚本能够继续执行，而不是中断。","title":"Nightwatch 测试用例失败继续执行","type":"posts"},{"content":" 元素常用验证方法 # 验证元素的值信息\nandesFormSection .assert.containsText(\u0026#39;@errorMessage\u0026#39;, \u0026#39;The email address is invalid.\u0026#39;) 验证元素是否可用\nandesFormSection .assert.attributeEquals(\u0026#39;@continueBtn\u0026#39;, \u0026#39;disabled\u0026#39;, \u0026#39;true\u0026#39;); 等待元素可用\nandesFormSection .expect.element(\u0026#39;@signInBtn\u0026#39;).to.be.visible.before(5000); # 或者 andesFormSection waitForElementVisible(\u0026#39;signInBtn\u0026#39;, 5000); 等待元素呈现\nandesFormSection .expect.element(\u0026#39;@signInBtn\u0026#39;).to.be.present.before(5000); # 或者 andesFormSection waitForElementPresent(\u0026#39;signInBtn\u0026#39;, 5000); 判断元素是否存在 # 用 Nightwatch 去判断一个 element 是否存在，如果存在执行如下操作，如果不存在做另外的操作。 这个在 Java 编写的自动化测试用例中可以用 try catch 可以解决，Nightwatch 试过不行。 另外看到 stackoverflow 上有通过判断 (result.status != -1)，没有解决我的问题。\n最后这样解决的，请看下面 tutorial.js\nconst tutorialCommands = { notShowTutorial: function() { const tutorialSection = this.section.tutorial; this.api.element(\u0026#39;css selector\u0026#39;, \u0026#39;.andes-dialog md-icon\u0026#39;, function(result) { if (result.value \u0026amp;\u0026amp; result.value.ELEMENT) { this.pause(2000); tutorialSection.click(\u0026#39;@doNotShowBtn\u0026#39;); this.pause(2000); tutorialSection.click(\u0026#39;@closeBtn\u0026#39;); } else { console.log(\u0026#39;no tutorial exists\u0026#39;); } }); } }; module.exports = { commands: [tutorialCommands], url: function() { return `https://shenxianpeng.github.io/`; }, sections: { tutorial: { selector: \u0026#39;.andes-dialog\u0026#39;, elements: { closeBtn: \u0026#39;md-icon\u0026#39;, doNotShowBtn: \u0026#39;md-checkbox .md-container\u0026#39; } } } }; 注意：这里的元素不能通过 section 的方式引用，例如这样，怀疑这是 Nightwatch 的 bug。\ntutorialSection.api.element(\u0026#39;css selector\u0026#39;, \u0026#39;@closeBtn\u0026#39;, function(result) { } ","date":"2017-10-26","externalUrl":null,"permalink":"/posts/nightwatch-cdetermine-if-element-exists/","section":"Posts","summary":"本文介绍了如何在 Nightwatch.js 中判断元素是否存在，并提供了示例代码。","title":"Nightwatch 验证元素是否存在","type":"posts"},{"content":"","date":"2017-10-25","externalUrl":null,"permalink":"/tags/blog/","section":"标签","summary":"","title":"Blog","type":"tags"},{"content":"如果想在 Hexo 文章中插入图片怎么做？\n网络上很容易搜到 Markdown 的语法是 ![Alt text](/path/to/img.jpg) 前面 Alt text 是指在图片下面命名，后面是图片的地址。那么如何配置？\n经过几番尝试得知：在你的 hexo 项目根目录下面 source 创建一个 images 文件夹， 把你以后用的到图片都放在这个目录下面就 OK 了。\n![示例图1](../images/color.png) ","date":"2017-10-25","externalUrl":null,"permalink":"/posts/insert-img-in-hexo-article/","section":"Posts","summary":"在 Hexo 博客文章中插入图片的方法和技巧，帮助你更好地展示内容。","title":"Hexo 博客文章中插入图片","type":"posts"},{"content":"安装 hexo-generator-feed 插件\nnpm install hexo-generator-feed --save 如果国内 npm 安装不成功，可以先安装 cnpm\nnpm install -g cnpm --registry=https://registry.npm.taobao.org 然后再\ncnpm install hexo-generator-feed --save 在 _config.yml 中配置这个插件\nfeed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: \u0026#39; \u0026#39; ","date":"2017-10-25","externalUrl":null,"permalink":"/posts/configure-rss-subscription-for-hexo/","section":"Posts","summary":"本文介绍如何在 Hexo 博客中配置 RSS 订阅功能，包括安装插件和配置文件。","title":"Hexo 配置 rss 订阅功能","type":"posts"},{"content":"在做 Nightwatch 自动化测试中，出现需要比较颜色的时候如何来做？ 基本的思路是首先需要取到这个 element 的颜色值，然后跟预期的颜色进行对比。 比如我要取下面这个会话窗口的颜色，选中这个图标，按 F12，查看这个图标的属性。发现Angular中的颜色属性不是 Elements 下，是在 Styles 下面，如何取到这个颜色值？\n这里会用到 getCssProperty 这个方法，具体如何使用，请看如下代码：\ngetChatColor: function(cb) { const chat = \u0026#39;[ng-click=\u0026#34;show()\u0026#34;]\u0026#39; this.getCssProperty(\u0026#39;@chat\u0026#39;, \u0026#39;background-color\u0026#39;, function(result) { cb(result.value); }); return this; }, 将上面的 getChatColor command 代码放到一个叫 chat.js 的 page 下面，然后在测试用例中这样调用这个 command\n\u0026#39;Test get color\u0026#39;: function (client) { var chat = client.page.chat(); let chatColor; chat.navigate(); chat.getChatColor(function(color) { chatColor = color; }); client.perform(function() { client.assert.equal(chatColor, \u0026#39;rgba(50, 104, 152, 1)\u0026#39;); }); } 截图中看到的 background color 是 rgb(50, 104, 152), 但是 getChatColor 返回指是rgba，rgb 和 rgba 之间需要转化一下，a 表示透明度，取值0~1之间。\n","date":"2017-10-25","externalUrl":null,"permalink":"/posts/nightwatch-auto-compare-colors/","section":"Posts","summary":"本文介绍了如何在 Nightwatch.js 中处理颜色比较，包括获取元素的颜色值并与预期颜色进行对比的示例代码。","title":"Nightwatch 自动化测试中比较颜色","type":"posts"},{"content":"如何在 JavaScript 通过接口自动生成和返回接口数据呢？\n在自动化测试中常常遇到接口测试，或是使用的数据需要从接口返回，那么如何来实现这种情况？\n例如我想通过 generateLicense 方法生成一个 license，然后在之后的自动化测试用例中使用这个生成的 license 继续做下一步的操作，例如注册 license 等。\n在 license.js 文件中创建一个 generateLicense 方法：\ngenerateLicense: function(success, day, capacity, code) { var request = require(\u0026#39;request\u0026#39;); var options = { method: \u0026#39;POST\u0026#39;, url: \u0026#39;https://generate-license/api/licenses\u0026#39;, headers: { \u0026#39;postman-token\u0026#39;: \u0026#39;d849e636-58c9-2705\u0026#39;, \u0026#39;cache-control\u0026#39;: \u0026#39;no-cache\u0026#39;, authorization: \u0026#39;Basic YWRtaW46U\u0026#39;, \u0026#39;content-type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: { company: \u0026#39;Google\u0026#39;, email: \u0026#39;5012962@qq.com\u0026#39;, expiration: day, capacity: capacity, phone: \u0026#39;89262518\u0026#39;, address: \u0026#39;Dalian\u0026#39;, code: code }, json: true }; request(options, function (error, response) { if (error) { console.log(error); return; } success(response); }); }, 对上面生成的 license 进行赋值，之后的测试用例中就可以使用 MVlicense 了。 使用中会涉及到异步操作，异步如何操作请看之前的文章。\nconst license = client.page.license(); let MVlicense; license.generateLicense(function(response) { MVlicense = response.body.data.license.license; }, 365, 10, \u0026#39;MV\u0026#39;); ","date":"2017-10-22","externalUrl":null,"permalink":"/posts/nightwatch-get-interface-return-data/","section":"Posts","summary":"本文介绍了如何在 Nightwatch.js 中通过接口自动生成和返回数据，并在测试用例中使用这些数据。","title":"Nightwatch 获取接口返回数据","type":"posts"},{"content":"在自动化测试中常常需要通过一个 command（或 function ）中返回的值来进行下一步的操作，JavaScript 与 JAVA 在调用返回值时有所不同，JS 中需要特定的写法来进行这种异步操作。\n以下面的 get License 数量为例，首先需要 get 一次 License 数量，然后进行一些列操作之后，再一次 get License 数量，比较这两次 License 数量值。\ngetLicenseNum 方法：\ngetLicenseNum: function (cb) { const license = \u0026#39;ul \u0026gt; li.license-id.ng-binding\u0026#39;; this.waitForElementVisible(license, 5000); this.api.elements(\u0026#39;css selector\u0026#39;, license, function (result) { cb(result.value.length); return this; }); } 对两次得到的 License num 进行比较：\n\u0026#39;JavaScrpit asynchronous operation\u0026#39;: function(client) { const license = client.page.license(); let num1, num2; license.getLicenseNum(function(num) { num1 = num; }); license.getLicenseNum(function(num) { num2 = num; }); client.perform(function() { client.assert.equal(num2 - num1, 1, \u0026#39;license number increase 1\u0026#39;); }); } ","date":"2017-10-21","externalUrl":null,"permalink":"/posts/nightwatch-async-operation/","section":"Posts","summary":"本文介绍了如何在 Nightwatch.js 中处理异步操作，包括获取 License 数量并进行比较的示例代码。","title":"Nightwatch 异步操作","type":"posts"},{"content":"在自动化测试中有这样一个场景，在一个输入框中输入一串字符，然后执行敲回车键，验证搜索结果，以 Google 搜索为例，代码如下：\n\u0026#39;search nightwatch and click ENTER key\u0026#39;: function(client) { client .url(\u0026#39;http://google.com\u0026#39;) .expect.element(\u0026#39;body\u0026#39;).to.be.present.before(1000); client.setValue(\u0026#39;input[type=text]\u0026#39;, [\u0026#39;nightwatch\u0026#39;, client.Keys.ENTER]) .pause(1000) .assert.containsText(\u0026#39;#main\u0026#39;, \u0026#39;Night Watch\u0026#39;); } 不能翻墙的可换成 baidu，相应的 element 需要改一下否则以上代码会报错。 上面的代码是执行一个按键操作，如果想做组合键操作怎么办呢？比如在 Google 搜索框中输入 nightwatch，然后按 ctrl+a 组合键来进行全选操作。还是以 Google 搜索为例，代码如下：\nclient.setValue(\u0026#39;input[type=text]\u0026#39;,[\u0026#39;nightwatch\u0026#39;, [client.Keys.CONTROL, \u0026#39;a\u0026#39;]]) 其他的组合键操作以此类推。\n其他按键 Keys 如下： Keys: { NULL, CANCEL, HELP, BACK_SPACE, TAB, CLEAR, RETURN, ENTER, SHIFT, CONTROL, ALT, PAUSE, ESCAPE, SPACE, PAGEUP, PAGEDOWN, END, HOME, LEFT_ARROW, UP_ARROW, RIGHT_ARROW, DOWN_ARROW, ARROW_LEFT, ARROW_UP, ARROW_RIGHT, ARROW_DOWN, INSERT, DELETE, SEMICOLON, EQUALS, NUMPAD0, NUMPAD1, NUMPAD2, NUMPAD3, NUMPAD4, NUMPAD5, NUMPAD6, NUMPAD7, NUMPAD8, NUMPAD9, MULTIPLY, ADD, SEPARATOR, SUBTRACT, DECIMAL, DIVIDE, F1, F2, F3, F4, F5, F6, F7, F8, F9, F10, F11, F12, COMMAND, META },\n","date":"2017-10-19","externalUrl":null,"permalink":"/posts/nightwatch-keyboard/","section":"Posts","summary":"本文介绍了如何在 Nightwatch.js 中模拟键盘操作，包括输入文本和组合键操作的示例代码。","title":"Nightwatch 模拟键盘操作","type":"posts"},{"content":"Nightwatch中文参考手册\n为何放弃 JAVA 改用 Nightwatch # 项目初期用的是 Java + Selenium + TestNG 自动化框架，由于之前推行的力度不够，加上繁重的功能测试和频繁的项目变更导致自动化测试代码跟不上开发的进度，大量的测试代码无法正在运行。 我们的产品采用的 AngularJS 开发，前端开发人员对js对Java更精通，以后的自动化脚本开发也可以一起编写。 Nightwatch 的环境配置和执行简单，只要 npm install、npm test 就可以运行起来，方便配置，运行和继续集成。 因此，与其维护不可用的代码不如好好整理，不如在项目领导和开发的强力支持下重新开始做一套可用的 E2E 测试。\n关于 Nightwatch 翻译 # 学习中发现 Nightwatch 没有比较完整的中文参考手册，因此决定学习之余翻译下官方文档，如有问题，欢迎纠正。\n","date":"2017-10-19","externalUrl":null,"permalink":"/posts/nightwatchjs-guilde/","section":"Posts","summary":"Nightwatch.js 是一个基于 Node.js 的自动化测试框架，本文提供了 Nightwatch.js 的中文参考手册和示例代码。","title":"Nightwatchjs 中文参考手册","type":"posts"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"沈显鹏 ( shenxianpeng)：DevOps 工程师。\nPython 爱好者，开源贡献者，公众号《DevOps攻城狮》作者。\n我创建了以下与 DevOps 和 Python 相关的开源项目：\n","externalUrl":null,"permalink":"/about/","section":"沈显鹏","summary":"\u003cp\u003e沈显鹏 (\n\n  \u003cspan class=\"relative inline-block align-text-bottom icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 496 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/\u003e\u003c/svg\u003e\n\n  \u003c/span\u003e\n\n \u003ca\n  href=\"https://github.com/shenxianpeng\"\n    target=\"_blank\"\n  \u003eshenxianpeng\u003c/a\u003e)：DevOps 工程师。\u003c/p\u003e","title":"沈显鹏","type":"about"},{"content":"译者：沈显鹏， @shenxianpeng。 简介：DevOps 工程师，Python 爱好者，开源贡献者。 开源项目：cpp-linter、commit-check、conventional-branch 和 devops-maturity。\n公众号：《DevOps攻城狮》。\n","externalUrl":null,"permalink":"/authors/shenxianpeng/","section":"作者列表","summary":"\u003cp\u003e\u003cstrong\u003e译者\u003c/strong\u003e：\u003ca\n  href=\"https://shenxianpeng.github.io\"\n    target=\"_blank\"\n  \u003e\u003cstrong\u003e沈显鹏\u003c/strong\u003e\u003c/a\u003e，\n\n  \u003cspan class=\"relative inline-block align-text-bottom icon\"\u003e\n    \u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 496 512\"\u003e\u003cpath fill=\"currentColor\" d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/\u003e\u003c/svg\u003e\n\n  \u003c/span\u003e\n\n\n\u003ca\n  href=\"https://github.com/shenxianpeng\"\n    target=\"_blank\"\n  \u003e@shenxianpeng\u003c/a\u003e。\n\u003cstrong\u003e简介\u003c/strong\u003e：DevOps 工程师，Python 爱好者，开源贡献者。\n\u003cstrong\u003e开源项目\u003c/strong\u003e：\u003ca\n  href=\"https://github.com/cpp-linter/cpp-linter\"\n    target=\"_blank\"\n  \u003ecpp-linter\u003c/a\u003e、\u003ca\n  href=\"https://github.com/devops-maturity/commit-check\"\n    target=\"_blank\"\n  \u003ecommit-check\u003c/a\u003e、\u003ca\n  href=\"https://github.com/devops-maturity/conventional-branch\"\n    target=\"_blank\"\n  \u003econventional-branch\u003c/a\u003e 和 \u003ca\n  href=\"https://github.com/devops-maturity/devops-maturity\"\n    target=\"_blank\"\n  \u003edevops-maturity\u003c/a\u003e。\u003c/p\u003e","title":"沈显鹏","type":"authors"},{"content":"DevOps工程师\n📧 xianpeng.shen@gmail.com 🌐 shenxianpeng.github.io\n🔗 LinkedIn | GitHub\n语言 # 中文：母语 英语：工作流利 立陶宛语：初级 个人总结 # DevOps/Build/Release 领域的 Tech Lead。 从零建立起团队的 DevOps，并在整个团队以及整个部门分享最佳实践。 从事过自动化测试、软件开发工作，从 2018 年开始担任 DevOps/Build/Release 工程师。 熟悉 Windows、Linux、AIX、Solaris、HP-UX 操作系统、软件开发生命周期、DevOps 工具链。 善于做 “Scan-Try-Scale”，乐于关注 DevOps 行业的最新发展和技术，愿意尝试并应用最佳实践到团队。 主要开发语言是 Python/Shell/Groovy 来完成一些 DevOps 的相关工作。 开源爱好者：业余时间创建并贡献 cpp-linter、commit-check、conventional-branch 等开源项目。目前 cpp-linter-action 已有 800+ 用户，被微软、Linux 基金会等知名项目使用。 知识分享者：七年来在我的 博客 和微信公众号 “DevOps攻城狮” 上分享数百篇技术文章，影响广泛的开发者群体。 工作经历 # 高级 DevOps 工程师 # Rocket Software, 立陶宛\n2024.07 - 至今\n主导先进 DevOps 实践，推动可扩展的交付体系建设。 DevOps 工程师 # Rocket Software, 大连\n2015 - 2024.06\n负责 MultiValue 产品的 CI/CD、基础设施管理和 DevOps 自动化。\n关键贡献：\n推动配置即代码（CaC）：将手工/Bamboo 构建迁移到 Jenkins，并构建共享库。 构建 Ansible 基础设施即代码（IaC），自动部署 Jenkins 与开发环境。 利用 Docker buildx、健康检查、CI/CD 和 Kubernetes 对产品进行容器化。 提出并推广 DevOps 成熟度徽章和 Conventional Commits。 使用 Jira + Python 自动化管理虚拟机，已在整个 BU 推广。 实现 U2、jBASE 等产品代码覆盖率报告。 Rocket Build 创新项目获奖（RB-218、RB-544）。 主导 Rocket Discover 自动化测试从 0 到 1 的落地。 自动化测试工程师 # 京东商城, 北京\n2012 - 2014\n编写自动化测试脚本并维护持续集成流程。 测试工程师 # SIMCOM（上海）\u0026amp; 东软（北京）\n2009 - 2011\n设计并执行测试用例，担任小型 QA 团队负责人，负责任务分配及经验分享。 项目经历 # pipeline-library 为 MVAS 构建 Jenkins 共享库，实现流水线即代码，提高 SDLC 一致性。\ndocker-images 为 MVAS 产品进行容器化，采用先进 DevOps 实践：buildx 构建、pytest 测试、健康检查和 Kubernetes 部署。\nansible-playbooks 以代码方式管理构建与开发基础设施，实现快速恢复与自动部署。\nU2Box CLI 使用 Go 开发的命令行工具，帮助快速构建 MV 开发/测试环境。\nMV 智能终端 基于 UOPY API 构建支持自动补全与高亮的命令行工具，荣获 Rocket Build 2019 一等奖与 CPO 奖。\nJira 虚拟机管理 结合 Jira 和 Python 脚本自动管理虚拟机资源，提升团队资源管理效率。\ncpp-linter C/C++ 代码格式化与静态分析的持续集成解决方案，广泛应用于工业界。\nCommit Check 开源替代 YACC，提供提交信息、分支命名、作者身份等检查机制。\nConventional Branch 推广 Git 分支命名规范，制定并维护相关规范与工具，广泛应用于工业界。\nDevOps Maturity 提供 DevOps 成熟度评估与徽章，帮助团队提升工程实践水平。\ngitstats 可视化 Git 仓库贡献情况与统计分析的工具。\natlassian-api-py Python 实现的 Atlassian API 库，便于集成 Jira 与 Bitbucket 的自动化流程。\n技能 # DevOps：★★★★★ (95%) 持续集成：★★★★★ (95%) Python：★★★★★ (95%) Shell：★★★★☆ (90%) Go/Groovy：★★★★☆ (80%) 兴趣 # 足球、游泳、写作 教育经历 # 2006 - 2009 辽宁交通高等专科学校 — 软件技术 ","externalUrl":null,"permalink":"/resume/","section":"沈显鹏","summary":"\u003cp\u003e\u003cstrong\u003eDevOps工程师\u003c/strong\u003e\u003cbr\u003e\n📧 \u003ca\n  href=\"mailto:xianpeng.shen@gmail.com\"\u003exianpeng.shen@gmail.com\u003c/a\u003e\n🌐 \u003ca\n  href=\"https://shenxianpeng.github.io\"\n    target=\"_blank\"\n  \u003eshenxianpeng.github.io\u003c/a\u003e\u003cbr\u003e\n🔗 \u003ca\n  href=\"https://www.linkedin.com/in/xianpeng-shen\"\n    target=\"_blank\"\n  \u003eLinkedIn\u003c/a\u003e | \u003ca\n  href=\"https://github.com/shenxianpeng\"\n    target=\"_blank\"\n  \u003eGitHub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\n\n\n\n\n\n\n\u003cfigure\u003e\n      \u003cimg class=\"my-0 rounded-md\" loading=\"lazy\" alt=\"\" src=\"/resume/sxp.jpg\"\u003e\n\n  \n\u003c/figure\u003e\n\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003e语言 \n    \u003cdiv id=\"语言\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#%e8%af%ad%e8%a8%80\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e中文\u003c/strong\u003e：母语\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e英语\u003c/strong\u003e：工作流利\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e立陶宛语\u003c/strong\u003e：初级\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\n\u003ch2 class=\"relative group\"\u003e个人总结 \n    \u003cdiv id=\"个人总结\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#%e4%b8%aa%e4%ba%ba%e6%80%bb%e7%bb%93\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDevOps/Build/Release 领域的 Tech Lead。\u003c/li\u003e\n\u003cli\u003e从零建立起团队的 DevOps，并在整个团队以及整个部门分享最佳实践。\u003c/li\u003e\n\u003cli\u003e从事过自动化测试、软件开发工作，从 2018 年开始担任 DevOps/Build/Release 工程师。\u003c/li\u003e\n\u003cli\u003e熟悉 Windows、Linux、AIX、Solaris、HP-UX 操作系统、软件开发生命周期、DevOps 工具链。\u003c/li\u003e\n\u003cli\u003e善于做 “Scan-Try-Scale”，乐于关注 DevOps 行业的最新发展和技术，愿意尝试并应用最佳实践到团队。\u003c/li\u003e\n\u003cli\u003e主要开发语言是 Python/Shell/Groovy 来完成一些 DevOps 的相关工作。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e开源爱好者\u003c/strong\u003e：业余时间创建并贡献 \u003ca\n  href=\"https://github.com/cpp-linter\"\n    target=\"_blank\"\n  \u003ecpp-linter\u003c/a\u003e、\u003ca\n  href=\"https://github.com/commit-check\"\n    target=\"_blank\"\n  \u003ecommit-check\u003c/a\u003e、\u003ca\n  href=\"https://github.com/conventional-branch\"\n    target=\"_blank\"\n  \u003econventional-branch\u003c/a\u003e 等开源项目。目前 cpp-linter-action 已有 800+ 用户，被微软、Linux 基金会等知名项目使用。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e知识分享者\u003c/strong\u003e：七年来在我的 \u003ca\n  href=\"https://shenxianpeng.github.io\"\n    target=\"_blank\"\n  \u003e博客\u003c/a\u003e 和微信公众号 \u003cstrong\u003e“DevOps攻城狮”\u003c/strong\u003e 上分享数百篇技术文章，影响广泛的开发者群体。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\n\u003ch2 class=\"relative group\"\u003e工作经历 \n    \u003cdiv id=\"工作经历\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#%e5%b7%a5%e4%bd%9c%e7%bb%8f%e5%8e%86\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\u003ch3 class=\"relative group\"\u003e高级 DevOps 工程师 \n    \u003cdiv id=\"高级-devops-工程师\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#%e9%ab%98%e7%ba%a7-devops-%e5%b7%a5%e7%a8%8b%e5%b8%88\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eRocket Software, 立陶宛\u003c/strong\u003e\u003cbr\u003e\n2024.07 - 至今\u003c/p\u003e","title":"沈显鹏","type":"resume"},{"content":"这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。\u0026#x1f680;\n你也可以用这些内容来定义 Hugo 的元数据，比如标题和描述。这些内容可以被用来增强 SEO 或其他目的。\n","externalUrl":null,"permalink":"/tags/advanced/","section":"标签","summary":"\u003cp\u003e这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。\u0026#x1f680;\u003c/p\u003e","title":"高级","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"作者列表","summary":"","title":"作者列表","type":"authors"}]